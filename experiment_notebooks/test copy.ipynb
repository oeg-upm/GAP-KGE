{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDFs download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting paperswithcode-client\n",
      "  Downloading paperswithcode_client-0.3.1-py3-none-any.whl (24 kB)\n",
      "Collecting tea-client==0.0.7\n",
      "  Downloading tea_client-0.0.7-py3-none-any.whl (11 kB)\n",
      "Collecting tea-console==0.0.6\n",
      "  Downloading tea_console-0.0.6-py3-none-any.whl (12 kB)\n",
      "Collecting typer==0.3.2\n",
      "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
      "Collecting httpx[http2]~=0.14.2\n",
      "  Downloading httpx-0.14.3-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic~=1.6.1\n",
      "  Downloading pydantic-1.6.2-py36.py37.py38-none-any.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.3/99.3 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tea~=0.1.2\n",
      "  Downloading tea-0.1.7-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzlocal~=2.1\n",
      "  Downloading tzlocal-2.1-py2.py3-none-any.whl (16 kB)\n",
      "Collecting rich~=9.11.0\n",
      "  Downloading rich-9.11.1-py3-none-any.whl (195 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.6/195.6 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytz~=2021.1\n",
      "  Using cached pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "Collecting click<7.2.0,>=7.1.1\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpcore==0.10.*\n",
      "  Downloading httpcore-0.10.2-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /home/checui/.local/lib/python3.10/site-packages (from httpx[http2]~=0.14.2->tea-client==0.0.7->paperswithcode-client) (1.3.0)\n",
      "Collecting chardet==3.*\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /home/checui/.local/lib/python3.10/site-packages (from httpx[http2]~=0.14.2->tea-client==0.0.7->paperswithcode-client) (2023.11.17)\n",
      "Collecting rfc3986[idna2008]<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting h2==3.*\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hpack<4,>=3.0\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting hyperframe<6,>=5.2.0\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting h11<0.10,>=0.8\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions<4.0.0,>=3.7.4\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /home/checui/.local/lib/python3.10/site-packages (from rich~=9.11.0->tea-console==0.0.6->paperswithcode-client) (0.4.6)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /home/checui/.local/lib/python3.10/site-packages (from rich~=9.11.0->tea-console==0.0.6->paperswithcode-client) (2.17.2)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tea~=0.1.2\n",
      "  Downloading tea-0.1.6-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading tea-0.1.5-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading tea-0.1.4-py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting psutil~=5.8.0\n",
      "  Downloading psutil-5.8.0.tar.gz (470 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m470.9/470.9 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: idna in /home/checui/.local/lib/python3.10/site-packages (from rfc3986[idna2008]<2,>=1.3->httpx[http2]~=0.14.2->tea-client==0.0.7->paperswithcode-client) (3.6)\n",
      "Building wheels for collected packages: psutil\n",
      "  Building wheel for psutil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psutil: filename=psutil-5.8.0-cp310-cp310-linux_x86_64.whl size=282801 sha256=3f76551727cf17e733e39680dff28fb5df15a2a839320a10447b8811cb3882d0\n",
      "  Stored in directory: /home/checui/.cache/pip/wheels/12/a3/6d/615295409067d58a62a069d30d296d61d3ac132605e3a9555c\n",
      "Successfully built psutil\n",
      "Installing collected packages: typing-extensions, rfc3986, pytz, hyperframe, hpack, h11, commonmark, chardet, tzlocal, rich, pydantic, psutil, httpcore, h2, click, typer, tea, httpx, tea-console, tea-client, paperswithcode-client\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2023.3.post1\n",
      "    Uninstalling pytz-2023.3.post1:\n",
      "      Successfully uninstalled pytz-2023.3.post1\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.10.4\n",
      "    Uninstalling pydantic-2.10.4:\n",
      "      Successfully uninstalled pydantic-2.10.4\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 5.9.6\n",
      "    Uninstalling psutil-5.9.6:\n",
      "      Successfully uninstalled psutil-5.9.6\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.2\n",
      "    Uninstalling httpcore-1.0.2:\n",
      "      Successfully uninstalled httpcore-1.0.2\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.7\n",
      "    Uninstalling click-8.1.7:\n",
      "      Successfully uninstalled click-8.1.7\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.9.0\n",
      "    Uninstalling typer-0.9.0:\n",
      "      Successfully uninstalled typer-0.9.0\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.25.2\n",
      "    Uninstalling httpx-0.25.2:\n",
      "      Successfully uninstalled httpx-0.25.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "yfinance 0.2.32 requires pytz>=2022.5, but you have pytz 2021.3 which is incompatible.\n",
      "ydata-profiling 4.6.2 requires pydantic>=2, but you have pydantic 1.6.2 which is incompatible.\n",
      "weasel 0.3.4 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.6.2 which is incompatible.\n",
      "wandb 0.19.1 requires pydantic<3,>=2.6, but you have pydantic 1.6.2 which is incompatible.\n",
      "wandb 0.19.1 requires typing-extensions<5,>=4.4; python_version < \"3.12\", but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "typeguard 4.1.2 requires typing-extensions>=4.7.0; python_version < \"3.12\", but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "thinc 8.2.2 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.6.2 which is incompatible.\n",
      "sqlalchemy 2.0.23 requires typing-extensions>=4.2.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "spacy 3.7.2 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.6.2 which is incompatible.\n",
      "pytype 2024.9.13 requires typing-extensions>=4.3.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "pydantic-core 2.27.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "openai 1.7.2 requires httpx<1,>=0.23.0, but you have httpx 0.14.3 which is incompatible.\n",
      "openai 1.7.2 requires pydantic<3,>=1.9.0, but you have pydantic 1.6.2 which is incompatible.\n",
      "openai 1.7.2 requires typing-extensions<5,>=4.7, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "nni 2.10.1 requires typeguard<3, but you have typeguard 4.1.2 which is incompatible.\n",
      "nni 2.10.1 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "mypy 1.11.2 requires typing-extensions>=4.6.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "llama-index 0.9.8.post1 requires typing-extensions>=4.5.0, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "flask 3.0.3 requires click>=8.1.3, but you have click 7.1.2 which is incompatible.\n",
      "confection 0.1.4 requires pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4, but you have pydantic 1.6.2 which is incompatible.\n",
      "cloudpathlib 0.16.0 requires typing_extensions>4; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "black 24.8.0 requires click>=8.0.0, but you have click 7.1.2 which is incompatible.\n",
      "black 24.8.0 requires typing-extensions>=4.0.1; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "async-lru 2.0.4 requires typing-extensions>=4.0.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "alembic 1.13.3 requires typing-extensions>=4, but you have typing-extensions 3.10.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed chardet-3.0.4 click-7.1.2 commonmark-0.9.1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 httpcore-0.10.2 httpx-0.14.3 hyperframe-5.2.0 paperswithcode-client-0.3.1 psutil-5.8.0 pydantic-1.6.2 pytz-2021.3 rfc3986-1.5.0 rich-9.11.1 tea-0.1.4 tea-client-0.0.7 tea-console-0.0.6 typer-0.3.2 typing-extensions-3.10.0.2 tzlocal-2.1\n"
     ]
    }
   ],
   "source": [
    "# use paperswithcode api\n",
    "!pip install paperswithcode-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m papers_list\u001b[38;5;241m.\u001b[39mappend(paper_entry)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Sleep to prevent rate limits\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "TASK_SLUG = \"knowledge-graph-embeddings\"\n",
    "PAPERS_URL = f\"https://paperswithcode.com/api/v1/tasks/{TASK_SLUG}/papers/\"\n",
    "\n",
    "# Fetch papers\n",
    "response = requests.get(PAPERS_URL)\n",
    "papers_data = response.json()\n",
    "\n",
    "information = ['tasks', 'datasets','implementations']\n",
    "# Display the first few papers\n",
    "papers_list= []\n",
    "\n",
    "def fetch_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error fetching {url}: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "for paper in papers_data[\"results\"][:40]:  # Limit output to 5 papers\n",
    "    paper_entry = {\n",
    "        \"Title\": paper[\"title\"],\n",
    "        \"Authors\": \", \".join(paper[\"authors\"]),\n",
    "        \"Abstract\": paper.get(\"abstract\", \"No abstract available\"),\n",
    "        \"PDF URL\": paper[\"url_pdf\"],\n",
    "        \"Datasets\": [],\n",
    "        \"Tasks\": [],\n",
    "    }\n",
    "    paper_slug = paper[\"id\"]\t\n",
    "\n",
    "    dataset_url = f\"https://paperswithcode.com/api/v1/papers/{paper_slug}/datasets/\"\n",
    "    dataset_data = fetch_data(dataset_url)\n",
    "    if dataset_data:\n",
    "        for dataset in dataset_data[\"results\"]:\n",
    "            paper_entry[\"Datasets\"].append({dataset[\"name\"]})\n",
    "    \n",
    "    taks_url = f\"https://paperswithcode.com/api/v1/papers/{paper_slug}/tasks/\"\n",
    "    task_data = fetch_data(taks_url)\n",
    "    if task_data:\n",
    "        for task in task_data[\"results\"]:\n",
    "            paper_entry[\"Tasks\"].append({task[\"name\"]})\n",
    "    # Append paper entry to the list\n",
    "    papers_list.append(paper_entry)\n",
    "\n",
    "    # Sleep to prevent rate limits\n",
    "    time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved data in 'papers_data.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Convert sets to lists before saving\n",
    "def convert_sets_to_lists(obj):\n",
    "    if isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_sets_to_lists(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_sets_to_lists(value) for key, value in obj.items()}\n",
    "    return obj  # Return original object if no conversion needed\n",
    "\n",
    "# Ensure `papers_list` is fully JSON serializable\n",
    "papers_list = convert_sets_to_lists(papers_list)\n",
    "\n",
    "# Save JSON\n",
    "with open(\"papers_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(papers_list, f, indent=4)\n",
    "\n",
    "print(\"Successfully saved data in 'papers_data.json'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: papers_pdfs/KG^2:_Learning_to_Reason_Science_Exam_Questions_with_Contextual_Knowledge_Graph_Embeddings.pdf\n",
      "Downloaded: papers_pdfs/Incorporating_Literals_into_Knowledge_Graph_Embeddings.pdf\n",
      "Downloaded: papers_pdfs/Adversarial_Contrastive_Estimation.pdf\n",
      "Downloaded: papers_pdfs/KBGAN:_Adversarial_Learning_for_Knowledge_Graph_Embeddings.pdf\n",
      "Downloaded: papers_pdfs/Convolutional_2D_Knowledge_Graph_Embeddings.pdf\n",
      "Downloaded: papers_pdfs/Answering_Visual-Relational_Queries_in_Web-Extracted_Knowledge_Graphs.pdf\n",
      "Downloaded: papers_pdfs/Expeditious_Generation_of_Knowledge_Graph_Embeddings.pdf\n",
      "Downloaded: papers_pdfs/Learning_Knowledge_Graph_Embeddings_with_Type_Regularizer.pdf\n",
      "Downloaded: papers_pdfs/Analysis_of_the_Impact_of_Negative_Sampling_on_Link_Prediction_in_Knowledge_Graphs.pdf\n",
      "Downloaded: papers_pdfs/DeepPath:_A_Reinforcement_Learning_Method_for_Knowledge_Graph_Reasoning.pdf\n",
      "Downloaded: papers_pdfs/Inducing_Interpretability_in_Knowledge_Graph_Embeddings.pdf\n",
      "Downloaded: papers_pdfs/Fast_Linear_Model_for_Knowledge_Graph_Embeddings.pdf\n",
      "Downloaded: papers_pdfs/Complex_and_Holographic_Embeddings_of_Knowledge_Graphs:_A_Comparison.pdf\n",
      "Downloaded: papers_pdfs/Multilingual_Knowledge_Graph_Embeddings_for_Cross-lingual_Knowledge_Alignment.pdf\n",
      "Downloaded: papers_pdfs/Learning_Symmetric_Collaborative_Dialogue_Agents_with_Dynamic_Knowledge_Graph_Embeddings.pdf\n",
      "Downloaded: papers_pdfs/Knowledge-Based_Distant_Regularization_in_Learning_Probabilistic_Models.pdf\n",
      "Downloaded: papers_pdfs/Embedding_Models_for_Episodic_Knowledge_Graphs.pdf\n",
      "Downloaded: papers_pdfs/Seq2RDF:_An_end-to-end_application_for_deriving_Triples_from_Natural_Language_Text.pdf\n",
      "Downloaded: papers_pdfs/Hypernetwork_Knowledge_Graph_Embeddings.pdf\n",
      "Downloaded: papers_pdfs/Multi-Hop_Knowledge_Graph_Reasoning_with_Reward_Shaping.pdf\n",
      "Downloaded: papers_pdfs/MOHONE:_Modeling_Higher_Order_Network_Effects_in_KnowledgeGraphs_via_Network_Infused_Embeddings.pdf\n",
      "Downloaded: papers_pdfs/DOLORES:_Deep_Contextualized_Knowledge_Graph_Embeddings.pdf\n",
      "Downloaded: papers_pdfs/Modelling_Salient_Features_as_Directions_in_Fine-Tuned_Semantic_Spaces.pdf\n",
      "Downloaded: papers_pdfs/Towards_Understanding_the_Geometry_of_Knowledge_Graph_Embeddings.pdf\n",
      "Downloaded: papers_pdfs/Multimodal_Named_Entity_Disambiguation_for_Noisy_Social_Media_Posts.pdf\n",
      "Downloaded: papers_pdfs/Recognizing_Mentions_of_Adverse_Drug_Reaction_in_Social_Media_Using_Knowledge-Infused_Recurrent_Models.pdf\n",
      "Downloaded: papers_pdfs/Sparsity_and_Noise:_Where_Knowledge_Graph_Embeddings_Fall_Short.pdf\n",
      "Downloaded: papers_pdfs/Entity_Hierarchy_Embedding.pdf\n",
      "Downloaded: papers_pdfs/Binarized_Knowledge_Graph_Embeddings.pdf\n",
      "Downloaded: papers_pdfs/Long-tail_Relation_Extraction_via_Knowledge_Graph_Embeddings_and_Graph_Convolution_Networks.pdf\n",
      "Downloaded: papers_pdfs/Quaternion_Knowledge_Graph_Embeddings.pdf\n",
      "Downloaded: papers_pdfs/Relation_Embedding_with_Dihedral_Group_in_Knowledge_Graph.pdf\n",
      "Downloaded: papers_pdfs/Learning_Attention-based_Embeddings_for_Relation_Prediction_in_Knowledge_Graphs.pdf\n",
      "Downloaded: papers_pdfs/Neural_Variational_Inference_For_Estimating_Uncertainty_in_Knowledge_Graph_Embeddings.pdf\n",
      "Downloaded: papers_pdfs/Augmenting_and_Tuning_Knowledge_Graph_Embeddings.pdf\n",
      "Downloaded: papers_pdfs/Adaptive_Margin_Ranking_Loss_for_Knowledge_Graph_Embeddings_via_a_Correntropy_Objective_Function.pdf\n",
      "Downloaded: papers_pdfs/Drug-Drug_Interaction_Prediction_Based_on_Knowledge_Graph_Embeddings_and_Convolutional-LSTM_Network.pdf\n",
      "Downloaded: papers_pdfs/Linking_Physicians_to_Medical_Research_Results_via_Knowledge_Graph_Embeddings_and_Twitter.pdf\n",
      "Downloaded: papers_pdfs/RDF2Vec:_RDF_Graph_Embeddings_and_Their_Applications.pdf\n",
      "Downloaded: papers_pdfs/HyperKG:_Hyperbolic_Knowledge_Graph_Embeddings_for_Knowledge_Base_Completion.pdf\n",
      "All available PDFs have been downloaded!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Load previously saved JSON file with paper metadata\n",
    "with open(\"papers_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    papers_list = json.load(f)\n",
    "\n",
    "# Create a folder to save PDFs\n",
    "PDF_FOLDER = \"papers_pdfs\"\n",
    "os.makedirs(PDF_FOLDER, exist_ok=True)\n",
    "\n",
    "# Function to download a PDF\n",
    "def download_pdf(pdf_url, title):\n",
    "    filename = f\"{PDF_FOLDER}/{title.replace(' ', '_')}.pdf\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        response = requests.get(pdf_url, stream=True)\n",
    "        response.raise_for_status()  # Raise an error if the request fails\n",
    "        \n",
    "        with open(filename, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        print(f\"Downloaded: {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download {title}: {e}\")\n",
    "        return None\n",
    "# Loop through papers and download PDFs\n",
    "for paper in papers_list:\n",
    "    pdf_url = paper.get(\"PDF URL\")\n",
    "    title = paper[\"Title\"]\n",
    "\n",
    "    if pdf_url and pdf_url.startswith(\"http\"):\n",
    "        pdf_path = download_pdf(pdf_url, title)\n",
    "        if pdf_path:\n",
    "            paper[\"Local PDF Path\"] = str(pdf_path)  # Store local path in JSON\n",
    "    else:\n",
    "        print(f\"No valid PDF URL for: {title}\")\n",
    "        paper[\"Local PDF Path\"] = None  # Store None if no valid PDF\n",
    "\n",
    "# Save the updated JSON with local paths\n",
    "with open(\"papers_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(papers_list, f, indent=4)\n",
    "\n",
    "print(\"All available PDFs have been downloaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on the pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"papers_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    papers_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, tokenizer, max_tokens=8000, overlap=200):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_tokens, len(tokens))\n",
    "        chunk = tokenizer.decode(tokens[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += max_tokens - overlap  # Overlapping context\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68cd99bd5a1f4546a53d00caa7029360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 1: Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "max_context_tokens = 8192 - 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = Path(os.getcwd())\n",
    "parent_dir = current_dir.parent\n",
    "\n",
    "paper = papers_list[0]\n",
    "\n",
    "\n",
    "with pdfplumber.open(str(current_dir /paper['Local PDF Path'])) as pdf:\n",
    "    text = \"\\n\".join(page.extract_text() for page in pdf.pages)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the name of datasets used in the paper?\"\n",
    "question2 = \"What are the tasks that the model is trained for?\"\n",
    "question3 = \"Who are the authors of the paper?\"\n",
    "questions = [question, question2, question3]\n",
    "responses_dataset = []\n",
    "responses_task = []\n",
    "responses_authors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: KG2: Learning to Reason Science Exam Questions\n",
      "with Contextual Knowledge Graph Embeddings\n",
      "Yuyu Zhang1∗, Hanjun Dai1∗, Kamil Toraman2, and Le Song1\n",
      "1College of Computing, Georgia Institute of Technology\n",
      "2Korea Advanced Institute of Science and Technology\n",
      "1{yuyu.zhang, hanjun.dai, lsong}@cc.gatech.edu\n",
      "2kvtoraman@kaist.ac.kr\n",
      "Abstract\n",
      "TheAI2ReasoningChallenge(ARC),anewbenchmarkdatasetforquestionanswering(QA)\n",
      "has been recently released. ARC only contains natural science questions authored for human\n",
      "exams, which are hard to answer and require advanced logic reasoning. On the ARC Chal-\n",
      "lengeSet, existingstate-of-the-artQAsystemsfailtosignificantlyoutperformrandombaseline,\n",
      "reflecting the difficult nature of this task. In this paper, we propose a novel framework for\n",
      "answering science exam questions, which mimics human solving process in an open-book exam.\n",
      "To address the reasoning challenge, we construct contextual knowledge graphs respectively for\n",
      "the question itself and supporting sentences. Our model learns to reason with neural embed-\n",
      "dings of both knowledge graphs. Experiments on the ARC Challenge Set show that our model\n",
      "outperforms the previous state-of-the-art QA systems.\n",
      "1 Introduction\n",
      "Question answering (QA) has been a long-standing challenge in the field of artificial intelligence.\n",
      "Numerous research works have pushed forward techniques for building QA systems. Many existing\n",
      "approaches achieve high performance on benchmark datasets. However, most of the questions in\n",
      "those datasets only require surface-level reasoning, and do not reveal the full-scale complexity and\n",
      "challenge of the question answering problem. Recently, the AI2 Reasoning Challenge (ARC) has\n",
      "been proposed [Clark et al., 2018], which is designed to pose a challenge to the QA community. On\n",
      "the ARC Challenge Set, several state-of-the-art QA systems, including leading neural models from\n",
      "the well-known SQuAD and SNLI tasks, only perform slightly better than the random baseline.\n",
      "This striking observation has demonstrated that QA is still far from being solved.\n",
      "Why it is so difficult to answer the questions in the ARC Challenge Set? 1) ARC consists of\n",
      "natural science questions, namely questions authored for human exams. All of these questions are\n",
      "drawn from real exams; 2) In order to encourage progress on hard questions, a Challenge Set has\n",
      "been partitioned from ARC. To be more specific, if a question could not be correctly answered by\n",
      "neither an information retrieval (IR) method nor a word co-occurrence method, it is sorted into\n",
      "∗Both authors contributed equally to the paper.\n",
      "1\n",
      "8102\n",
      "yaM\n",
      "13\n",
      "]GL.sc[\n",
      "1v39321.5081:viXra\n",
      "the Challenge Set, otherwise the Easy Set. To illustrate the difference, consider the following two\n",
      "examples from both sets respectively, where the bold answers correspond to the correct choices:\n",
      "• ARC Easy Set:\n",
      "Which property of air does a barometer measure? (A) speed (B) pressure (C)\n",
      "humidity (D) temperature\n",
      "This question is correctly answered by both the IR and word co-occurrence methods.1 The\n",
      "IR method finds sentences relevant to the correct answer in the reference corpus, e.g., “Air\n",
      "pressurewillbemeasuredwithabarometer”. Duetothesubstantialwordoverlap,thequestion\n",
      "can be easily solved. Similarly, the word co-occurrence method finds that “barometer” and\n",
      "“pressure” co-occur frequently in the corpus, leading to the correct answer.\n",
      "• ARC Challenge Set:\n",
      "Which property of a mineral can be determined just by looking at it? (A) luster\n",
      "(B) mass (C) weight (D) hardness\n",
      "NeithertheIRmethodnorthewordco-occurrencemethodcancorrectlyanswerthisquestion.\n",
      "There are no sentences in the corpus similar to “A material’s luster can be determined by\n",
      "looking at it”. Also, “mineral” often co-occurs with distractor options (e.g., mass, hardness),\n",
      "which confuses the word co-occurrence method.\n",
      "Fromtheexamplesabove,weseethatsurface-levelreasoningmethodsarenotabletosolveques-\n",
      "tions in the Challenge Set, even the required knowledge is already covered in the reference corpus.\n",
      "The ARC Corpus, a large science-related text corpus collected from the Web and released together\n",
      "withARC,mentionsknowledgerelevanttoabout95%oftheARCChallengequestions[Clarketal.,\n",
      "2018]. However, the IR method with the ARC Corpus, as listed in Table 1, only achieves 20.26\n",
      "test score, which underperforms the random baseline. Collecting more sentences into the corpus\n",
      "would not solve the challenge. Actually we tried to use the entire Web as the reference corpus with\n",
      "Google Search API, and select the answer option with the most number of hits. This only slightly\n",
      "improves the score to 21.58.\n",
      "To tackle the ARC Challenge, we believe that there is no shortcut to get around advanced logic\n",
      "reasoning and deeper text comprehension. These questions target at students of age 8 through\n",
      "13 years old, and should be relatively easy for human to solve. For an adult with basic reasoning\n",
      "capability, even she forgets about the knowledge learned in grade school, she can still ace most of\n",
      "these questions in an open-book exam, by searching relevant supporting texts and reasoning over\n",
      "them.\n",
      "Inspired by the human problem solving process, we propose a neural reasoning engine named\n",
      "KG2 foransweringscienceexamquestions: readthequestion,generatehypothesisbycombiningthe\n",
      "questionstemandansweroption,findsupportingsentencesinthecorpus,andverifythehypothesis.\n",
      "For effective and efficient reasoning, we represent both hypothesis and supporting sentences in\n",
      "knowledge graphs. For example, in the supporting graph, “luster” is linked to “brightness”, and\n",
      "“brightness” connects to “look”, which is consistent with the hypothesis graph. Therefore, such\n",
      "reasoningpatternsongraphscanbelearnedbyourdifferentiableneuralengine. Experimentsonthe\n",
      "1Note that even it is correctly answered by only one of them, ARC would exclude it from the Challenge Set.\n",
      "2\n",
      "ARC Challenge Set show that our model achieves score that surpasses the previous state-of-the-art\n",
      "results.\n",
      "In summary, the contributions of this work are: 1) We propose a novel differential neural\n",
      "programming framework for reasoning about science exam questions; 2) Our method sets the new\n",
      "state of the art on the ARC Challenge Set; 3) We decompose the remaining difficulties towards\n",
      "solving the ARC Challenge, facilitating the community to engage with the dataset and progress on\n",
      "the challenging task.\n",
      "2 Related Work\n",
      "Science QA: For elementary science QA, simple IR-based methods have been proposed for science\n",
      "exams [Clark et al., 2016]. Markov Logic Networks [Richardson and Domingos, 2006] has been\n",
      "used to reason over a small set of logical rules [Khot et al., 2015]. Jansen et al. [2016] has analyzed\n",
      "knowledge and inference requirements for science exam questions.\n",
      "The work most related to us is DGEM [Khot et al., 2018], a neural entailment model which\n",
      "also employs Open IE to generate hypothesis graph. Our key contributions over DGEM: 1) DGEM\n",
      "is designed for single sentence entailment, while we aggregate multiple supporting sentences for\n",
      "reasoning; 2) DGEM has no structured representation of supporting facts, while our model learns\n",
      "to reason over the paired hypothesis and supporting graphs together.\n",
      "Graph Embedding: We employ graph embedding techniques for reasoning over knowledge\n",
      "graphs. Graph embedding has provided the representational flexibility for neural models in many\n",
      "NLP tasks, such as dialog system [He et al., 2017], question answering [Zhang et al., 2017], link\n",
      "prediction [Bordes et al., 2013] and triple classification [Feng et al., 2016]. In our paper, we extend\n",
      "this technique to mimic the reasoning process on graph ranking problem.\n",
      "3 Task\n",
      "(cid:8) (cid:0) (1) (m)(cid:1) (cid:9)n\n",
      "The ARC Challenge Set consists of science exam questions D = q, c,...,c, a, where\n",
      "i i i i i=1\n",
      "(j)\n",
      "q is the question stem, c is the j-th answer option corresponding to q (typically 4-way multiple\n",
      "i i i\n",
      "(j)\n",
      "choices), and a is the label of correct answer. Both q and c are in text format. Among the\n",
      "i i i\n",
      "multiple choices, only one of them is the correct answer and others are distractors. With the\n",
      "question stem and options, the goal is to find the correct answer. Accompanied with ARC, the\n",
      "ARCCorpusisalsoprovided,providing14Mscience-relatedsentencesfromtheWebwithknowledge\n",
      "relevant to ARC. The use of the ARC Corpus is optional for the ARC Challenge.\n",
      "4 Approach\n",
      "4.1 Generating Hypothesis\n",
      "A hypothesis h is a statement that combines a question stem q and an answer option c, which helps\n",
      "us understand what is being asked and what is the target to be verified. For example, consider the\n",
      "question stem “Which of these occurs due to the rotation of Earth?” and one of the answer options\n",
      "“day and night”. The hypothesis to be generated from them should be: “Day and night occurs due\n",
      "to the rotation of Earth”.\n",
      "3\n",
      "To automatically generate hypothesis, we first identify the wh-word (e.g., which, what, where,\n",
      "etc.) in the question stem, and replace it with the answer option. If there is no wh-word found,\n",
      "we just append the answer option behind the question stem. We create several rules to handle\n",
      "special cases and make hypothesis more natural. For example, “Which of these” and “Which of the\n",
      "following” should be replaced as a whole when they appear in the question stem. We successfully\n",
      "generatehypothesisformostquestions,however,therearestillafewcornercasesrequiringadvanced\n",
      "rewording, which should be negligible.\n",
      "4.2 Searching Potential Supports\n",
      "Toverifyahypothesis,welookforsupportsinthereferencecorpus. Althoughthecorpusistypically\n",
      "gigantic, we only need to focus on a tiny part of it, which is relevant to the question we are solving.\n",
      "Therefore,weusethegeneratedhypothesisasaquerytosearchtheentirecorpus. Thetopretrieved\n",
      "sentences are treated as potential supports for the hypothesis. In order to efficiently search the\n",
      "corpus, we build a local search engine on top of ElasticSearch [Gormley and Tong, 2015]. Since\n",
      "the corpus sentences are not as clean as questions, we filter noisy sentences that contain negation\n",
      "words (e.g., not, except, etc.) or unexpected characters or simply too long, and then pick up the\n",
      "top 20 sentences for verifying the hypothesis.\n",
      "4.3 Constructing Knowledge Graphs\n",
      "Many questions in the ARC Challenge Set require advanced reasoning on multiple supporting\n",
      "sentences. To aggregate knowledge across sentences, we employ Open IE [Banko et al., 2007,\n",
      "Christensen et al., 2011, Pal et al., 2016] v4 2 to extract relation triples from each sentence, and\n",
      "collect them to construct a contextual knowledge graph.\n",
      "More specifically, each relation triple is represented as T(s,p,o ), where s is the subject, p is the\n",
      "i\n",
      "predicate, ando isthei-thobject. Weconstructthegraphbyaddingnodess, pando, andadding\n",
      "i i\n",
      "directed edges with labels subj and obj. If there is adverbial of time or location extracted by Open\n",
      "IE, we add an edge with label time or loc in the knowledge graph. Words in each graph node are\n",
      "lemmatized. Similarly, we construct another knowledge graph for the corresponding hypothesis,\n",
      "which is paired with the supporting knowledge graph. Refer to Appendix A for examples of our\n",
      "generated graphs.\n",
      "4.4 Learning with Graph Embeddings\n",
      "Givenaquestionq andacandidatechoicec,weconstructthecorrespondinghypothesisgraphGhypo\n",
      "q,c\n",
      "and supporting graph Gsupp by aggregating the relation triples mentioned in Section 4.3. Thus,\n",
      "q,c\n",
      "choosing the right answer for question q becomes a graph ranking problem. A good graph scoring\n",
      "i\n",
      "functionf : Ghypo×Gsupp (cid:55)→ Rshouldassignthehighestscoretothecorrecthypothesis-supporting\n",
      "graph pair. Without loss of generality, we use point-wise ranking objective, where f(·) becomes a\n",
      "binary classifier.\n",
      "Toimplementthegraphscoringfunction,weadapttherecentadvancesingraphembedding[Dai\n",
      "et al., 2016, Gilmer et al., 2017] to our problem. Specifically, let G = (V,E) be a knowledge graph,\n",
      "and V ∈ V be the set of predicate nodes. We associate each node v ∈ V with an embedding vector\n",
      "p\n",
      "2https://github.com/allenai/openie-standalone\n",
      "4\n",
      "µ that captures the local information, which is computed recursively using the equation:\n",
      "v\n",
      "µ(t) = h(cid:0) x,µ(t−1),{(µ(t−1),e )} (cid:1) (1)\n",
      "v v v u u,v (u,v,eu,v)∈E\n",
      "Here x encodes the text feature of node generated by LSTM that is jointly trained with the\n",
      "v\n",
      "supervision. The edge type e can be time, loc, etc. We use a two-layer neural network for\n",
      "u,v\n",
      "(T)\n",
      "the function h(·). Eq. (1) iterates for T steps, and we use µ = µ as the node embedding\n",
      "v v\n",
      "representation. Finally, the scoring function f(·) is defined as:\n",
      "f(Ghypo,Gsupp) = f({µ u} u∈Vphypo,{µ v} v∈Vpsupp)\n",
      "(cid:16) (cid:17)\n",
      "= σ max\n",
      "µ(cid:62) uµv\n",
      "−0.5, (2)\n",
      "u,v (cid:107)µu(cid:107)(cid:107)µv(cid:107)\n",
      "where σ(·) is the sigmoid function, and the −0.5 shift is used to center the matching score at\n",
      "zero. Eq. (2) is making max inner product search between all pairs of predicate node embeddings.\n",
      "This mimics the procedure of reasoning on the most relevant hypothesis and corresponding sup-\n",
      "porting evidence, since each embedding vector already captures the information within its T-hop\n",
      "neighborhood.\n",
      "5 Experiments\n",
      "We compare our method against several recently published baseline models, including state-of-the-\n",
      "art neural models from the well-known SQuAD and SNLI tasks.\n",
      "5.1 Setup\n",
      "We use the ARC Challenge Set [Clark et al., 2018] for all experiments. This dataset consists of\n",
      "2,590 questions drawn from a variety of human exams. We use the original train / development /\n",
      "test split. The test set is held-out for model evaluation, which contains 1,172 questions. For each\n",
      "question, a QA system receives one point if it selects the correct answer, and 1/k points if it reports\n",
      "a k-way tie (i.e., chooses multiple answers) that includes the correct answer. The ARC Corpus can\n",
      "be optionally used for all models.\n",
      "5.2 Baselines\n",
      "Guess-all / Random: This naive baseline just selects all answer options, getting 1/k scores for\n",
      "eachquestionwithk answeroptions. Randomselectingwillalsoconvergetothisscoreafterenough\n",
      "trials.\n",
      "IR-ARC:IR-basedmethodsendsquestionstempluseachoptionasaquerytoasearchengine. For\n",
      "IR-ARC, the search engine is built on top of the ARC Corpus, and the search score is determined\n",
      "by the ElasticSearch score of the top retrieved sentence. The option with the highest search score\n",
      "is finally selected.\n",
      "IR-Google: This is similar to IR-ARC, but uses Google Search API 3 to retrieve documents from\n",
      "the entire Web, instead of just searching on the ARC Corpus. IR-Google uses the number of hits\n",
      "as the search score.\n",
      "3https://developers.google.com/custom-search\n",
      "5\n",
      "Table 1: Test performance of different QA systems on the ARC Challenge Set. The ARC Corpus\n",
      "is used in DecompAttn, DGEM, BiDAF and KG2.\n",
      "Method Test Scores\n",
      "IR-ARC 20.26\n",
      "IR-Google 21.58\n",
      "TupleInference 23.83\n",
      "DecompAttn 24.34\n",
      "Guess-all / Random 25.02\n",
      "DGEM-OpenIE 26.41\n",
      "BiDAF 26.54\n",
      "TableILP 26.97\n",
      "KG2 31.70\n",
      "TableILP:Thismethod[Khashabietal.,2016]performstable-basedreasoning,whichisformulated\n",
      "as an Integer Linear Program (ILP).\n",
      "TupleInference: This model [Khot et al., 2017] searches for graph that best connects the terms\n",
      "in the question with an answer choice via the knowledge extracted by Open IE.\n",
      "DecompAttn: It is a neural entailment model [Parikh et al., 2016] adapted to multiple-choice QA\n",
      "by assigning entailment score to the pair of hypothesis and single supporting sentence [Clark et al.,\n",
      "2018]. The answer option with the highest score is selected. DecompAttn is a top performer on\n",
      "SNLI [Bowman et al., 2015].\n",
      "DGEM-OpenIE: DGEM [Khot et al., 2018] is also a neural model for sentence-level entailment,\n",
      "but uses Open IE to create structured representation of the hypothesis. On the SciTail task [Khot\n",
      "et al., 2018], DGEM is a top performer. In Clark et al. [2018], there is another version of DGEM,\n",
      "which uses a proprietary parser together with Open IE and achieves 27.11 test score. For fair\n",
      "comparison, we only list publicly available models in Table 1.\n",
      "BiDAF: This model [Seo et al., 2016] is for span prediction QA, and has been adapted to multiple-\n",
      "choice QA [Clark et al., 2018]. BiDAF is a top performer on SQuAD [Rajpurkar et al., 2016].\n",
      "5.3 Results and Analysis\n",
      "Table 1 summarizes the test scores of all baseline models and our method. It is striking to see that\n",
      "none of the baseline methods perform significantly better than the random baseline, where the 95%\n",
      "confidence interval is ±2.5%. Our method KG2 achieves 31.70, which substantially improves the\n",
      "previous state of the art by 17.5%.\n",
      "Nevertheless, we are still far from “passing” the exam. To dissect the difficulties, we randomly\n",
      "sample 100 questions for investigation and report the results in Figure 1. More than half of the\n",
      "questions are lack of support: even human couldn’t solve them by only referring to the supporting\n",
      "sentences. This may be caused by the limited coverage of the corpus, and the retrieval bias where\n",
      "sentences with low word overlap can partially explain concept which is indispensable for reasoning.\n",
      "External knowledge sources may help on these questions. 12% questions have lost key information\n",
      "in graph, due to the failure of Open IE. Sentence parsing may be helpful since it reserves more\n",
      "text. 21% questions require very complex reasoning, and only 15% questions are “learnable” given\n",
      "6\n",
      "insufficient\n",
      "support (51%)\n",
      "others (1%)\n",
      "failed\n",
      "open IE (12%)\n",
      "learnable (15%)\n",
      "complex\n",
      "reasoning (21%)\n",
      "Figure 1: Distribution of various difficulties in solving the ARC Challenge Set.\n",
      "the current framework. This gives us an estimated upper bound when we correctly answer all the\n",
      "learnable questions, and just randomly guess the others, which should be 36.25. Improving the\n",
      "learning algorithm should bring our current result closer to this upper bound.\n",
      "6 Conclusion and Future Work\n",
      "We present a neural reasoning engine for answering science exam questions, which learns to reason\n",
      "overcontextualknowledgegraphs. Experimentalresultsshowthatourmethodoutperformsexisting\n",
      "QA systems on the ARC Challenge Set. In the future, we will explore how to exploit external\n",
      "knowledge sources, and try to improve the quality of open IE by sentence parsing.\n",
      "7\n",
      "References\n",
      "Michele Banko, Michael J Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni.\n",
      "Open information extraction from the web. In IJCAI, volume 7, pages 2670–2676, 2007.\n",
      "Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.\n",
      "Translating embeddings for modeling multi-relational data. In Advances in neural information\n",
      "processing systems, pages 2787–2795, 2013.\n",
      "Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large anno-\n",
      "tated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326, 2015.\n",
      "Janara Christensen, Stephen Soderland, Oren Etzioni, et al. An analysis of open information\n",
      "extraction based on semantic role labeling. In Proceedings of the sixth international conference\n",
      "on Knowledge capture, pages 113–120. ACM, 2011.\n",
      "Peter Clark, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter D Turney, and\n",
      "Daniel Khashabi. Combining retrieval, statistics, and inference to answer elementary science\n",
      "questions. In AAAI, pages 2580–2586, 2016.\n",
      "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\n",
      "Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\n",
      "arXiv preprint arXiv:1803.05457, 2018.\n",
      "Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for struc-\n",
      "tured data. In International Conference on Machine Learning, pages 2702–2711, 2016.\n",
      "Jun Feng, Minlie Huang, Yang Yang, et al. Gake: Graph aware knowledge embedding. In Pro-\n",
      "ceedings of COLING 2016, the 26th International Conference on Computational Linguistics:\n",
      "Technical Papers, pages 641–651, 2016.\n",
      "Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural\n",
      "message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.\n",
      "Clinton Gormley and Zachary Tong. Elasticsearch: The Definitive Guide: A Distributed Real-Time\n",
      "Search and Analytics Engine. ” O’Reilly Media, Inc.”, 2015.\n",
      "He He, Anusha Balakrishnan, Mihail Eric, and Percy Liang. Learning symmetric collaborative\n",
      "dialogue agents with dynamic knowledge graph embeddings. arXiv preprint arXiv:1704.07130,\n",
      "2017.\n",
      "Peter Jansen, Niranjan Balasubramanian, Mihai Surdeanu, and Peter Clark. What’s in an expla-\n",
      "nation? characterizing knowledge and inference requirements for elementary science exams. In\n",
      "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics:\n",
      "Technical Papers, pages 2956–2965, 2016.\n",
      "Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Peter Clark, Oren Etzioni, and Dan Roth.\n",
      "Question answering via integer programming over semi-structured knowledge. arXiv preprint\n",
      "arXiv:1604.06076, 2016.\n",
      "8\n",
      "Tushar Khot, Niranjan Balasubramanian, Eric Gribkoff, Ashish Sabharwal, Peter Clark, and\n",
      "Oren Etzioni. Markov logic networks for natural language question answering. arXiv preprint\n",
      "arXiv:1507.03045, 2015.\n",
      "Tushar Khot, Ashish Sabharwal, and Peter Clark. Answering complex questions using open infor-\n",
      "mation extraction. arXiv preprint arXiv:1704.05572, 2017.\n",
      "Tushar Khot, Ashish Sabharwal, and Peter Clark. Scitail: A textual entailment dataset from\n",
      "science question answering. In Proceedings of AAAI, 2018.\n",
      "Harinder Pal et al. Demonyms and compound relational nouns in nominal open ie. In Proceedings\n",
      "of the 5th Workshop on Automated Knowledge Base Construction, pages 35–39, 2016.\n",
      "Ankur P Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "model for natural language inference. arXiv preprint arXiv:1606.01933, 2016.\n",
      "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\n",
      "for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n",
      "Matthew Richardson and Pedro Domingos. Markov logic networks. Machine learning, 62(1-2):\n",
      "107–136, 2006.\n",
      "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention\n",
      "flow for machine comprehension. arXiv preprint arXiv:1611.01603, 2016.\n",
      "Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J Smola, and Le Song. Variational rea-\n",
      "soning for question answering with knowledge graph. arXiv preprint arXiv:1709.04071, 2017.\n",
      "9\n",
      "Appendix\n",
      "A Examples of Knowledge Graphs\n",
      "To illustrate how we construct knowledge graphs from hypothesis and supporting sentences, here\n",
      "we present some examples.\n",
      "WefirstshowarelativelysimpleexampleinFigure2. Weseeapairofhypothesisandsupporting\n",
      "graphs. The hypothesis is “seed of oak comes from fruit”, as shown in Figure 2a. Note that the\n",
      "verb “comes” is lemmatized and becomes “come” in the graph. The supporting knowledge graph\n",
      "is plotted in Figure 2b, where we obtain knowledge including “fruit contains seed”, “fruit is part of\n",
      "tree”, and “oak is kind of tree”. With the supporting knowledge graph, we should be able to infer\n",
      "that the hypothesis is true.\n",
      "kind of tree\n",
      "obj\n",
      "be\n",
      "from fruit\n",
      "sub\n",
      "obj\n",
      "come seed oak\n",
      "sub\n",
      "seed of oak obj be su ob bj part of tree\n",
      "fruit sub obj\n",
      "(a) Knowledge graph for hypothesis part of plant sub be\n",
      "contain\n",
      "(b) Knowledge graph for supports\n",
      "Figure 2: Example of knowledge graphs for paired hypothesis and supports.\n",
      "Note that the knowledge graphs can be very complicated when the question stem has multiple\n",
      "sentences, or there is rich information in the supporting sentences extracted by Open IE. We show\n",
      "another example in Figure 3, which has a heavier supporting graph than the previous example.\n",
      "This is actually common in the ARC Challenge Set. In this example, the hypothesis is “day\n",
      "and night occurs due to rotation of earth”, as plotted in Figure 3a. Looking at the supporting\n",
      "graph in Figure 3b, we can find key information for this question, such as “day and night occurs\n",
      "because earth rotates”,“day and night causes earth rotation on its axis”,“day and night is caused by\n",
      "earth’s rotation”, etc. Withthesupportingknowledgegraph, weshouldhavenecessaryinformation\n",
      "to verify the hypothesis.\n",
      "10\n",
      "day and night\n",
      "sub\n",
      "occur\n",
      "obj\n",
      "due to rotation of earth\n",
      "(a) Knowledge graph for hypothesis\n",
      "rotation of earth\n",
      "sub\n",
      "cause\n",
      "obj\n",
      "by rotation of earth on its axis night and day phenomenon of day and night\n",
      "obj\n",
      "obj\n",
      "be cause be\n",
      "sub sub\n",
      "alternation between day and night primary effect of earth's rotation\n",
      "responsible for our day and night\n",
      "obj\n",
      "be\n",
      "sub\n",
      "to create day and night\n",
      "because earth rotates obj\n",
      "earth's rotation on its axis\n",
      "sub rotate\n",
      "obj by rotation of earth cause sub\n",
      "due to rotation of earth on its imaginary axis obj earth far beyond cause day and night\n",
      "occur be cause sub obj\n",
      "obj obj go\n",
      "sub sub to create sub\n",
      "happen obj impact of rotation on earth\n",
      "sub\n",
      "obj day and night sub\n",
      "sub cause be cause obj\n",
      "earth rotation on its axis sub obj\n",
      "time sub sub by rotation of earth on\n",
      "occur make\n",
      "sub\n",
      "obj cause be cause be result of earth complete rotation on its axis\n",
      "obj sun obj\n",
      "be\n",
      "by earth's rotation\n",
      "sub sub\n",
      "because earth rotate on it's axis obj sequence of day and night on earth\n",
      "earth's rotation\n",
      "result of earth rotate on its axis\n",
      "(b) Knowledge graph for supports\n",
      "Figure 3: Another example of knowledge graphs for paired hypothesis and supports.\n",
      "11<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  45895,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['ARC', 'SQuAD', 'SNLI', 'SciTail']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: KG2: Learning to Reason Science Exam Questions\n",
      "with Contextual Knowledge Graph Embeddings\n",
      "Yuyu Zhang1∗, Hanjun Dai1∗, Kamil Toraman2, and Le Song1\n",
      "1College of Computing, Georgia Institute of Technology\n",
      "2Korea Advanced Institute of Science and Technology\n",
      "1{yuyu.zhang, hanjun.dai, lsong}@cc.gatech.edu\n",
      "2kvtoraman@kaist.ac.kr\n",
      "Abstract\n",
      "TheAI2ReasoningChallenge(ARC),anewbenchmarkdatasetforquestionanswering(QA)\n",
      "has been recently released. ARC only contains natural science questions authored for human\n",
      "exams, which are hard to answer and require advanced logic reasoning. On the ARC Chal-\n",
      "lengeSet, existingstate-of-the-artQAsystemsfailtosignificantlyoutperformrandombaseline,\n",
      "reflecting the difficult nature of this task. In this paper, we propose a novel framework for\n",
      "answering science exam questions, which mimics human solving process in an open-book exam.\n",
      "To address the reasoning challenge, we construct contextual knowledge graphs respectively for\n",
      "the question itself and supporting sentences. Our model learns to reason with neural embed-\n",
      "dings of both knowledge graphs. Experiments on the ARC Challenge Set show that our model\n",
      "outperforms the previous state-of-the-art QA systems.\n",
      "1 Introduction\n",
      "Question answering (QA) has been a long-standing challenge in the field of artificial intelligence.\n",
      "Numerous research works have pushed forward techniques for building QA systems. Many existing\n",
      "approaches achieve high performance on benchmark datasets. However, most of the questions in\n",
      "those datasets only require surface-level reasoning, and do not reveal the full-scale complexity and\n",
      "challenge of the question answering problem. Recently, the AI2 Reasoning Challenge (ARC) has\n",
      "been proposed [Clark et al., 2018], which is designed to pose a challenge to the QA community. On\n",
      "the ARC Challenge Set, several state-of-the-art QA systems, including leading neural models from\n",
      "the well-known SQuAD and SNLI tasks, only perform slightly better than the random baseline.\n",
      "This striking observation has demonstrated that QA is still far from being solved.\n",
      "Why it is so difficult to answer the questions in the ARC Challenge Set? 1) ARC consists of\n",
      "natural science questions, namely questions authored for human exams. All of these questions are\n",
      "drawn from real exams; 2) In order to encourage progress on hard questions, a Challenge Set has\n",
      "been partitioned from ARC. To be more specific, if a question could not be correctly answered by\n",
      "neither an information retrieval (IR) method nor a word co-occurrence method, it is sorted into\n",
      "∗Both authors contributed equally to the paper.\n",
      "1\n",
      "8102\n",
      "yaM\n",
      "13\n",
      "]GL.sc[\n",
      "1v39321.5081:viXra\n",
      "the Challenge Set, otherwise the Easy Set. To illustrate the difference, consider the following two\n",
      "examples from both sets respectively, where the bold answers correspond to the correct choices:\n",
      "• ARC Easy Set:\n",
      "Which property of air does a barometer measure? (A) speed (B) pressure (C)\n",
      "humidity (D) temperature\n",
      "This question is correctly answered by both the IR and word co-occurrence methods.1 The\n",
      "IR method finds sentences relevant to the correct answer in the reference corpus, e.g., “Air\n",
      "pressurewillbemeasuredwithabarometer”. Duetothesubstantialwordoverlap,thequestion\n",
      "can be easily solved. Similarly, the word co-occurrence method finds that “barometer” and\n",
      "“pressure” co-occur frequently in the corpus, leading to the correct answer.\n",
      "• ARC Challenge Set:\n",
      "Which property of a mineral can be determined just by looking at it? (A) luster\n",
      "(B) mass (C) weight (D) hardness\n",
      "NeithertheIRmethodnorthewordco-occurrencemethodcancorrectlyanswerthisquestion.\n",
      "There are no sentences in the corpus similar to “A material’s luster can be determined by\n",
      "looking at it”. Also, “mineral” often co-occurs with distractor options (e.g., mass, hardness),\n",
      "which confuses the word co-occurrence method.\n",
      "Fromtheexamplesabove,weseethatsurface-levelreasoningmethodsarenotabletosolveques-\n",
      "tions in the Challenge Set, even the required knowledge is already covered in the reference corpus.\n",
      "The ARC Corpus, a large science-related text corpus collected from the Web and released together\n",
      "withARC,mentionsknowledgerelevanttoabout95%oftheARCChallengequestions[Clarketal.,\n",
      "2018]. However, the IR method with the ARC Corpus, as listed in Table 1, only achieves 20.26\n",
      "test score, which underperforms the random baseline. Collecting more sentences into the corpus\n",
      "would not solve the challenge. Actually we tried to use the entire Web as the reference corpus with\n",
      "Google Search API, and select the answer option with the most number of hits. This only slightly\n",
      "improves the score to 21.58.\n",
      "To tackle the ARC Challenge, we believe that there is no shortcut to get around advanced logic\n",
      "reasoning and deeper text comprehension. These questions target at students of age 8 through\n",
      "13 years old, and should be relatively easy for human to solve. For an adult with basic reasoning\n",
      "capability, even she forgets about the knowledge learned in grade school, she can still ace most of\n",
      "these questions in an open-book exam, by searching relevant supporting texts and reasoning over\n",
      "them.\n",
      "Inspired by the human problem solving process, we propose a neural reasoning engine named\n",
      "KG2 foransweringscienceexamquestions: readthequestion,generatehypothesisbycombiningthe\n",
      "questionstemandansweroption,findsupportingsentencesinthecorpus,andverifythehypothesis.\n",
      "For effective and efficient reasoning, we represent both hypothesis and supporting sentences in\n",
      "knowledge graphs. For example, in the supporting graph, “luster” is linked to “brightness”, and\n",
      "“brightness” connects to “look”, which is consistent with the hypothesis graph. Therefore, such\n",
      "reasoningpatternsongraphscanbelearnedbyourdifferentiableneuralengine. Experimentsonthe\n",
      "1Note that even it is correctly answered by only one of them, ARC would exclude it from the Challenge Set.\n",
      "2\n",
      "ARC Challenge Set show that our model achieves score that surpasses the previous state-of-the-art\n",
      "results.\n",
      "In summary, the contributions of this work are: 1) We propose a novel differential neural\n",
      "programming framework for reasoning about science exam questions; 2) Our method sets the new\n",
      "state of the art on the ARC Challenge Set; 3) We decompose the remaining difficulties towards\n",
      "solving the ARC Challenge, facilitating the community to engage with the dataset and progress on\n",
      "the challenging task.\n",
      "2 Related Work\n",
      "Science QA: For elementary science QA, simple IR-based methods have been proposed for science\n",
      "exams [Clark et al., 2016]. Markov Logic Networks [Richardson and Domingos, 2006] has been\n",
      "used to reason over a small set of logical rules [Khot et al., 2015]. Jansen et al. [2016] has analyzed\n",
      "knowledge and inference requirements for science exam questions.\n",
      "The work most related to us is DGEM [Khot et al., 2018], a neural entailment model which\n",
      "also employs Open IE to generate hypothesis graph. Our key contributions over DGEM: 1) DGEM\n",
      "is designed for single sentence entailment, while we aggregate multiple supporting sentences for\n",
      "reasoning; 2) DGEM has no structured representation of supporting facts, while our model learns\n",
      "to reason over the paired hypothesis and supporting graphs together.\n",
      "Graph Embedding: We employ graph embedding techniques for reasoning over knowledge\n",
      "graphs. Graph embedding has provided the representational flexibility for neural models in many\n",
      "NLP tasks, such as dialog system [He et al., 2017], question answering [Zhang et al., 2017], link\n",
      "prediction [Bordes et al., 2013] and triple classification [Feng et al., 2016]. In our paper, we extend\n",
      "this technique to mimic the reasoning process on graph ranking problem.\n",
      "3 Task\n",
      "(cid:8) (cid:0) (1) (m)(cid:1) (cid:9)n\n",
      "The ARC Challenge Set consists of science exam questions D = q, c,...,c, a, where\n",
      "i i i i i=1\n",
      "(j)\n",
      "q is the question stem, c is the j-th answer option corresponding to q (typically 4-way multiple\n",
      "i i i\n",
      "(j)\n",
      "choices), and a is the label of correct answer. Both q and c are in text format. Among the\n",
      "i i i\n",
      "multiple choices, only one of them is the correct answer and others are distractors. With the\n",
      "question stem and options, the goal is to find the correct answer. Accompanied with ARC, the\n",
      "ARCCorpusisalsoprovided,providing14Mscience-relatedsentencesfromtheWebwithknowledge\n",
      "relevant to ARC. The use of the ARC Corpus is optional for the ARC Challenge.\n",
      "4 Approach\n",
      "4.1 Generating Hypothesis\n",
      "A hypothesis h is a statement that combines a question stem q and an answer option c, which helps\n",
      "us understand what is being asked and what is the target to be verified. For example, consider the\n",
      "question stem “Which of these occurs due to the rotation of Earth?” and one of the answer options\n",
      "“day and night”. The hypothesis to be generated from them should be: “Day and night occurs due\n",
      "to the rotation of Earth”.\n",
      "3\n",
      "To automatically generate hypothesis, we first identify the wh-word (e.g., which, what, where,\n",
      "etc.) in the question stem, and replace it with the answer option. If there is no wh-word found,\n",
      "we just append the answer option behind the question stem. We create several rules to handle\n",
      "special cases and make hypothesis more natural. For example, “Which of these” and “Which of the\n",
      "following” should be replaced as a whole when they appear in the question stem. We successfully\n",
      "generatehypothesisformostquestions,however,therearestillafewcornercasesrequiringadvanced\n",
      "rewording, which should be negligible.\n",
      "4.2 Searching Potential Supports\n",
      "Toverifyahypothesis,welookforsupportsinthereferencecorpus. Althoughthecorpusistypically\n",
      "gigantic, we only need to focus on a tiny part of it, which is relevant to the question we are solving.\n",
      "Therefore,weusethegeneratedhypothesisasaquerytosearchtheentirecorpus. Thetopretrieved\n",
      "sentences are treated as potential supports for the hypothesis. In order to efficiently search the\n",
      "corpus, we build a local search engine on top of ElasticSearch [Gormley and Tong, 2015]. Since\n",
      "the corpus sentences are not as clean as questions, we filter noisy sentences that contain negation\n",
      "words (e.g., not, except, etc.) or unexpected characters or simply too long, and then pick up the\n",
      "top 20 sentences for verifying the hypothesis.\n",
      "4.3 Constructing Knowledge Graphs\n",
      "Many questions in the ARC Challenge Set require advanced reasoning on multiple supporting\n",
      "sentences. To aggregate knowledge across sentences, we employ Open IE [Banko et al., 2007,\n",
      "Christensen et al., 2011, Pal et al., 2016] v4 2 to extract relation triples from each sentence, and\n",
      "collect them to construct a contextual knowledge graph.\n",
      "More specifically, each relation triple is represented as T(s,p,o ), where s is the subject, p is the\n",
      "i\n",
      "predicate, ando isthei-thobject. Weconstructthegraphbyaddingnodess, pando, andadding\n",
      "i i\n",
      "directed edges with labels subj and obj. If there is adverbial of time or location extracted by Open\n",
      "IE, we add an edge with label time or loc in the knowledge graph. Words in each graph node are\n",
      "lemmatized. Similarly, we construct another knowledge graph for the corresponding hypothesis,\n",
      "which is paired with the supporting knowledge graph. Refer to Appendix A for examples of our\n",
      "generated graphs.\n",
      "4.4 Learning with Graph Embeddings\n",
      "Givenaquestionq andacandidatechoicec,weconstructthecorrespondinghypothesisgraphGhypo\n",
      "q,c\n",
      "and supporting graph Gsupp by aggregating the relation triples mentioned in Section 4.3. Thus,\n",
      "q,c\n",
      "choosing the right answer for question q becomes a graph ranking problem. A good graph scoring\n",
      "i\n",
      "functionf : Ghypo×Gsupp (cid:55)→ Rshouldassignthehighestscoretothecorrecthypothesis-supporting\n",
      "graph pair. Without loss of generality, we use point-wise ranking objective, where f(·) becomes a\n",
      "binary classifier.\n",
      "Toimplementthegraphscoringfunction,weadapttherecentadvancesingraphembedding[Dai\n",
      "et al., 2016, Gilmer et al., 2017] to our problem. Specifically, let G = (V,E) be a knowledge graph,\n",
      "and V ∈ V be the set of predicate nodes. We associate each node v ∈ V with an embedding vector\n",
      "p\n",
      "2https://github.com/allenai/openie-standalone\n",
      "4\n",
      "µ that captures the local information, which is computed recursively using the equation:\n",
      "v\n",
      "µ(t) = h(cid:0) x,µ(t−1),{(µ(t−1),e )} (cid:1) (1)\n",
      "v v v u u,v (u,v,eu,v)∈E\n",
      "Here x encodes the text feature of node generated by LSTM that is jointly trained with the\n",
      "v\n",
      "supervision. The edge type e can be time, loc, etc. We use a two-layer neural network for\n",
      "u,v\n",
      "(T)\n",
      "the function h(·). Eq. (1) iterates for T steps, and we use µ = µ as the node embedding\n",
      "v v\n",
      "representation. Finally, the scoring function f(·) is defined as:\n",
      "f(Ghypo,Gsupp) = f({µ u} u∈Vphypo,{µ v} v∈Vpsupp)\n",
      "(cid:16) (cid:17)\n",
      "= σ max\n",
      "µ(cid:62) uµv\n",
      "−0.5, (2)\n",
      "u,v (cid:107)µu(cid:107)(cid:107)µv(cid:107)\n",
      "where σ(·) is the sigmoid function, and the −0.5 shift is used to center the matching score at\n",
      "zero. Eq. (2) is making max inner product search between all pairs of predicate node embeddings.\n",
      "This mimics the procedure of reasoning on the most relevant hypothesis and corresponding sup-\n",
      "porting evidence, since each embedding vector already captures the information within its T-hop\n",
      "neighborhood.\n",
      "5 Experiments\n",
      "We compare our method against several recently published baseline models, including state-of-the-\n",
      "art neural models from the well-known SQuAD and SNLI tasks.\n",
      "5.1 Setup\n",
      "We use the ARC Challenge Set [Clark et al., 2018] for all experiments. This dataset consists of\n",
      "2,590 questions drawn from a variety of human exams. We use the original train / development /\n",
      "test split. The test set is held-out for model evaluation, which contains 1,172 questions. For each\n",
      "question, a QA system receives one point if it selects the correct answer, and 1/k points if it reports\n",
      "a k-way tie (i.e., chooses multiple answers) that includes the correct answer. The ARC Corpus can\n",
      "be optionally used for all models.\n",
      "5.2 Baselines\n",
      "Guess-all / Random: This naive baseline just selects all answer options, getting 1/k scores for\n",
      "eachquestionwithk answeroptions. Randomselectingwillalsoconvergetothisscoreafterenough\n",
      "trials.\n",
      "IR-ARC:IR-basedmethodsendsquestionstempluseachoptionasaquerytoasearchengine. For\n",
      "IR-ARC, the search engine is built on top of the ARC Corpus, and the search score is determined\n",
      "by the ElasticSearch score of the top retrieved sentence. The option with the highest search score\n",
      "is finally selected.\n",
      "IR-Google: This is similar to IR-ARC, but uses Google Search API 3 to retrieve documents from\n",
      "the entire Web, instead of just searching on the ARC Corpus. IR-Google uses the number of hits\n",
      "as the search score.\n",
      "3https://developers.google.com/custom-search\n",
      "5\n",
      "Table 1: Test performance of different QA systems on the ARC Challenge Set. The ARC Corpus\n",
      "is used in DecompAttn, DGEM, BiDAF and KG2.\n",
      "Method Test Scores\n",
      "IR-ARC 20.26\n",
      "IR-Google 21.58\n",
      "TupleInference 23.83\n",
      "DecompAttn 24.34\n",
      "Guess-all / Random 25.02\n",
      "DGEM-OpenIE 26.41\n",
      "BiDAF 26.54\n",
      "TableILP 26.97\n",
      "KG2 31.70\n",
      "TableILP:Thismethod[Khashabietal.,2016]performstable-basedreasoning,whichisformulated\n",
      "as an Integer Linear Program (ILP).\n",
      "TupleInference: This model [Khot et al., 2017] searches for graph that best connects the terms\n",
      "in the question with an answer choice via the knowledge extracted by Open IE.\n",
      "DecompAttn: It is a neural entailment model [Parikh et al., 2016] adapted to multiple-choice QA\n",
      "by assigning entailment score to the pair of hypothesis and single supporting sentence [Clark et al.,\n",
      "2018]. The answer option with the highest score is selected. DecompAttn is a top performer on\n",
      "SNLI [Bowman et al., 2015].\n",
      "DGEM-OpenIE: DGEM [Khot et al., 2018] is also a neural model for sentence-level entailment,\n",
      "but uses Open IE to create structured representation of the hypothesis. On the SciTail task [Khot\n",
      "et al., 2018], DGEM is a top performer. In Clark et al. [2018], there is another version of DGEM,\n",
      "which uses a proprietary parser together with Open IE and achieves 27.11 test score. For fair\n",
      "comparison, we only list publicly available models in Table 1.\n",
      "BiDAF: This model [Seo et al., 2016] is for span prediction QA, and has been adapted to multiple-\n",
      "choice QA [Clark et al., 2018]. BiDAF is a top performer on SQuAD [Rajpurkar et al., 2016].\n",
      "5.3 Results and Analysis\n",
      "Table 1 summarizes the test scores of all baseline models and our method. It is striking to see that\n",
      "none of the baseline methods perform significantly better than the random baseline, where the 95%\n",
      "confidence interval is ±2.5%. Our method KG2 achieves 31.70, which substantially improves the\n",
      "previous state of the art by 17.5%.\n",
      "Nevertheless, we are still far from “passing” the exam. To dissect the difficulties, we randomly\n",
      "sample 100 questions for investigation and report the results in Figure 1. More than half of the\n",
      "questions are lack of support: even human couldn’t solve them by only referring to the supporting\n",
      "sentences. This may be caused by the limited coverage of the corpus, and the retrieval bias where\n",
      "sentences with low word overlap can partially explain concept which is indispensable for reasoning.\n",
      "External knowledge sources may help on these questions. 12% questions have lost key information\n",
      "in graph, due to the failure of Open IE. Sentence parsing may be helpful since it reserves more\n",
      "text. 21% questions require very complex reasoning, and only 15% questions are “learnable” given\n",
      "6\n",
      "insufficient\n",
      "support (51%)\n",
      "others (1%)\n",
      "failed\n",
      "open IE (12%)\n",
      "learnable (15%)\n",
      "complex\n",
      "reasoning (21%)\n",
      "Figure 1: Distribution of various difficulties in solving the ARC Challenge Set.\n",
      "the current framework. This gives us an estimated upper bound when we correctly answer all the\n",
      "learnable questions, and just randomly guess the others, which should be 36.25. Improving the\n",
      "learning algorithm should bring our current result closer to this upper bound.\n",
      "6 Conclusion and Future Work\n",
      "We present a neural reasoning engine for answering science exam questions, which learns to reason\n",
      "overcontextualknowledgegraphs. Experimentalresultsshowthatourmethodoutperformsexisting\n",
      "QA systems on the ARC Challenge Set. In the future, we will explore how to exploit external\n",
      "knowledge sources, and try to improve the quality of open IE by sentence parsing.\n",
      "7\n",
      "References\n",
      "Michele Banko, Michael J Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni.\n",
      "Open information extraction from the web. In IJCAI, volume 7, pages 2670–2676, 2007.\n",
      "Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.\n",
      "Translating embeddings for modeling multi-relational data. In Advances in neural information\n",
      "processing systems, pages 2787–2795, 2013.\n",
      "Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large anno-\n",
      "tated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326, 2015.\n",
      "Janara Christensen, Stephen Soderland, Oren Etzioni, et al. An analysis of open information\n",
      "extraction based on semantic role labeling. In Proceedings of the sixth international conference\n",
      "on Knowledge capture, pages 113–120. ACM, 2011.\n",
      "Peter Clark, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter D Turney, and\n",
      "Daniel Khashabi. Combining retrieval, statistics, and inference to answer elementary science\n",
      "questions. In AAAI, pages 2580–2586, 2016.\n",
      "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\n",
      "Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\n",
      "arXiv preprint arXiv:1803.05457, 2018.\n",
      "Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for struc-\n",
      "tured data. In International Conference on Machine Learning, pages 2702–2711, 2016.\n",
      "Jun Feng, Minlie Huang, Yang Yang, et al. Gake: Graph aware knowledge embedding. In Pro-\n",
      "ceedings of COLING 2016, the 26th International Conference on Computational Linguistics:\n",
      "Technical Papers, pages 641–651, 2016.\n",
      "Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural\n",
      "message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.\n",
      "Clinton Gormley and Zachary Tong. Elasticsearch: The Definitive Guide: A Distributed Real-Time\n",
      "Search and Analytics Engine. ” O’Reilly Media, Inc.”, 2015.\n",
      "He He, Anusha Balakrishnan, Mihail Eric, and Percy Liang. Learning symmetric collaborative\n",
      "dialogue agents with dynamic knowledge graph embeddings. arXiv preprint arXiv:1704.07130,\n",
      "2017.\n",
      "Peter Jansen, Niranjan Balasubramanian, Mihai Surdeanu, and Peter Clark. What’s in an expla-\n",
      "nation? characterizing knowledge and inference requirements for elementary science exams. In\n",
      "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics:\n",
      "Technical Papers, pages 2956–2965, 2016.\n",
      "Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Peter Clark, Oren Etzioni, and Dan Roth.\n",
      "Question answering via integer programming over semi-structured knowledge. arXiv preprint\n",
      "arXiv:1604.06076, 2016.\n",
      "8\n",
      "Tushar Khot, Niranjan Balasubramanian, Eric Gribkoff, Ashish Sabharwal, Peter Clark, and\n",
      "Oren Etzioni. Markov logic networks for natural language question answering. arXiv preprint\n",
      "arXiv:1507.03045, 2015.\n",
      "Tushar Khot, Ashish Sabharwal, and Peter Clark. Answering complex questions using open infor-\n",
      "mation extraction. arXiv preprint arXiv:1704.05572, 2017.\n",
      "Tushar Khot, Ashish Sabharwal, and Peter Clark. Scitail: A textual entailment dataset from\n",
      "science question answering. In Proceedings of AAAI, 2018.\n",
      "Harinder Pal et al. Demonyms and compound relational nouns in nominal open ie. In Proceedings\n",
      "of the 5th Workshop on Automated Knowledge Base Construction, pages 35–39, 2016.\n",
      "Ankur P Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "model for natural language inference. arXiv preprint arXiv:1606.01933, 2016.\n",
      "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\n",
      "for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n",
      "Matthew Richardson and Pedro Domingos. Markov logic networks. Machine learning, 62(1-2):\n",
      "107–136, 2006.\n",
      "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention\n",
      "flow for machine comprehension. arXiv preprint arXiv:1611.01603, 2016.\n",
      "Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J Smola, and Le Song. Variational rea-\n",
      "soning for question answering with knowledge graph. arXiv preprint arXiv:1709.04071, 2017.\n",
      "9\n",
      "Appendix\n",
      "A Examples of Knowledge Graphs\n",
      "To illustrate how we construct knowledge graphs from hypothesis and supporting sentences, here\n",
      "we present some examples.\n",
      "WefirstshowarelativelysimpleexampleinFigure2. Weseeapairofhypothesisandsupporting\n",
      "graphs. The hypothesis is “seed of oak comes from fruit”, as shown in Figure 2a. Note that the\n",
      "verb “comes” is lemmatized and becomes “come” in the graph. The supporting knowledge graph\n",
      "is plotted in Figure 2b, where we obtain knowledge including “fruit contains seed”, “fruit is part of\n",
      "tree”, and “oak is kind of tree”. With the supporting knowledge graph, we should be able to infer\n",
      "that the hypothesis is true.\n",
      "kind of tree\n",
      "obj\n",
      "be\n",
      "from fruit\n",
      "sub\n",
      "obj\n",
      "come seed oak\n",
      "sub\n",
      "seed of oak obj be su ob bj part of tree\n",
      "fruit sub obj\n",
      "(a) Knowledge graph for hypothesis part of plant sub be\n",
      "contain\n",
      "(b) Knowledge graph for supports\n",
      "Figure 2: Example of knowledge graphs for paired hypothesis and supports.\n",
      "Note that the knowledge graphs can be very complicated when the question stem has multiple\n",
      "sentences, or there is rich information in the supporting sentences extracted by Open IE. We show\n",
      "another example in Figure 3, which has a heavier supporting graph than the previous example.\n",
      "This is actually common in the ARC Challenge Set. In this example, the hypothesis is “day\n",
      "and night occurs due to rotation of earth”, as plotted in Figure 3a. Looking at the supporting\n",
      "graph in Figure 3b, we can find key information for this question, such as “day and night occurs\n",
      "because earth rotates”,“day and night causes earth rotation on its axis”,“day and night is caused by\n",
      "earth’s rotation”, etc. Withthesupportingknowledgegraph, weshouldhavenecessaryinformation\n",
      "to verify the hypothesis.\n",
      "10\n",
      "day and night\n",
      "sub\n",
      "occur\n",
      "obj\n",
      "due to rotation of earth\n",
      "(a) Knowledge graph for hypothesis\n",
      "rotation of earth\n",
      "sub\n",
      "cause\n",
      "obj\n",
      "by rotation of earth on its axis night and day phenomenon of day and night\n",
      "obj\n",
      "obj\n",
      "be cause be\n",
      "sub sub\n",
      "alternation between day and night primary effect of earth's rotation\n",
      "responsible for our day and night\n",
      "obj\n",
      "be\n",
      "sub\n",
      "to create day and night\n",
      "because earth rotates obj\n",
      "earth's rotation on its axis\n",
      "sub rotate\n",
      "obj by rotation of earth cause sub\n",
      "due to rotation of earth on its imaginary axis obj earth far beyond cause day and night\n",
      "occur be cause sub obj\n",
      "obj obj go\n",
      "sub sub to create sub\n",
      "happen obj impact of rotation on earth\n",
      "sub\n",
      "obj day and night sub\n",
      "sub cause be cause obj\n",
      "earth rotation on its axis sub obj\n",
      "time sub sub by rotation of earth on\n",
      "occur make\n",
      "sub\n",
      "obj cause be cause be result of earth complete rotation on its axis\n",
      "obj sun obj\n",
      "be\n",
      "by earth's rotation\n",
      "sub sub\n",
      "because earth rotate on it's axis obj sequence of day and night on earth\n",
      "earth's rotation\n",
      "result of earth rotate on its axis\n",
      "(b) Knowledge graph for supports\n",
      "Figure 3: Another example of knowledge graphs for paired hypothesis and supports.\n",
      "11<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  25624,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Generating Hypothesis', 'Searching Potential Supports', 'Constructing Knowledge Graphs', 'Learning with Graph Embeddings']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: KG2: Learning to Reason Science Exam Questions\n",
      "with Contextual Knowledge Graph Embeddings\n",
      "Yuyu Zhang1∗, Hanjun Dai1∗, Kamil Toraman2, and Le Song1\n",
      "1College of Computing, Georgia Institute of Technology\n",
      "2Korea Advanced Institute of Science and Technology\n",
      "1{yuyu.zhang, hanjun.dai, lsong}@cc.gatech.edu\n",
      "2kvtoraman@kaist.ac.kr\n",
      "Abstract\n",
      "TheAI2ReasoningChallenge(ARC),anewbenchmarkdatasetforquestionanswering(QA)\n",
      "has been recently released. ARC only contains natural science questions authored for human\n",
      "exams, which are hard to answer and require advanced logic reasoning. On the ARC Chal-\n",
      "lengeSet, existingstate-of-the-artQAsystemsfailtosignificantlyoutperformrandombaseline,\n",
      "reflecting the difficult nature of this task. In this paper, we propose a novel framework for\n",
      "answering science exam questions, which mimics human solving process in an open-book exam.\n",
      "To address the reasoning challenge, we construct contextual knowledge graphs respectively for\n",
      "the question itself and supporting sentences. Our model learns to reason with neural embed-\n",
      "dings of both knowledge graphs. Experiments on the ARC Challenge Set show that our model\n",
      "outperforms the previous state-of-the-art QA systems.\n",
      "1 Introduction\n",
      "Question answering (QA) has been a long-standing challenge in the field of artificial intelligence.\n",
      "Numerous research works have pushed forward techniques for building QA systems. Many existing\n",
      "approaches achieve high performance on benchmark datasets. However, most of the questions in\n",
      "those datasets only require surface-level reasoning, and do not reveal the full-scale complexity and\n",
      "challenge of the question answering problem. Recently, the AI2 Reasoning Challenge (ARC) has\n",
      "been proposed [Clark et al., 2018], which is designed to pose a challenge to the QA community. On\n",
      "the ARC Challenge Set, several state-of-the-art QA systems, including leading neural models from\n",
      "the well-known SQuAD and SNLI tasks, only perform slightly better than the random baseline.\n",
      "This striking observation has demonstrated that QA is still far from being solved.\n",
      "Why it is so difficult to answer the questions in the ARC Challenge Set? 1) ARC consists of\n",
      "natural science questions, namely questions authored for human exams. All of these questions are\n",
      "drawn from real exams; 2) In order to encourage progress on hard questions, a Challenge Set has\n",
      "been partitioned from ARC. To be more specific, if a question could not be correctly answered by\n",
      "neither an information retrieval (IR) method nor a word co-occurrence method, it is sorted into\n",
      "∗Both authors contributed equally to the paper.\n",
      "1\n",
      "8102\n",
      "yaM\n",
      "13\n",
      "]GL.sc[\n",
      "1v39321.5081:viXra\n",
      "the Challenge Set, otherwise the Easy Set. To illustrate the difference, consider the following two\n",
      "examples from both sets respectively, where the bold answers correspond to the correct choices:\n",
      "• ARC Easy Set:\n",
      "Which property of air does a barometer measure? (A) speed (B) pressure (C)\n",
      "humidity (D) temperature\n",
      "This question is correctly answered by both the IR and word co-occurrence methods.1 The\n",
      "IR method finds sentences relevant to the correct answer in the reference corpus, e.g., “Air\n",
      "pressurewillbemeasuredwithabarometer”. Duetothesubstantialwordoverlap,thequestion\n",
      "can be easily solved. Similarly, the word co-occurrence method finds that “barometer” and\n",
      "“pressure” co-occur frequently in the corpus, leading to the correct answer.\n",
      "• ARC Challenge Set:\n",
      "Which property of a mineral can be determined just by looking at it? (A) luster\n",
      "(B) mass (C) weight (D) hardness\n",
      "NeithertheIRmethodnorthewordco-occurrencemethodcancorrectlyanswerthisquestion.\n",
      "There are no sentences in the corpus similar to “A material’s luster can be determined by\n",
      "looking at it”. Also, “mineral” often co-occurs with distractor options (e.g., mass, hardness),\n",
      "which confuses the word co-occurrence method.\n",
      "Fromtheexamplesabove,weseethatsurface-levelreasoningmethodsarenotabletosolveques-\n",
      "tions in the Challenge Set, even the required knowledge is already covered in the reference corpus.\n",
      "The ARC Corpus, a large science-related text corpus collected from the Web and released together\n",
      "withARC,mentionsknowledgerelevanttoabout95%oftheARCChallengequestions[Clarketal.,\n",
      "2018]. However, the IR method with the ARC Corpus, as listed in Table 1, only achieves 20.26\n",
      "test score, which underperforms the random baseline. Collecting more sentences into the corpus\n",
      "would not solve the challenge. Actually we tried to use the entire Web as the reference corpus with\n",
      "Google Search API, and select the answer option with the most number of hits. This only slightly\n",
      "improves the score to 21.58.\n",
      "To tackle the ARC Challenge, we believe that there is no shortcut to get around advanced logic\n",
      "reasoning and deeper text comprehension. These questions target at students of age 8 through\n",
      "13 years old, and should be relatively easy for human to solve. For an adult with basic reasoning\n",
      "capability, even she forgets about the knowledge learned in grade school, she can still ace most of\n",
      "these questions in an open-book exam, by searching relevant supporting texts and reasoning over\n",
      "them.\n",
      "Inspired by the human problem solving process, we propose a neural reasoning engine named\n",
      "KG2 foransweringscienceexamquestions: readthequestion,generatehypothesisbycombiningthe\n",
      "questionstemandansweroption,findsupportingsentencesinthecorpus,andverifythehypothesis.\n",
      "For effective and efficient reasoning, we represent both hypothesis and supporting sentences in\n",
      "knowledge graphs. For example, in the supporting graph, “luster” is linked to “brightness”, and\n",
      "“brightness” connects to “look”, which is consistent with the hypothesis graph. Therefore, such\n",
      "reasoningpatternsongraphscanbelearnedbyourdifferentiableneuralengine. Experimentsonthe\n",
      "1Note that even it is correctly answered by only one of them, ARC would exclude it from the Challenge Set.\n",
      "2\n",
      "ARC Challenge Set show that our model achieves score that surpasses the previous state-of-the-art\n",
      "results.\n",
      "In summary, the contributions of this work are: 1) We propose a novel differential neural\n",
      "programming framework for reasoning about science exam questions; 2) Our method sets the new\n",
      "state of the art on the ARC Challenge Set; 3) We decompose the remaining difficulties towards\n",
      "solving the ARC Challenge, facilitating the community to engage with the dataset and progress on\n",
      "the challenging task.\n",
      "2 Related Work\n",
      "Science QA: For elementary science QA, simple IR-based methods have been proposed for science\n",
      "exams [Clark et al., 2016]. Markov Logic Networks [Richardson and Domingos, 2006] has been\n",
      "used to reason over a small set of logical rules [Khot et al., 2015]. Jansen et al. [2016] has analyzed\n",
      "knowledge and inference requirements for science exam questions.\n",
      "The work most related to us is DGEM [Khot et al., 2018], a neural entailment model which\n",
      "also employs Open IE to generate hypothesis graph. Our key contributions over DGEM: 1) DGEM\n",
      "is designed for single sentence entailment, while we aggregate multiple supporting sentences for\n",
      "reasoning; 2) DGEM has no structured representation of supporting facts, while our model learns\n",
      "to reason over the paired hypothesis and supporting graphs together.\n",
      "Graph Embedding: We employ graph embedding techniques for reasoning over knowledge\n",
      "graphs. Graph embedding has provided the representational flexibility for neural models in many\n",
      "NLP tasks, such as dialog system [He et al., 2017], question answering [Zhang et al., 2017], link\n",
      "prediction [Bordes et al., 2013] and triple classification [Feng et al., 2016]. In our paper, we extend\n",
      "this technique to mimic the reasoning process on graph ranking problem.\n",
      "3 Task\n",
      "(cid:8) (cid:0) (1) (m)(cid:1) (cid:9)n\n",
      "The ARC Challenge Set consists of science exam questions D = q, c,...,c, a, where\n",
      "i i i i i=1\n",
      "(j)\n",
      "q is the question stem, c is the j-th answer option corresponding to q (typically 4-way multiple\n",
      "i i i\n",
      "(j)\n",
      "choices), and a is the label of correct answer. Both q and c are in text format. Among the\n",
      "i i i\n",
      "multiple choices, only one of them is the correct answer and others are distractors. With the\n",
      "question stem and options, the goal is to find the correct answer. Accompanied with ARC, the\n",
      "ARCCorpusisalsoprovided,providing14Mscience-relatedsentencesfromtheWebwithknowledge\n",
      "relevant to ARC. The use of the ARC Corpus is optional for the ARC Challenge.\n",
      "4 Approach\n",
      "4.1 Generating Hypothesis\n",
      "A hypothesis h is a statement that combines a question stem q and an answer option c, which helps\n",
      "us understand what is being asked and what is the target to be verified. For example, consider the\n",
      "question stem “Which of these occurs due to the rotation of Earth?” and one of the answer options\n",
      "“day and night”. The hypothesis to be generated from them should be: “Day and night occurs due\n",
      "to the rotation of Earth”.\n",
      "3\n",
      "To automatically generate hypothesis, we first identify the wh-word (e.g., which, what, where,\n",
      "etc.) in the question stem, and replace it with the answer option. If there is no wh-word found,\n",
      "we just append the answer option behind the question stem. We create several rules to handle\n",
      "special cases and make hypothesis more natural. For example, “Which of these” and “Which of the\n",
      "following” should be replaced as a whole when they appear in the question stem. We successfully\n",
      "generatehypothesisformostquestions,however,therearestillafewcornercasesrequiringadvanced\n",
      "rewording, which should be negligible.\n",
      "4.2 Searching Potential Supports\n",
      "Toverifyahypothesis,welookforsupportsinthereferencecorpus. Althoughthecorpusistypically\n",
      "gigantic, we only need to focus on a tiny part of it, which is relevant to the question we are solving.\n",
      "Therefore,weusethegeneratedhypothesisasaquerytosearchtheentirecorpus. Thetopretrieved\n",
      "sentences are treated as potential supports for the hypothesis. In order to efficiently search the\n",
      "corpus, we build a local search engine on top of ElasticSearch [Gormley and Tong, 2015]. Since\n",
      "the corpus sentences are not as clean as questions, we filter noisy sentences that contain negation\n",
      "words (e.g., not, except, etc.) or unexpected characters or simply too long, and then pick up the\n",
      "top 20 sentences for verifying the hypothesis.\n",
      "4.3 Constructing Knowledge Graphs\n",
      "Many questions in the ARC Challenge Set require advanced reasoning on multiple supporting\n",
      "sentences. To aggregate knowledge across sentences, we employ Open IE [Banko et al., 2007,\n",
      "Christensen et al., 2011, Pal et al., 2016] v4 2 to extract relation triples from each sentence, and\n",
      "collect them to construct a contextual knowledge graph.\n",
      "More specifically, each relation triple is represented as T(s,p,o ), where s is the subject, p is the\n",
      "i\n",
      "predicate, ando isthei-thobject. Weconstructthegraphbyaddingnodess, pando, andadding\n",
      "i i\n",
      "directed edges with labels subj and obj. If there is adverbial of time or location extracted by Open\n",
      "IE, we add an edge with label time or loc in the knowledge graph. Words in each graph node are\n",
      "lemmatized. Similarly, we construct another knowledge graph for the corresponding hypothesis,\n",
      "which is paired with the supporting knowledge graph. Refer to Appendix A for examples of our\n",
      "generated graphs.\n",
      "4.4 Learning with Graph Embeddings\n",
      "Givenaquestionq andacandidatechoicec,weconstructthecorrespondinghypothesisgraphGhypo\n",
      "q,c\n",
      "and supporting graph Gsupp by aggregating the relation triples mentioned in Section 4.3. Thus,\n",
      "q,c\n",
      "choosing the right answer for question q becomes a graph ranking problem. A good graph scoring\n",
      "i\n",
      "functionf : Ghypo×Gsupp (cid:55)→ Rshouldassignthehighestscoretothecorrecthypothesis-supporting\n",
      "graph pair. Without loss of generality, we use point-wise ranking objective, where f(·) becomes a\n",
      "binary classifier.\n",
      "Toimplementthegraphscoringfunction,weadapttherecentadvancesingraphembedding[Dai\n",
      "et al., 2016, Gilmer et al., 2017] to our problem. Specifically, let G = (V,E) be a knowledge graph,\n",
      "and V ∈ V be the set of predicate nodes. We associate each node v ∈ V with an embedding vector\n",
      "p\n",
      "2https://github.com/allenai/openie-standalone\n",
      "4\n",
      "µ that captures the local information, which is computed recursively using the equation:\n",
      "v\n",
      "µ(t) = h(cid:0) x,µ(t−1),{(µ(t−1),e )} (cid:1) (1)\n",
      "v v v u u,v (u,v,eu,v)∈E\n",
      "Here x encodes the text feature of node generated by LSTM that is jointly trained with the\n",
      "v\n",
      "supervision. The edge type e can be time, loc, etc. We use a two-layer neural network for\n",
      "u,v\n",
      "(T)\n",
      "the function h(·). Eq. (1) iterates for T steps, and we use µ = µ as the node embedding\n",
      "v v\n",
      "representation. Finally, the scoring function f(·) is defined as:\n",
      "f(Ghypo,Gsupp) = f({µ u} u∈Vphypo,{µ v} v∈Vpsupp)\n",
      "(cid:16) (cid:17)\n",
      "= σ max\n",
      "µ(cid:62) uµv\n",
      "−0.5, (2)\n",
      "u,v (cid:107)µu(cid:107)(cid:107)µv(cid:107)\n",
      "where σ(·) is the sigmoid function, and the −0.5 shift is used to center the matching score at\n",
      "zero. Eq. (2) is making max inner product search between all pairs of predicate node embeddings.\n",
      "This mimics the procedure of reasoning on the most relevant hypothesis and corresponding sup-\n",
      "porting evidence, since each embedding vector already captures the information within its T-hop\n",
      "neighborhood.\n",
      "5 Experiments\n",
      "We compare our method against several recently published baseline models, including state-of-the-\n",
      "art neural models from the well-known SQuAD and SNLI tasks.\n",
      "5.1 Setup\n",
      "We use the ARC Challenge Set [Clark et al., 2018] for all experiments. This dataset consists of\n",
      "2,590 questions drawn from a variety of human exams. We use the original train / development /\n",
      "test split. The test set is held-out for model evaluation, which contains 1,172 questions. For each\n",
      "question, a QA system receives one point if it selects the correct answer, and 1/k points if it reports\n",
      "a k-way tie (i.e., chooses multiple answers) that includes the correct answer. The ARC Corpus can\n",
      "be optionally used for all models.\n",
      "5.2 Baselines\n",
      "Guess-all / Random: This naive baseline just selects all answer options, getting 1/k scores for\n",
      "eachquestionwithk answeroptions. Randomselectingwillalsoconvergetothisscoreafterenough\n",
      "trials.\n",
      "IR-ARC:IR-basedmethodsendsquestionstempluseachoptionasaquerytoasearchengine. For\n",
      "IR-ARC, the search engine is built on top of the ARC Corpus, and the search score is determined\n",
      "by the ElasticSearch score of the top retrieved sentence. The option with the highest search score\n",
      "is finally selected.\n",
      "IR-Google: This is similar to IR-ARC, but uses Google Search API 3 to retrieve documents from\n",
      "the entire Web, instead of just searching on the ARC Corpus. IR-Google uses the number of hits\n",
      "as the search score.\n",
      "3https://developers.google.com/custom-search\n",
      "5\n",
      "Table 1: Test performance of different QA systems on the ARC Challenge Set. The ARC Corpus\n",
      "is used in DecompAttn, DGEM, BiDAF and KG2.\n",
      "Method Test Scores\n",
      "IR-ARC 20.26\n",
      "IR-Google 21.58\n",
      "TupleInference 23.83\n",
      "DecompAttn 24.34\n",
      "Guess-all / Random 25.02\n",
      "DGEM-OpenIE 26.41\n",
      "BiDAF 26.54\n",
      "TableILP 26.97\n",
      "KG2 31.70\n",
      "TableILP:Thismethod[Khashabietal.,2016]performstable-basedreasoning,whichisformulated\n",
      "as an Integer Linear Program (ILP).\n",
      "TupleInference: This model [Khot et al., 2017] searches for graph that best connects the terms\n",
      "in the question with an answer choice via the knowledge extracted by Open IE.\n",
      "DecompAttn: It is a neural entailment model [Parikh et al., 2016] adapted to multiple-choice QA\n",
      "by assigning entailment score to the pair of hypothesis and single supporting sentence [Clark et al.,\n",
      "2018]. The answer option with the highest score is selected. DecompAttn is a top performer on\n",
      "SNLI [Bowman et al., 2015].\n",
      "DGEM-OpenIE: DGEM [Khot et al., 2018] is also a neural model for sentence-level entailment,\n",
      "but uses Open IE to create structured representation of the hypothesis. On the SciTail task [Khot\n",
      "et al., 2018], DGEM is a top performer. In Clark et al. [2018], there is another version of DGEM,\n",
      "which uses a proprietary parser together with Open IE and achieves 27.11 test score. For fair\n",
      "comparison, we only list publicly available models in Table 1.\n",
      "BiDAF: This model [Seo et al., 2016] is for span prediction QA, and has been adapted to multiple-\n",
      "choice QA [Clark et al., 2018]. BiDAF is a top performer on SQuAD [Rajpurkar et al., 2016].\n",
      "5.3 Results and Analysis\n",
      "Table 1 summarizes the test scores of all baseline models and our method. It is striking to see that\n",
      "none of the baseline methods perform significantly better than the random baseline, where the 95%\n",
      "confidence interval is ±2.5%. Our method KG2 achieves 31.70, which substantially improves the\n",
      "previous state of the art by 17.5%.\n",
      "Nevertheless, we are still far from “passing” the exam. To dissect the difficulties, we randomly\n",
      "sample 100 questions for investigation and report the results in Figure 1. More than half of the\n",
      "questions are lack of support: even human couldn’t solve them by only referring to the supporting\n",
      "sentences. This may be caused by the limited coverage of the corpus, and the retrieval bias where\n",
      "sentences with low word overlap can partially explain concept which is indispensable for reasoning.\n",
      "External knowledge sources may help on these questions. 12% questions have lost key information\n",
      "in graph, due to the failure of Open IE. Sentence parsing may be helpful since it reserves more\n",
      "text. 21% questions require very complex reasoning, and only 15% questions are “learnable” given\n",
      "6\n",
      "insufficient\n",
      "support (51%)\n",
      "others (1%)\n",
      "failed\n",
      "open IE (12%)\n",
      "learnable (15%)\n",
      "complex\n",
      "reasoning (21%)\n",
      "Figure 1: Distribution of various difficulties in solving the ARC Challenge Set.\n",
      "the current framework. This gives us an estimated upper bound when we correctly answer all the\n",
      "learnable questions, and just randomly guess the others, which should be 36.25. Improving the\n",
      "learning algorithm should bring our current result closer to this upper bound.\n",
      "6 Conclusion and Future Work\n",
      "We present a neural reasoning engine for answering science exam questions, which learns to reason\n",
      "overcontextualknowledgegraphs. Experimentalresultsshowthatourmethodoutperformsexisting\n",
      "QA systems on the ARC Challenge Set. In the future, we will explore how to exploit external\n",
      "knowledge sources, and try to improve the quality of open IE by sentence parsing.\n",
      "7\n",
      "References\n",
      "Michele Banko, Michael J Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni.\n",
      "Open information extraction from the web. In IJCAI, volume 7, pages 2670–2676, 2007.\n",
      "Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.\n",
      "Translating embeddings for modeling multi-relational data. In Advances in neural information\n",
      "processing systems, pages 2787–2795, 2013.\n",
      "Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large anno-\n",
      "tated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326, 2015.\n",
      "Janara Christensen, Stephen Soderland, Oren Etzioni, et al. An analysis of open information\n",
      "extraction based on semantic role labeling. In Proceedings of the sixth international conference\n",
      "on Knowledge capture, pages 113–120. ACM, 2011.\n",
      "Peter Clark, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter D Turney, and\n",
      "Daniel Khashabi. Combining retrieval, statistics, and inference to answer elementary science\n",
      "questions. In AAAI, pages 2580–2586, 2016.\n",
      "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\n",
      "Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\n",
      "arXiv preprint arXiv:1803.05457, 2018.\n",
      "Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for struc-\n",
      "tured data. In International Conference on Machine Learning, pages 2702–2711, 2016.\n",
      "Jun Feng, Minlie Huang, Yang Yang, et al. Gake: Graph aware knowledge embedding. In Pro-\n",
      "ceedings of COLING 2016, the 26th International Conference on Computational Linguistics:\n",
      "Technical Papers, pages 641–651, 2016.\n",
      "Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural\n",
      "message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.\n",
      "Clinton Gormley and Zachary Tong. Elasticsearch: The Definitive Guide: A Distributed Real-Time\n",
      "Search and Analytics Engine. ” O’Reilly Media, Inc.”, 2015.\n",
      "He He, Anusha Balakrishnan, Mihail Eric, and Percy Liang. Learning symmetric collaborative\n",
      "dialogue agents with dynamic knowledge graph embeddings. arXiv preprint arXiv:1704.07130,\n",
      "2017.\n",
      "Peter Jansen, Niranjan Balasubramanian, Mihai Surdeanu, and Peter Clark. What’s in an expla-\n",
      "nation? characterizing knowledge and inference requirements for elementary science exams. In\n",
      "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics:\n",
      "Technical Papers, pages 2956–2965, 2016.\n",
      "Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Peter Clark, Oren Etzioni, and Dan Roth.\n",
      "Question answering via integer programming over semi-structured knowledge. arXiv preprint\n",
      "arXiv:1604.06076, 2016.\n",
      "8\n",
      "Tushar Khot, Niranjan Balasubramanian, Eric Gribkoff, Ashish Sabharwal, Peter Clark, and\n",
      "Oren Etzioni. Markov logic networks for natural language question answering. arXiv preprint\n",
      "arXiv:1507.03045, 2015.\n",
      "Tushar Khot, Ashish Sabharwal, and Peter Clark. Answering complex questions using open infor-\n",
      "mation extraction. arXiv preprint arXiv:1704.05572, 2017.\n",
      "Tushar Khot, Ashish Sabharwal, and Peter Clark. Scitail: A textual entailment dataset from\n",
      "science question answering. In Proceedings of AAAI, 2018.\n",
      "Harinder Pal et al. Demonyms and compound relational nouns in nominal open ie. In Proceedings\n",
      "of the 5th Workshop on Automated Knowledge Base Construction, pages 35–39, 2016.\n",
      "Ankur P Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "model for natural language inference. arXiv preprint arXiv:1606.01933, 2016.\n",
      "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions\n",
      "for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n",
      "Matthew Richardson and Pedro Domingos. Markov logic networks. Machine learning, 62(1-2):\n",
      "107–136, 2006.\n",
      "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention\n",
      "flow for machine comprehension. arXiv preprint arXiv:1611.01603, 2016.\n",
      "Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J Smola, and Le Song. Variational rea-\n",
      "soning for question answering with knowledge graph. arXiv preprint arXiv:1709.04071, 2017.\n",
      "9\n",
      "Appendix\n",
      "A Examples of Knowledge Graphs\n",
      "To illustrate how we construct knowledge graphs from hypothesis and supporting sentences, here\n",
      "we present some examples.\n",
      "WefirstshowarelativelysimpleexampleinFigure2. Weseeapairofhypothesisandsupporting\n",
      "graphs. The hypothesis is “seed of oak comes from fruit”, as shown in Figure 2a. Note that the\n",
      "verb “comes” is lemmatized and becomes “come” in the graph. The supporting knowledge graph\n",
      "is plotted in Figure 2b, where we obtain knowledge including “fruit contains seed”, “fruit is part of\n",
      "tree”, and “oak is kind of tree”. With the supporting knowledge graph, we should be able to infer\n",
      "that the hypothesis is true.\n",
      "kind of tree\n",
      "obj\n",
      "be\n",
      "from fruit\n",
      "sub\n",
      "obj\n",
      "come seed oak\n",
      "sub\n",
      "seed of oak obj be su ob bj part of tree\n",
      "fruit sub obj\n",
      "(a) Knowledge graph for hypothesis part of plant sub be\n",
      "contain\n",
      "(b) Knowledge graph for supports\n",
      "Figure 2: Example of knowledge graphs for paired hypothesis and supports.\n",
      "Note that the knowledge graphs can be very complicated when the question stem has multiple\n",
      "sentences, or there is rich information in the supporting sentences extracted by Open IE. We show\n",
      "another example in Figure 3, which has a heavier supporting graph than the previous example.\n",
      "This is actually common in the ARC Challenge Set. In this example, the hypothesis is “day\n",
      "and night occurs due to rotation of earth”, as plotted in Figure 3a. Looking at the supporting\n",
      "graph in Figure 3b, we can find key information for this question, such as “day and night occurs\n",
      "because earth rotates”,“day and night causes earth rotation on its axis”,“day and night is caused by\n",
      "earth’s rotation”, etc. Withthesupportingknowledgegraph, weshouldhavenecessaryinformation\n",
      "to verify the hypothesis.\n",
      "10\n",
      "day and night\n",
      "sub\n",
      "occur\n",
      "obj\n",
      "due to rotation of earth\n",
      "(a) Knowledge graph for hypothesis\n",
      "rotation of earth\n",
      "sub\n",
      "cause\n",
      "obj\n",
      "by rotation of earth on its axis night and day phenomenon of day and night\n",
      "obj\n",
      "obj\n",
      "be cause be\n",
      "sub sub\n",
      "alternation between day and night primary effect of earth's rotation\n",
      "responsible for our day and night\n",
      "obj\n",
      "be\n",
      "sub\n",
      "to create day and night\n",
      "because earth rotates obj\n",
      "earth's rotation on its axis\n",
      "sub rotate\n",
      "obj by rotation of earth cause sub\n",
      "due to rotation of earth on its imaginary axis obj earth far beyond cause day and night\n",
      "occur be cause sub obj\n",
      "obj obj go\n",
      "sub sub to create sub\n",
      "happen obj impact of rotation on earth\n",
      "sub\n",
      "obj day and night sub\n",
      "sub cause be cause obj\n",
      "earth rotation on its axis sub obj\n",
      "time sub sub by rotation of earth on\n",
      "occur make\n",
      "sub\n",
      "obj cause be cause be result of earth complete rotation on its axis\n",
      "obj sun obj\n",
      "be\n",
      "by earth's rotation\n",
      "sub sub\n",
      "because earth rotate on it's axis obj sequence of day and night on earth\n",
      "earth's rotation\n",
      "result of earth rotate on its axis\n",
      "(b) Knowledge graph for supports\n",
      "Figure 3: Another example of knowledge graphs for paired hypothesis and supports.\n",
      "11<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  19508,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Yuyu Zhang', 'Hanjun Dai', 'Kamil Toraman', 'Le Song']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Incorporating Literals into Knowledge Graph Embeddings\n",
      "AgustinusKristiadi∗4,MohammadAsifKhan∗1,DenisLukovnikov1,JensLehmann1,2,AsjaFischer3\n",
      "1 SDAGroup,UniversityofBonn\n",
      "2 EISDepartment,FraunhoferIAIS\n",
      "3 Ruhr-UniversityBochum\n",
      "4 UniversityofTu¨bingen\n",
      "agustinus.kristiadi@uni-tuebingen.de,s6mokhan@uni-bonn.de,lukovnik@cs.uni-bonn.de,\n",
      "jens.lehmann@uni-bonn.de,jens.lehmann@iais.fraunhofer.de,asja.fischer@rub.de\n",
      "Abstract\n",
      "DoeHigh\n",
      "Knowledge graphs are composed of different el-\n",
      "School\n",
      "ements: entity nodes, relation edges, and literal\n",
      "n\n",
      "t\n",
      "pro\n",
      "eib\n",
      "rd sue ots\n",
      "e\n",
      "n. )vE\n",
      "a\n",
      "ala nuc deh\n",
      "t(\n",
      "hl ei e.t ge\n",
      "r.\n",
      "era btl\n",
      "h\n",
      "yen eo\n",
      "h\n",
      "nd\n",
      "e\n",
      "ce\n",
      "i\n",
      "ogc dho etn sota\n",
      "f\n",
      "inin\n",
      "a\n",
      "fs\n",
      "on\n",
      "ra men\n",
      "n at\n",
      "te\n",
      "i\n",
      "itn oyt ni oty\n",
      "f\n",
      "w’s\n",
      "hty\n",
      "ia cpt he- studiesAt studies\n",
      "At\n",
      "in general cannot be represented by relations be-\n",
      "tween entities alone. However, most of the ex- knows? Jane\n",
      "isting embedding- or latent-feature-based methods John\n",
      "for knowledge graph analysis only consider entity\n",
      "n\n",
      "i tn\n",
      "ho\n",
      "f\n",
      "id soe\n",
      "r\n",
      "ps\n",
      "m\n",
      "aa pan\n",
      "t\n",
      "eid\n",
      "ro\n",
      ",nr we pl ea rt\n",
      "o\n",
      "ei\n",
      "v\n",
      "xo\n",
      "i\n",
      "tn\n",
      "d\n",
      "enee\n",
      "d\n",
      "ddg ebe xys i,\n",
      "sl\n",
      "ta\n",
      "i\n",
      "itn ned grat llh asu teis\n",
      "n\n",
      "ntd too fean aco\n",
      "tc\n",
      "ut\n",
      "o\n",
      "rt eua nk mte\n",
      ".\n",
      "et th\n",
      "I\n",
      "hne\n",
      "-\n",
      "b\n",
      "irth\n",
      "Y\n",
      "e a\n",
      "birthYear\n",
      "r\n",
      "ods for link prediction by a simple portable mod-\n",
      "2001 2000\n",
      "ule for incorporating literals, which we name Lit-\n",
      "eralE.Unlikeinconcurrentmethodswhereliterals\n",
      "Figure1: Literals(box)encodeinformationthatcannotberepre-\n",
      "areincorporatedbyaddingaliteral-dependentterm\n",
      "sentedbyrelationsalone,andareusefulforlinkpredictiontask.For\n",
      "totheoutputofthescoringfunctionandthusonly\n",
      "instance,byconsideringbothbirthYearliteralsandthefactthat\n",
      "indirectlyaffecttheentityembeddings,LiteralEdi- John and Jane both study at Doe High School, we can be\n",
      "rectlyenrichestheseembeddingswithinformation moreconfidentthattherelationknowsbetweenJohnandJane\n",
      "fromliteralsviaalearnableparametrizedfunction. exists.\n",
      "Thisfunctioncanbeeasilyintegratedintothescor-\n",
      "ingfunctionofexistingmethodsandlearnedalong\n",
      "with the entity embeddings in an end-to-end man-\n",
      "DBpedia[Lehmannetal., 2015], Freebase[Bollackeretal.,\n",
      "ner. In an extensive empirical study over three 2008], YAGO3 [Mahdisoltani et al., 2014], and the Google\n",
      "datasets, we evaluate LiteralE-extended versions Knowledge Graph [Dong et al., 2014]. There are differ-\n",
      "of various state-of-the-art latent feature methods ent knowledge representation paradigms for modeling KGs\n",
      "for link prediction and demonstrate that LiteralE suchastheResourceDescriptionFramework(RDF)and(la-\n",
      "presents an effective way to improve their perfor- beled)propertygraphs. Withinthispaper,weconsideraKG\n",
      "mance. Fortheseexperiments,weaugmentedstan- to be a set of triples, where each triple connects an entity\n",
      "darddatasetswiththeirliterals,whichwepublicly (shown as circle in Figure 1) to another entity or a literal\n",
      "provideastestbedsforfurtherresearch. Moreover, (the latter shown as rectangle in Figure 1) via relationships.\n",
      "we show that LiteralE leads to an qualitative im- SuchKGscanberepresentedbytheRDFandpropertygraph\n",
      "provementoftheembeddingsandthatitcanbeeas- paradigms,i.e.themethodspresentedinthispaperareappli-\n",
      "ilyextendedtohandleliteralsfromdifferentmodal- cabletoboth.Togiveaconcreteexample,theKGdepictedin\n",
      "ities. Figure 1 includes the triples (John, Doe High School,\n",
      "studiesAt) and (Jane, 2000, birthYear). The first\n",
      "tripleexpressestherelationshipbetweenanentityandanother\n",
      "1 Introduction entity. Thesecondtripleexpressesarelationshipbetweenan\n",
      "entityandaliteral1.\n",
      "Knowledge graphs (KGs) form the backbone of a range of\n",
      "applications,forinstanceintheareasofsearch,questionan- Knowledge graphs aim to capture factual knowledge\n",
      "swering and data integration. Some well known KGs are\n",
      "1FormoreinformationabouttheRDFconceptsseehttps://\n",
      "∗Equalcontribution www.w3.org/TR/rdf11-concepts\n",
      "9102\n",
      "luJ\n",
      "81\n",
      "]IA.sc[\n",
      "3v43900.2081:viXra\n",
      "within a particular domain. However, they are often incom- model interactions between an embedding of an entity\n",
      "plete since, e.g., more information is provided for popular andallitsliteralvaluesandcanbetrainedend-to-end.\n",
      "than for unknown entities or because the KG is partially or\n",
      "• We evaluate LiteralE on standard link prediction\n",
      "fullygeneratedviaanautomaticextractionprocess. Asare-\n",
      "datasets: FB15k,FB15k-237andYAGO3-10. Weex-\n",
      "sult,KGsrelyheavilyonmethodspredictingunknowntriples\n",
      "tended FB15k and FB15k-237 with literals, in order\n",
      "givenallknowntriples.Thisproblemisusuallyreferredtoas\n",
      "to allow for direct comparison against other methods\n",
      "linkprediction. Thecloselyrelatedproblemofdetectingin-\n",
      "on these standard datasets. We provide these literal-\n",
      "correct triples in KGs is referred to as link correction and is\n",
      "extended versions (augmented with numerical and tex-\n",
      "relevantforimprovingthequalityofaKG.\n",
      "tual literals) and hope they can serve as a testbed for\n",
      "Due to the importance of the problem, many methods for\n",
      "futureresearchontheinclusionofliteralsinKGmodel-\n",
      "link prediction and correction in KGs have been developed.\n",
      "ing.2\n",
      "Thetwomainclassesofthesemethodsaregraphfeatureand\n",
      "latent feature methods [Nickel et al., 2016]. Graph feature • Basedonexperimentalresultsontheextendeddatasets,\n",
      "methodspredicttheexistenceoftriplesbasedonfeaturesdi- weshowthatexploitingtheinformationprovidedbylit-\n",
      "rectly observed in the KG, such as the neighborhood of an erals significantly increases the link prediction perfor-\n",
      "entity and paths to other entities. They are well suited for mance of existing latent feature methods as well as the\n",
      "modelinglocalgraphpatterns. Inlatentfeaturemodels,low- qualityoftheirembeddings.\n",
      "dimensional, latentrepresentations(alsocalledembeddings)\n",
      "Thispaperisorganizedasfollows. InSection2wereview\n",
      "of entities and relations are learned. These embeddings in-\n",
      "severallatentfeaturemethodsforlinkpredictioninKGs. In\n",
      "corporate the KG structure, can capture global patterns, and\n",
      "Section3wepresentLiteralE,ourapproachforincorporating\n",
      "allow to compute the likeliness of a given triple in terms of\n",
      "literals into existing latent feature methods. We give a brief\n",
      "aprobabilityorscorefunction. However, mostoftherecent\n",
      "review of the related literatures and contrast LiteralE with\n",
      "work on latent feature models only takes entities and their\n",
      "othermethodsincorporatingliteralsinSection4. Ourexperi-\n",
      "relations to other entities into account. Therefore, they are\n",
      "mentmethodologyisdescribedinSection5,andinSection6\n",
      "missing the additional information encoded in literals. For\n",
      "wepresentourexperimentresults. Finally, weconcludeour\n",
      "example, Figure 1 shows two entities with both structural\n",
      "paperinSection7.\n",
      "(visiting the same school) as well as literal (birth years) in-\n",
      "Our implementation of the proposed methods and all\n",
      "formation. Tomaximizetheaccuracyofpredictingaknows\n",
      "datasets are publicly available at: https://github.\n",
      "relationbetweentheseentities,structuralandliteralinforma-\n",
      "com/SmartDataAnalytics/LiteralE.\n",
      "tion should be combined as people visiting the same school\n",
      "and having similar age tend to have a higher probability of\n",
      "knowingeachother. 2 Preliminaries\n",
      "Inthispaper,weinvestigatetheadvantageobtainedbyin-\n",
      "Inthefollowingwewilldescribethelinkpredictionproblem\n",
      "corporating the additional information provided by literals\n",
      "moreformallyandgiveabriefoverviewoverwell-knownla-\n",
      "intolatentfeaturemethods. WeintroduceLiteralE,amethod\n",
      "tentfeaturemethods.\n",
      "to enrich entity embeddings with their literal information.\n",
      "Givenanentityembedding,weincorporateitscorresponding\n",
      "2.1 ProblemDescription\n",
      "literalsusingalearnableparametricfunction,whichgetsthe\n",
      "vanillaembeddingandthe entity’sliteralsasinput, andout- Link prediction is defined as the task of deciding whether\n",
      "putsaliteral-enrichedembedding. Thisembeddingcanthen a fact (represented by a triple) is true or false given a KG.\n",
      "replace the vanilla embedding in any latent feature model, Moreformally, letE = {e 1,···,e Ne}bethesetofentities,\n",
      "withoutchangingitsoriginalscoringfunctionandtheresult- R = {r 1,···,r Nr} be the set of relations connecting two\n",
      "ingsystemcanbejointlytrainedwithstochasticgradientde- entities,D = {d 1,···,d Nd}bethesetofrelationsconnect-\n",
      "scent,oranyothergradientbasedalgorithmofchoice,inan ing an entity and a literal, i.e., the data relations, and L be\n",
      "end-to-end manner. Therefore, LiteralE can be seen as an thesetofallliteralvalues. AknowledgegraphG isasubset\n",
      "extensionmodulethatcanbeuniversallycombinedwithany of (E ×E ×R)∪(E ×L×D) representing the facts that\n",
      "existinglatentfeaturemethod. Withinthispaper,wemainly areassumedtohold. Linkpredictioncanbeformulatedbya\n",
      "focus on numerical literals. However, we demonstrate that functionψ :E×E×R→Rmappingeachpossiblefactrep-\n",
      "theprinciplecanbedirectlygeneralizedtootherliteraltypes, resentedbythecorrespondingtriple(e i,e j,r k)∈E×E×R\n",
      "suchastextualandimageinformation,e.g.byprovidinglow- to a score value, where a higher value implies the triple is\n",
      "dimensionalvectorrepresentationofimageortext[Xieetal., morelikelytobetrue.\n",
      "2016;Xuetal.,2016]asanadditionalinputtoLiteralE.\n",
      "Ourcontributionsinthispaperarethreefold: 2.2 LatentFeatureMethods\n",
      "• WeintroduceLiteralE,auniversalapproachtoenrichla- In general, latent feature methods are a class of methods in\n",
      "tentfeaturemethodswithliteralinformationviaalearn- whichlowdimensionalvectorrepresentationsofentitiesand\n",
      "ableparametricfunction. Incontrasttootherlatentfea- relations, called embeddings or latent features, are learned.\n",
      "turemodelsincludingliterals,ourapproachdoesnotre-\n",
      "quirespecificpriorknowledge,doesnotrelyonafixed 2A literal-extended version of YAGO3-10 is provided\n",
      "functiontocombineentityembeddingsandliterals,can by[Pezeshkpouretal.,2017].\n",
      "LetH betheembeddingdimension. Wedefineascorefunc- Triple Embeddings LiteralE ScoreFunc. Score\n",
      "tionf : RH ×RH ×RH → Rthatmapsatripleofembed-\n",
      "li\n",
      "dings(e,e,r )toascoref(e,e,r )thatcorrelateswith\n",
      "i j k i j k\n",
      "the truth value of the triple. In latent feature methods, the ei g\n",
      "scoreofanytriple(e,e,r )∈E×E×Risthendefinedas\n",
      "i j k\n",
      "ψ(e i,e j,r k)d=ef f(e i,e j,r k). rk f y\n",
      "Latent feature methods for link predictions are well stud-\n",
      "ied. These methods follow a score-based approach as de-\n",
      "scribedabovebutmakeuseofdifferentkindofscoringfunc- ej g\n",
      "tions f. In this paper we study the potential benefit of in-\n",
      "corporating numerical literals in three state of the art meth-\n",
      "lj\n",
      "ods: DistMult [Dong et al., 2014], ComplEx [Trouillon et\n",
      "al.,2016],andConvE[Dettmersetal.,2018],whicharede- Figure2: OverviewonhowLiteralEisappliedtothebasescoring\n",
      "functionf.LiteralEtakestheembeddingandthecorrespondinglit-\n",
      "scribed in the following. Note however, that these are just\n",
      "eralsasinput, andcombinesthemviaalearnablefunctiong. The\n",
      "anexemplarychoiceofmethodsandourapproachforincor-\n",
      "outputisajointembeddingwhichisfurtherusedinthescorefunc-\n",
      "poratingliteralscaneasilybeadoptedtootherlatentfeature\n",
      "tionf.\n",
      "methods.\n",
      "The DistMult scoring function is defined as diagonal bi-\n",
      "linearinteractionbetweenthetwoentityembeddingsandthe\n",
      "i-th entity and the k-th data relation exists in the KGs, and\n",
      "relationembeddingcorrespondingtoagiventriple,asfollows\n",
      "zero otherwise. We will refer to the i-th row l of L as the\n",
      "(cid:124) i\n",
      "f DistMult(e i,e j,r k)=(cid:104)e i,e j,r k(cid:105)=e i diag(r k)e j. (1) literal vector of the i-th entity. As an illustration, consider\n",
      "theKGpartdepictedinFigure1andimaginethatthereonly\n",
      "Observe that DistMult is cheap to implement, both in terms\n",
      "exist three data relations in this specific KG: heightCm,\n",
      "ofcomputationalandspacecomplexity.\n",
      "birthYear,andcountryArea. FortheentityJohnwe\n",
      "ComplExcanbeseenasDistMultanalogueinthecomplex\n",
      "will then have the literal vector (0,2001,0) in the particular\n",
      "space. The embedding vectors have two parts: the real part\n",
      "row corresponding to John in matrix L, as John only has\n",
      "Re(e) and Re(r), and the imaginary part Im(e) and Im(r),\n",
      "literalinformationforbirthYear.3\n",
      "respectively. Thescoringfunctionisdefinedas\n",
      "AtthecoreofLiteralEisafunctiong :RH ×RNd →RH\n",
      "f ComplEx(e i,e j,r k)=Re((cid:104)e i,¯e j,r k(cid:105)) that takes an entity’s embedding and a literal vector as in-\n",
      "=(cid:104)Re(e ),Re(e ),Re(r )(cid:105) puts and maps them to a vector of the same dimension as\n",
      "i j k\n",
      "+(cid:104)Im(e ),Im(e ),Re(r )(cid:105) (2) the entity embedding. This vector forms an literal-enriched\n",
      "i j k\n",
      "embedding vector that can replace the original embedding\n",
      "+(cid:104)Re(e ),Im(e ),Im(r )(cid:105)\n",
      "i j k vector in the scoring function of any latent feature model.\n",
      "−(cid:104)Im(e i),Re(e j),Im(r k)(cid:105). For example, in our experiments, we replace every entity\n",
      "embedding e with elit = g(e,l ) in the scoring functions\n",
      "ComplExthushastwicethenumberofparameterscompared i i i i\n",
      "of DistMult and ConvE. For ComplEx, where the embed-\n",
      "toDistMultbutprovidesthebenefitofmodelingasymmetric\n",
      "relationshipsbetter,asdiscussedby[Trouillonetal.,2016]. dings have a real and an imaginary part, we use two sep-\n",
      "arate functions to map Re(e ) and Im(e ) to their literal-\n",
      "ConvEemploysaconvolutionalneuralnetworktoextract i i\n",
      "extended counterparts. Aside of these changes regarding\n",
      "features from entity and relation embeddings. Let h be a\n",
      "nonlinearfunction,ω ∈ Rk×m×n beconvolutionfilters,and the entity embeddings, the score functions are the same as\n",
      "W ∈ Rkmn×H beaweightmatrix. TheConvEscorefunc- described before in eq. (1), eq. (2), and eq. (3). For in-\n",
      "stance,theLiteralE-extendedversionofDistMultisgivenby\n",
      "tionisthendefinedby\n",
      "f (elit,elit,r ).\n",
      "DistMult i j k\n",
      "f (e,e,r )=h(vec(h([e,r ]∗ω))W)e, (3)\n",
      "ConvE i j k i k j Wewillnowdescribethefunctiong indetail. First,since\n",
      "where vec(·) is the vectorization of output of convolutional we would like g to be flexible, we need it to be learnable.\n",
      "filters. By employing deep feature extractors in the form Second, we would like g to be able to decide whether the\n",
      "of nonlinear convolutional layers, ConvE is able to encode additional literal information is useful or not, and adapt ac-\n",
      "more expressive features while remaining highly parameter cordingly,e.g. byincorporatingorignoringthatinformation.\n",
      "efficient. Wethereforetakecuefromthegatingmechanismpresentin\n",
      "RNNs, such as the gated recurrent unit (GRU) [Cho et al.,\n",
      "3 LiteralE 2014],andletgbedefinedby\n",
      "Ourmethodof incorporating literalsintoexistinglatentfea-\n",
      "ture methods, which we call LiteralE, is a simple, modular, g :RH ×RNd →RH\n",
      "and universal extension which can potentially enhance the e,l(cid:55)→z(cid:12)h+(1−z)(cid:12)e, (4)\n",
      "performanceofarbitrarylatentfeaturemethods.\n",
      "LetL ∈ RNe×Nd beamatrix, whereeachentryL\n",
      "ik\n",
      "con-\n",
      "tainsthek-thliteralvalueofthei-thentityifatriplewiththe 3Notethatinpractice,wenormalizetheliteralvalues.\n",
      "where(cid:12)isthepointwisemultiplicationand Table 1: Model complexity in terms of number of parameters of\n",
      "methodsforincorporatingliterals.Wedenotethenumberofparam-\n",
      "z=σ(WT e+WTl+b) etersofbasemodels(e.g.DistMult)withΓ. Furthermore,Z isthe\n",
      "ze zl\n",
      "numberofhiddenunitsinaneuralnetwork(e.g.inLiteralE-MLP\n",
      "h=h(WT[e,l]). (5)\n",
      "h andMTKGNN’sAttributeNetworks).Weleaveoutbiasparameters\n",
      "Note that W\n",
      "h\n",
      "∈ RH+Nd×H, W\n",
      "ze\n",
      "∈ RH×H, W\n",
      "zl\n",
      "∈ forclarity.\n",
      "RNd×H, and b ∈ RH are the parameters of g, σ is the sig-\n",
      "moidfunction, andhisacomponent-wisenonlinearity(e.g. Model NumberofParameters\n",
      "thehyperbolictangent). KBLN Γ+N N\n",
      "r d\n",
      "LiteralE introduces some overhead in the number of pa- MTKGNN Γ+N H +2(2HZ+Z)\n",
      "d\n",
      "rameters compared to the base method. This overhead is\n",
      "equal to the number of parameters of the function g and is LiteralE Γ+2H2+2N dH +H\n",
      "comparedtothatofotherapproachesfortheincorporationof\n",
      "literalsinTable1. Specifically,thereare2H2+2N H +H\n",
      "d KBLRN[Garcia-DuranandNiepert,2017]handlesliterals\n",
      "additionalparameterscorrespondingtothedimensionalityof\n",
      "in a separate function added to the vanilla scoring function\n",
      "W,W,W,andbineq.(5). Thus,withthischoiceofg\n",
      "h ze zl andthusdoesnotincorporateliteralsintotheentityembed-\n",
      "andgivenH,thenumberofadditionalparametersofLiteralE\n",
      "dingsthemselves. Theconstructionoffeaturesfromthenu-\n",
      "growsinO(N ),thatis,lineartothenumberofdatarelations\n",
      "d mericalliteralsisbasedonapriorknowledge: thedifference\n",
      "in the KG. Furthermore, the additional space complexity of\n",
      "betweenthenumericalliteralsofthesubjectandobjectentity\n",
      "LiteralE is in O(N N ) as one needs to store the matrix L.\n",
      "e d isagoodpredictorforagivenrelation.\n",
      "Lastly,theadditionalcomputationalcomplexityofLiteralEis\n",
      "These features then serve as input to a fixed radial basis\n",
      "onlyattributedtothecostofthreematrixmultiplicationand\n",
      "function (RBF), which is added to the score function of the\n",
      "onevectoraddition.\n",
      "base method (DistMult). In contrast, LiteralE incorporates\n",
      "In summary, with our method LiteralE, we propose to re-\n",
      "literal information directly into the entity embeddings4, and\n",
      "placethescorefunctionf (e,e,r )fromthehostmethod\n",
      "X i j k does not use any prior knowledge about the meaning of nu-\n",
      "X withthefunctioncomposition\n",
      "mericalliterals.\n",
      "f (g(e,l ),g(e,l ),r ) MTKGNN [Tay et al., 2017] extends ERMLP [Dong et\n",
      "X i i j j k\n",
      "al., 2014] and incorporates numerical literals by introduc-\n",
      "as illustrated in Figure 2. This new scoring function can be\n",
      "ing an additional learning task, more precisely, the task of\n",
      "trainedbygradientdescentbasedoptimizationusingthesame\n",
      "predicting the literal value for a given entity. This multi-\n",
      "trainingprocedureasbefore.\n",
      "task learning approach of MTKGNN requires an additional\n",
      "attribute-specific training procedure. Therefore, adding an-\n",
      "4 RelatedWork\n",
      "other type or modality of literals is not straightforward and\n",
      "In the last years, several efforts to incorporate literals into costlyasanotherlearningtaskneedstobedevised.Similarto\n",
      "KGembeddingmethodshavebeenmade. [Toutanovaetal., MTKGNN, TransEA [Wu and Wang, 2018] extends TransE\n",
      "2015]and[Tuetal.,2017]makeuseoftextualliteralsofen- by adding a numerical attribute prediction loss to the rela-\n",
      "titiesinadditiontorelationalembeddings. Morespecifically tionalloss.\n",
      "theylearnadditionalentityembeddingsfromtheirtextualde- Lastly, the model recently proposed by [Thoma et al.,\n",
      "scriptionandusetheminanadditiveterminthescorefunc- 2017]canbeseenasaspecialcaseofLiteralEwherethefunc-\n",
      "tion of latent distant methods. [Xie et al., 2016] and [Xu et tionusedinsteadofthefunctiong definedabovetocombine\n",
      "al., 2016] also proposed methods to incorporate textual lit- literals of entities with their entity embeddings is a concate-\n",
      "erals into latent distance methods such as TransE by encod- nationfollowedbysingularvaluedecomposition. Thus,they\n",
      "ingtextualliteralswithrecurrentorconvolutionalneuralnet- useafixedfunctiontocombinetherepresentations,whereas\n",
      "works. [RuobingXie,2017]useimageliteralsintheirmodel LiteralEemploysanadaptablefunctionandisthereforemore\n",
      "by projecting entities’ image features into an entity embed- flexible. Furthermore, theyonlyconsiderimageandtextlit-\n",
      "dingsspace.However,allofthoseapproachesdonotconsider eralsbutnonumericalliterals.\n",
      "numericalliterals.MultiModal[Pezeshkpouretal.,2017]ex-\n",
      "tends DistMult to also predict the likeliness of (subject, re- 5 Experiments\n",
      "lation, literal)-triples, by replacing the object embedding in\n",
      "In the following we will describe the training approach, the\n",
      "standardDistMultbyitsliteralembedding(whereliteralsof\n",
      "datasets, the experimental setup, and the evaluation metrics\n",
      "differentmodalitiesaretakenintoaccount). Bydoingsolit-\n",
      "appliedinourexperiments.\n",
      "erals are incorporated into entity embeddings in an implicit\n",
      "manner. [Sun et al., 2017], proposes to employ literals to 5.1 Training\n",
      "refine the joint embeddings in entity alignment tasks: They\n",
      "We use the same training approach as [Dettmers et al.,\n",
      "useliteralstoclustertogetherentitieswhichhavehighliteral\n",
      "2018] for all the tested methods. That is, for every given\n",
      "correlations,thusonlyindirectlyusetheliteralinformationin\n",
      "theentityembeddings. Incontrasttoalltheaforementioned 4Note,thatincorporatingtheliteralinformationintotheembed-\n",
      "works, LiteralE combines the literals into the entity embed- dingsalsoseemsadvantageousforentitydisambiguationorcluster-\n",
      "dingdirectlyandexplicitlybythefunctiongdefinedabove. ing.\n",
      "Table 2: Number of entities, relations, literals, and triples, for all format), etc. To enrich FB15k and FB15k-237 with these\n",
      "datasetsusedinthispaper.\n",
      "literals,wecreatedaSPARQLendpointforFreebaseandex-\n",
      "tracted literals of all entities contained in FB15k. We fur-\n",
      "Dataset FB15k FB15k-237 YAGO3-10 ther filtered the extracted literals based on their frequency,\n",
      "#Entities(N ) 14,951 14,541 123,182\n",
      "i.eweonlyconsiderdatarelationsd ∈ D thatoccuratleast\n",
      "e\n",
      "in 5 triples in FB15k. We also remove all key and ID rela-\n",
      "#Relations(N ) 1,345 237 37\n",
      "r\n",
      "tionssincetheirvaluesarenotmeaningfulasquantities. For\n",
      "#Datarel.(N ) 121 121 5\n",
      "d\n",
      "#Literals(|L|) 18,741 18,741 111,406 YAGO3-10, weusenumericalliteralsprovidedbyYAGO3-\n",
      "10-plus [Pezeshkpour et al., 2017], which is publicly avail-\n",
      "#Relationaltriples 592,213 310,116 1,089,040\n",
      "able.5 In case an entity has multiple literal values for a par-\n",
      "#Literaltriples 70,257 70,257 111,406\n",
      "ticulardatarelation,wearbitrarilyselectoneofthem. Some\n",
      "statisticsofthedatasetsareprovidedinTable2.\n",
      "triple (e,e,r ) in the KG, we compute the score for\n",
      "i j k\n",
      "(e,e(cid:48),r ),∀e(cid:48) ∈E usingthe(originalorLiteralE-extended) 5.3 ExperimentalSetup\n",
      "i j k j\n",
      "scoringfunctionf,andapplythesigmoidfunctiontothere-\n",
      "WeimplementedLieralEontopofConvE’scodebase,which\n",
      "sultingscore(i.e.p = σ◦f), suchthatitcanbeinterpreted\n",
      "ispubliclyavailable6. Thehyperparametersusedinallofour\n",
      "asprobabilityofexistenceofagiventriple.\n",
      "experimentsacrossalldatasetsare:learningrate0.001,batch\n",
      "Let p ∈ [0,1]Ne be the probability vector, collecting the\n",
      "size128, embeddingsize200, embeddingdropoutprobabil-\n",
      "resultingprobabilitieswithrespecttoalle(cid:48) ∈ E. Themodel\n",
      "j ity 0.2, and label smoothing 0.1. Additionally for ConvE,\n",
      "is then trained by minimizing the binary cross-entropy loss\n",
      "we used feature map dropout with probability 0.2 and pro-\n",
      "between the probability vector p and the vector of ground\n",
      "jection layer dropout with probability 0.3. Note, that these\n",
      "truth labels y ∈ {0,1}Ne indicating the existence of triples\n",
      "hyperparametervaluesarethesameasintheexperimentsof\n",
      "(e,e(cid:48),r ),∀e(cid:48) ∈E intheKG.Thatis,weminimize\n",
      "i j k j [Dettmersetal.,2018].\n",
      "1\n",
      "(cid:88)Ne ExceptforexperimentswithConvE,werunallofourex-\n",
      "L(p,y)=− (y log(p )+(1−y )log(1−p )), perimentsforamaximumof100epochsasweobservedthat\n",
      "x x x x\n",
      "N\n",
      "e thisissufficientforconvergence.ForConvE,weusedatmost\n",
      "x=1\n",
      "(6) 1000epochs, asdescribedintheoriginalpaper[Dettmerset\n",
      "where p x and y x are the predicted probability and the al.,2018]. Weapplyearlystoppinginalloftheexperiments\n",
      "given truth value for the x-th element of our candidate set by monitoring the Mean Reciprocal Rank (MRR) metric on\n",
      "{(e i,e(cid:48) j,r k),e(cid:48)\n",
      "j\n",
      "∈E}. WeuseAdam[KingmaandBa,2015] thevalidationseteverythreeepochs.\n",
      "tooptimizethislossfunction. To validate our approach and to eliminate the effect of\n",
      "Note, the above procedure of considering all triples different environment setups, we re-implemented the re-\n",
      "(e i,e(cid:48) j,r k), ∀e(cid:48)\n",
      "j\n",
      "∈ E if there is any triple (e i,e j,r k) with latedmodels,KBLN[Garcia-DuranandNiepert,2017],and\n",
      "heade iandrelationr kinthetrainingsetisreferredtoas1-N MTKGNN [Tay et al., 2017] as baselines. Note that we did\n",
      "scoring[Dettmersetal.,2018]asforeachtriple,wecompute notre-implementKBLRN[Garcia-DuranandNiepert,2017]\n",
      "scoresofN := N e = |E|triples. Thisisincontrastwith1-1 since the sub-model KBLN (i.e. the KBLRN model without\n",
      "scoring, whereoneprimarilyconsidersthetrainingexample making use of the relational information provided by graph\n",
      "(e i,e j,r k)andappliessomeotherstrategyfornegativesam- feature methods) is directly comparable to LiteralE.7 As in\n",
      "pling(i.e.forthegenerationofnon-existingtriples).Werefer [Dettmersetal.,2018],weusea1-Ntrainingapproach,while\n",
      "the reader to [Dettmers et al., 2018] for a further discussion KBLN and MTKGNN uses a 1-1 approach. Therefore, the\n",
      "regardingthis. RelNetinMTKGNNwhichisaneuralnetworkisinfeasible\n",
      "tobeimplementedinourenvironment. Thus, asopposedto\n",
      "5.2 Datasets\n",
      "neural network, we use DistMult as base model in our re-\n",
      "We use three widely used datasets for evaluating link pre- implementation of an MTKGNN-like method. While this\n",
      "dictionperformance: FB15k,FB15k-237,andYAGO3-10. changedoesnotallowtoevaluatetheperformanceoftheorig-\n",
      "FB15k [Bordes et al., 2013] is a subset of Freebase where inalMTKGNNmodel, itmakesourMTKGNN-likemethod\n",
      "mosttriplesarerelatedtomoviesandsports. Asdiscussedby directlycomparabletotheothermethodsthatweconsiderin\n",
      "[Dettmers et al., 2018], FB15k has a large number of test our experiments, since it uses the same base score function.\n",
      "triples which can simply be obtained by inverting training All in all, due to these differences in the loss function and\n",
      "triples. This results in a biased test set, for which a simple the overall framework which are necessary to make KBLN\n",
      "modelwhichissymmetricwithrespecttoobjectandsubject andMTKGNNcomparabletoLiteralE,theresultswereport\n",
      "entity is capable of achieving excellent results. To address for them might differ from those reported in the respective\n",
      "this problem, FB15k-237 [Toutanova and Chen, 2015] was original papers. In addition, we obtain slightly different re-\n",
      "createdbyremovinginverserelationsfromFB15k. YAGO3- sultscomparedto[Dettmersetal.,2018]forDistMult,Com-\n",
      "10 [Mahdisoltani et al., 2014] is a subset of the YAGO3\n",
      "knowledge graph which mostly consists of triples related to 5https://github.com/pouyapez/multim-kb-embeddings\n",
      "people. 6https://github.com/TimDettmers/ConvE\n",
      "Inthiswork,weonlyconsidernumericalliterals,e.g. lon- 7<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,    605,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['FB15k', 'FB15k-237', 'YAGO3-10']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: withrespecttoobjectandsubject andMTKGNNcomparabletoLiteralE,theresultswereport\n",
      "entity is capable of achieving excellent results. To address for them might differ from those reported in the respective\n",
      "this problem, FB15k-237 [Toutanova and Chen, 2015] was original papers. In addition, we obtain slightly different re-\n",
      "createdbyremovinginverserelationsfromFB15k. YAGO3- sultscomparedto[Dettmersetal.,2018]forDistMult,Com-\n",
      "10 [Mahdisoltani et al., 2014] is a subset of the YAGO3\n",
      "knowledge graph which mostly consists of triples related to 5https://github.com/pouyapez/multim-kb-embeddings\n",
      "people. 6https://github.com/TimDettmers/ConvE\n",
      "Inthiswork,weonlyconsidernumericalliterals,e.g. lon- 7Note,thatLiteralEcouldalsobeextendedtoincorporategraph\n",
      "gitude,latitude,population,age,dateofbirth(inUNIXtime featuresasanadditionalinputtog.\n",
      "Table3: LinkpredictionresultsonFB15k,FB15k-237,andYAGO3-10. Thebestvaluescomparingourimplementationofbasemodels,\n",
      "KBLN,MTKGNNandLiteralEarehighlightedinboldtext.Onlynumericalliteralsareusedintheexperiments.\n",
      "FB15k\n",
      "Models MR MRR Hits@1 Hits@3 Hits@10\n",
      "DistMult 108 0.671 0.589 0.723 0.818\n",
      "ComplEx 127 0.695 0.618 0.744 0.833\n",
      "ConvE 49 0.692 0.596 0.760 0.853\n",
      "KBLN[Garcia-DuranandNiepert,2017] 129 0.739 0.668 0.788 0.859\n",
      "MTKGNN[Tayetal.,2017] 87 0.669 0.586 0.722 0.82\n",
      "DistMult-LiteralE 68 0.676 0.589 0.733 0.825\n",
      "ComplEx-LiteralE 80 0.746 0.686 0.782 0.853\n",
      "ConvE-LiteralE 43 0.733 0.656 0.785 0.863\n",
      "FB15k-237\n",
      "Models MR MRR Hits@1 Hits@3 Hits@10\n",
      "DistMult 633 0.282 0.203 0.309 0.438\n",
      "ComplEx 652 0.290 0.212 0.317 0.445\n",
      "ConvE 297 0.313 0.228 0.344 0.479\n",
      "KBLN[Garcia-DuranandNiepert,2017] 358 0.301 0.215 0.333 0.468\n",
      "MTKGNN[Tayetal.,2017] 532 0.285 0.204 0.312 0.445\n",
      "DistMult-LiteralE 280 0.317 0.232 0.348 0.483\n",
      "ComplEx-LiteralE 357 0.305 0.222 0.336 0.466\n",
      "ConvE-LiteralE 255 0.303 0.219 0.33 0.471\n",
      "YAGO3-10\n",
      "Models MR MRR Hits@1 Hits@3 Hits@10\n",
      "DistMult 2943 0.466 0.377 0.514 0.653\n",
      "ComplEx 3768 0.493 0.411 0.536 0.649\n",
      "ConvE 2141 0.505 0.422 0.554 0.660\n",
      "KBLN[Garcia-DuranandNiepert,2017] 2666 0.487 0.405 0.531 0.642\n",
      "MTKGNN[Tayetal.,2017] 2970 0.481 0.398 0.527 0.634\n",
      "DistMult-LiteralE 1642 0.479 0.4 0.525 0.627\n",
      "ComplEx-LiteralE 2508 0.485 0.412 0.527 0.618\n",
      "ConvE-LiteralE 1037 0.525 0.448 0.572 0.659\n",
      "Table4: ThelinkpredictionperformanceofLiteralEemployinga Table 5: Link prediction results for DistMult-LiteralE on FB15k-\n",
      "simplelineartransformationg. 237,withbothnumericalandtextliterals. “DM”and“L”standfor\n",
      "lin\n",
      "DistMult and LiteralE respectively, while “N” and “T” denote the\n",
      "usageofnumericalandtextliterals,respectively.\n",
      "Datasets Functions MRR Hits@1 Hits@10\n",
      "FB15k DistMult-g 0.583 0.476 0.771\n",
      "lin Models MRR Hits@1 MRRIncr.\n",
      "ComplEx-g 0.765 0.705 0.871\n",
      "lin\n",
      "ConvE-g 0.66 0.556 0.836 DM 0.241 0.155 -\n",
      "lin\n",
      "DM-L(N) 0.317 0.232 0+31.54%\n",
      "FB15k-237 DistMult-g 0.314 0.228 0.483\n",
      "lin DM-L(N+T) 0.32 0.234 +32.78%\n",
      "ComplEx-g 0.299 0.214 0.467\n",
      "lin\n",
      "ConvE-g 0.314 0.228 0.483\n",
      "lin\n",
      "YAGO3-10 DistMult-g 0.504 0.422 0.653 variantbasedonasimple(butstilllearnable)lineartransfor-\n",
      "lin\n",
      "ComplEx-g\n",
      "lin\n",
      "0.509 0.433 0.653 mation, dubbed g lin. That is, g\n",
      "lin\n",
      ": RH × RNd → RH is\n",
      "ConvE-g\n",
      "lin\n",
      "0.506 0.422 0.664 defined by e,l (cid:55)→ WT[e,l], where W ∈ RH+Nd×H is a\n",
      "learnableweightmatrix. TheresultsarepresentedinTable4\n",
      "(cf. Table3).\n",
      "plExandConvEforallthreedatasets(ourresultsaremostly The proposed g leads to better results than g in 5 out\n",
      "lin\n",
      "comparableorslightlybetterandinsomecaseworse). This of 9 experiments. While LiteralE with g provides a consis-\n",
      "could be attributed to the hyperparameter tuning performed tentperformanceimprovementforallbasemodels,DistMult-\n",
      "in[Dettmersetal.,2018].\n",
      "g shows a decreased performance compared to DistMult\n",
      "lin\n",
      "on FB15k. This might be explained by the fact that – as\n",
      "5.4 Evaluation\n",
      "[ToutanovaandChen,2015]alreadyreported–FB15kcon-\n",
      "For the evaluation of the performance of the different meth- tainstriplesinthetestsetthathaveaninverseanalog(i.e.the\n",
      "odsonthelinkpredictiontask,wefollowthestandardsetup tripleresultingfromchangingthepositionofsubjectandob-\n",
      "used in other studies. For each triple (e i,e j,r k) in the test jectentity)inthetrainingset. Thepredictionforsuchtriples\n",
      "set,wegenerateasetofcorruptedtriplesbyeitherreplacing cangetdifficultiftheinversehasadifferentlabel. Sincethe\n",
      "thesubjectentitye i ortheobjectentitye j withanyotheren- vanilla DistMult already has difficulties in modeling asym-\n",
      "titye(cid:48) ∈E. Wefurthercomputethescoresofthesecorrupted metric relations on FB15k, adding literals using a naive g\n",
      "lin\n",
      "triples along with the score of the true triple. To evaluate might only introduce noise, resulting in even lower perfor-\n",
      "themodel,werankalltripleswithrespecttotheirscoresand mance. Ontheotherhand,g leadstobetterresultsthangin\n",
      "lin\n",
      "use the following standard evaluation metrics: Mean Rank combinationwithComplExonFB15k.\n",
      "(MR),MeanReciprocalRank(MRR),Hits@1,Hits@3,and In general, the results show that for performance-\n",
      "Hits@10. maximizationpurpose, itmakessensetoinvestigatetheper-\n",
      "formanceofLiteralE incombinationwithdifferenttransfor-\n",
      "6 Results mation functions. Given the right choice of transformation\n",
      "functionforincorporatingliterals, LiteralEalwaysimproves\n",
      "6.1 LinkPrediction\n",
      "theperformanceofthebasemodel.\n",
      "The results of our experiments for link prediction are sum-\n",
      "marized in Table 3. In general, LiteralE improves the base\n",
      "6.3 ExperimentwithTextLiterals\n",
      "models (DistMult, ComplEx, and ConvE) significantly. For\n",
      "instance,wefoundthatimplementingLiteralEontopofDist- LiteralE, as described in Section 3 can easily be extended\n",
      "MultimprovestheMRRscoreby0.74%,12.41%,and2.7% to other types of literals, e.g. text and images. In this sec-\n",
      "fortheFB15k,FB15k-237,andYAGO3-10dataset,respec- tionthisisbrieflydemonstratedfortextliterals. First, letus\n",
      "tively. We also observed that the improvements brought by assume that text literals are represented by vectors in RNt,\n",
      "LiteralE when combined with ComplEx and ConvE are not i.e. as resulting from document embedding techniques [Le\n",
      "as impressive as for DistMult, which might be attributed to and Mikolov, 2014].8 Subsequently, let us redefine g to be\n",
      "the fact that these base models already achieve higher per- a function mapping RH ×RNd ×RNt to RH. Specifically,\n",
      "formance than DistMult. Compared to other methods that weredefineW h(eq.(5))tobeinRH+Nd+Nt×H andemploy\n",
      "incorporate literals, namely KBLN and MTKGNN, LiteralE an additional gating weight matrix W\n",
      "zt\n",
      "∈ RNt×H to han-\n",
      "achievesacompetitiveorevenbetterperformanceinourex- dletheadditionaltextliteral. Note,thatthissimpleextension\n",
      "periments. Moreover, note that, LiteralE directly and ex- schemecanbeusedtoextendLiteralEtoincorporateliterals\n",
      "plicitlymodifiestheembeddingvectors,whereasKBLNand ofanyothertype(e.g.imageliterals)aslongasthoseliterals\n",
      "MTKGNNdonot. Thus,LiteralEembeddingscouldbemore areencodedasvectorsinRN,forsomeN.\n",
      "useful for tasks other than link prediction. This will be dis- TheresultsforextendingDistMult-LiteralEwiththeenti-\n",
      "cussedfurtherinSection6.4. ties’ text literals (i.e. the entity description) are presented in\n",
      "Table 5. We found that incorporating text literals results in\n",
      "6.2 ComparisontoaSimpleLiteralEBaseline\n",
      "Tovalidateourchoiceofthefunctiong,wecomparetheper- 8WeusespaCy’spretrainedGloVeembeddingmodel. Available\n",
      "formance of LiteralE with the g proposed in Section 3 to its athttps://spacy.io\n",
      "Table6: ComparisonofnearestneighborsofselectedentitiesfromFB15k-237embeddedin(i)DistMult’slatentspace,(ii)KBLN’slatent\n",
      "space,(iii)MTKGNN’slatentspace,(iv)theliteralspace,whereeachentityisrepresentedonlybyitsliterals,and(v)theDisMult-LiteralE’s\n",
      "latentspace.\n",
      "Entity Methods NearestNeighbors\n",
      "NorthAmerica DistMult LatinAmerica,Pyrenees,Americas\n",
      "KBLN HouseofHanover,HouseofStuart,HouseofRomanov\n",
      "MTKGNN LatinAmerica,PanamaCity,Pyrenees\n",
      "Num. lits. only SovietUnion,LatinAmerica,Africa\n",
      "LiteralE Americas,LatinAmerica,Asia\n",
      "Philippines DistMult Peru,Thailand,Kuwait\n",
      "KBLN HouseofRomanov,HouseofHanover,HouseofStuart\n",
      "MTKGNN Thailand,Kuwait,Peru\n",
      "Num. lits. only Peru,Poland,Pakistan\n",
      "LiteralE Thailand,Taiwan,Greece\n",
      "RomanRepublic DistMult RepublicofVenice,IsraelDefenseForce,ByzantineEmpire\n",
      "KBLN RepublicofVenice,Carthage,Retinol\n",
      "MTKGNN RepublicofVenice,Carthage,NorthIsland\n",
      "Num. lits. only Alexandria,Yerevan,Cologne\n",
      "LiteralE RomanEmpire,KingdomofGreece,ByzantineEmpire\n",
      "afurtherincreaseofthelinkpredictionperformanceofDist- AmericaisclosetoPyreneesandRoman Repulicis\n",
      "MultonFB15k-237. close to North Island. This findings demonstrates the\n",
      "advantageofincorporatingliteralsontheembeddinglevel(as\n",
      "6.4 NearestNeighborAnalysis\n",
      "donebyLiterelE)overincorporatingthematthescoreorloss\n",
      "Forafurtherqualitativeinvestigation,wepresentthenearest function(asdonebyKBLNandMTKGNN,respectively).\n",
      "neighbors of some entities in the space of literals, the latent Wheninspectingthenearestneighborhoodofthesameen-\n",
      "space learned by (i) DistMult, (ii) KBLN, (iii) MTKGNN, tities when represented only by their literal vectors, it be-\n",
      "and(iv)DisMult-LiteralEinTable6.9 comesclearthatthesevectorsthemselvesarealreadycontain-\n",
      "In the embedding space of DistMult, geographical en- ingusefulinformationindicatingtheclosenessofsimilaren-\n",
      "tities such as North America and Philippines are tities. Forexample,geographicalentitieshavelongitude\n",
      "close to other entities of the same type. However, these and latitude literals, while city, nation, and empire enti-\n",
      "neighboringentitiesarenotintuitive,e.g.North America ties have date founded and date dissolved literals,\n",
      "is close to Pyrenees, whereas Philippines is close which can explain the closeness of two entities given only\n",
      "to Peru and Kuwait. When we inspected the embed- their literal vectors. Note however, that the nearest neigh-\n",
      "ding space of DistMult-LiteralE that also takes literals in- bours in the literal space do not coincide with and are less\n",
      "formation into account, these nearest neighbors (shown in informative than the nearest neighbours in the LiteralE em-\n",
      "bold font in Table 6) become more intuitive, i.e they con- beddingspace.\n",
      "sist of entities geographically close to each others. Fur- All in all, our observations suggest that integrating the\n",
      "thermore, we found that DistMult-LiteralE’s embeddings literal information into entity embeddings indeed improves\n",
      "showclearqualitativeadvantagecomparedtothatofvanilla their quality, which makes LiteralE embeddings promising\n",
      "DistMult also for entities from other types, e.g. compar- forentityresolutionandclusteringtasks.\n",
      "ing the nearest neighbors of Roman Republic which is\n",
      "of type ‘empire’. In contrast, KBLN’s embeddings tend 7 ConclusionandFutureWork\n",
      "to be close to the embeddings of unrelated entities: both\n",
      "North AmericaandPhilippinesareclosetotheen- Inthispaper,weintroducedLiteralE:asimplemethodtoin-\n",
      "tities House of Romanov, House of Hanover, and corporate literals into latent feature methods for knowledge\n",
      "House of Stuart, while Roman Republic is close graph analysis. It corresponds to a learnable function that\n",
      "to Retinol. Similarly, the embeddings of MTKGNN are mergesentityembeddingswiththeirliteralinformationavail-\n",
      "alsoclosetotheembeddingofunrelatedentities,e.g.,North able in the knowledge graph. The resulting literal-enriched\n",
      "latent features can replace the vanilla entity embedding in\n",
      "9ThebasemodelforallofthesemethodsisDistMult. any latent feature method, without any further modification.\n",
      "Therefore, LiteralE can be seen as an universal extension [Mahdisoltanietal.,2014] Farzaneh Mahdisoltani, Joanna Biega,\n",
      "module. Weshowedthataugmentingvariousstate-of-the-art andFabianSuchanek. Yago3: Aknowledgebasefrommultilin-\n",
      "models (DistMult, ComplEx, and ConvE) with LiteralE sig- gualwikipedias. In7thBiennialConferenceonInnovativeData\n",
      "nificantlyimprovestheirlinkpredictionperformance. More- SystemsResearch.CIDRConference,2014.\n",
      "over, as exemplarily demonstrated for text literals, LiteralE [Nickeletal.,2016] Maximilian Nickel, Kevin Murphy, Volker\n",
      "canbeeasilyextendedothertypesofliterals. Infuturework, Tresp, and Evgeniy Gabrilovich. A review of relational ma-\n",
      "LiteralEshallbefurtherbeextendedtoaccommodateliterals chinelearningforknowledgegraphs. ProceedingsoftheIEEE,\n",
      "from the image domain. This can be achieved by extracting 104(1):11–33,2016.\n",
      "latentrepresentationsfromimages(forexamplewithconvo- [Pezeshkpouretal.,2017] Pouya Pezeshkpour, CA Irvine, Liyan\n",
      "lutional neural networks), and providing them as additional Chen,andSameerSingh.Embeddingmultimodalrelationaldata.\n",
      "inputs to LiteralE for merging them with the vanilla entitiy 2017.\n",
      "embeddings.Furthermore,ourfindingthatLiteralEimproves [RuobingXie,2017] Huanbo Luan Maosong Sun Ruobing Xie,\n",
      "the quality of the entity embeddings makes it a promising ZhiyuanLiu. Image-embodiedknowledgerepresentationlearn-\n",
      "candidateforimprovingothertasksinthefieldofknowledge ing. InProceedingsoftheTwenty-SixthInternationalJointCon-\n",
      "graphanalysis,suchasentityresolutionandknowledgegraph ference on Artificial Intelligence, IJCAI-17, pages 3140–3146,\n",
      "clustering. 2017.\n",
      "[Sunetal.,2017] Zequn Sun, Wei Hu, and Chengkai Li. Cross-\n",
      "References lingual entity alignment via joint attribute-preserving embed-\n",
      "ding.InInternationalSemanticWebConference,pages628–644.\n",
      "[Bollackeretal.,2008] KurtBollacker,ColinEvans,PraveenPari-\n",
      "Springer,2017.\n",
      "tosh,TimSturge,andJamieTaylor. Freebase: acollaboratively\n",
      "createdgraphdatabaseforstructuringhumanknowledge.InPro- [Tayetal.,2017] Yi Tay, Luu Anh Tuan, Minh C Phan, and\n",
      "ceedingsofthe2008ACMSIGMODinternationalconferenceon SiuCheungHui. Multi-taskneuralnetworkfornon-discreteat-\n",
      "Managementofdata,pages1247–1250.AcM,2008. tribute prediction in knowledge graphs. In Proceedings of the\n",
      "2017ACMonConferenceonInformationandKnowledgeMan-\n",
      "[Bordesetal.,2013] Antoine Bordes, Nicolas Usunier, Alberto\n",
      "agement,pages1029–1038.ACM,2017.\n",
      "Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Trans-\n",
      "lating embeddings for modeling multi-relational data. In Ad- [Thomaetal.,2017] SteffenThoma,AchimRettinger,andFabian\n",
      "vances in neural information processing systems, pages 2787– Both. Towardsholisticconceptrepresentations: Embeddingre-\n",
      "2795,2013. lationalknowledge,visualattributes,anddistributionalwordse-\n",
      "mantics. InInternationalSemanticWebConference,pages694–\n",
      "[Choetal.,2014] Kyunghyun Cho, Bart van Merrienboer, Caglar\n",
      "710.Springer,2017.\n",
      "Gulcehre,DzmitryBahdanau,FethiBougares,HolgerSchwenk,\n",
      "andYoshuaBengio. Learningphraserepresentationsusingrnn [ToutanovaandChen,2015] Kristina Toutanova and Danqi Chen.\n",
      "encoder–decoderforstatisticalmachinetranslation. InProceed- Observedversuslatentfeaturesforknowledgebaseandtextin-\n",
      "ings of the 2014 Conference on Empirical Methods in Natural ference. InProceedingsofthe3rdWorkshoponContinuousVec-\n",
      "LanguageProcessing(EMNLP),pages1724–1734,Doha,Qatar, torSpaceModelsandtheirCompositionality,pages57–66,2015.\n",
      "October2014.AssociationforComputationalLinguistics. [Toutanovaetal.,2015] Kristina Toutanova, Danqi Chen, Patrick\n",
      "[Dettmersetal.,2018] Tim Dettmers, Minervini Pasquale, Stene- Pantel,HoifungPoon,PallaviChoudhury,andMichaelGamon.\n",
      "torpPontus,andSebastianRiedel. Convolutional2dknowledge Representing text for joint embedding of text and knowledge\n",
      "graphembeddings. InProceedingsofthe32thAAAIConference bases. InEMNLP,volume15,pages1499–1509,2015.\n",
      "onArtificialIntelligence,February2018. [Trouillonetal.,2016] The´oTrouillon,JohannesWelbl,Sebastian\n",
      "[Dongetal.,2014] Xin Dong, Evgeniy Gabrilovich, Geremy Riedel, E´ricGaussier, andGuillaumeBouchard. Complexem-\n",
      "Heitz,WilkoHorn,NiLao,KevinMurphy,ThomasStrohmann, beddingsforsimplelinkprediction. InInternationalConference\n",
      "Shaohua Sun, and Wei Zhang. Knowledge vault: A web-scale onMachineLearning,pages2071–2080,2016.\n",
      "approach to probabilistic knowledge fusion. In Proceedings of\n",
      "[Tuetal.,2017] CunchaoTu,HanLiu,ZhiyuanLiu,andMaosong\n",
      "the20thACMSIGKDDinternationalconferenceonKnowledge\n",
      "Sun. Cane:Context-awarenetworkembeddingforrelationmod-\n",
      "discoveryanddatamining,pages601–610.ACM,2014.\n",
      "eling. In Proceedings of the 55th Annual Meeting of the Asso-\n",
      "[Garcia-DuranandNiepert,2017] Alberto Garcia-Duran and ciationforComputationalLinguistics(Volume1: LongPapers),\n",
      "Mathias Niepert. KBLRN: End-to-end learning of knowledge volume1,pages1722–1731,2017.\n",
      "base representations with latent, relational, and numerical\n",
      "[WuandWang,2018] YanrongWuandZhichunWang.Knowledge\n",
      "features. arXivpreprintarXiv:1709.04676,2017.\n",
      "graphembeddingwithnumericattributesofentities.InProceed-\n",
      "[KingmaandBa,2015] DiederikPKingmaandJimmyBa. Adam: ingsofTheThirdWorkshoponRepresentationLearningforNLP,\n",
      "Amethodforstochasticoptimization. In3rdInternationalCon- pages132–136,2018.\n",
      "ferenceforLearningRepresentations.ICLR,2015.\n",
      "[Xieetal.,2016] RuobingXie,ZhiyuanLiu,JiaJia,HuanboLuan,\n",
      "[LeandMikolov,2014] QuocLeandTomasMikolov. Distributed andMaosongSun. Representationlearningofknowledgegraphs\n",
      "representations of sentences and documents. In International withentitydescriptions. InAAAI,pages2659–2665,2016.\n",
      "conferenceonmachinelearning,pages1188–1196,2014.\n",
      "[Xuetal.,2016] JiachengXu, KanChen, XipengQiu,andXuan-\n",
      "[Lehmannetal.,2015] Jens Lehmann, Robert Isele, Max Jakob, jingHuang. Knowledgegraphrepresentationwithjointlystruc-\n",
      "AnjaJentzsch,DimitrisKontokostas,PabloNMendes,Sebastian turalandtextualencoding. InIJCAI,2016.\n",
      "Hellmann, Mohamed Morsey, Patrick Van Kleef, So¨ren Auer,\n",
      "et al. DBpedia–a large-scale, multilingual knowledge base ex-\n",
      "tractedfromwikipedia. SemanticWeb,6(2):167–195,2015.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,    605,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['FB15k', 'FB15k-237', 'YAGO3-10']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Incorporating Literals into Knowledge Graph Embeddings\n",
      "AgustinusKristiadi∗4,MohammadAsifKhan∗1,DenisLukovnikov1,JensLehmann1,2,AsjaFischer3\n",
      "1 SDAGroup,UniversityofBonn\n",
      "2 EISDepartment,FraunhoferIAIS\n",
      "3 Ruhr-UniversityBochum\n",
      "4 UniversityofTu¨bingen\n",
      "agustinus.kristiadi@uni-tuebingen.de,s6mokhan@uni-bonn.de,lukovnik@cs.uni-bonn.de,\n",
      "jens.lehmann@uni-bonn.de,jens.lehmann@iais.fraunhofer.de,asja.fischer@rub.de\n",
      "Abstract\n",
      "DoeHigh\n",
      "Knowledge graphs are composed of different el-\n",
      "School\n",
      "ements: entity nodes, relation edges, and literal\n",
      "n\n",
      "t\n",
      "pro\n",
      "eib\n",
      "rd sue ots\n",
      "e\n",
      "n. )vE\n",
      "a\n",
      "ala nuc deh\n",
      "t(\n",
      "hl ei e.t ge\n",
      "r.\n",
      "era btl\n",
      "h\n",
      "yen eo\n",
      "h\n",
      "nd\n",
      "e\n",
      "ce\n",
      "i\n",
      "ogc dho etn sota\n",
      "f\n",
      "inin\n",
      "a\n",
      "fs\n",
      "on\n",
      "ra men\n",
      "n at\n",
      "te\n",
      "i\n",
      "itn oyt ni oty\n",
      "f\n",
      "w’s\n",
      "hty\n",
      "ia cpt he- studiesAt studies\n",
      "At\n",
      "in general cannot be represented by relations be-\n",
      "tween entities alone. However, most of the ex- knows? Jane\n",
      "isting embedding- or latent-feature-based methods John\n",
      "for knowledge graph analysis only consider entity\n",
      "n\n",
      "i tn\n",
      "ho\n",
      "f\n",
      "id soe\n",
      "r\n",
      "ps\n",
      "m\n",
      "aa pan\n",
      "t\n",
      "eid\n",
      "ro\n",
      ",nr we pl ea rt\n",
      "o\n",
      "ei\n",
      "v\n",
      "xo\n",
      "i\n",
      "tn\n",
      "d\n",
      "enee\n",
      "d\n",
      "ddg ebe xys i,\n",
      "sl\n",
      "ta\n",
      "i\n",
      "itn ned grat llh asu teis\n",
      "n\n",
      "ntd too fean aco\n",
      "tc\n",
      "ut\n",
      "o\n",
      "rt eua nk mte\n",
      ".\n",
      "et th\n",
      "I\n",
      "hne\n",
      "-\n",
      "b\n",
      "irth\n",
      "Y\n",
      "e a\n",
      "birthYear\n",
      "r\n",
      "ods for link prediction by a simple portable mod-\n",
      "2001 2000\n",
      "ule for incorporating literals, which we name Lit-\n",
      "eralE.Unlikeinconcurrentmethodswhereliterals\n",
      "Figure1: Literals(box)encodeinformationthatcannotberepre-\n",
      "areincorporatedbyaddingaliteral-dependentterm\n",
      "sentedbyrelationsalone,andareusefulforlinkpredictiontask.For\n",
      "totheoutputofthescoringfunctionandthusonly\n",
      "instance,byconsideringbothbirthYearliteralsandthefactthat\n",
      "indirectlyaffecttheentityembeddings,LiteralEdi- John and Jane both study at Doe High School, we can be\n",
      "rectlyenrichestheseembeddingswithinformation moreconfidentthattherelationknowsbetweenJohnandJane\n",
      "fromliteralsviaalearnableparametrizedfunction. exists.\n",
      "Thisfunctioncanbeeasilyintegratedintothescor-\n",
      "ingfunctionofexistingmethodsandlearnedalong\n",
      "with the entity embeddings in an end-to-end man-\n",
      "DBpedia[Lehmannetal., 2015], Freebase[Bollackeretal.,\n",
      "ner. In an extensive empirical study over three 2008], YAGO3 [Mahdisoltani et al., 2014], and the Google\n",
      "datasets, we evaluate LiteralE-extended versions Knowledge Graph [Dong et al., 2014]. There are differ-\n",
      "of various state-of-the-art latent feature methods ent knowledge representation paradigms for modeling KGs\n",
      "for link prediction and demonstrate that LiteralE suchastheResourceDescriptionFramework(RDF)and(la-\n",
      "presents an effective way to improve their perfor- beled)propertygraphs. Withinthispaper,weconsideraKG\n",
      "mance. Fortheseexperiments,weaugmentedstan- to be a set of triples, where each triple connects an entity\n",
      "darddatasetswiththeirliterals,whichwepublicly (shown as circle in Figure 1) to another entity or a literal\n",
      "provideastestbedsforfurtherresearch. Moreover, (the latter shown as rectangle in Figure 1) via relationships.\n",
      "we show that LiteralE leads to an qualitative im- SuchKGscanberepresentedbytheRDFandpropertygraph\n",
      "provementoftheembeddingsandthatitcanbeeas- paradigms,i.e.themethodspresentedinthispaperareappli-\n",
      "ilyextendedtohandleliteralsfromdifferentmodal- cabletoboth.Togiveaconcreteexample,theKGdepictedin\n",
      "ities. Figure 1 includes the triples (John, Doe High School,\n",
      "studiesAt) and (Jane, 2000, birthYear). The first\n",
      "tripleexpressestherelationshipbetweenanentityandanother\n",
      "1 Introduction entity. Thesecondtripleexpressesarelationshipbetweenan\n",
      "entityandaliteral1.\n",
      "Knowledge graphs (KGs) form the backbone of a range of\n",
      "applications,forinstanceintheareasofsearch,questionan- Knowledge graphs aim to capture factual knowledge\n",
      "swering and data integration. Some well known KGs are\n",
      "1FormoreinformationabouttheRDFconceptsseehttps://\n",
      "∗Equalcontribution www.w3.org/TR/rdf11-concepts\n",
      "9102\n",
      "luJ\n",
      "81\n",
      "]IA.sc[\n",
      "3v43900.2081:viXra\n",
      "within a particular domain. However, they are often incom- model interactions between an embedding of an entity\n",
      "plete since, e.g., more information is provided for popular andallitsliteralvaluesandcanbetrainedend-to-end.\n",
      "than for unknown entities or because the KG is partially or\n",
      "• We evaluate LiteralE on standard link prediction\n",
      "fullygeneratedviaanautomaticextractionprocess. Asare-\n",
      "datasets: FB15k,FB15k-237andYAGO3-10. Weex-\n",
      "sult,KGsrelyheavilyonmethodspredictingunknowntriples\n",
      "tended FB15k and FB15k-237 with literals, in order\n",
      "givenallknowntriples.Thisproblemisusuallyreferredtoas\n",
      "to allow for direct comparison against other methods\n",
      "linkprediction. Thecloselyrelatedproblemofdetectingin-\n",
      "on these standard datasets. We provide these literal-\n",
      "correct triples in KGs is referred to as link correction and is\n",
      "extended versions (augmented with numerical and tex-\n",
      "relevantforimprovingthequalityofaKG.\n",
      "tual literals) and hope they can serve as a testbed for\n",
      "Due to the importance of the problem, many methods for\n",
      "futureresearchontheinclusionofliteralsinKGmodel-\n",
      "link prediction and correction in KGs have been developed.\n",
      "ing.2\n",
      "Thetwomainclassesofthesemethodsaregraphfeatureand\n",
      "latent feature methods [Nickel et al., 2016]. Graph feature • Basedonexperimentalresultsontheextendeddatasets,\n",
      "methodspredicttheexistenceoftriplesbasedonfeaturesdi- weshowthatexploitingtheinformationprovidedbylit-\n",
      "rectly observed in the KG, such as the neighborhood of an erals significantly increases the link prediction perfor-\n",
      "entity and paths to other entities. They are well suited for mance of existing latent feature methods as well as the\n",
      "modelinglocalgraphpatterns. Inlatentfeaturemodels,low- qualityoftheirembeddings.\n",
      "dimensional, latentrepresentations(alsocalledembeddings)\n",
      "Thispaperisorganizedasfollows. InSection2wereview\n",
      "of entities and relations are learned. These embeddings in-\n",
      "severallatentfeaturemethodsforlinkpredictioninKGs. In\n",
      "corporate the KG structure, can capture global patterns, and\n",
      "Section3wepresentLiteralE,ourapproachforincorporating\n",
      "allow to compute the likeliness of a given triple in terms of\n",
      "literals into existing latent feature methods. We give a brief\n",
      "aprobabilityorscorefunction. However, mostoftherecent\n",
      "review of the related literatures and contrast LiteralE with\n",
      "work on latent feature models only takes entities and their\n",
      "othermethodsincorporatingliteralsinSection4. Ourexperi-\n",
      "relations to other entities into account. Therefore, they are\n",
      "mentmethodologyisdescribedinSection5,andinSection6\n",
      "missing the additional information encoded in literals. For\n",
      "wepresentourexperimentresults. Finally, weconcludeour\n",
      "example, Figure 1 shows two entities with both structural\n",
      "paperinSection7.\n",
      "(visiting the same school) as well as literal (birth years) in-\n",
      "Our implementation of the proposed methods and all\n",
      "formation. Tomaximizetheaccuracyofpredictingaknows\n",
      "datasets are publicly available at: https://github.\n",
      "relationbetweentheseentities,structuralandliteralinforma-\n",
      "com/SmartDataAnalytics/LiteralE.\n",
      "tion should be combined as people visiting the same school\n",
      "and having similar age tend to have a higher probability of\n",
      "knowingeachother. 2 Preliminaries\n",
      "Inthispaper,weinvestigatetheadvantageobtainedbyin-\n",
      "Inthefollowingwewilldescribethelinkpredictionproblem\n",
      "corporating the additional information provided by literals\n",
      "moreformallyandgiveabriefoverviewoverwell-knownla-\n",
      "intolatentfeaturemethods. WeintroduceLiteralE,amethod\n",
      "tentfeaturemethods.\n",
      "to enrich entity embeddings with their literal information.\n",
      "Givenanentityembedding,weincorporateitscorresponding\n",
      "2.1 ProblemDescription\n",
      "literalsusingalearnableparametricfunction,whichgetsthe\n",
      "vanillaembeddingandthe entity’sliteralsasinput, andout- Link prediction is defined as the task of deciding whether\n",
      "putsaliteral-enrichedembedding. Thisembeddingcanthen a fact (represented by a triple) is true or false given a KG.\n",
      "replace the vanilla embedding in any latent feature model, Moreformally, letE = {e 1,···,e Ne}bethesetofentities,\n",
      "withoutchangingitsoriginalscoringfunctionandtheresult- R = {r 1,···,r Nr} be the set of relations connecting two\n",
      "ingsystemcanbejointlytrainedwithstochasticgradientde- entities,D = {d 1,···,d Nd}bethesetofrelationsconnect-\n",
      "scent,oranyothergradientbasedalgorithmofchoice,inan ing an entity and a literal, i.e., the data relations, and L be\n",
      "end-to-end manner. Therefore, LiteralE can be seen as an thesetofallliteralvalues. AknowledgegraphG isasubset\n",
      "extensionmodulethatcanbeuniversallycombinedwithany of (E ×E ×R)∪(E ×L×D) representing the facts that\n",
      "existinglatentfeaturemethod. Withinthispaper,wemainly areassumedtohold. Linkpredictioncanbeformulatedbya\n",
      "focus on numerical literals. However, we demonstrate that functionψ :E×E×R→Rmappingeachpossiblefactrep-\n",
      "theprinciplecanbedirectlygeneralizedtootherliteraltypes, resentedbythecorrespondingtriple(e i,e j,r k)∈E×E×R\n",
      "suchastextualandimageinformation,e.g.byprovidinglow- to a score value, where a higher value implies the triple is\n",
      "dimensionalvectorrepresentationofimageortext[Xieetal., morelikelytobetrue.\n",
      "2016;Xuetal.,2016]asanadditionalinputtoLiteralE.\n",
      "Ourcontributionsinthispaperarethreefold: 2.2 LatentFeatureMethods\n",
      "• WeintroduceLiteralE,auniversalapproachtoenrichla- In general, latent feature methods are a class of methods in\n",
      "tentfeaturemethodswithliteralinformationviaalearn- whichlowdimensionalvectorrepresentationsofentitiesand\n",
      "ableparametricfunction. Incontrasttootherlatentfea- relations, called embeddings or latent features, are learned.\n",
      "turemodelsincludingliterals,ourapproachdoesnotre-\n",
      "quirespecificpriorknowledge,doesnotrelyonafixed 2A literal-extended version of YAGO3-10 is provided\n",
      "functiontocombineentityembeddingsandliterals,can by[Pezeshkpouretal.,2017].\n",
      "LetH betheembeddingdimension. Wedefineascorefunc- Triple Embeddings LiteralE ScoreFunc. Score\n",
      "tionf : RH ×RH ×RH → Rthatmapsatripleofembed-\n",
      "li\n",
      "dings(e,e,r )toascoref(e,e,r )thatcorrelateswith\n",
      "i j k i j k\n",
      "the truth value of the triple. In latent feature methods, the ei g\n",
      "scoreofanytriple(e,e,r )∈E×E×Risthendefinedas\n",
      "i j k\n",
      "ψ(e i,e j,r k)d=ef f(e i,e j,r k). rk f y\n",
      "Latent feature methods for link predictions are well stud-\n",
      "ied. These methods follow a score-based approach as de-\n",
      "scribedabovebutmakeuseofdifferentkindofscoringfunc- ej g\n",
      "tions f. In this paper we study the potential benefit of in-\n",
      "corporating numerical literals in three state of the art meth-\n",
      "lj\n",
      "ods: DistMult [Dong et al., 2014], ComplEx [Trouillon et\n",
      "al.,2016],andConvE[Dettmersetal.,2018],whicharede- Figure2: OverviewonhowLiteralEisappliedtothebasescoring\n",
      "functionf.LiteralEtakestheembeddingandthecorrespondinglit-\n",
      "scribed in the following. Note however, that these are just\n",
      "eralsasinput, andcombinesthemviaalearnablefunctiong. The\n",
      "anexemplarychoiceofmethodsandourapproachforincor-\n",
      "outputisajointembeddingwhichisfurtherusedinthescorefunc-\n",
      "poratingliteralscaneasilybeadoptedtootherlatentfeature\n",
      "tionf.\n",
      "methods.\n",
      "The DistMult scoring function is defined as diagonal bi-\n",
      "linearinteractionbetweenthetwoentityembeddingsandthe\n",
      "i-th entity and the k-th data relation exists in the KGs, and\n",
      "relationembeddingcorrespondingtoagiventriple,asfollows\n",
      "zero otherwise. We will refer to the i-th row l of L as the\n",
      "(cid:124) i\n",
      "f DistMult(e i,e j,r k)=(cid:104)e i,e j,r k(cid:105)=e i diag(r k)e j. (1) literal vector of the i-th entity. As an illustration, consider\n",
      "theKGpartdepictedinFigure1andimaginethatthereonly\n",
      "Observe that DistMult is cheap to implement, both in terms\n",
      "exist three data relations in this specific KG: heightCm,\n",
      "ofcomputationalandspacecomplexity.\n",
      "birthYear,andcountryArea. FortheentityJohnwe\n",
      "ComplExcanbeseenasDistMultanalogueinthecomplex\n",
      "will then have the literal vector (0,2001,0) in the particular\n",
      "space. The embedding vectors have two parts: the real part\n",
      "row corresponding to John in matrix L, as John only has\n",
      "Re(e) and Re(r), and the imaginary part Im(e) and Im(r),\n",
      "literalinformationforbirthYear.3\n",
      "respectively. Thescoringfunctionisdefinedas\n",
      "AtthecoreofLiteralEisafunctiong :RH ×RNd →RH\n",
      "f ComplEx(e i,e j,r k)=Re((cid:104)e i,¯e j,r k(cid:105)) that takes an entity’s embedding and a literal vector as in-\n",
      "=(cid:104)Re(e ),Re(e ),Re(r )(cid:105) puts and maps them to a vector of the same dimension as\n",
      "i j k\n",
      "+(cid:104)Im(e ),Im(e ),Re(r )(cid:105) (2) the entity embedding. This vector forms an literal-enriched\n",
      "i j k\n",
      "embedding vector that can replace the original embedding\n",
      "+(cid:104)Re(e ),Im(e ),Im(r )(cid:105)\n",
      "i j k vector in the scoring function of any latent feature model.\n",
      "−(cid:104)Im(e i),Re(e j),Im(r k)(cid:105). For example, in our experiments, we replace every entity\n",
      "embedding e with elit = g(e,l ) in the scoring functions\n",
      "ComplExthushastwicethenumberofparameterscompared i i i i\n",
      "of DistMult and ConvE. For ComplEx, where the embed-\n",
      "toDistMultbutprovidesthebenefitofmodelingasymmetric\n",
      "relationshipsbetter,asdiscussedby[Trouillonetal.,2016]. dings have a real and an imaginary part, we use two sep-\n",
      "arate functions to map Re(e ) and Im(e ) to their literal-\n",
      "ConvEemploysaconvolutionalneuralnetworktoextract i i\n",
      "extended counterparts. Aside of these changes regarding\n",
      "features from entity and relation embeddings. Let h be a\n",
      "nonlinearfunction,ω ∈ Rk×m×n beconvolutionfilters,and the entity embeddings, the score functions are the same as\n",
      "W ∈ Rkmn×H beaweightmatrix. TheConvEscorefunc- described before in eq. (1), eq. (2), and eq. (3). For in-\n",
      "stance,theLiteralE-extendedversionofDistMultisgivenby\n",
      "tionisthendefinedby\n",
      "f (elit,elit,r ).\n",
      "DistMult i j k\n",
      "f (e,e,r )=h(vec(h([e,r ]∗ω))W)e, (3)\n",
      "ConvE i j k i k j Wewillnowdescribethefunctiong indetail. First,since\n",
      "where vec(·) is the vectorization of output of convolutional we would like g to be flexible, we need it to be learnable.\n",
      "filters. By employing deep feature extractors in the form Second, we would like g to be able to decide whether the\n",
      "of nonlinear convolutional layers, ConvE is able to encode additional literal information is useful or not, and adapt ac-\n",
      "more expressive features while remaining highly parameter cordingly,e.g. byincorporatingorignoringthatinformation.\n",
      "efficient. Wethereforetakecuefromthegatingmechanismpresentin\n",
      "RNNs, such as the gated recurrent unit (GRU) [Cho et al.,\n",
      "3 LiteralE 2014],andletgbedefinedby\n",
      "Ourmethodof incorporating literalsintoexistinglatentfea-\n",
      "ture methods, which we call LiteralE, is a simple, modular, g :RH ×RNd →RH\n",
      "and universal extension which can potentially enhance the e,l(cid:55)→z(cid:12)h+(1−z)(cid:12)e, (4)\n",
      "performanceofarbitrarylatentfeaturemethods.\n",
      "LetL ∈ RNe×Nd beamatrix, whereeachentryL\n",
      "ik\n",
      "con-\n",
      "tainsthek-thliteralvalueofthei-thentityifatriplewiththe 3Notethatinpractice,wenormalizetheliteralvalues.\n",
      "where(cid:12)isthepointwisemultiplicationand Table 1: Model complexity in terms of number of parameters of\n",
      "methodsforincorporatingliterals.Wedenotethenumberofparam-\n",
      "z=σ(WT e+WTl+b) etersofbasemodels(e.g.DistMult)withΓ. Furthermore,Z isthe\n",
      "ze zl\n",
      "numberofhiddenunitsinaneuralnetwork(e.g.inLiteralE-MLP\n",
      "h=h(WT[e,l]). (5)\n",
      "h andMTKGNN’sAttributeNetworks).Weleaveoutbiasparameters\n",
      "Note that W\n",
      "h\n",
      "∈ RH+Nd×H, W\n",
      "ze\n",
      "∈ RH×H, W\n",
      "zl\n",
      "∈ forclarity.\n",
      "RNd×H, and b ∈ RH are the parameters of g, σ is the sig-\n",
      "moidfunction, andhisacomponent-wisenonlinearity(e.g. Model NumberofParameters\n",
      "thehyperbolictangent). KBLN Γ+N N\n",
      "r d\n",
      "LiteralE introduces some overhead in the number of pa- MTKGNN Γ+N H +2(2HZ+Z)\n",
      "d\n",
      "rameters compared to the base method. This overhead is\n",
      "equal to the number of parameters of the function g and is LiteralE Γ+2H2+2N dH +H\n",
      "comparedtothatofotherapproachesfortheincorporationof\n",
      "literalsinTable1. Specifically,thereare2H2+2N H +H\n",
      "d KBLRN[Garcia-DuranandNiepert,2017]handlesliterals\n",
      "additionalparameterscorrespondingtothedimensionalityof\n",
      "in a separate function added to the vanilla scoring function\n",
      "W,W,W,andbineq.(5). Thus,withthischoiceofg\n",
      "h ze zl andthusdoesnotincorporateliteralsintotheentityembed-\n",
      "andgivenH,thenumberofadditionalparametersofLiteralE\n",
      "dingsthemselves. Theconstructionoffeaturesfromthenu-\n",
      "growsinO(N ),thatis,lineartothenumberofdatarelations\n",
      "d mericalliteralsisbasedonapriorknowledge: thedifference\n",
      "in the KG. Furthermore, the additional space complexity of\n",
      "betweenthenumericalliteralsofthesubjectandobjectentity\n",
      "LiteralE is in O(N N ) as one needs to store the matrix L.\n",
      "e d isagoodpredictorforagivenrelation.\n",
      "Lastly,theadditionalcomputationalcomplexityofLiteralEis\n",
      "These features then serve as input to a fixed radial basis\n",
      "onlyattributedtothecostofthreematrixmultiplicationand\n",
      "function (RBF), which is added to the score function of the\n",
      "onevectoraddition.\n",
      "base method (DistMult). In contrast, LiteralE incorporates\n",
      "In summary, with our method LiteralE, we propose to re-\n",
      "literal information directly into the entity embeddings4, and\n",
      "placethescorefunctionf (e,e,r )fromthehostmethod\n",
      "X i j k does not use any prior knowledge about the meaning of nu-\n",
      "X withthefunctioncomposition\n",
      "mericalliterals.\n",
      "f (g(e,l ),g(e,l ),r ) MTKGNN [Tay et al., 2017] extends ERMLP [Dong et\n",
      "X i i j j k\n",
      "al., 2014] and incorporates numerical literals by introduc-\n",
      "as illustrated in Figure 2. This new scoring function can be\n",
      "ing an additional learning task, more precisely, the task of\n",
      "trainedbygradientdescentbasedoptimizationusingthesame\n",
      "predicting the literal value for a given entity. This multi-\n",
      "trainingprocedureasbefore.\n",
      "task learning approach of MTKGNN requires an additional\n",
      "attribute-specific training procedure. Therefore, adding an-\n",
      "4 RelatedWork\n",
      "other type or modality of literals is not straightforward and\n",
      "In the last years, several efforts to incorporate literals into costlyasanotherlearningtaskneedstobedevised.Similarto\n",
      "KGembeddingmethodshavebeenmade. [Toutanovaetal., MTKGNN, TransEA [Wu and Wang, 2018] extends TransE\n",
      "2015]and[Tuetal.,2017]makeuseoftextualliteralsofen- by adding a numerical attribute prediction loss to the rela-\n",
      "titiesinadditiontorelationalembeddings. Morespecifically tionalloss.\n",
      "theylearnadditionalentityembeddingsfromtheirtextualde- Lastly, the model recently proposed by [Thoma et al.,\n",
      "scriptionandusetheminanadditiveterminthescorefunc- 2017]canbeseenasaspecialcaseofLiteralEwherethefunc-\n",
      "tion of latent distant methods. [Xie et al., 2016] and [Xu et tionusedinsteadofthefunctiong definedabovetocombine\n",
      "al., 2016] also proposed methods to incorporate textual lit- literals of entities with their entity embeddings is a concate-\n",
      "erals into latent distance methods such as TransE by encod- nationfollowedbysingularvaluedecomposition. Thus,they\n",
      "ingtextualliteralswithrecurrentorconvolutionalneuralnet- useafixedfunctiontocombinetherepresentations,whereas\n",
      "works. [RuobingXie,2017]useimageliteralsintheirmodel LiteralEemploysanadaptablefunctionandisthereforemore\n",
      "by projecting entities’ image features into an entity embed- flexible. Furthermore, theyonlyconsiderimageandtextlit-\n",
      "dingsspace.However,allofthoseapproachesdonotconsider eralsbutnonumericalliterals.\n",
      "numericalliterals.MultiModal[Pezeshkpouretal.,2017]ex-\n",
      "tends DistMult to also predict the likeliness of (subject, re- 5 Experiments\n",
      "lation, literal)-triples, by replacing the object embedding in\n",
      "In the following we will describe the training approach, the\n",
      "standardDistMultbyitsliteralembedding(whereliteralsof\n",
      "datasets, the experimental setup, and the evaluation metrics\n",
      "differentmodalitiesaretakenintoaccount). Bydoingsolit-\n",
      "appliedinourexperiments.\n",
      "erals are incorporated into entity embeddings in an implicit\n",
      "manner. [Sun et al., 2017], proposes to employ literals to 5.1 Training\n",
      "refine the joint embeddings in entity alignment tasks: They\n",
      "We use the same training approach as [Dettmers et al.,\n",
      "useliteralstoclustertogetherentitieswhichhavehighliteral\n",
      "2018] for all the tested methods. That is, for every given\n",
      "correlations,thusonlyindirectlyusetheliteralinformationin\n",
      "theentityembeddings. Incontrasttoalltheaforementioned 4Note,thatincorporatingtheliteralinformationintotheembed-\n",
      "works, LiteralE combines the literals into the entity embed- dingsalsoseemsadvantageousforentitydisambiguationorcluster-\n",
      "dingdirectlyandexplicitlybythefunctiongdefinedabove. ing.\n",
      "Table 2: Number of entities, relations, literals, and triples, for all format), etc. To enrich FB15k and FB15k-237 with these\n",
      "datasetsusedinthispaper.\n",
      "literals,wecreatedaSPARQLendpointforFreebaseandex-\n",
      "tracted literals of all entities contained in FB15k. We fur-\n",
      "Dataset FB15k FB15k-237 YAGO3-10 ther filtered the extracted literals based on their frequency,\n",
      "#Entities(N ) 14,951 14,541 123,182\n",
      "i.eweonlyconsiderdatarelationsd ∈ D thatoccuratleast\n",
      "e\n",
      "in 5 triples in FB15k. We also remove all key and ID rela-\n",
      "#Relations(N ) 1,345 237 37\n",
      "r\n",
      "tionssincetheirvaluesarenotmeaningfulasquantities. For\n",
      "#Datarel.(N ) 121 121 5\n",
      "d\n",
      "#Literals(|L|) 18,741 18,741 111,406 YAGO3-10, weusenumericalliteralsprovidedbyYAGO3-\n",
      "10-plus [Pezeshkpour et al., 2017], which is publicly avail-\n",
      "#Relationaltriples 592,213 310,116 1,089,040\n",
      "able.5 In case an entity has multiple literal values for a par-\n",
      "#Literaltriples 70,257 70,257 111,406\n",
      "ticulardatarelation,wearbitrarilyselectoneofthem. Some\n",
      "statisticsofthedatasetsareprovidedinTable2.\n",
      "triple (e,e,r ) in the KG, we compute the score for\n",
      "i j k\n",
      "(e,e(cid:48),r ),∀e(cid:48) ∈E usingthe(originalorLiteralE-extended) 5.3 ExperimentalSetup\n",
      "i j k j\n",
      "scoringfunctionf,andapplythesigmoidfunctiontothere-\n",
      "WeimplementedLieralEontopofConvE’scodebase,which\n",
      "sultingscore(i.e.p = σ◦f), suchthatitcanbeinterpreted\n",
      "ispubliclyavailable6. Thehyperparametersusedinallofour\n",
      "asprobabilityofexistenceofagiventriple.\n",
      "experimentsacrossalldatasetsare:learningrate0.001,batch\n",
      "Let p ∈ [0,1]Ne be the probability vector, collecting the\n",
      "size128, embeddingsize200, embeddingdropoutprobabil-\n",
      "resultingprobabilitieswithrespecttoalle(cid:48) ∈ E. Themodel\n",
      "j ity 0.2, and label smoothing 0.1. Additionally for ConvE,\n",
      "is then trained by minimizing the binary cross-entropy loss\n",
      "we used feature map dropout with probability 0.2 and pro-\n",
      "between the probability vector p and the vector of ground\n",
      "jection layer dropout with probability 0.3. Note, that these\n",
      "truth labels y ∈ {0,1}Ne indicating the existence of triples\n",
      "hyperparametervaluesarethesameasintheexperimentsof\n",
      "(e,e(cid:48),r ),∀e(cid:48) ∈E intheKG.Thatis,weminimize\n",
      "i j k j [Dettmersetal.,2018].\n",
      "1\n",
      "(cid:88)Ne ExceptforexperimentswithConvE,werunallofourex-\n",
      "L(p,y)=− (y log(p )+(1−y )log(1−p )), perimentsforamaximumof100epochsasweobservedthat\n",
      "x x x x\n",
      "N\n",
      "e thisissufficientforconvergence.ForConvE,weusedatmost\n",
      "x=1\n",
      "(6) 1000epochs, asdescribedintheoriginalpaper[Dettmerset\n",
      "where p x and y x are the predicted probability and the al.,2018]. Weapplyearlystoppinginalloftheexperiments\n",
      "given truth value for the x-th element of our candidate set by monitoring the Mean Reciprocal Rank (MRR) metric on\n",
      "{(e i,e(cid:48) j,r k),e(cid:48)\n",
      "j\n",
      "∈E}. WeuseAdam[KingmaandBa,2015] thevalidationseteverythreeepochs.\n",
      "tooptimizethislossfunction. To validate our approach and to eliminate the effect of\n",
      "Note, the above procedure of considering all triples different environment setups, we re-implemented the re-\n",
      "(e i,e(cid:48) j,r k), ∀e(cid:48)\n",
      "j\n",
      "∈ E if there is any triple (e i,e j,r k) with latedmodels,KBLN[Garcia-DuranandNiepert,2017],and\n",
      "heade iandrelationr kinthetrainingsetisreferredtoas1-N MTKGNN [Tay et al., 2017] as baselines. Note that we did\n",
      "scoring[Dettmersetal.,2018]asforeachtriple,wecompute notre-implementKBLRN[Garcia-DuranandNiepert,2017]\n",
      "scoresofN := N e = |E|triples. Thisisincontrastwith1-1 since the sub-model KBLN (i.e. the KBLRN model without\n",
      "scoring, whereoneprimarilyconsidersthetrainingexample making use of the relational information provided by graph\n",
      "(e i,e j,r k)andappliessomeotherstrategyfornegativesam- feature methods) is directly comparable to LiteralE.7 As in\n",
      "pling(i.e.forthegenerationofnon-existingtriples).Werefer [Dettmersetal.,2018],weusea1-Ntrainingapproach,while\n",
      "the reader to [Dettmers et al., 2018] for a further discussion KBLN and MTKGNN uses a 1-1 approach. Therefore, the\n",
      "regardingthis. RelNetinMTKGNNwhichisaneuralnetworkisinfeasible\n",
      "tobeimplementedinourenvironment. Thus, asopposedto\n",
      "5.2 Datasets\n",
      "neural network, we use DistMult as base model in our re-\n",
      "We use three widely used datasets for evaluating link pre- implementation of an MTKGNN-like method. While this\n",
      "dictionperformance: FB15k,FB15k-237,andYAGO3-10. changedoesnotallowtoevaluatetheperformanceoftheorig-\n",
      "FB15k [Bordes et al., 2013] is a subset of Freebase where inalMTKGNNmodel, itmakesourMTKGNN-likemethod\n",
      "mosttriplesarerelatedtomoviesandsports. Asdiscussedby directlycomparabletotheothermethodsthatweconsiderin\n",
      "[Dettmers et al., 2018], FB15k has a large number of test our experiments, since it uses the same base score function.\n",
      "triples which can simply be obtained by inverting training All in all, due to these differences in the loss function and\n",
      "triples. This results in a biased test set, for which a simple the overall framework which are necessary to make KBLN\n",
      "modelwhichissymmetricwithrespecttoobjectandsubject andMTKGNNcomparabletoLiteralE,theresultswereport\n",
      "entity is capable of achieving excellent results. To address for them might differ from those reported in the respective\n",
      "this problem, FB15k-237 [Toutanova and Chen, 2015] was original papers. In addition, we obtain slightly different re-\n",
      "createdbyremovinginverserelationsfromFB15k. YAGO3- sultscomparedto[Dettmersetal.,2018]forDistMult,Com-\n",
      "10 [Mahdisoltani et al., 2014] is a subset of the YAGO3\n",
      "knowledge graph which mostly consists of triples related to 5https://github.com/pouyapez/multim-kb-embeddings\n",
      "people. 6https://github.com/TimDettmers/ConvE\n",
      "Inthiswork,weonlyconsidernumericalliterals,e.g. lon- 7<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  20212,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Link prediction']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: withrespecttoobjectandsubject andMTKGNNcomparabletoLiteralE,theresultswereport\n",
      "entity is capable of achieving excellent results. To address for them might differ from those reported in the respective\n",
      "this problem, FB15k-237 [Toutanova and Chen, 2015] was original papers. In addition, we obtain slightly different re-\n",
      "createdbyremovinginverserelationsfromFB15k. YAGO3- sultscomparedto[Dettmersetal.,2018]forDistMult,Com-\n",
      "10 [Mahdisoltani et al., 2014] is a subset of the YAGO3\n",
      "knowledge graph which mostly consists of triples related to 5https://github.com/pouyapez/multim-kb-embeddings\n",
      "people. 6https://github.com/TimDettmers/ConvE\n",
      "Inthiswork,weonlyconsidernumericalliterals,e.g. lon- 7Note,thatLiteralEcouldalsobeextendedtoincorporategraph\n",
      "gitude,latitude,population,age,dateofbirth(inUNIXtime featuresasanadditionalinputtog.\n",
      "Table3: LinkpredictionresultsonFB15k,FB15k-237,andYAGO3-10. Thebestvaluescomparingourimplementationofbasemodels,\n",
      "KBLN,MTKGNNandLiteralEarehighlightedinboldtext.Onlynumericalliteralsareusedintheexperiments.\n",
      "FB15k\n",
      "Models MR MRR Hits@1 Hits@3 Hits@10\n",
      "DistMult 108 0.671 0.589 0.723 0.818\n",
      "ComplEx 127 0.695 0.618 0.744 0.833\n",
      "ConvE 49 0.692 0.596 0.760 0.853\n",
      "KBLN[Garcia-DuranandNiepert,2017] 129 0.739 0.668 0.788 0.859\n",
      "MTKGNN[Tayetal.,2017] 87 0.669 0.586 0.722 0.82\n",
      "DistMult-LiteralE 68 0.676 0.589 0.733 0.825\n",
      "ComplEx-LiteralE 80 0.746 0.686 0.782 0.853\n",
      "ConvE-LiteralE 43 0.733 0.656 0.785 0.863\n",
      "FB15k-237\n",
      "Models MR MRR Hits@1 Hits@3 Hits@10\n",
      "DistMult 633 0.282 0.203 0.309 0.438\n",
      "ComplEx 652 0.290 0.212 0.317 0.445\n",
      "ConvE 297 0.313 0.228 0.344 0.479\n",
      "KBLN[Garcia-DuranandNiepert,2017] 358 0.301 0.215 0.333 0.468\n",
      "MTKGNN[Tayetal.,2017] 532 0.285 0.204 0.312 0.445\n",
      "DistMult-LiteralE 280 0.317 0.232 0.348 0.483\n",
      "ComplEx-LiteralE 357 0.305 0.222 0.336 0.466\n",
      "ConvE-LiteralE 255 0.303 0.219 0.33 0.471\n",
      "YAGO3-10\n",
      "Models MR MRR Hits@1 Hits@3 Hits@10\n",
      "DistMult 2943 0.466 0.377 0.514 0.653\n",
      "ComplEx 3768 0.493 0.411 0.536 0.649\n",
      "ConvE 2141 0.505 0.422 0.554 0.660\n",
      "KBLN[Garcia-DuranandNiepert,2017] 2666 0.487 0.405 0.531 0.642\n",
      "MTKGNN[Tayetal.,2017] 2970 0.481 0.398 0.527 0.634\n",
      "DistMult-LiteralE 1642 0.479 0.4 0.525 0.627\n",
      "ComplEx-LiteralE 2508 0.485 0.412 0.527 0.618\n",
      "ConvE-LiteralE 1037 0.525 0.448 0.572 0.659\n",
      "Table4: ThelinkpredictionperformanceofLiteralEemployinga Table 5: Link prediction results for DistMult-LiteralE on FB15k-\n",
      "simplelineartransformationg. 237,withbothnumericalandtextliterals. “DM”and“L”standfor\n",
      "lin\n",
      "DistMult and LiteralE respectively, while “N” and “T” denote the\n",
      "usageofnumericalandtextliterals,respectively.\n",
      "Datasets Functions MRR Hits@1 Hits@10\n",
      "FB15k DistMult-g 0.583 0.476 0.771\n",
      "lin Models MRR Hits@1 MRRIncr.\n",
      "ComplEx-g 0.765 0.705 0.871\n",
      "lin\n",
      "ConvE-g 0.66 0.556 0.836 DM 0.241 0.155 -\n",
      "lin\n",
      "DM-L(N) 0.317 0.232 0+31.54%\n",
      "FB15k-237 DistMult-g 0.314 0.228 0.483\n",
      "lin DM-L(N+T) 0.32 0.234 +32.78%\n",
      "ComplEx-g 0.299 0.214 0.467\n",
      "lin\n",
      "ConvE-g 0.314 0.228 0.483\n",
      "lin\n",
      "YAGO3-10 DistMult-g 0.504 0.422 0.653 variantbasedonasimple(butstilllearnable)lineartransfor-\n",
      "lin\n",
      "ComplEx-g\n",
      "lin\n",
      "0.509 0.433 0.653 mation, dubbed g lin. That is, g\n",
      "lin\n",
      ": RH × RNd → RH is\n",
      "ConvE-g\n",
      "lin\n",
      "0.506 0.422 0.664 defined by e,l (cid:55)→ WT[e,l], where W ∈ RH+Nd×H is a\n",
      "learnableweightmatrix. TheresultsarepresentedinTable4\n",
      "(cf. Table3).\n",
      "plExandConvEforallthreedatasets(ourresultsaremostly The proposed g leads to better results than g in 5 out\n",
      "lin\n",
      "comparableorslightlybetterandinsomecaseworse). This of 9 experiments. While LiteralE with g provides a consis-\n",
      "could be attributed to the hyperparameter tuning performed tentperformanceimprovementforallbasemodels,DistMult-\n",
      "in[Dettmersetal.,2018].\n",
      "g shows a decreased performance compared to DistMult\n",
      "lin\n",
      "on FB15k. This might be explained by the fact that – as\n",
      "5.4 Evaluation\n",
      "[ToutanovaandChen,2015]alreadyreported–FB15kcon-\n",
      "For the evaluation of the performance of the different meth- tainstriplesinthetestsetthathaveaninverseanalog(i.e.the\n",
      "odsonthelinkpredictiontask,wefollowthestandardsetup tripleresultingfromchangingthepositionofsubjectandob-\n",
      "used in other studies. For each triple (e i,e j,r k) in the test jectentity)inthetrainingset. Thepredictionforsuchtriples\n",
      "set,wegenerateasetofcorruptedtriplesbyeitherreplacing cangetdifficultiftheinversehasadifferentlabel. Sincethe\n",
      "thesubjectentitye i ortheobjectentitye j withanyotheren- vanilla DistMult already has difficulties in modeling asym-\n",
      "titye(cid:48) ∈E. Wefurthercomputethescoresofthesecorrupted metric relations on FB15k, adding literals using a naive g\n",
      "lin\n",
      "triples along with the score of the true triple. To evaluate might only introduce noise, resulting in even lower perfor-\n",
      "themodel,werankalltripleswithrespecttotheirscoresand mance. Ontheotherhand,g leadstobetterresultsthangin\n",
      "lin\n",
      "use the following standard evaluation metrics: Mean Rank combinationwithComplExonFB15k.\n",
      "(MR),MeanReciprocalRank(MRR),Hits@1,Hits@3,and In general, the results show that for performance-\n",
      "Hits@10. maximizationpurpose, itmakessensetoinvestigatetheper-\n",
      "formanceofLiteralE incombinationwithdifferenttransfor-\n",
      "6 Results mation functions. Given the right choice of transformation\n",
      "functionforincorporatingliterals, LiteralEalwaysimproves\n",
      "6.1 LinkPrediction\n",
      "theperformanceofthebasemodel.\n",
      "The results of our experiments for link prediction are sum-\n",
      "marized in Table 3. In general, LiteralE improves the base\n",
      "6.3 ExperimentwithTextLiterals\n",
      "models (DistMult, ComplEx, and ConvE) significantly. For\n",
      "instance,wefoundthatimplementingLiteralEontopofDist- LiteralE, as described in Section 3 can easily be extended\n",
      "MultimprovestheMRRscoreby0.74%,12.41%,and2.7% to other types of literals, e.g. text and images. In this sec-\n",
      "fortheFB15k,FB15k-237,andYAGO3-10dataset,respec- tionthisisbrieflydemonstratedfortextliterals. First, letus\n",
      "tively. We also observed that the improvements brought by assume that text literals are represented by vectors in RNt,\n",
      "LiteralE when combined with ComplEx and ConvE are not i.e. as resulting from document embedding techniques [Le\n",
      "as impressive as for DistMult, which might be attributed to and Mikolov, 2014].8 Subsequently, let us redefine g to be\n",
      "the fact that these base models already achieve higher per- a function mapping RH ×RNd ×RNt to RH. Specifically,\n",
      "formance than DistMult. Compared to other methods that weredefineW h(eq.(5))tobeinRH+Nd+Nt×H andemploy\n",
      "incorporate literals, namely KBLN and MTKGNN, LiteralE an additional gating weight matrix W\n",
      "zt\n",
      "∈ RNt×H to han-\n",
      "achievesacompetitiveorevenbetterperformanceinourex- dletheadditionaltextliteral. Note,thatthissimpleextension\n",
      "periments. Moreover, note that, LiteralE directly and ex- schemecanbeusedtoextendLiteralEtoincorporateliterals\n",
      "plicitlymodifiestheembeddingvectors,whereasKBLNand ofanyothertype(e.g.imageliterals)aslongasthoseliterals\n",
      "MTKGNNdonot. Thus,LiteralEembeddingscouldbemore areencodedasvectorsinRN,forsomeN.\n",
      "useful for tasks other than link prediction. This will be dis- TheresultsforextendingDistMult-LiteralEwiththeenti-\n",
      "cussedfurtherinSection6.4. ties’ text literals (i.e. the entity description) are presented in\n",
      "Table 5. We found that incorporating text literals results in\n",
      "6.2 ComparisontoaSimpleLiteralEBaseline\n",
      "Tovalidateourchoiceofthefunctiong,wecomparetheper- 8WeusespaCy’spretrainedGloVeembeddingmodel. Available\n",
      "formance of LiteralE with the g proposed in Section 3 to its athttps://spacy.io\n",
      "Table6: ComparisonofnearestneighborsofselectedentitiesfromFB15k-237embeddedin(i)DistMult’slatentspace,(ii)KBLN’slatent\n",
      "space,(iii)MTKGNN’slatentspace,(iv)theliteralspace,whereeachentityisrepresentedonlybyitsliterals,and(v)theDisMult-LiteralE’s\n",
      "latentspace.\n",
      "Entity Methods NearestNeighbors\n",
      "NorthAmerica DistMult LatinAmerica,Pyrenees,Americas\n",
      "KBLN HouseofHanover,HouseofStuart,HouseofRomanov\n",
      "MTKGNN LatinAmerica,PanamaCity,Pyrenees\n",
      "Num. lits. only SovietUnion,LatinAmerica,Africa\n",
      "LiteralE Americas,LatinAmerica,Asia\n",
      "Philippines DistMult Peru,Thailand,Kuwait\n",
      "KBLN HouseofRomanov,HouseofHanover,HouseofStuart\n",
      "MTKGNN Thailand,Kuwait,Peru\n",
      "Num. lits. only Peru,Poland,Pakistan\n",
      "LiteralE Thailand,Taiwan,Greece\n",
      "RomanRepublic DistMult RepublicofVenice,IsraelDefenseForce,ByzantineEmpire\n",
      "KBLN RepublicofVenice,Carthage,Retinol\n",
      "MTKGNN RepublicofVenice,Carthage,NorthIsland\n",
      "Num. lits. only Alexandria,Yerevan,Cologne\n",
      "LiteralE RomanEmpire,KingdomofGreece,ByzantineEmpire\n",
      "afurtherincreaseofthelinkpredictionperformanceofDist- AmericaisclosetoPyreneesandRoman Repulicis\n",
      "MultonFB15k-237. close to North Island. This findings demonstrates the\n",
      "advantageofincorporatingliteralsontheembeddinglevel(as\n",
      "6.4 NearestNeighborAnalysis\n",
      "donebyLiterelE)overincorporatingthematthescoreorloss\n",
      "Forafurtherqualitativeinvestigation,wepresentthenearest function(asdonebyKBLNandMTKGNN,respectively).\n",
      "neighbors of some entities in the space of literals, the latent Wheninspectingthenearestneighborhoodofthesameen-\n",
      "space learned by (i) DistMult, (ii) KBLN, (iii) MTKGNN, tities when represented only by their literal vectors, it be-\n",
      "and(iv)DisMult-LiteralEinTable6.9 comesclearthatthesevectorsthemselvesarealreadycontain-\n",
      "In the embedding space of DistMult, geographical en- ingusefulinformationindicatingtheclosenessofsimilaren-\n",
      "tities such as North America and Philippines are tities. Forexample,geographicalentitieshavelongitude\n",
      "close to other entities of the same type. However, these and latitude literals, while city, nation, and empire enti-\n",
      "neighboringentitiesarenotintuitive,e.g.North America ties have date founded and date dissolved literals,\n",
      "is close to Pyrenees, whereas Philippines is close which can explain the closeness of two entities given only\n",
      "to Peru and Kuwait. When we inspected the embed- their literal vectors. Note however, that the nearest neigh-\n",
      "ding space of DistMult-LiteralE that also takes literals in- bours in the literal space do not coincide with and are less\n",
      "formation into account, these nearest neighbors (shown in informative than the nearest neighbours in the LiteralE em-\n",
      "bold font in Table 6) become more intuitive, i.e they con- beddingspace.\n",
      "sist of entities geographically close to each others. Fur- All in all, our observations suggest that integrating the\n",
      "thermore, we found that DistMult-LiteralE’s embeddings literal information into entity embeddings indeed improves\n",
      "showclearqualitativeadvantagecomparedtothatofvanilla their quality, which makes LiteralE embeddings promising\n",
      "DistMult also for entities from other types, e.g. compar- forentityresolutionandclusteringtasks.\n",
      "ing the nearest neighbors of Roman Republic which is\n",
      "of type ‘empire’. In contrast, KBLN’s embeddings tend 7 ConclusionandFutureWork\n",
      "to be close to the embeddings of unrelated entities: both\n",
      "North AmericaandPhilippinesareclosetotheen- Inthispaper,weintroducedLiteralE:asimplemethodtoin-\n",
      "tities House of Romanov, House of Hanover, and corporate literals into latent feature methods for knowledge\n",
      "House of Stuart, while Roman Republic is close graph analysis. It corresponds to a learnable function that\n",
      "to Retinol. Similarly, the embeddings of MTKGNN are mergesentityembeddingswiththeirliteralinformationavail-\n",
      "alsoclosetotheembeddingofunrelatedentities,e.g.,North able in the knowledge graph. The resulting literal-enriched\n",
      "latent features can replace the vanilla entity embedding in\n",
      "9ThebasemodelforallofthesemethodsisDistMult. any latent feature method, without any further modification.\n",
      "Therefore, LiteralE can be seen as an universal extension [Mahdisoltanietal.,2014] Farzaneh Mahdisoltani, Joanna Biega,\n",
      "module. Weshowedthataugmentingvariousstate-of-the-art andFabianSuchanek. Yago3: Aknowledgebasefrommultilin-\n",
      "models (DistMult, ComplEx, and ConvE) with LiteralE sig- gualwikipedias. In7thBiennialConferenceonInnovativeData\n",
      "nificantlyimprovestheirlinkpredictionperformance. More- SystemsResearch.CIDRConference,2014.\n",
      "over, as exemplarily demonstrated for text literals, LiteralE [Nickeletal.,2016] Maximilian Nickel, Kevin Murphy, Volker\n",
      "canbeeasilyextendedothertypesofliterals. Infuturework, Tresp, and Evgeniy Gabrilovich. A review of relational ma-\n",
      "LiteralEshallbefurtherbeextendedtoaccommodateliterals chinelearningforknowledgegraphs. ProceedingsoftheIEEE,\n",
      "from the image domain. This can be achieved by extracting 104(1):11–33,2016.\n",
      "latentrepresentationsfromimages(forexamplewithconvo- [Pezeshkpouretal.,2017] Pouya Pezeshkpour, CA Irvine, Liyan\n",
      "lutional neural networks), and providing them as additional Chen,andSameerSingh.Embeddingmultimodalrelationaldata.\n",
      "inputs to LiteralE for merging them with the vanilla entitiy 2017.\n",
      "embeddings.Furthermore,ourfindingthatLiteralEimproves [RuobingXie,2017] Huanbo Luan Maosong Sun Ruobing Xie,\n",
      "the quality of the entity embeddings makes it a promising ZhiyuanLiu. Image-embodiedknowledgerepresentationlearn-\n",
      "candidateforimprovingothertasksinthefieldofknowledge ing. InProceedingsoftheTwenty-SixthInternationalJointCon-\n",
      "graphanalysis,suchasentityresolutionandknowledgegraph ference on Artificial Intelligence, IJCAI-17, pages 3140–3146,\n",
      "clustering. 2017.\n",
      "[Sunetal.,2017] Zequn Sun, Wei Hu, and Chengkai Li. Cross-\n",
      "References lingual entity alignment via joint attribute-preserving embed-\n",
      "ding.InInternationalSemanticWebConference,pages628–644.\n",
      "[Bollackeretal.,2008] KurtBollacker,ColinEvans,PraveenPari-\n",
      "Springer,2017.\n",
      "tosh,TimSturge,andJamieTaylor. Freebase: acollaboratively\n",
      "createdgraphdatabaseforstructuringhumanknowledge.InPro- [Tayetal.,2017] Yi Tay, Luu Anh Tuan, Minh C Phan, and\n",
      "ceedingsofthe2008ACMSIGMODinternationalconferenceon SiuCheungHui. Multi-taskneuralnetworkfornon-discreteat-\n",
      "Managementofdata,pages1247–1250.AcM,2008. tribute prediction in knowledge graphs. In Proceedings of the\n",
      "2017ACMonConferenceonInformationandKnowledgeMan-\n",
      "[Bordesetal.,2013] Antoine Bordes, Nicolas Usunier, Alberto\n",
      "agement,pages1029–1038.ACM,2017.\n",
      "Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Trans-\n",
      "lating embeddings for modeling multi-relational data. In Ad- [Thomaetal.,2017] SteffenThoma,AchimRettinger,andFabian\n",
      "vances in neural information processing systems, pages 2787– Both. Towardsholisticconceptrepresentations: Embeddingre-\n",
      "2795,2013. lationalknowledge,visualattributes,anddistributionalwordse-\n",
      "mantics. InInternationalSemanticWebConference,pages694–\n",
      "[Choetal.,2014] Kyunghyun Cho, Bart van Merrienboer, Caglar\n",
      "710.Springer,2017.\n",
      "Gulcehre,DzmitryBahdanau,FethiBougares,HolgerSchwenk,\n",
      "andYoshuaBengio. Learningphraserepresentationsusingrnn [ToutanovaandChen,2015] Kristina Toutanova and Danqi Chen.\n",
      "encoder–decoderforstatisticalmachinetranslation. InProceed- Observedversuslatentfeaturesforknowledgebaseandtextin-\n",
      "ings of the 2014 Conference on Empirical Methods in Natural ference. InProceedingsofthe3rdWorkshoponContinuousVec-\n",
      "LanguageProcessing(EMNLP),pages1724–1734,Doha,Qatar, torSpaceModelsandtheirCompositionality,pages57–66,2015.\n",
      "October2014.AssociationforComputationalLinguistics. [Toutanovaetal.,2015] Kristina Toutanova, Danqi Chen, Patrick\n",
      "[Dettmersetal.,2018] Tim Dettmers, Minervini Pasquale, Stene- Pantel,HoifungPoon,PallaviChoudhury,andMichaelGamon.\n",
      "torpPontus,andSebastianRiedel. Convolutional2dknowledge Representing text for joint embedding of text and knowledge\n",
      "graphembeddings. InProceedingsofthe32thAAAIConference bases. InEMNLP,volume15,pages1499–1509,2015.\n",
      "onArtificialIntelligence,February2018. [Trouillonetal.,2016] The´oTrouillon,JohannesWelbl,Sebastian\n",
      "[Dongetal.,2014] Xin Dong, Evgeniy Gabrilovich, Geremy Riedel, E´ricGaussier, andGuillaumeBouchard. Complexem-\n",
      "Heitz,WilkoHorn,NiLao,KevinMurphy,ThomasStrohmann, beddingsforsimplelinkprediction. InInternationalConference\n",
      "Shaohua Sun, and Wei Zhang. Knowledge vault: A web-scale onMachineLearning,pages2071–2080,2016.\n",
      "approach to probabilistic knowledge fusion. In Proceedings of\n",
      "[Tuetal.,2017] CunchaoTu,HanLiu,ZhiyuanLiu,andMaosong\n",
      "the20thACMSIGKDDinternationalconferenceonKnowledge\n",
      "Sun. Cane:Context-awarenetworkembeddingforrelationmod-\n",
      "discoveryanddatamining,pages601–610.ACM,2014.\n",
      "eling. In Proceedings of the 55th Annual Meeting of the Asso-\n",
      "[Garcia-DuranandNiepert,2017] Alberto Garcia-Duran and ciationforComputationalLinguistics(Volume1: LongPapers),\n",
      "Mathias Niepert. KBLRN: End-to-end learning of knowledge volume1,pages1722–1731,2017.\n",
      "base representations with latent, relational, and numerical\n",
      "[WuandWang,2018] YanrongWuandZhichunWang.Knowledge\n",
      "features. arXivpreprintarXiv:1709.04676,2017.\n",
      "graphembeddingwithnumericattributesofentities.InProceed-\n",
      "[KingmaandBa,2015] DiederikPKingmaandJimmyBa. Adam: ingsofTheThirdWorkshoponRepresentationLearningforNLP,\n",
      "Amethodforstochasticoptimization. In3rdInternationalCon- pages132–136,2018.\n",
      "ferenceforLearningRepresentations.ICLR,2015.\n",
      "[Xieetal.,2016] RuobingXie,ZhiyuanLiu,JiaJia,HuanboLuan,\n",
      "[LeandMikolov,2014] QuocLeandTomasMikolov. Distributed andMaosongSun. Representationlearningofknowledgegraphs\n",
      "representations of sentences and documents. In International withentitydescriptions. InAAAI,pages2659–2665,2016.\n",
      "conferenceonmachinelearning,pages1188–1196,2014.\n",
      "[Xuetal.,2016] JiachengXu, KanChen, XipengQiu,andXuan-\n",
      "[Lehmannetal.,2015] Jens Lehmann, Robert Isele, Max Jakob, jingHuang. Knowledgegraphrepresentationwithjointlystruc-\n",
      "AnjaJentzsch,DimitrisKontokostas,PabloNMendes,Sebastian turalandtextualencoding. InIJCAI,2016.\n",
      "Hellmann, Mohamed Morsey, Patrick Van Kleef, So¨ren Auer,\n",
      "et al. DBpedia–a large-scale, multilingual knowledge base ex-\n",
      "tractedfromwikipedia. SemanticWeb,6(2):167–195,2015.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  20212,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Link prediction']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Incorporating Literals into Knowledge Graph Embeddings\n",
      "AgustinusKristiadi∗4,MohammadAsifKhan∗1,DenisLukovnikov1,JensLehmann1,2,AsjaFischer3\n",
      "1 SDAGroup,UniversityofBonn\n",
      "2 EISDepartment,FraunhoferIAIS\n",
      "3 Ruhr-UniversityBochum\n",
      "4 UniversityofTu¨bingen\n",
      "agustinus.kristiadi@uni-tuebingen.de,s6mokhan@uni-bonn.de,lukovnik@cs.uni-bonn.de,\n",
      "jens.lehmann@uni-bonn.de,jens.lehmann@iais.fraunhofer.de,asja.fischer@rub.de\n",
      "Abstract\n",
      "DoeHigh\n",
      "Knowledge graphs are composed of different el-\n",
      "School\n",
      "ements: entity nodes, relation edges, and literal\n",
      "n\n",
      "t\n",
      "pro\n",
      "eib\n",
      "rd sue ots\n",
      "e\n",
      "n. )vE\n",
      "a\n",
      "ala nuc deh\n",
      "t(\n",
      "hl ei e.t ge\n",
      "r.\n",
      "era btl\n",
      "h\n",
      "yen eo\n",
      "h\n",
      "nd\n",
      "e\n",
      "ce\n",
      "i\n",
      "ogc dho etn sota\n",
      "f\n",
      "inin\n",
      "a\n",
      "fs\n",
      "on\n",
      "ra men\n",
      "n at\n",
      "te\n",
      "i\n",
      "itn oyt ni oty\n",
      "f\n",
      "w’s\n",
      "hty\n",
      "ia cpt he- studiesAt studies\n",
      "At\n",
      "in general cannot be represented by relations be-\n",
      "tween entities alone. However, most of the ex- knows? Jane\n",
      "isting embedding- or latent-feature-based methods John\n",
      "for knowledge graph analysis only consider entity\n",
      "n\n",
      "i tn\n",
      "ho\n",
      "f\n",
      "id soe\n",
      "r\n",
      "ps\n",
      "m\n",
      "aa pan\n",
      "t\n",
      "eid\n",
      "ro\n",
      ",nr we pl ea rt\n",
      "o\n",
      "ei\n",
      "v\n",
      "xo\n",
      "i\n",
      "tn\n",
      "d\n",
      "enee\n",
      "d\n",
      "ddg ebe xys i,\n",
      "sl\n",
      "ta\n",
      "i\n",
      "itn ned grat llh asu teis\n",
      "n\n",
      "ntd too fean aco\n",
      "tc\n",
      "ut\n",
      "o\n",
      "rt eua nk mte\n",
      ".\n",
      "et th\n",
      "I\n",
      "hne\n",
      "-\n",
      "b\n",
      "irth\n",
      "Y\n",
      "e a\n",
      "birthYear\n",
      "r\n",
      "ods for link prediction by a simple portable mod-\n",
      "2001 2000\n",
      "ule for incorporating literals, which we name Lit-\n",
      "eralE.Unlikeinconcurrentmethodswhereliterals\n",
      "Figure1: Literals(box)encodeinformationthatcannotberepre-\n",
      "areincorporatedbyaddingaliteral-dependentterm\n",
      "sentedbyrelationsalone,andareusefulforlinkpredictiontask.For\n",
      "totheoutputofthescoringfunctionandthusonly\n",
      "instance,byconsideringbothbirthYearliteralsandthefactthat\n",
      "indirectlyaffecttheentityembeddings,LiteralEdi- John and Jane both study at Doe High School, we can be\n",
      "rectlyenrichestheseembeddingswithinformation moreconfidentthattherelationknowsbetweenJohnandJane\n",
      "fromliteralsviaalearnableparametrizedfunction. exists.\n",
      "Thisfunctioncanbeeasilyintegratedintothescor-\n",
      "ingfunctionofexistingmethodsandlearnedalong\n",
      "with the entity embeddings in an end-to-end man-\n",
      "DBpedia[Lehmannetal., 2015], Freebase[Bollackeretal.,\n",
      "ner. In an extensive empirical study over three 2008], YAGO3 [Mahdisoltani et al., 2014], and the Google\n",
      "datasets, we evaluate LiteralE-extended versions Knowledge Graph [Dong et al., 2014]. There are differ-\n",
      "of various state-of-the-art latent feature methods ent knowledge representation paradigms for modeling KGs\n",
      "for link prediction and demonstrate that LiteralE suchastheResourceDescriptionFramework(RDF)and(la-\n",
      "presents an effective way to improve their perfor- beled)propertygraphs. Withinthispaper,weconsideraKG\n",
      "mance. Fortheseexperiments,weaugmentedstan- to be a set of triples, where each triple connects an entity\n",
      "darddatasetswiththeirliterals,whichwepublicly (shown as circle in Figure 1) to another entity or a literal\n",
      "provideastestbedsforfurtherresearch. Moreover, (the latter shown as rectangle in Figure 1) via relationships.\n",
      "we show that LiteralE leads to an qualitative im- SuchKGscanberepresentedbytheRDFandpropertygraph\n",
      "provementoftheembeddingsandthatitcanbeeas- paradigms,i.e.themethodspresentedinthispaperareappli-\n",
      "ilyextendedtohandleliteralsfromdifferentmodal- cabletoboth.Togiveaconcreteexample,theKGdepictedin\n",
      "ities. Figure 1 includes the triples (John, Doe High School,\n",
      "studiesAt) and (Jane, 2000, birthYear). The first\n",
      "tripleexpressestherelationshipbetweenanentityandanother\n",
      "1 Introduction entity. Thesecondtripleexpressesarelationshipbetweenan\n",
      "entityandaliteral1.\n",
      "Knowledge graphs (KGs) form the backbone of a range of\n",
      "applications,forinstanceintheareasofsearch,questionan- Knowledge graphs aim to capture factual knowledge\n",
      "swering and data integration. Some well known KGs are\n",
      "1FormoreinformationabouttheRDFconceptsseehttps://\n",
      "∗Equalcontribution www.w3.org/TR/rdf11-concepts\n",
      "9102\n",
      "luJ\n",
      "81\n",
      "]IA.sc[\n",
      "3v43900.2081:viXra\n",
      "within a particular domain. However, they are often incom- model interactions between an embedding of an entity\n",
      "plete since, e.g., more information is provided for popular andallitsliteralvaluesandcanbetrainedend-to-end.\n",
      "than for unknown entities or because the KG is partially or\n",
      "• We evaluate LiteralE on standard link prediction\n",
      "fullygeneratedviaanautomaticextractionprocess. Asare-\n",
      "datasets: FB15k,FB15k-237andYAGO3-10. Weex-\n",
      "sult,KGsrelyheavilyonmethodspredictingunknowntriples\n",
      "tended FB15k and FB15k-237 with literals, in order\n",
      "givenallknowntriples.Thisproblemisusuallyreferredtoas\n",
      "to allow for direct comparison against other methods\n",
      "linkprediction. Thecloselyrelatedproblemofdetectingin-\n",
      "on these standard datasets. We provide these literal-\n",
      "correct triples in KGs is referred to as link correction and is\n",
      "extended versions (augmented with numerical and tex-\n",
      "relevantforimprovingthequalityofaKG.\n",
      "tual literals) and hope they can serve as a testbed for\n",
      "Due to the importance of the problem, many methods for\n",
      "futureresearchontheinclusionofliteralsinKGmodel-\n",
      "link prediction and correction in KGs have been developed.\n",
      "ing.2\n",
      "Thetwomainclassesofthesemethodsaregraphfeatureand\n",
      "latent feature methods [Nickel et al., 2016]. Graph feature • Basedonexperimentalresultsontheextendeddatasets,\n",
      "methodspredicttheexistenceoftriplesbasedonfeaturesdi- weshowthatexploitingtheinformationprovidedbylit-\n",
      "rectly observed in the KG, such as the neighborhood of an erals significantly increases the link prediction perfor-\n",
      "entity and paths to other entities. They are well suited for mance of existing latent feature methods as well as the\n",
      "modelinglocalgraphpatterns. Inlatentfeaturemodels,low- qualityoftheirembeddings.\n",
      "dimensional, latentrepresentations(alsocalledembeddings)\n",
      "Thispaperisorganizedasfollows. InSection2wereview\n",
      "of entities and relations are learned. These embeddings in-\n",
      "severallatentfeaturemethodsforlinkpredictioninKGs. In\n",
      "corporate the KG structure, can capture global patterns, and\n",
      "Section3wepresentLiteralE,ourapproachforincorporating\n",
      "allow to compute the likeliness of a given triple in terms of\n",
      "literals into existing latent feature methods. We give a brief\n",
      "aprobabilityorscorefunction. However, mostoftherecent\n",
      "review of the related literatures and contrast LiteralE with\n",
      "work on latent feature models only takes entities and their\n",
      "othermethodsincorporatingliteralsinSection4. Ourexperi-\n",
      "relations to other entities into account. Therefore, they are\n",
      "mentmethodologyisdescribedinSection5,andinSection6\n",
      "missing the additional information encoded in literals. For\n",
      "wepresentourexperimentresults. Finally, weconcludeour\n",
      "example, Figure 1 shows two entities with both structural\n",
      "paperinSection7.\n",
      "(visiting the same school) as well as literal (birth years) in-\n",
      "Our implementation of the proposed methods and all\n",
      "formation. Tomaximizetheaccuracyofpredictingaknows\n",
      "datasets are publicly available at: https://github.\n",
      "relationbetweentheseentities,structuralandliteralinforma-\n",
      "com/SmartDataAnalytics/LiteralE.\n",
      "tion should be combined as people visiting the same school\n",
      "and having similar age tend to have a higher probability of\n",
      "knowingeachother. 2 Preliminaries\n",
      "Inthispaper,weinvestigatetheadvantageobtainedbyin-\n",
      "Inthefollowingwewilldescribethelinkpredictionproblem\n",
      "corporating the additional information provided by literals\n",
      "moreformallyandgiveabriefoverviewoverwell-knownla-\n",
      "intolatentfeaturemethods. WeintroduceLiteralE,amethod\n",
      "tentfeaturemethods.\n",
      "to enrich entity embeddings with their literal information.\n",
      "Givenanentityembedding,weincorporateitscorresponding\n",
      "2.1 ProblemDescription\n",
      "literalsusingalearnableparametricfunction,whichgetsthe\n",
      "vanillaembeddingandthe entity’sliteralsasinput, andout- Link prediction is defined as the task of deciding whether\n",
      "putsaliteral-enrichedembedding. Thisembeddingcanthen a fact (represented by a triple) is true or false given a KG.\n",
      "replace the vanilla embedding in any latent feature model, Moreformally, letE = {e 1,···,e Ne}bethesetofentities,\n",
      "withoutchangingitsoriginalscoringfunctionandtheresult- R = {r 1,···,r Nr} be the set of relations connecting two\n",
      "ingsystemcanbejointlytrainedwithstochasticgradientde- entities,D = {d 1,···,d Nd}bethesetofrelationsconnect-\n",
      "scent,oranyothergradientbasedalgorithmofchoice,inan ing an entity and a literal, i.e., the data relations, and L be\n",
      "end-to-end manner. Therefore, LiteralE can be seen as an thesetofallliteralvalues. AknowledgegraphG isasubset\n",
      "extensionmodulethatcanbeuniversallycombinedwithany of (E ×E ×R)∪(E ×L×D) representing the facts that\n",
      "existinglatentfeaturemethod. Withinthispaper,wemainly areassumedtohold. Linkpredictioncanbeformulatedbya\n",
      "focus on numerical literals. However, we demonstrate that functionψ :E×E×R→Rmappingeachpossiblefactrep-\n",
      "theprinciplecanbedirectlygeneralizedtootherliteraltypes, resentedbythecorrespondingtriple(e i,e j,r k)∈E×E×R\n",
      "suchastextualandimageinformation,e.g.byprovidinglow- to a score value, where a higher value implies the triple is\n",
      "dimensionalvectorrepresentationofimageortext[Xieetal., morelikelytobetrue.\n",
      "2016;Xuetal.,2016]asanadditionalinputtoLiteralE.\n",
      "Ourcontributionsinthispaperarethreefold: 2.2 LatentFeatureMethods\n",
      "• WeintroduceLiteralE,auniversalapproachtoenrichla- In general, latent feature methods are a class of methods in\n",
      "tentfeaturemethodswithliteralinformationviaalearn- whichlowdimensionalvectorrepresentationsofentitiesand\n",
      "ableparametricfunction. Incontrasttootherlatentfea- relations, called embeddings or latent features, are learned.\n",
      "turemodelsincludingliterals,ourapproachdoesnotre-\n",
      "quirespecificpriorknowledge,doesnotrelyonafixed 2A literal-extended version of YAGO3-10 is provided\n",
      "functiontocombineentityembeddingsandliterals,can by[Pezeshkpouretal.,2017].\n",
      "LetH betheembeddingdimension. Wedefineascorefunc- Triple Embeddings LiteralE ScoreFunc. Score\n",
      "tionf : RH ×RH ×RH → Rthatmapsatripleofembed-\n",
      "li\n",
      "dings(e,e,r )toascoref(e,e,r )thatcorrelateswith\n",
      "i j k i j k\n",
      "the truth value of the triple. In latent feature methods, the ei g\n",
      "scoreofanytriple(e,e,r )∈E×E×Risthendefinedas\n",
      "i j k\n",
      "ψ(e i,e j,r k)d=ef f(e i,e j,r k). rk f y\n",
      "Latent feature methods for link predictions are well stud-\n",
      "ied. These methods follow a score-based approach as de-\n",
      "scribedabovebutmakeuseofdifferentkindofscoringfunc- ej g\n",
      "tions f. In this paper we study the potential benefit of in-\n",
      "corporating numerical literals in three state of the art meth-\n",
      "lj\n",
      "ods: DistMult [Dong et al., 2014], ComplEx [Trouillon et\n",
      "al.,2016],andConvE[Dettmersetal.,2018],whicharede- Figure2: OverviewonhowLiteralEisappliedtothebasescoring\n",
      "functionf.LiteralEtakestheembeddingandthecorrespondinglit-\n",
      "scribed in the following. Note however, that these are just\n",
      "eralsasinput, andcombinesthemviaalearnablefunctiong. The\n",
      "anexemplarychoiceofmethodsandourapproachforincor-\n",
      "outputisajointembeddingwhichisfurtherusedinthescorefunc-\n",
      "poratingliteralscaneasilybeadoptedtootherlatentfeature\n",
      "tionf.\n",
      "methods.\n",
      "The DistMult scoring function is defined as diagonal bi-\n",
      "linearinteractionbetweenthetwoentityembeddingsandthe\n",
      "i-th entity and the k-th data relation exists in the KGs, and\n",
      "relationembeddingcorrespondingtoagiventriple,asfollows\n",
      "zero otherwise. We will refer to the i-th row l of L as the\n",
      "(cid:124) i\n",
      "f DistMult(e i,e j,r k)=(cid:104)e i,e j,r k(cid:105)=e i diag(r k)e j. (1) literal vector of the i-th entity. As an illustration, consider\n",
      "theKGpartdepictedinFigure1andimaginethatthereonly\n",
      "Observe that DistMult is cheap to implement, both in terms\n",
      "exist three data relations in this specific KG: heightCm,\n",
      "ofcomputationalandspacecomplexity.\n",
      "birthYear,andcountryArea. FortheentityJohnwe\n",
      "ComplExcanbeseenasDistMultanalogueinthecomplex\n",
      "will then have the literal vector (0,2001,0) in the particular\n",
      "space. The embedding vectors have two parts: the real part\n",
      "row corresponding to John in matrix L, as John only has\n",
      "Re(e) and Re(r), and the imaginary part Im(e) and Im(r),\n",
      "literalinformationforbirthYear.3\n",
      "respectively. Thescoringfunctionisdefinedas\n",
      "AtthecoreofLiteralEisafunctiong :RH ×RNd →RH\n",
      "f ComplEx(e i,e j,r k)=Re((cid:104)e i,¯e j,r k(cid:105)) that takes an entity’s embedding and a literal vector as in-\n",
      "=(cid:104)Re(e ),Re(e ),Re(r )(cid:105) puts and maps them to a vector of the same dimension as\n",
      "i j k\n",
      "+(cid:104)Im(e ),Im(e ),Re(r )(cid:105) (2) the entity embedding. This vector forms an literal-enriched\n",
      "i j k\n",
      "embedding vector that can replace the original embedding\n",
      "+(cid:104)Re(e ),Im(e ),Im(r )(cid:105)\n",
      "i j k vector in the scoring function of any latent feature model.\n",
      "−(cid:104)Im(e i),Re(e j),Im(r k)(cid:105). For example, in our experiments, we replace every entity\n",
      "embedding e with elit = g(e,l ) in the scoring functions\n",
      "ComplExthushastwicethenumberofparameterscompared i i i i\n",
      "of DistMult and ConvE. For ComplEx, where the embed-\n",
      "toDistMultbutprovidesthebenefitofmodelingasymmetric\n",
      "relationshipsbetter,asdiscussedby[Trouillonetal.,2016]. dings have a real and an imaginary part, we use two sep-\n",
      "arate functions to map Re(e ) and Im(e ) to their literal-\n",
      "ConvEemploysaconvolutionalneuralnetworktoextract i i\n",
      "extended counterparts. Aside of these changes regarding\n",
      "features from entity and relation embeddings. Let h be a\n",
      "nonlinearfunction,ω ∈ Rk×m×n beconvolutionfilters,and the entity embeddings, the score functions are the same as\n",
      "W ∈ Rkmn×H beaweightmatrix. TheConvEscorefunc- described before in eq. (1), eq. (2), and eq. (3). For in-\n",
      "stance,theLiteralE-extendedversionofDistMultisgivenby\n",
      "tionisthendefinedby\n",
      "f (elit,elit,r ).\n",
      "DistMult i j k\n",
      "f (e,e,r )=h(vec(h([e,r ]∗ω))W)e, (3)\n",
      "ConvE i j k i k j Wewillnowdescribethefunctiong indetail. First,since\n",
      "where vec(·) is the vectorization of output of convolutional we would like g to be flexible, we need it to be learnable.\n",
      "filters. By employing deep feature extractors in the form Second, we would like g to be able to decide whether the\n",
      "of nonlinear convolutional layers, ConvE is able to encode additional literal information is useful or not, and adapt ac-\n",
      "more expressive features while remaining highly parameter cordingly,e.g. byincorporatingorignoringthatinformation.\n",
      "efficient. Wethereforetakecuefromthegatingmechanismpresentin\n",
      "RNNs, such as the gated recurrent unit (GRU) [Cho et al.,\n",
      "3 LiteralE 2014],andletgbedefinedby\n",
      "Ourmethodof incorporating literalsintoexistinglatentfea-\n",
      "ture methods, which we call LiteralE, is a simple, modular, g :RH ×RNd →RH\n",
      "and universal extension which can potentially enhance the e,l(cid:55)→z(cid:12)h+(1−z)(cid:12)e, (4)\n",
      "performanceofarbitrarylatentfeaturemethods.\n",
      "LetL ∈ RNe×Nd beamatrix, whereeachentryL\n",
      "ik\n",
      "con-\n",
      "tainsthek-thliteralvalueofthei-thentityifatriplewiththe 3Notethatinpractice,wenormalizetheliteralvalues.\n",
      "where(cid:12)isthepointwisemultiplicationand Table 1: Model complexity in terms of number of parameters of\n",
      "methodsforincorporatingliterals.Wedenotethenumberofparam-\n",
      "z=σ(WT e+WTl+b) etersofbasemodels(e.g.DistMult)withΓ. Furthermore,Z isthe\n",
      "ze zl\n",
      "numberofhiddenunitsinaneuralnetwork(e.g.inLiteralE-MLP\n",
      "h=h(WT[e,l]). (5)\n",
      "h andMTKGNN’sAttributeNetworks).Weleaveoutbiasparameters\n",
      "Note that W\n",
      "h\n",
      "∈ RH+Nd×H, W\n",
      "ze\n",
      "∈ RH×H, W\n",
      "zl\n",
      "∈ forclarity.\n",
      "RNd×H, and b ∈ RH are the parameters of g, σ is the sig-\n",
      "moidfunction, andhisacomponent-wisenonlinearity(e.g. Model NumberofParameters\n",
      "thehyperbolictangent). KBLN Γ+N N\n",
      "r d\n",
      "LiteralE introduces some overhead in the number of pa- MTKGNN Γ+N H +2(2HZ+Z)\n",
      "d\n",
      "rameters compared to the base method. This overhead is\n",
      "equal to the number of parameters of the function g and is LiteralE Γ+2H2+2N dH +H\n",
      "comparedtothatofotherapproachesfortheincorporationof\n",
      "literalsinTable1. Specifically,thereare2H2+2N H +H\n",
      "d KBLRN[Garcia-DuranandNiepert,2017]handlesliterals\n",
      "additionalparameterscorrespondingtothedimensionalityof\n",
      "in a separate function added to the vanilla scoring function\n",
      "W,W,W,andbineq.(5). Thus,withthischoiceofg\n",
      "h ze zl andthusdoesnotincorporateliteralsintotheentityembed-\n",
      "andgivenH,thenumberofadditionalparametersofLiteralE\n",
      "dingsthemselves. Theconstructionoffeaturesfromthenu-\n",
      "growsinO(N ),thatis,lineartothenumberofdatarelations\n",
      "d mericalliteralsisbasedonapriorknowledge: thedifference\n",
      "in the KG. Furthermore, the additional space complexity of\n",
      "betweenthenumericalliteralsofthesubjectandobjectentity\n",
      "LiteralE is in O(N N ) as one needs to store the matrix L.\n",
      "e d isagoodpredictorforagivenrelation.\n",
      "Lastly,theadditionalcomputationalcomplexityofLiteralEis\n",
      "These features then serve as input to a fixed radial basis\n",
      "onlyattributedtothecostofthreematrixmultiplicationand\n",
      "function (RBF), which is added to the score function of the\n",
      "onevectoraddition.\n",
      "base method (DistMult). In contrast, LiteralE incorporates\n",
      "In summary, with our method LiteralE, we propose to re-\n",
      "literal information directly into the entity embeddings4, and\n",
      "placethescorefunctionf (e,e,r )fromthehostmethod\n",
      "X i j k does not use any prior knowledge about the meaning of nu-\n",
      "X withthefunctioncomposition\n",
      "mericalliterals.\n",
      "f (g(e,l ),g(e,l ),r ) MTKGNN [Tay et al., 2017] extends ERMLP [Dong et\n",
      "X i i j j k\n",
      "al., 2014] and incorporates numerical literals by introduc-\n",
      "as illustrated in Figure 2. This new scoring function can be\n",
      "ing an additional learning task, more precisely, the task of\n",
      "trainedbygradientdescentbasedoptimizationusingthesame\n",
      "predicting the literal value for a given entity. This multi-\n",
      "trainingprocedureasbefore.\n",
      "task learning approach of MTKGNN requires an additional\n",
      "attribute-specific training procedure. Therefore, adding an-\n",
      "4 RelatedWork\n",
      "other type or modality of literals is not straightforward and\n",
      "In the last years, several efforts to incorporate literals into costlyasanotherlearningtaskneedstobedevised.Similarto\n",
      "KGembeddingmethodshavebeenmade. [Toutanovaetal., MTKGNN, TransEA [Wu and Wang, 2018] extends TransE\n",
      "2015]and[Tuetal.,2017]makeuseoftextualliteralsofen- by adding a numerical attribute prediction loss to the rela-\n",
      "titiesinadditiontorelationalembeddings. Morespecifically tionalloss.\n",
      "theylearnadditionalentityembeddingsfromtheirtextualde- Lastly, the model recently proposed by [Thoma et al.,\n",
      "scriptionandusetheminanadditiveterminthescorefunc- 2017]canbeseenasaspecialcaseofLiteralEwherethefunc-\n",
      "tion of latent distant methods. [Xie et al., 2016] and [Xu et tionusedinsteadofthefunctiong definedabovetocombine\n",
      "al., 2016] also proposed methods to incorporate textual lit- literals of entities with their entity embeddings is a concate-\n",
      "erals into latent distance methods such as TransE by encod- nationfollowedbysingularvaluedecomposition. Thus,they\n",
      "ingtextualliteralswithrecurrentorconvolutionalneuralnet- useafixedfunctiontocombinetherepresentations,whereas\n",
      "works. [RuobingXie,2017]useimageliteralsintheirmodel LiteralEemploysanadaptablefunctionandisthereforemore\n",
      "by projecting entities’ image features into an entity embed- flexible. Furthermore, theyonlyconsiderimageandtextlit-\n",
      "dingsspace.However,allofthoseapproachesdonotconsider eralsbutnonumericalliterals.\n",
      "numericalliterals.MultiModal[Pezeshkpouretal.,2017]ex-\n",
      "tends DistMult to also predict the likeliness of (subject, re- 5 Experiments\n",
      "lation, literal)-triples, by replacing the object embedding in\n",
      "In the following we will describe the training approach, the\n",
      "standardDistMultbyitsliteralembedding(whereliteralsof\n",
      "datasets, the experimental setup, and the evaluation metrics\n",
      "differentmodalitiesaretakenintoaccount). Bydoingsolit-\n",
      "appliedinourexperiments.\n",
      "erals are incorporated into entity embeddings in an implicit\n",
      "manner. [Sun et al., 2017], proposes to employ literals to 5.1 Training\n",
      "refine the joint embeddings in entity alignment tasks: They\n",
      "We use the same training approach as [Dettmers et al.,\n",
      "useliteralstoclustertogetherentitieswhichhavehighliteral\n",
      "2018] for all the tested methods. That is, for every given\n",
      "correlations,thusonlyindirectlyusetheliteralinformationin\n",
      "theentityembeddings. Incontrasttoalltheaforementioned 4Note,thatincorporatingtheliteralinformationintotheembed-\n",
      "works, LiteralE combines the literals into the entity embed- dingsalsoseemsadvantageousforentitydisambiguationorcluster-\n",
      "dingdirectlyandexplicitlybythefunctiongdefinedabove. ing.\n",
      "Table 2: Number of entities, relations, literals, and triples, for all format), etc. To enrich FB15k and FB15k-237 with these\n",
      "datasetsusedinthispaper.\n",
      "literals,wecreatedaSPARQLendpointforFreebaseandex-\n",
      "tracted literals of all entities contained in FB15k. We fur-\n",
      "Dataset FB15k FB15k-237 YAGO3-10 ther filtered the extracted literals based on their frequency,\n",
      "#Entities(N ) 14,951 14,541 123,182\n",
      "i.eweonlyconsiderdatarelationsd ∈ D thatoccuratleast\n",
      "e\n",
      "in 5 triples in FB15k. We also remove all key and ID rela-\n",
      "#Relations(N ) 1,345 237 37\n",
      "r\n",
      "tionssincetheirvaluesarenotmeaningfulasquantities. For\n",
      "#Datarel.(N ) 121 121 5\n",
      "d\n",
      "#Literals(|L|) 18,741 18,741 111,406 YAGO3-10, weusenumericalliteralsprovidedbyYAGO3-\n",
      "10-plus [Pezeshkpour et al., 2017], which is publicly avail-\n",
      "#Relationaltriples 592,213 310,116 1,089,040\n",
      "able.5 In case an entity has multiple literal values for a par-\n",
      "#Literaltriples 70,257 70,257 111,406\n",
      "ticulardatarelation,wearbitrarilyselectoneofthem. Some\n",
      "statisticsofthedatasetsareprovidedinTable2.\n",
      "triple (e,e,r ) in the KG, we compute the score for\n",
      "i j k\n",
      "(e,e(cid:48),r ),∀e(cid:48) ∈E usingthe(originalorLiteralE-extended) 5.3 ExperimentalSetup\n",
      "i j k j\n",
      "scoringfunctionf,andapplythesigmoidfunctiontothere-\n",
      "WeimplementedLieralEontopofConvE’scodebase,which\n",
      "sultingscore(i.e.p = σ◦f), suchthatitcanbeinterpreted\n",
      "ispubliclyavailable6. Thehyperparametersusedinallofour\n",
      "asprobabilityofexistenceofagiventriple.\n",
      "experimentsacrossalldatasetsare:learningrate0.001,batch\n",
      "Let p ∈ [0,1]Ne be the probability vector, collecting the\n",
      "size128, embeddingsize200, embeddingdropoutprobabil-\n",
      "resultingprobabilitieswithrespecttoalle(cid:48) ∈ E. Themodel\n",
      "j ity 0.2, and label smoothing 0.1. Additionally for ConvE,\n",
      "is then trained by minimizing the binary cross-entropy loss\n",
      "we used feature map dropout with probability 0.2 and pro-\n",
      "between the probability vector p and the vector of ground\n",
      "jection layer dropout with probability 0.3. Note, that these\n",
      "truth labels y ∈ {0,1}Ne indicating the existence of triples\n",
      "hyperparametervaluesarethesameasintheexperimentsof\n",
      "(e,e(cid:48),r ),∀e(cid:48) ∈E intheKG.Thatis,weminimize\n",
      "i j k j [Dettmersetal.,2018].\n",
      "1\n",
      "(cid:88)Ne ExceptforexperimentswithConvE,werunallofourex-\n",
      "L(p,y)=− (y log(p )+(1−y )log(1−p )), perimentsforamaximumof100epochsasweobservedthat\n",
      "x x x x\n",
      "N\n",
      "e thisissufficientforconvergence.ForConvE,weusedatmost\n",
      "x=1\n",
      "(6) 1000epochs, asdescribedintheoriginalpaper[Dettmerset\n",
      "where p x and y x are the predicted probability and the al.,2018]. Weapplyearlystoppinginalloftheexperiments\n",
      "given truth value for the x-th element of our candidate set by monitoring the Mean Reciprocal Rank (MRR) metric on\n",
      "{(e i,e(cid:48) j,r k),e(cid:48)\n",
      "j\n",
      "∈E}. WeuseAdam[KingmaandBa,2015] thevalidationseteverythreeepochs.\n",
      "tooptimizethislossfunction. To validate our approach and to eliminate the effect of\n",
      "Note, the above procedure of considering all triples different environment setups, we re-implemented the re-\n",
      "(e i,e(cid:48) j,r k), ∀e(cid:48)\n",
      "j\n",
      "∈ E if there is any triple (e i,e j,r k) with latedmodels,KBLN[Garcia-DuranandNiepert,2017],and\n",
      "heade iandrelationr kinthetrainingsetisreferredtoas1-N MTKGNN [Tay et al., 2017] as baselines. Note that we did\n",
      "scoring[Dettmersetal.,2018]asforeachtriple,wecompute notre-implementKBLRN[Garcia-DuranandNiepert,2017]\n",
      "scoresofN := N e = |E|triples. Thisisincontrastwith1-1 since the sub-model KBLN (i.e. the KBLRN model without\n",
      "scoring, whereoneprimarilyconsidersthetrainingexample making use of the relational information provided by graph\n",
      "(e i,e j,r k)andappliessomeotherstrategyfornegativesam- feature methods) is directly comparable to LiteralE.7 As in\n",
      "pling(i.e.forthegenerationofnon-existingtriples).Werefer [Dettmersetal.,2018],weusea1-Ntrainingapproach,while\n",
      "the reader to [Dettmers et al., 2018] for a further discussion KBLN and MTKGNN uses a 1-1 approach. Therefore, the\n",
      "regardingthis. RelNetinMTKGNNwhichisaneuralnetworkisinfeasible\n",
      "tobeimplementedinourenvironment. Thus, asopposedto\n",
      "5.2 Datasets\n",
      "neural network, we use DistMult as base model in our re-\n",
      "We use three widely used datasets for evaluating link pre- implementation of an MTKGNN-like method. While this\n",
      "dictionperformance: FB15k,FB15k-237,andYAGO3-10. changedoesnotallowtoevaluatetheperformanceoftheorig-\n",
      "FB15k [Bordes et al., 2013] is a subset of Freebase where inalMTKGNNmodel, itmakesourMTKGNN-likemethod\n",
      "mosttriplesarerelatedtomoviesandsports. Asdiscussedby directlycomparabletotheothermethodsthatweconsiderin\n",
      "[Dettmers et al., 2018], FB15k has a large number of test our experiments, since it uses the same base score function.\n",
      "triples which can simply be obtained by inverting training All in all, due to these differences in the loss function and\n",
      "triples. This results in a biased test set, for which a simple the overall framework which are necessary to make KBLN\n",
      "modelwhichissymmetricwithrespecttoobjectandsubject andMTKGNNcomparabletoLiteralE,theresultswereport\n",
      "entity is capable of achieving excellent results. To address for them might differ from those reported in the respective\n",
      "this problem, FB15k-237 [Toutanova and Chen, 2015] was original papers. In addition, we obtain slightly different re-\n",
      "createdbyremovinginverserelationsfromFB15k. YAGO3- sultscomparedto[Dettmersetal.,2018]forDistMult,Com-\n",
      "10 [Mahdisoltani et al., 2014] is a subset of the YAGO3\n",
      "knowledge graph which mostly consists of triples related to 5https://github.com/pouyapez/multim-kb-embeddings\n",
      "people. 6https://github.com/TimDettmers/ConvE\n",
      "Inthiswork,weonlyconsidernumericalliterals,e.g. lon- 7<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  63016,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Agustinus Kristiadi', 'Mohammad Asif Khan', 'Denis Lukovnikov', 'Jens Lehmann', 'Asja Fischer']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: withrespecttoobjectandsubject andMTKGNNcomparabletoLiteralE,theresultswereport\n",
      "entity is capable of achieving excellent results. To address for them might differ from those reported in the respective\n",
      "this problem, FB15k-237 [Toutanova and Chen, 2015] was original papers. In addition, we obtain slightly different re-\n",
      "createdbyremovinginverserelationsfromFB15k. YAGO3- sultscomparedto[Dettmersetal.,2018]forDistMult,Com-\n",
      "10 [Mahdisoltani et al., 2014] is a subset of the YAGO3\n",
      "knowledge graph which mostly consists of triples related to 5https://github.com/pouyapez/multim-kb-embeddings\n",
      "people. 6https://github.com/TimDettmers/ConvE\n",
      "Inthiswork,weonlyconsidernumericalliterals,e.g. lon- 7Note,thatLiteralEcouldalsobeextendedtoincorporategraph\n",
      "gitude,latitude,population,age,dateofbirth(inUNIXtime featuresasanadditionalinputtog.\n",
      "Table3: LinkpredictionresultsonFB15k,FB15k-237,andYAGO3-10. Thebestvaluescomparingourimplementationofbasemodels,\n",
      "KBLN,MTKGNNandLiteralEarehighlightedinboldtext.Onlynumericalliteralsareusedintheexperiments.\n",
      "FB15k\n",
      "Models MR MRR Hits@1 Hits@3 Hits@10\n",
      "DistMult 108 0.671 0.589 0.723 0.818\n",
      "ComplEx 127 0.695 0.618 0.744 0.833\n",
      "ConvE 49 0.692 0.596 0.760 0.853\n",
      "KBLN[Garcia-DuranandNiepert,2017] 129 0.739 0.668 0.788 0.859\n",
      "MTKGNN[Tayetal.,2017] 87 0.669 0.586 0.722 0.82\n",
      "DistMult-LiteralE 68 0.676 0.589 0.733 0.825\n",
      "ComplEx-LiteralE 80 0.746 0.686 0.782 0.853\n",
      "ConvE-LiteralE 43 0.733 0.656 0.785 0.863\n",
      "FB15k-237\n",
      "Models MR MRR Hits@1 Hits@3 Hits@10\n",
      "DistMult 633 0.282 0.203 0.309 0.438\n",
      "ComplEx 652 0.290 0.212 0.317 0.445\n",
      "ConvE 297 0.313 0.228 0.344 0.479\n",
      "KBLN[Garcia-DuranandNiepert,2017] 358 0.301 0.215 0.333 0.468\n",
      "MTKGNN[Tayetal.,2017] 532 0.285 0.204 0.312 0.445\n",
      "DistMult-LiteralE 280 0.317 0.232 0.348 0.483\n",
      "ComplEx-LiteralE 357 0.305 0.222 0.336 0.466\n",
      "ConvE-LiteralE 255 0.303 0.219 0.33 0.471\n",
      "YAGO3-10\n",
      "Models MR MRR Hits@1 Hits@3 Hits@10\n",
      "DistMult 2943 0.466 0.377 0.514 0.653\n",
      "ComplEx 3768 0.493 0.411 0.536 0.649\n",
      "ConvE 2141 0.505 0.422 0.554 0.660\n",
      "KBLN[Garcia-DuranandNiepert,2017] 2666 0.487 0.405 0.531 0.642\n",
      "MTKGNN[Tayetal.,2017] 2970 0.481 0.398 0.527 0.634\n",
      "DistMult-LiteralE 1642 0.479 0.4 0.525 0.627\n",
      "ComplEx-LiteralE 2508 0.485 0.412 0.527 0.618\n",
      "ConvE-LiteralE 1037 0.525 0.448 0.572 0.659\n",
      "Table4: ThelinkpredictionperformanceofLiteralEemployinga Table 5: Link prediction results for DistMult-LiteralE on FB15k-\n",
      "simplelineartransformationg. 237,withbothnumericalandtextliterals. “DM”and“L”standfor\n",
      "lin\n",
      "DistMult and LiteralE respectively, while “N” and “T” denote the\n",
      "usageofnumericalandtextliterals,respectively.\n",
      "Datasets Functions MRR Hits@1 Hits@10\n",
      "FB15k DistMult-g 0.583 0.476 0.771\n",
      "lin Models MRR Hits@1 MRRIncr.\n",
      "ComplEx-g 0.765 0.705 0.871\n",
      "lin\n",
      "ConvE-g 0.66 0.556 0.836 DM 0.241 0.155 -\n",
      "lin\n",
      "DM-L(N) 0.317 0.232 0+31.54%\n",
      "FB15k-237 DistMult-g 0.314 0.228 0.483\n",
      "lin DM-L(N+T) 0.32 0.234 +32.78%\n",
      "ComplEx-g 0.299 0.214 0.467\n",
      "lin\n",
      "ConvE-g 0.314 0.228 0.483\n",
      "lin\n",
      "YAGO3-10 DistMult-g 0.504 0.422 0.653 variantbasedonasimple(butstilllearnable)lineartransfor-\n",
      "lin\n",
      "ComplEx-g\n",
      "lin\n",
      "0.509 0.433 0.653 mation, dubbed g lin. That is, g\n",
      "lin\n",
      ": RH × RNd → RH is\n",
      "ConvE-g\n",
      "lin\n",
      "0.506 0.422 0.664 defined by e,l (cid:55)→ WT[e,l], where W ∈ RH+Nd×H is a\n",
      "learnableweightmatrix. TheresultsarepresentedinTable4\n",
      "(cf. Table3).\n",
      "plExandConvEforallthreedatasets(ourresultsaremostly The proposed g leads to better results than g in 5 out\n",
      "lin\n",
      "comparableorslightlybetterandinsomecaseworse). This of 9 experiments. While LiteralE with g provides a consis-\n",
      "could be attributed to the hyperparameter tuning performed tentperformanceimprovementforallbasemodels,DistMult-\n",
      "in[Dettmersetal.,2018].\n",
      "g shows a decreased performance compared to DistMult\n",
      "lin\n",
      "on FB15k. This might be explained by the fact that – as\n",
      "5.4 Evaluation\n",
      "[ToutanovaandChen,2015]alreadyreported–FB15kcon-\n",
      "For the evaluation of the performance of the different meth- tainstriplesinthetestsetthathaveaninverseanalog(i.e.the\n",
      "odsonthelinkpredictiontask,wefollowthestandardsetup tripleresultingfromchangingthepositionofsubjectandob-\n",
      "used in other studies. For each triple (e i,e j,r k) in the test jectentity)inthetrainingset. Thepredictionforsuchtriples\n",
      "set,wegenerateasetofcorruptedtriplesbyeitherreplacing cangetdifficultiftheinversehasadifferentlabel. Sincethe\n",
      "thesubjectentitye i ortheobjectentitye j withanyotheren- vanilla DistMult already has difficulties in modeling asym-\n",
      "titye(cid:48) ∈E. Wefurthercomputethescoresofthesecorrupted metric relations on FB15k, adding literals using a naive g\n",
      "lin\n",
      "triples along with the score of the true triple. To evaluate might only introduce noise, resulting in even lower perfor-\n",
      "themodel,werankalltripleswithrespecttotheirscoresand mance. Ontheotherhand,g leadstobetterresultsthangin\n",
      "lin\n",
      "use the following standard evaluation metrics: Mean Rank combinationwithComplExonFB15k.\n",
      "(MR),MeanReciprocalRank(MRR),Hits@1,Hits@3,and In general, the results show that for performance-\n",
      "Hits@10. maximizationpurpose, itmakessensetoinvestigatetheper-\n",
      "formanceofLiteralE incombinationwithdifferenttransfor-\n",
      "6 Results mation functions. Given the right choice of transformation\n",
      "functionforincorporatingliterals, LiteralEalwaysimproves\n",
      "6.1 LinkPrediction\n",
      "theperformanceofthebasemodel.\n",
      "The results of our experiments for link prediction are sum-\n",
      "marized in Table 3. In general, LiteralE improves the base\n",
      "6.3 ExperimentwithTextLiterals\n",
      "models (DistMult, ComplEx, and ConvE) significantly. For\n",
      "instance,wefoundthatimplementingLiteralEontopofDist- LiteralE, as described in Section 3 can easily be extended\n",
      "MultimprovestheMRRscoreby0.74%,12.41%,and2.7% to other types of literals, e.g. text and images. In this sec-\n",
      "fortheFB15k,FB15k-237,andYAGO3-10dataset,respec- tionthisisbrieflydemonstratedfortextliterals. First, letus\n",
      "tively. We also observed that the improvements brought by assume that text literals are represented by vectors in RNt,\n",
      "LiteralE when combined with ComplEx and ConvE are not i.e. as resulting from document embedding techniques [Le\n",
      "as impressive as for DistMult, which might be attributed to and Mikolov, 2014].8 Subsequently, let us redefine g to be\n",
      "the fact that these base models already achieve higher per- a function mapping RH ×RNd ×RNt to RH. Specifically,\n",
      "formance than DistMult. Compared to other methods that weredefineW h(eq.(5))tobeinRH+Nd+Nt×H andemploy\n",
      "incorporate literals, namely KBLN and MTKGNN, LiteralE an additional gating weight matrix W\n",
      "zt\n",
      "∈ RNt×H to han-\n",
      "achievesacompetitiveorevenbetterperformanceinourex- dletheadditionaltextliteral. Note,thatthissimpleextension\n",
      "periments. Moreover, note that, LiteralE directly and ex- schemecanbeusedtoextendLiteralEtoincorporateliterals\n",
      "plicitlymodifiestheembeddingvectors,whereasKBLNand ofanyothertype(e.g.imageliterals)aslongasthoseliterals\n",
      "MTKGNNdonot. Thus,LiteralEembeddingscouldbemore areencodedasvectorsinRN,forsomeN.\n",
      "useful for tasks other than link prediction. This will be dis- TheresultsforextendingDistMult-LiteralEwiththeenti-\n",
      "cussedfurtherinSection6.4. ties’ text literals (i.e. the entity description) are presented in\n",
      "Table 5. We found that incorporating text literals results in\n",
      "6.2 ComparisontoaSimpleLiteralEBaseline\n",
      "Tovalidateourchoiceofthefunctiong,wecomparetheper- 8WeusespaCy’spretrainedGloVeembeddingmodel. Available\n",
      "formance of LiteralE with the g proposed in Section 3 to its athttps://spacy.io\n",
      "Table6: ComparisonofnearestneighborsofselectedentitiesfromFB15k-237embeddedin(i)DistMult’slatentspace,(ii)KBLN’slatent\n",
      "space,(iii)MTKGNN’slatentspace,(iv)theliteralspace,whereeachentityisrepresentedonlybyitsliterals,and(v)theDisMult-LiteralE’s\n",
      "latentspace.\n",
      "Entity Methods NearestNeighbors\n",
      "NorthAmerica DistMult LatinAmerica,Pyrenees,Americas\n",
      "KBLN HouseofHanover,HouseofStuart,HouseofRomanov\n",
      "MTKGNN LatinAmerica,PanamaCity,Pyrenees\n",
      "Num. lits. only SovietUnion,LatinAmerica,Africa\n",
      "LiteralE Americas,LatinAmerica,Asia\n",
      "Philippines DistMult Peru,Thailand,Kuwait\n",
      "KBLN HouseofRomanov,HouseofHanover,HouseofStuart\n",
      "MTKGNN Thailand,Kuwait,Peru\n",
      "Num. lits. only Peru,Poland,Pakistan\n",
      "LiteralE Thailand,Taiwan,Greece\n",
      "RomanRepublic DistMult RepublicofVenice,IsraelDefenseForce,ByzantineEmpire\n",
      "KBLN RepublicofVenice,Carthage,Retinol\n",
      "MTKGNN RepublicofVenice,Carthage,NorthIsland\n",
      "Num. lits. only Alexandria,Yerevan,Cologne\n",
      "LiteralE RomanEmpire,KingdomofGreece,ByzantineEmpire\n",
      "afurtherincreaseofthelinkpredictionperformanceofDist- AmericaisclosetoPyreneesandRoman Repulicis\n",
      "MultonFB15k-237. close to North Island. This findings demonstrates the\n",
      "advantageofincorporatingliteralsontheembeddinglevel(as\n",
      "6.4 NearestNeighborAnalysis\n",
      "donebyLiterelE)overincorporatingthematthescoreorloss\n",
      "Forafurtherqualitativeinvestigation,wepresentthenearest function(asdonebyKBLNandMTKGNN,respectively).\n",
      "neighbors of some entities in the space of literals, the latent Wheninspectingthenearestneighborhoodofthesameen-\n",
      "space learned by (i) DistMult, (ii) KBLN, (iii) MTKGNN, tities when represented only by their literal vectors, it be-\n",
      "and(iv)DisMult-LiteralEinTable6.9 comesclearthatthesevectorsthemselvesarealreadycontain-\n",
      "In the embedding space of DistMult, geographical en- ingusefulinformationindicatingtheclosenessofsimilaren-\n",
      "tities such as North America and Philippines are tities. Forexample,geographicalentitieshavelongitude\n",
      "close to other entities of the same type. However, these and latitude literals, while city, nation, and empire enti-\n",
      "neighboringentitiesarenotintuitive,e.g.North America ties have date founded and date dissolved literals,\n",
      "is close to Pyrenees, whereas Philippines is close which can explain the closeness of two entities given only\n",
      "to Peru and Kuwait. When we inspected the embed- their literal vectors. Note however, that the nearest neigh-\n",
      "ding space of DistMult-LiteralE that also takes literals in- bours in the literal space do not coincide with and are less\n",
      "formation into account, these nearest neighbors (shown in informative than the nearest neighbours in the LiteralE em-\n",
      "bold font in Table 6) become more intuitive, i.e they con- beddingspace.\n",
      "sist of entities geographically close to each others. Fur- All in all, our observations suggest that integrating the\n",
      "thermore, we found that DistMult-LiteralE’s embeddings literal information into entity embeddings indeed improves\n",
      "showclearqualitativeadvantagecomparedtothatofvanilla their quality, which makes LiteralE embeddings promising\n",
      "DistMult also for entities from other types, e.g. compar- forentityresolutionandclusteringtasks.\n",
      "ing the nearest neighbors of Roman Republic which is\n",
      "of type ‘empire’. In contrast, KBLN’s embeddings tend 7 ConclusionandFutureWork\n",
      "to be close to the embeddings of unrelated entities: both\n",
      "North AmericaandPhilippinesareclosetotheen- Inthispaper,weintroducedLiteralE:asimplemethodtoin-\n",
      "tities House of Romanov, House of Hanover, and corporate literals into latent feature methods for knowledge\n",
      "House of Stuart, while Roman Republic is close graph analysis. It corresponds to a learnable function that\n",
      "to Retinol. Similarly, the embeddings of MTKGNN are mergesentityembeddingswiththeirliteralinformationavail-\n",
      "alsoclosetotheembeddingofunrelatedentities,e.g.,North able in the knowledge graph. The resulting literal-enriched\n",
      "latent features can replace the vanilla entity embedding in\n",
      "9ThebasemodelforallofthesemethodsisDistMult. any latent feature method, without any further modification.\n",
      "Therefore, LiteralE can be seen as an universal extension [Mahdisoltanietal.,2014] Farzaneh Mahdisoltani, Joanna Biega,\n",
      "module. Weshowedthataugmentingvariousstate-of-the-art andFabianSuchanek. Yago3: Aknowledgebasefrommultilin-\n",
      "models (DistMult, ComplEx, and ConvE) with LiteralE sig- gualwikipedias. In7thBiennialConferenceonInnovativeData\n",
      "nificantlyimprovestheirlinkpredictionperformance. More- SystemsResearch.CIDRConference,2014.\n",
      "over, as exemplarily demonstrated for text literals, LiteralE [Nickeletal.,2016] Maximilian Nickel, Kevin Murphy, Volker\n",
      "canbeeasilyextendedothertypesofliterals. Infuturework, Tresp, and Evgeniy Gabrilovich. A review of relational ma-\n",
      "LiteralEshallbefurtherbeextendedtoaccommodateliterals chinelearningforknowledgegraphs. ProceedingsoftheIEEE,\n",
      "from the image domain. This can be achieved by extracting 104(1):11–33,2016.\n",
      "latentrepresentationsfromimages(forexamplewithconvo- [Pezeshkpouretal.,2017] Pouya Pezeshkpour, CA Irvine, Liyan\n",
      "lutional neural networks), and providing them as additional Chen,andSameerSingh.Embeddingmultimodalrelationaldata.\n",
      "inputs to LiteralE for merging them with the vanilla entitiy 2017.\n",
      "embeddings.Furthermore,ourfindingthatLiteralEimproves [RuobingXie,2017] Huanbo Luan Maosong Sun Ruobing Xie,\n",
      "the quality of the entity embeddings makes it a promising ZhiyuanLiu. Image-embodiedknowledgerepresentationlearn-\n",
      "candidateforimprovingothertasksinthefieldofknowledge ing. InProceedingsoftheTwenty-SixthInternationalJointCon-\n",
      "graphanalysis,suchasentityresolutionandknowledgegraph ference on Artificial Intelligence, IJCAI-17, pages 3140–3146,\n",
      "clustering. 2017.\n",
      "[Sunetal.,2017] Zequn Sun, Wei Hu, and Chengkai Li. Cross-\n",
      "References lingual entity alignment via joint attribute-preserving embed-\n",
      "ding.InInternationalSemanticWebConference,pages628–644.\n",
      "[Bollackeretal.,2008] KurtBollacker,ColinEvans,PraveenPari-\n",
      "Springer,2017.\n",
      "tosh,TimSturge,andJamieTaylor. Freebase: acollaboratively\n",
      "createdgraphdatabaseforstructuringhumanknowledge.InPro- [Tayetal.,2017] Yi Tay, Luu Anh Tuan, Minh C Phan, and\n",
      "ceedingsofthe2008ACMSIGMODinternationalconferenceon SiuCheungHui. Multi-taskneuralnetworkfornon-discreteat-\n",
      "Managementofdata,pages1247–1250.AcM,2008. tribute prediction in knowledge graphs. In Proceedings of the\n",
      "2017ACMonConferenceonInformationandKnowledgeMan-\n",
      "[Bordesetal.,2013] Antoine Bordes, Nicolas Usunier, Alberto\n",
      "agement,pages1029–1038.ACM,2017.\n",
      "Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Trans-\n",
      "lating embeddings for modeling multi-relational data. In Ad- [Thomaetal.,2017] SteffenThoma,AchimRettinger,andFabian\n",
      "vances in neural information processing systems, pages 2787– Both. Towardsholisticconceptrepresentations: Embeddingre-\n",
      "2795,2013. lationalknowledge,visualattributes,anddistributionalwordse-\n",
      "mantics. InInternationalSemanticWebConference,pages694–\n",
      "[Choetal.,2014] Kyunghyun Cho, Bart van Merrienboer, Caglar\n",
      "710.Springer,2017.\n",
      "Gulcehre,DzmitryBahdanau,FethiBougares,HolgerSchwenk,\n",
      "andYoshuaBengio. Learningphraserepresentationsusingrnn [ToutanovaandChen,2015] Kristina Toutanova and Danqi Chen.\n",
      "encoder–decoderforstatisticalmachinetranslation. InProceed- Observedversuslatentfeaturesforknowledgebaseandtextin-\n",
      "ings of the 2014 Conference on Empirical Methods in Natural ference. InProceedingsofthe3rdWorkshoponContinuousVec-\n",
      "LanguageProcessing(EMNLP),pages1724–1734,Doha,Qatar, torSpaceModelsandtheirCompositionality,pages57–66,2015.\n",
      "October2014.AssociationforComputationalLinguistics. [Toutanovaetal.,2015] Kristina Toutanova, Danqi Chen, Patrick\n",
      "[Dettmersetal.,2018] Tim Dettmers, Minervini Pasquale, Stene- Pantel,HoifungPoon,PallaviChoudhury,andMichaelGamon.\n",
      "torpPontus,andSebastianRiedel. Convolutional2dknowledge Representing text for joint embedding of text and knowledge\n",
      "graphembeddings. InProceedingsofthe32thAAAIConference bases. InEMNLP,volume15,pages1499–1509,2015.\n",
      "onArtificialIntelligence,February2018. [Trouillonetal.,2016] The´oTrouillon,JohannesWelbl,Sebastian\n",
      "[Dongetal.,2014] Xin Dong, Evgeniy Gabrilovich, Geremy Riedel, E´ricGaussier, andGuillaumeBouchard. Complexem-\n",
      "Heitz,WilkoHorn,NiLao,KevinMurphy,ThomasStrohmann, beddingsforsimplelinkprediction. InInternationalConference\n",
      "Shaohua Sun, and Wei Zhang. Knowledge vault: A web-scale onMachineLearning,pages2071–2080,2016.\n",
      "approach to probabilistic knowledge fusion. In Proceedings of\n",
      "[Tuetal.,2017] CunchaoTu,HanLiu,ZhiyuanLiu,andMaosong\n",
      "the20thACMSIGKDDinternationalconferenceonKnowledge\n",
      "Sun. Cane:Context-awarenetworkembeddingforrelationmod-\n",
      "discoveryanddatamining,pages601–610.ACM,2014.\n",
      "eling. In Proceedings of the 55th Annual Meeting of the Asso-\n",
      "[Garcia-DuranandNiepert,2017] Alberto Garcia-Duran and ciationforComputationalLinguistics(Volume1: LongPapers),\n",
      "Mathias Niepert. KBLRN: End-to-end learning of knowledge volume1,pages1722–1731,2017.\n",
      "base representations with latent, relational, and numerical\n",
      "[WuandWang,2018] YanrongWuandZhichunWang.Knowledge\n",
      "features. arXivpreprintarXiv:1709.04676,2017.\n",
      "graphembeddingwithnumericattributesofentities.InProceed-\n",
      "[KingmaandBa,2015] DiederikPKingmaandJimmyBa. Adam: ingsofTheThirdWorkshoponRepresentationLearningforNLP,\n",
      "Amethodforstochasticoptimization. In3rdInternationalCon- pages132–136,2018.\n",
      "ferenceforLearningRepresentations.ICLR,2015.\n",
      "[Xieetal.,2016] RuobingXie,ZhiyuanLiu,JiaJia,HuanboLuan,\n",
      "[LeandMikolov,2014] QuocLeandTomasMikolov. Distributed andMaosongSun. Representationlearningofknowledgegraphs\n",
      "representations of sentences and documents. In International withentitydescriptions. InAAAI,pages2659–2665,2016.\n",
      "conferenceonmachinelearning,pages1188–1196,2014.\n",
      "[Xuetal.,2016] JiachengXu, KanChen, XipengQiu,andXuan-\n",
      "[Lehmannetal.,2015] Jens Lehmann, Robert Isele, Max Jakob, jingHuang. Knowledgegraphrepresentationwithjointlystruc-\n",
      "AnjaJentzsch,DimitrisKontokostas,PabloNMendes,Sebastian turalandtextualencoding. InIJCAI,2016.\n",
      "Hellmann, Mohamed Morsey, Patrick Van Kleef, So¨ren Auer,\n",
      "et al. DBpedia–a large-scale, multilingual knowledge base ex-\n",
      "tractedfromwikipedia. SemanticWeb,6(2):167–195,2015.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  27992,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Pouya Pezeshkpour', 'CA Irvine', 'Liyan Chen', 'Sameer Singh']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Adversarial Contrastive Estimation\n",
      "AvishekJoeyBose1,2,∗† HuanLing1,2,∗† YanshuaiCao1,∗\n",
      "1BorealisAI 2UniversityofToronto\n",
      "{joey.bose,huan.ling}@mail.utoronto.ca\n",
      "{yanshuai.cao}@borealisai.com\n",
      "Abstract modelingneed,ascertainassumptionsarebestex-\n",
      "pressed as some score or energy in margin based\n",
      "Learning by contrasting positive and neg- or un-normalized probability models (Smith and\n",
      "ativesamplesisageneralstrategyadopted Eisner, 2005). For example, modeling entity re-\n",
      "by many methods. Noise contrastive lationsastranslationsorvariantsthereofinavec-\n",
      "estimation (NCE) for word embeddings torspacenaturallyleadstoadistance-basedscore\n",
      "andtranslatingembeddingsforknowledge tobeminimizedforobservedentity-relation-entity\n",
      "graphs are examples in NLP employing triplets(Bordesetal.,2013).\n",
      "this approach. In this work, we view Given a scoring function, the gradient of the\n",
      "contrastive learning as an abstraction of model’sparametersonobservedpositiveexamples\n",
      "all such methods and augment the neg- can be readily computed, but the negative phase\n",
      "ative sampler into a mixture distribution requiresadesigndecisiononhowtosampledata.\n",
      "containing an adversarially learned sam- In noise contrastive estimation for word embed-\n",
      "pler. The resulting adaptive sampler finds dings, a negative example is formed by replacing\n",
      "harder negative examples, which forces acomponentofapositivepairbyrandomlyselect-\n",
      "the main model to learn a better represen- ingasampledwordfromthevocabulary,resulting\n",
      "tation of the data. We evaluate our pro- in a fictitious word-context pair which would be\n",
      "posalonlearningwordembeddings,order unlikelytoactuallyexistinthedataset. Thisnega-\n",
      "embeddingsandknowledgegraphembed- tive sampling by corruption approach is also used\n",
      "dingsandobservebothfasterconvergence inlearningknowledgegraphembeddings(Bordes\n",
      "andimprovedresultsonmultiplemetrics. etal.,2013;Linetal.,2015;Jietal.,2015;Wang\n",
      "et al., 2014; Trouillon et al., 2016; Yang et al.,\n",
      "1 Introduction 2014; Dettmers et al., 2017), order embeddings\n",
      "(Vendrovetal.,2016),captiongeneration(Daiand\n",
      "Many models learn by contrasting losses on ob-\n",
      "Lin,2017),etc.\n",
      "served positive examples with those on some fic-\n",
      "Typicallythecorruptiondistributionisthesame\n",
      "titiousnegativeexamples,tryingtodecreasesome\n",
      "for all inputs like in skip-gram or CBOW NCE,\n",
      "score on positive ones while increasing it on neg-\n",
      "rather than being a conditional distribution that\n",
      "ative ones. There are multiple reasons why such\n",
      "takes into account information about the input\n",
      "contrastive learning approach is needed. Com-\n",
      "sampleunderconsideration. Furthermore,thecor-\n",
      "putational tractability is one. For instance, in-\n",
      "ruption process usually only encodes a human\n",
      "steadofusingsoftmaxtopredictawordforlearn-\n",
      "prior as to what constitutes a hard negative sam-\n",
      "ing word embeddings, noise contrastive estima-\n",
      "ple,ratherthanbeinglearnedfromdata. Forthese\n",
      "tion (NCE) (Dyer, 2014; Mnih and Teh, 2012)\n",
      "two reasons, the simple fixed corruption process\n",
      "can be used in skip-gram or CBOW word em-\n",
      "often yields only easy negative examples. Easy\n",
      "bedding models (Gutmann and Hyva¨rinen, 2012;\n",
      "negativesaresub-optimalforlearningdiscrimina-\n",
      "Mikolov et al., 2013; Mnih and Kavukcuoglu,\n",
      "tive representation as they do not force the model\n",
      "2013; Vaswani et al., 2013). Another reason is\n",
      "tofindcriticalcharacteristicsofobservedpositive\n",
      "∗authorscontributedequally data, which has been independently discovered in\n",
      "†WorkdonewhileauthorwasaninternatBorealisAI applications outside NLP previously (Shrivastava\n",
      "8102\n",
      "guA\n",
      "2\n",
      "]LC.sc[\n",
      "3v24630.5081:viXra\n",
      "etal.,2016). Evenifhardnegativesareoccasion- with respect to some joint distribution over pos-\n",
      "ally reached, the infrequency means slow conver- itive and negative samples. Furthermore, by\n",
      "gence. Designingamoresophisticatedcorruption the law of total expectation, and the fact that\n",
      "process could be fruitful, but requires costly trial- given x+, the negative sampling is not depen-\n",
      "and-errorbyahumanexpert. dent on the positive label, i.e. p(y+,y−|x+) =\n",
      "In this work, we propose to augment the sim- p(y+|x+)p(y−|x+),Eq.1canbere-writtenas\n",
      "plecorruptionnoiseprocessinvariousembedding\n",
      "models with an adversarially learned conditional E p(x+)[E p(y+|x+)p(y−|x+)l ω(x+,y+,y−)] (2)\n",
      "distribution, forming a mixture negative sampler\n",
      "that adapts to the underlying data and the em- Separableloss\n",
      "bedding model training progress. The resulting Inthecasewherethelossdecomposesintoasum\n",
      "methodisreferredtoasadversarialcontrastivees- of scores on positive and negative tuples such as\n",
      "timation (ACE). The adaptive conditional model l (x+,y+,y−) = s (x+,y+)−s˜ (x+,y−),then\n",
      "ω ω ω\n",
      "engagesinaminimaxgamewiththeprimaryem- Expression.2becomes\n",
      "beddingmodel,muchlikeinGenerativeAdversar-\n",
      "ial Networks (GANs) (Goodfellow et al., 2014a), E [E s (x,y)−E s˜ (x,y)]\n",
      "p+(x) p+(y|x) ω p−(y|x) ω\n",
      "where a discriminator net (D), tries to distinguish (3)\n",
      "samples produced by a generator (G) from real where we moved the + and − to p for notational\n",
      "data(Goodfellowetal.,2014b). InACE,themain brevity. Learning by stochastic gradient descent\n",
      "model learns to distinguish between a real posi- aims to adjust ω to pushing down s (x,y) on\n",
      "ω\n",
      "tive example and a negative sample selected by samples from p+ while pushing up s˜ (x,y) on\n",
      "ω\n",
      "themixtureofafixedNCEsamplerandanadver- samples from p−. Note that for generality, the\n",
      "sarial generator. The main model and the genera- scoringfunctionfornegativesamples,denotedby\n",
      "tor takes alternating turns to update their parame- s˜, could be slightly different from s. For in-\n",
      "ω ω\n",
      "ters. In fact, our method can be viewed as a con- stance, s˜could contain a margin as in the case of\n",
      "ditionalGAN(MirzaandOsindero,2014)ondis- OrderEmbeddingsinSec.4.2.\n",
      "creteinputs,withamixturegeneratorconsistingof\n",
      "Nonseparableloss\n",
      "a learned and a fixed distribution, with additional\n",
      "techniques introduced to achieve stable and con- Eq. 1 is the general form that we would like to\n",
      "vergenttrainingofembeddingmodels. consider because for certain problems, the loss\n",
      "InourproposedACEapproach,theconditional function cannot be separated into sums of terms\n",
      "sampler finds harder negatives than NCE, while containing only positive (x+,y+) and terms with\n",
      "being able to gracefully fall back to NCE when- negatives (x+,y−). An example of such a non-\n",
      "ever the generator cannot find hard negatives. We separable loss is the triplet ranking loss (Schroff\n",
      "demonstratetheefficacyandgeneralityofthepro- et al., 2015): l ω = max(0,η + s ω(x+,y+) −\n",
      "posed method on three different learning tasks, s ω(x+,y−)), which does not decompose due to\n",
      "word embeddings (Mikolov et al., 2013), order therectification.\n",
      "embeddings(Vendrovetal.,2016)andknowledge\n",
      "Noisecontrastiveestimation\n",
      "graphembeddings(Jietal.,2015).\n",
      "The typical NCE approach in tasks such as word\n",
      "2 Method embeddings (Mikolov et al., 2013), order embed-\n",
      "dings(Vendrovetal.,2016),andknowledgegraph\n",
      "2.1 Background: contrastivelearning\n",
      "embeddingscanbeviewedasaspecialcaseofEq.\n",
      "In the most general form, our method applies to 2 by taking p(y−|x+) to be some unconditional\n",
      "supervised learning problems with a contrastive\n",
      "p (y).\n",
      "nce\n",
      "objectiveofthefollowingform:\n",
      "Thisleadstoefficientcomputationduringtrain-\n",
      "L(ω) = E l (x+,y+,y−) (1) ing,however,p nce(y)sacrificesthesamplingeffi-\n",
      "p(x+,y+,y−) ω\n",
      "ciencyoflearningasthenegativesproducedusing\n",
      "where l (x+,y+,y−) captures both the model afixeddistributionarenottailoredtowardx+,and\n",
      "ω\n",
      "with parameters ω and the loss that scores a asaresultarenotnecessarilyhardnegativeexam-\n",
      "positive tuple (x+,y+) against a negative one ples. Thus, the model is not forced to discover\n",
      "(x+,y−). E (.) denotes expectation discriminative representation of observed positive\n",
      "p(x+,y+,y−)\n",
      "data. As training progresses, more and more neg- Instead, we use the REINFORCE (Williams,\n",
      "ativeexamplesarecorrectlylearned,theprobabil- 1992)gradientestimatorfor∇ L(θ,x):\n",
      "θ\n",
      "ityofdrawingahardnegativeexamplediminishes\n",
      "(1−λ)E(cid:2) −l (x,y+,y−)∇ log(g (y−|x))(cid:3) (6)\n",
      "further,causingslowconvergence. ω θ θ\n",
      "where the expectation E is with respect to\n",
      "2.2 Adversarialmixturenoise\n",
      "p(y+,y−|x) = p(y+|x)g (y−|x), and the dis-\n",
      "θ\n",
      "To remedy the above mentioned problem of a criminatorlossl (x,y+,y−)actsasthereward.\n",
      "ω\n",
      "fixed unconditional negative sampler, we propose\n",
      "With a separable loss, the (conditional) value\n",
      "toaugmentitintoamixtureone,λp (y)+(1−\n",
      "nce functionoftheminimaxgameis:\n",
      "λ)g (y|x), where g is a conditional distribution\n",
      "θ θ\n",
      "with a learnable parameter θ and λ is a hyper- L(ω,θ;x) = E s (x,y)\n",
      "p+(y|x) ω\n",
      "parameter. The objective in Expression. 2 can −E s˜ (x,y)−E s˜ (x,y) (7)\n",
      "thenbewrittenas(conditionedonxfornotational\n",
      "pnce(y) ω g θ(y|x) ω\n",
      "brevity): and only the last term depends on the generator\n",
      "parameterω. Hence,withaseparableloss,there-\n",
      "L(ω,θ;x) = λE l (x,y+,y−) wardis−s˜(x+,y−). Thisreductiondoesnothap-\n",
      "p(y+|x)pnce(y−) ω\n",
      "+(1−λ)E l (x,y+,y−) (4) penwithanon-separableloss,andwehavetouse\n",
      "p(y+|x)g (y−|x) ω\n",
      "θ l (x,y+,y−).\n",
      "ω\n",
      "Welearn(ω,θ)inaGAN-styleminimaxgame:\n",
      "2.4 Entropyandtrainingstability\n",
      "minmaxV(ω,θ) = minmaxE L(ω,θ;x) GAN training can suffer from instability and de-\n",
      "p+(x)\n",
      "ω θ ω θ generacy where the generator probability mass\n",
      "(5)\n",
      "collapses to a few modes or points. Much work\n",
      "The embedding model behind l (x,y+,y−) is\n",
      "ω has been done to stabilize GAN training in the\n",
      "similar to the discriminator in (conditional) GAN\n",
      "continuous case (Arjovsky et al., 2017; Gulrajani\n",
      "(or critic in Wasserstein (Arjovsky et al., 2017)\n",
      "et al., 2017; Cao et al., 2018). In ACE, if the\n",
      "or Energy-based GAN (Zhao et al., 2016), while\n",
      "generator g probability mass collapses to a few\n",
      "g (y|x) acts as the generator. Henceforth, we θ\n",
      "θ candidates, then after the discriminator success-\n",
      "willusethetermdiscriminator(D)andembedding\n",
      "fullylearnsaboutthesenegatives,g cannotadapt\n",
      "modelinterchangeably,andrefertog asthegen- θ\n",
      "θ to select new hard negatives, because the REIN-\n",
      "erator.\n",
      "FORCEgradientestimatorEq.6reliesong being\n",
      "θ\n",
      "able to explore other candidates during sampling.\n",
      "2.3 Learningthegenerator\n",
      "Therefore,iftheg probabilitymasscollapses,in-\n",
      "θ\n",
      "ThereisoneimportantdistinctiontotypicalGAN:\n",
      "stead of leading to oscillation as in typical GAN,\n",
      "g (y|x)definesacategoricaldistributionoverpos-\n",
      "θ themin-maxgameinACEreachesanequilibrium\n",
      "sibleyvalues,andsamplesaredrawnaccordingly;\n",
      "wherethediscriminatorwinsandg cannolonger\n",
      "θ\n",
      "in contrast to typical GAN over continuous data\n",
      "adapt,thenACEfallsbacktoNCEsincethenega-\n",
      "space such as images, where samples are gener-\n",
      "tivesamplerhasanothermixturecomponentfrom\n",
      "ated by an implicit generative model that warps\n",
      "NCE.\n",
      "noise vectors into data points. Due to the discrete\n",
      "ThisbehaviorofgracefullyfallingbacktoNCE\n",
      "samplingstep,g cannotlearnbyreceivinggradi-\n",
      "θ is more desirable than the alternative of stalled\n",
      "ent through the discriminator. One possible solu- training if p−(y|x) does not have a simple p\n",
      "nce\n",
      "tion is to use the Gumbel-softmax reparametriza-\n",
      "mixturecomponent. However, wewouldstilllike\n",
      "tion trick (Jang et al., 2016; Maddison et al.,\n",
      "to avoid such collapse, as the adversarial samples\n",
      "2016),whichgivesadifferentiableapproximation.\n",
      "provide greater learning signals than NCE sam-\n",
      "However,thisdifferentiabilitycomesatthecostof\n",
      "ples. To this end, we propose to use a regularizer\n",
      "drawing N Gumbel samples per each categorical\n",
      "to encourage the categorical distribution g (y|x)\n",
      "θ\n",
      "sample,whereN isthenumberofcategories. For\n",
      "tohavehighentropy. Inordertomakethethereg-\n",
      "word embeddings, N is the vocabulary size, and\n",
      "ularizerinterpretableanditshyperparameterseasy\n",
      "for knowledge graph embeddings, N is the num-\n",
      "totune,wedesignthefollowingform:\n",
      "berofentities,bothleadingtoinfeasiblecomputa-\n",
      "tionalrequirements. R (x) = max(0,c−H(g (y|x))) (8)\n",
      "ent θ\n",
      "whereH(g (y|x))istheentropyofthecategorical self-criticalbaselinemethod(Rennieetal.,2016),\n",
      "θ\n",
      "distributiong (y|x),andc = log(k)istheentropy where the baseline is b(x) = l (y+,y(cid:63),x), or\n",
      "θ ω\n",
      "of a uniform distribution over k choices, and k is b(x) = −s˜ (y(cid:63),x)intheseparablelosscase,and\n",
      "ω\n",
      "a hyper-parameter. Intuitively, R expresses the y(cid:63) = argmax g (y |x). In other words, the base-\n",
      "ent i θ i\n",
      "priorthatthegeneratorshouldspreaditsmassover lineistherewardofthemostlikelysampleaccord-\n",
      "morethank choicesforeachx. ingtothegenerator.\n",
      "2.5 Handlingfalsenegatives 2.7 Improvingexplorationing by\n",
      "θ\n",
      "Duringnegativesampling,p−(y|x)couldactually leveragingNCEsamples\n",
      "produce y that forms a positive pair that exists in\n",
      "In Sec. 2.4 we touched on the need for sufficient\n",
      "the training set, i.e., a false negative. This possi-\n",
      "exploration in g. It is possible to also leverage\n",
      "θ\n",
      "bility exists in NCE already, but since p is not\n",
      "nce negative samples from NCE to help the gener-\n",
      "adaptive,theprobabilityofsamplingafalsenega-\n",
      "ator learn. This is essentially off-policy explo-\n",
      "tive is low. Hence in NCE, the score on this false\n",
      "ration in reinforcement learning since NCE sam-\n",
      "negative (true observation) pair is pushed up less\n",
      "plesarenotdrawnaccordingtog (y|x). Thegen-\n",
      "θ\n",
      "inthenegativetermthaninthepositiveterm.\n",
      "erator learning can use importance re-weighting\n",
      "However, with the adaptive sampler, g (y|x),\n",
      "ω to leverage those samples. The resulting REIN-\n",
      "falsenegativesbecomeamuchmoresevereissue.\n",
      "FORCE gradient estimator is basically the same\n",
      "g (y|x)canlearntoconcentrateitsmassonafew\n",
      "ω asEq.6exceptthattherewardsarereweightedby\n",
      "false negatives, significantly canceling the learn- g (y−|x)/p (y−), and the expectation is with\n",
      "θ nce\n",
      "ing of those observations in the positive phase. respect to p(y+|x)p (y−). This additional off-\n",
      "nce\n",
      "Theentropyregularizationreducesthisproblemas\n",
      "policy learning term provides gradient informa-\n",
      "itforcesthegeneratortospreaditsmass,hencere- tionforgeneratorlearningifg (y−|x)isnotzero,\n",
      "θ\n",
      "ducingthechanceofafalsenegative.\n",
      "meaning that for it to be effective in helping ex-\n",
      "To further alleviate this problem, whenever\n",
      "ploration,thegeneratorcannotbecollapsedatthe\n",
      "computationally feasible, we apply an additional\n",
      "first place. Hence, in practice, this term is only\n",
      "two-steptechnique. First,wemaintainahashmap\n",
      "usedtofurtherhelpontopoftheentropyregular-\n",
      "of the training data in memory, and use it to effi-\n",
      "ization,butitdoesnotreplaceit.\n",
      "ciently detect if a negative sample (x+,y−) is an\n",
      "actual observation. If so, its contribution to the 3 RelatedWork\n",
      "lossisgivenazeroweightinωlearningstep. Sec-\n",
      "ond,toupdateθinthegeneratorlearningstep,the Smith and Eisner (2005) proposed contrastive es-\n",
      "reward for false negative samples are replaced by timation as a way for unsupervised learning of\n",
      "a large penalty, so that the REINFORCE gradient log-linearmodelsbytakingimplicitevidencefrom\n",
      "update would steer g away from those samples. user-defined neighborhoods around observed dat-\n",
      "θ\n",
      "The second step is needed to prevent null compu- apoints. Gutmann and Hyva¨rinen (2010) intro-\n",
      "tation where g learns to sample false negatives duced NCE as an alternative to the hierarchical\n",
      "θ\n",
      "which are subsequently ignored by the discrimi- softmax. IntheworksofMnihandTeh(2012)and\n",
      "natorupdateforω. MnihandKavukcuoglu(2013),NCEisappliedto\n",
      "log-bilinear models and Vaswani et al. (2013) ap-\n",
      "2.6 VarianceReduction\n",
      "pliedNCEtoneuralprobabilisticlanguagemodels\n",
      "The basic REINFORCE gradient estimator is (Yoshuaetal.,2003). Comparedtotheseprevious\n",
      "poised with high variance, so in practice one of- NCE methods that rely on simple fixed sampling\n",
      "ten needs to apply variance reduction techniques. heuristics,ACEusesanadaptivesamplerthatpro-\n",
      "The most basic form of variance reduction is to duceshardernegatives.\n",
      "subtract a baseline from the reward. As long as In the domain of max-margin estimation for\n",
      "thebaselineisnotafunctionofactions(i.e.,sam- structured prediction (Taskar et al., 2005), loss\n",
      "ples y− being drawn), the REINFORCE gradi- augmented MAP inference plays the role of find-\n",
      "ent estimator remains unbiased. More advanced inghardnegatives(thehardest). However,thisin-\n",
      "gradient estimators exist that also reduce vari- ferenceisonlytractableinalimitedclassofmod-\n",
      "ance (Grathwohl et al., 2017; Tucker et al., 2017; els such structured SVM (Tsochantaridis et al.,\n",
      "Liu et al., 2018), but for simplicity we use the 2005). Compared to those models that use exact\n",
      "maximization to find the hardest negative config- is that the negative context words are sampled in\n",
      "uration each time, the generator in ACE can be the same way, rather than tailored toward the ac-\n",
      "viewed as learning an approximate amortized in- tual target word. To apply ACE to this problem\n",
      "ference network. Concurrently to this work, Tu we first define the value function for the minimax\n",
      "andGimpel(2018)proposesaverysimilarframe- game,V(D,G),asfollows:\n",
      "work,usingalearnedinferencenetworkforStruc-\n",
      "tured prediction energy networks (SPEN) (Be- V(D,G) = E [logD(w,w )]\n",
      "p+(wc) c t\n",
      "langerandMcCallum,2016). −E [−log(1−D(w,w ))] (10)\n",
      "pnce(wc) c t\n",
      "Concurrent with our work, there have been\n",
      "−E [−log(1−D(w,w ))]\n",
      "otherinterestsinapplyingtheGANtoNLPprob- g θ(wc|wt) c t\n",
      "lems (Fedus et al., 2018; Wang et al., 2018; Cai\n",
      "withD = p(y = 1|w,w )andG = g (w |w ).\n",
      "andWang,2017). Knowledgegraphmodelsnatu- t c θ c t\n",
      "rally lend to a GAN setup, and has been the sub-\n",
      "Implementationdetails\n",
      "ject of study in Wang et al. (2018) and Cai and\n",
      "For our experiments, we train all our models on\n",
      "Wang (2017). These two concurrent works are\n",
      "a single pass of the May 2017 dump of the En-\n",
      "most closely related to one of the three tasks on\n",
      "glish Wikipedia with lowercased unigrams. The\n",
      "whichwestudyACEinthiswork. Besidesamore\n",
      "vocabulary size is restricted to the top 150k most\n",
      "general formulation that applies to problems be-\n",
      "frequent words when training from scratch while\n",
      "yond those considered in Wang et al. (2018) and\n",
      "forfinetuningweusethesamevocabularyasPen-\n",
      "Cai and Wang (2017), the techniques introduced\n",
      "nington et al. (2014), which is 400k of the most\n",
      "in our work on handling false negatives and en-\n",
      "frequent words. We use 5 NCE samples for each\n",
      "tropy regularization lead to improved experimen-\n",
      "positivesampleand1adversarialsampleinawin-\n",
      "talresultsasshowninSec.5.4.\n",
      "dowsizeof10andthesamepositivesubsampling\n",
      "4 ApplicationofACEonthreetasks schemeproposedbyMikolovetal.(2013). Learn-\n",
      "ing for both G and D uses Adam (Kingma and\n",
      "4.1 WordEmbeddings Ba, 2014) optimizer with its default parameters.\n",
      "Our conditional discriminator is modeled using\n",
      "Wordembeddingslearnavectorrepresentationof\n",
      "the Skip-Gram architecture, which is a two layer\n",
      "wordsfromco-occurrencesinatextcorpus. NCE\n",
      "neuralnetworkwithalinearmappingbetweenthe\n",
      "casts this learning problem as a binary classifica-\n",
      "layers. The generator network consists of an em-\n",
      "tion where the model tries to distinguish positive\n",
      "bedding layer followed by two small hidden lay-\n",
      "word and context pairs, from negative noise sam-\n",
      "ers,followedbyanoutputsoftmaxlayer. Thefirst\n",
      "ples composed of word and false context pairs.\n",
      "layer of the generator shares its weights with the\n",
      "The NCE objective in Skip-gram (Mikolov et al.,\n",
      "second embedding layer in the discriminator net-\n",
      "2013) for word embeddings is a separable loss of\n",
      "work,whichwefindreallyspeedsupconvergence\n",
      "theform:\n",
      "(cid:88) asthegeneratordoesnothavetorelearnitsownset\n",
      "L = − [logp(y = 1|w,w+)\n",
      "t c of embeddings. The difference between the dis-\n",
      "wt∈V criminatorandgeneratoristhatasigmoidnonlin-\n",
      "(9)\n",
      "K\n",
      "(cid:88) earityisusedafterthesecondlayerinthediscrim-\n",
      "+ logp(y = 0|w,w−)]\n",
      "t c inator, while in the generator, a softmax layer is\n",
      "c=1\n",
      "usedtodefineacategoricaldistributionovernega-\n",
      "Here, w+ is sampled from the set of true con- tive word candidates. We find that controlling the\n",
      "c\n",
      "texts and w− ∼ Q is sampled k times from a generator entropy is critical for finetuning exper-\n",
      "c\n",
      "iments as otherwise the generator collapses to its\n",
      "fixed noise distribution. Mikolov et al. (2013) in-\n",
      "favorite negative sample. The word embeddings\n",
      "troduced a further simplification of NCE, called\n",
      "are taken to be the first dense matrix in the dis-\n",
      "“Negative Sampling” (Dyer, 2014). With respect\n",
      "criminator.\n",
      "to our ACE framework, the difference between\n",
      "NCE and Negative Sampling is inconsequential,\n",
      "4.2 OrderEmbeddingsHypernymPrediction\n",
      "sowecontinuethediscussionusingNCE.Adraw-\n",
      "back of this sampling scheme is that it favors As introduced in Vendrov et al. (2016), ordered\n",
      "more common words as context. Another issue representations over hierarchy can be learned by\n",
      "order embeddings. An example task for such or- take TransD as an example, and modify its noise\n",
      "dered representation is hypernym prediction. A contrastivelearningtoACE,anddemonstratesig-\n",
      "hypernympairisapairofconceptswherethefirst nificantimprovementinsampleefficiencyandlink\n",
      "concept is a specialization or an instance of the predictionresults.\n",
      "second.\n",
      "Implementationdetails\n",
      "Forcompleteness,webrieflydescribeorderem-\n",
      "Let a positive entity-relation-entity triplet be de-\n",
      "beddings,thenanalyzeACEonthehypernympre-\n",
      "notedbyξ+ = (h+,r+,t+),andanegativetriplet\n",
      "diction task. In order embeddings, each entity is\n",
      "represented by a vector in RN, the score for a couldeitherhaveitsheadortailbeanegativesam-\n",
      "ple,i.e. ξ− = (h−,r+,t+)orξ− = (h+,r+,t−).\n",
      "positive ordered pair of entities (x,y) is defined\n",
      "by s (x,y) = ||max(0,y − x)||2 and, score for In either case, the general formulation in Sec. 2.1\n",
      "ω\n",
      "a negative ordered pair (x+,y−) is defined by stillapplies. Thenon-separablelossfunctiontakes\n",
      "s˜ (x+,y−) = max{0,η−s(x+,y−)},whereisη ontheform:\n",
      "ω\n",
      "isthemargin. Letf(u)betheembeddingfunction\n",
      "l = max(0,η+s (ξ+)−s (ξ−)) (12)\n",
      "ω ω\n",
      "which takes an entity as input and outputs an em-\n",
      "bedding vector. We define P as a set of positive Thescoringruleis:\n",
      "pairs and N as negative pairs, the separable loss\n",
      "functionfororderembeddingtaskisdefinedby: s = (cid:107)h ⊥+r−t ⊥(cid:107) (13)\n",
      "(cid:88) (cid:88) where r is the embedding vector for r, and h is\n",
      "L= s (f(u),f(v)))+ s˜(f(u),f(v)) ⊥\n",
      "ω\n",
      "projection of the embedding of h onto the space\n",
      "(u,v)∈P (u,v)∈N\n",
      "of r by h = h + r h(cid:62)h, where r and h are\n",
      "(11) ⊥ p p p p\n",
      "projection parameters of the model. t is defined\n",
      "⊥\n",
      "Implementationdetails inasimilarwaythroughparameterst,t andr.\n",
      "p p\n",
      "Ourgeneratorforthistaskisjustalinearfullycon- Theformofthegeneratorg θ(t−|r+,h+)ischo-\n",
      "nectedsoftmaxlayer,takinganembeddingvector sen to be f θ(h ⊥,h ⊥ +r), where f θ is a feedfor-\n",
      "fromdiscriminatorasinputandoutputtingacate- wardneuralnetthatconcatenatesitstwoinputar-\n",
      "goricaldistributionovertheentityset. Forthedis- guments,thenpropagatesthroughtwohiddenlay-\n",
      "criminator,weinheritallmodelsettingfromVen- ers,followedbyafinalsoftmaxoutputlayer. Asa\n",
      "drov et al. (2016): we use 50 dimensions hidden functionof(r+,h+),g θ sharesparameterwiththe\n",
      "state and bash size 1000, a learning rate of 0.01 discriminator, as the inputs to f θ are the embed-\n",
      "andtheAdamoptimizer. Forthegenerator,weuse dingvectors. Duringgeneratorlearning, onlyθ is\n",
      "a batch size of 1000, a learning rate 0.01 and the updatedandtheTransDmodelembeddingparam-\n",
      "Adamoptimizer. Weapplyweightdecaywithrate etersarefrozen.\n",
      "0.1andentropylossregularizationasdescribedin\n",
      "5 Experiments\n",
      "Sec. 2.4. Wehandlefalsenegativeasdescribedin\n",
      "Sec. 2.5. After cross validation, variance reduc- We evaluate ACE with experiments on word\n",
      "tionandleveragingNCEsamplesdoesnotgreatly embeddings, order embeddings, and knowledge\n",
      "affecttheorderembeddingtask. graph embeddings tasks. In short, whenever\n",
      "the original learning objective is contrastive (all\n",
      "4.3 KnowledgeGraphEmbeddings\n",
      "tasks except Glove fine-tuning) our results con-\n",
      "Knowledgegraphscontainentityandrelationdata sistently show that ACE improves over NCE. In\n",
      "of the form (head entity, relation, tail entity), and somecases,weincludeadditionalcomparisonsto\n",
      "the goal is to learn from observed positive entity the state-of-art results on the task to put the sig-\n",
      "relations and predict missing links (a.k.a. link nificance of such improvements in context: the\n",
      "prediction). There have been many works on generic ACE can often make a reasonable base-\n",
      "knowledge graph embeddings, e.g. TransE (Bor- line competitive with SOTA methods that are op-\n",
      "desetal.,2013),TransR(Linetal.,2015),TransH timizedforthetask.\n",
      "(Wangetal.,2014),TransD(Jietal.,2015),Com- For word embeddings, we evaluate models\n",
      "plex(Trouillonetal.,2016),DistMult(Yangetal., trained from scratch as well as fine-tuned Glove\n",
      "2014)andConvE(Dettmersetal.,2017). Manyof models(Penningtonetal.,2014)onwordsimilar-\n",
      "themuseacontrastivelearningobjective. Herewe ity tasks that consist of computing the similarity\n",
      "Figure 1: Left: Order embedding Accuracy plot. Figure 2: losscurveonNCEnegativepairsandACE\n",
      "Right:OrderembeddingdiscriminatorLossplotonNCE negativepairs. Left: withoutentropyandweightdecay.\n",
      "samplednegativepairsandpositivepairs. Right:withentropyandweightdecay\n",
      "Figure 3: Left: Rare Word, Right: WS353 similarity scores during the first Figure 4: Training from scratch losses\n",
      "epochoftraining. ontheDiscriminator\n",
      "between word pairs where the ground truth is an NCE by 40.4% and 45.7%. We also evaluate\n",
      "average of human scores. We choose the Rare our model qualitatively by inspecting the nearest\n",
      "word dataset (Luong et al., 2013) and WordSim- neighbors of selected words in Table. 1. We first\n",
      "353 (Finkelstein et al., 2001) by virtue of our hy- present the five nearest neighbors to each word to\n",
      "pothesisthatACElearnsbetterrepresentationsfor show that both NCE and ACE models learn sen-\n",
      "both rare and frequent words. We also qualita- sible embeddings. We then show that ACE em-\n",
      "tivelyevaluateACEwordembeddingsbyinspect- beddings have much better semantic relevance in\n",
      "ingthenearestneighborsofselectedwords. alargerneighborhood(nearestneighbor45-50).\n",
      "For the hypernym prediction task, following\n",
      "V<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  27685,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Rare Word', 'WS353', 'English Wikipedia']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: riminator\n",
      "between word pairs where the ground truth is an NCE by 40.4% and 45.7%. We also evaluate\n",
      "average of human scores. We choose the Rare our model qualitatively by inspecting the nearest\n",
      "word dataset (Luong et al., 2013) and WordSim- neighbors of selected words in Table. 1. We first\n",
      "353 (Finkelstein et al., 2001) by virtue of our hy- present the five nearest neighbors to each word to\n",
      "pothesisthatACElearnsbetterrepresentationsfor show that both NCE and ACE models learn sen-\n",
      "both rare and frequent words. We also qualita- sible embeddings. We then show that ACE em-\n",
      "tivelyevaluateACEwordembeddingsbyinspect- beddings have much better semantic relevance in\n",
      "ingthenearestneighborsofselectedwords. alargerneighborhood(nearestneighbor45-50).\n",
      "For the hypernym prediction task, following\n",
      "Vendrov et al. (2016), hypernym pairs are created 5.2 FinetuningWordEmbeddings\n",
      "from the WordNet hierarchy’s transitive closure.\n",
      "We take off-the-shelf pre-trained Glove embed-\n",
      "Weusethereleasedrandomdevelopmentsplitand\n",
      "dings which were trained using 6 billion tokens\n",
      "test split from Vendrov et al. (2016), which both\n",
      "(Pennington et al., 2014) and fine-tune them us-\n",
      "contain4000edges.\n",
      "ingouralgorithm. Itisinterestingtonotethatthe\n",
      "For knowledge graph embeddings, we use original Glove objective does not fit into the con-\n",
      "TransD (Ji et al., 2015) as our base model, and trastive learning framework, but nonetheless we\n",
      "perform ablation study to analyze the behavior of find that they benefit from ACE. In fact, we ob-\n",
      "ACE with various add-on features, and confirm servethattrainingsuchthat75%ofthewordsap-\n",
      "thatentropyregularizationiscrucialforgoodper- pear as positive contexts is sufficient to beat the\n",
      "formance in ACE. We also obtain link prediction largestdimensionalitypre-trainedGlovemodelon\n",
      "resultsthatarecompetitiveorsuperiortothestate- word similarity tasks. We evaluate our perfor-\n",
      "of-artsontheWN18dataset(Bordesetal.,2014). mance on the Rare Word and WordSim353 data.\n",
      "AscanbeseenfromourresultsinTable2,ACEon\n",
      "5.1 TrainingWordEmbeddingsfromscratch RWisnotalwaysbetterandforthe100dand300d\n",
      "Glove embeddings is marginally worse. How-\n",
      "In this experiment, we empirically observe that\n",
      "ever,onWordSim353ACEdoesconsiderablybet-\n",
      "training word embeddings using ACE converges\n",
      "ter across the board to the point where 50d Glove\n",
      "significantly faster than NCE after one epoch. As\n",
      "embeddings outperform the 300d baseline Glove\n",
      "shown in Fig. 3 both ACE (a mixture of p and\n",
      "nce\n",
      "model.\n",
      "g ) and just g (denoted by ADV) significantly\n",
      "θ θ\n",
      "outperforms the NCE baseline, with an absolute\n",
      "5.3 HypernymPrediction\n",
      "improvementof73.1%and58.5%respectivelyon\n",
      "RW score. We note similar results on WordSim- As shown in Table 3, with ACE training, our\n",
      "353 dataset where ACE and ADV outperforms method achieves a 1.5% improvement on accu-\n",
      "Queen King Computer Man Woman\n",
      "Skip-GramNCETop5 princess prince computers woman girl\n",
      "king queen computing boy man\n",
      "empress kings software girl prostitute\n",
      "pxqueen emperor microcomputer stranger person\n",
      "monarch monarch mainframe person divorcee\n",
      "Skip-GramNCETop45-50 sambiria eraric hypercard angiomata suitor\n",
      "phongsri mumbere neurotechnology someone nymphomaniac\n",
      "safrit empress lgp bespectacled barmaid\n",
      "mcelvoy saxonvm pcs hero redheaded\n",
      "tsarina pretender keystroke clown jew\n",
      "Skip-GramACETop5 princess prince software woman girl\n",
      "prince vi computers girl herself\n",
      "elizabeth kings applications tells man\n",
      "duke duke computing dead lover\n",
      "consort iii hardware boy tells\n",
      "Skip-GramACETop45-50 baron earl files kid aunt\n",
      "abbey holy information told maid\n",
      "throne cardinal device revenge wife\n",
      "marie aragon design magic lady\n",
      "victoria princes compatible angry bride\n",
      "Table1: Top5NearestNeighborsofWordsfollowedbyNeighbors45-50fordifferentModels.\n",
      "RW WS353 Method Accuracy(%)\n",
      "SkipgramOnlyNCEbaseline 18.90 31.35\n",
      "order-embeddings 90.6\n",
      "Skipgram+OnlyADV 29.96 58.05\n",
      "Skipgram+ACE 32.71 55.00 order-embeddings+OurACE 92.0\n",
      "Glove-50(Recomputedbasedon(Penningtonetal.,2014)) 34.02 49.51\n",
      "Glove-100(Recomputedbasedon(Penningtonetal.,2014)) 36.64 52.76\n",
      "Table3: OrderEmbeddingPerformance\n",
      "Glove-300(Recomputedbasedon(Penningtonetal.,2014)) 41.18 60.12\n",
      "Glove-50+ACE 35.60 60.46\n",
      "Glove-100+ACE 36.51 63.29\n",
      "Glove-300+ACE 40.57 66.50\n",
      "Table 2: Spearman score (ρ ∗ 100) on RW and\n",
      "model(discriminator)weapplyACEtoisTransD\n",
      "WS353 Datasets. We trained a skipgram model\n",
      "(Ji et al., 2015). Fig. 5 shows validation per-\n",
      "from scratch under various settings for only 1\n",
      "formance as training progresses. All variants of\n",
      "epoch on wikipedia. For finetuned models we re-\n",
      "ACE converges to better results than base NCE.\n",
      "computed the scores based on the publicly avail-\n",
      "AmongACEvariants,allmethodsthatincludeen-\n",
      "able6BtokensGlovemodelsandwefinetunedun-\n",
      "tropyregularizationsignificantlyoutperformwith-\n",
      "tilroughly75%ofthevocabularywasseen.\n",
      "out entropy regularization. Without the self crit-\n",
      "ical baseline variance reduction, learning could\n",
      "racy over Vendrov et al. (2016) without tunning progress faster at the beginning but the final per-\n",
      "any of the discriminator’s hyperparameters. We formancesuffersslightly. Thebestperformanceis\n",
      "further report training curve in Fig. 1, we report obtainedwithouttheadditionaloff-policylearning\n",
      "loss curve on randomly sampled pairs. We stress ofthegenerator.\n",
      "thatintheACEmodel,wetrainrandompairsand\n",
      "generatorgeneratedpairsjointly,asshowninFig. Table. 4 shows the final test results on WN18\n",
      "2,hardnegativeshelptheorderembeddingmodel link prediction task. It is interesting to note that\n",
      "convergesfaster. ACEimprovesMRRscoremoresignificantlythan\n",
      "hit@10. AsMRRisalotmoresensitivetothetop\n",
      "rankings, i.e., howthecorrectconfigurationranks\n",
      "5.4 AblationStudyandImprovingTransD\n",
      "amongthecompetitivealternatives,thisisconsis-\n",
      "To analyze different aspects of ACE, we perform tentwiththefactthatACEsampleshardnegatives\n",
      "an ablation study on the knowledge graph em- andforcesthebasemodeltolearnamorediscrim-\n",
      "bedding task. As described in Sec. 4.3, the base inativerepresentationofthepositiveexamples.\n",
      "faster than on the NCE negatives. After adding\n",
      "entropy regularization and weight decay, the gen-\n",
      "eratorworksasexpected.\n",
      "6 Limitations\n",
      "When the generator softmax is large, the current\n",
      "implementation of ACE training is computation-\n",
      "ally expensive. Although ACE converges faster\n",
      "per iteration, it may converge more slowly on\n",
      "wall-clocktimedependingonthecostofthesoft-\n",
      "max. However, embeddings are typically used as\n",
      "pre-trained building blocks for subsequent tasks.\n",
      "Thus,theirlearningisusuallythepre-computation\n",
      "step for the more complex downstream models\n",
      "Figure 5: Ablation study: measuring validation\n",
      "and spending more time is justified, especially\n",
      "Mean Reciprocal Rank (MRR) on WN18 dataset\n",
      "with GPU acceleration. We believe that the com-\n",
      "astrainingprogresses.\n",
      "putational cost could potentially be reduced via\n",
      "some existing techniques such as the “augment\n",
      "MRR hit@10\n",
      "and reduce” variational inference of (Ruiz et al.,\n",
      "ACE(Ent+SC) 0.792 0.945\n",
      "ACE(Ent+SC+IW) 0.768 0.949 2018), adaptive softmax (Grave et al., 2016), or\n",
      "NCETransD(ours) 0.527 0.947 the “sparsely-gated” softmax of Shazeer et al.\n",
      "NCETransD((Jietal.,2015)) - 0.925\n",
      "(2017),butleavethattofuturework.\n",
      "KBGAN(DISTMULT)((CaiandWang,2017)) 0.772 0.948\n",
      "KBGAN(COMPLEX)((CaiandWang,2017)) 0.779 0.948 Another limitation is on the theoretical front.\n",
      "Wangetal.((Wangetal.,2018)) - 0.93\n",
      "As noted in Goodfellow (2014), GAN learning\n",
      "COMPLEX((Trouillonetal.,2016)) 0.941 0.947\n",
      "does not implement maximum likelihood estima-\n",
      "tion (MLE), while NCE has MLE as an asymp-\n",
      "Table 4: WN18 experiments: the first portion of\n",
      "totic limit. To the best of our knowledge, more\n",
      "the table contains results where the base model is\n",
      "distantconnectionsbetweenGANandMLEtrain-\n",
      "TransD, the last separated line is the COMPLEX\n",
      "ing are not known, and tools for analyzing the\n",
      "embedding model (Trouillon et al., 2016), which\n",
      "equilibriumofamin-maxgamewhereplayersare\n",
      "achieves the SOTA on this dataset. Among all\n",
      "parametrizedbydeepneuralnetsarecurrentlynot\n",
      "TransDbasedmodels(thebestresultsinthisgroup\n",
      "availabletothebestofourknowledge.\n",
      "isunderlined),ACEimprovesoverbasicNCEand\n",
      "another GAN based approach KBGAN. The gap 7 Conclusion\n",
      "on MRR is likely due to the difference between\n",
      "In this paper, we propose Adversarial Contrastive\n",
      "TransDandCOMPLEXmodels.\n",
      "Estimation as a general technique for improving\n",
      "5.5 HardNegativeAnalysis\n",
      "supervised learning problems that learn by con-\n",
      "To better understand the effect of the adversarial trasting observed and fictitious samples. Specifi-\n",
      "samplesproposedbythegeneratorweplotthedis- cally, weuseageneratornetworkinaconditional\n",
      "criminator loss on both p and g samples. In GAN like setting to propose hard negative exam-\n",
      "nce θ\n",
      "this context, a harder sample means a higher loss ples for our discriminator model. We find that a\n",
      "assigned by the discriminator. Fig. 4 shows that mixture distribution of randomly sampling neg-\n",
      "discriminatorlossforthewordembeddingtaskon ative examples along with an adaptive negative\n",
      "g samples are always higher than on p sam- sampler leads to improved performances on a va-\n",
      "θ nce\n",
      "ples, confirming that the generator is indeed sam- rietyofembeddingtasks. Wevalidateourhypoth-\n",
      "plinghardernegatives. esisthathardnegativeexamplesarecriticaltoop-\n",
      "For Hypernym Prediction task, Fig.2 shows dis- timal learning and can be proposed via our ACE\n",
      "criminator loss on negative pairs sampled from framework. Finally, we find that controlling the\n",
      "NCE and ACE respectively. The higher the loss entropy of the generator through a regularization\n",
      "theharderthenegativepairis. Asindicatedinthe term and properly handling false negatives is cru-\n",
      "leftplot,lossontheACEnegativetermscollapses cialforsuccessfultraining.\n",
      "Acknowledgments Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,\n",
      "Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-\n",
      "We would like to thank Teng Long for providing tan Ruppin. 2001. Placing search in context: The\n",
      "the initial baseline code on knowledge graph em- conceptrevisited. InProceedingsofthe10thinter-\n",
      "nationalconferenceonWorldWideWeb,pages406–\n",
      "beddings,MatthewE.Taylorforproofreadingthe\n",
      "414.ACM.\n",
      "manuscript and Jackie Chi Kit Cheung for sug-\n",
      "gestions on preparing for the ACL oral presenta- Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\n",
      "tion. Additionally,wewouldalsoliketoacknowl- BingXu,DavidWarde-Farley,SherjilOzair,Aaron\n",
      "Courville, and Yoshua Bengio. 2014a. Generative\n",
      "edge Jordana Feldman for editing a blog post on\n",
      "adversarial nets. In Z. Ghahramani, M. Welling,\n",
      "this work and April Cooper for creating the art-\n",
      "C. Cortes, N. D. Lawrence, and K. Q. Weinberger,\n",
      "worksfortheblogpost. Finally,weappreciatethe editors,AdvancesinNeuralInformationProcessing\n",
      "broaderBorealisAIteamfordiscussionandemo- Systems27,pages2672–2680.\n",
      "tionalsupport.\n",
      "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\n",
      "BingXu,DavidWarde-Farley,SherjilOzair,Aaron\n",
      "Courville, and Yoshua Bengio. 2014b. Generative\n",
      "References adversarialnets. InAdvancesinneuralinformation\n",
      "processingsystems,pages2672–2680.\n",
      "Martin Arjovsky, Soumith Chintala, and Le´on Bot-\n",
      "tou. 2017. Wasserstein GAN. arXiv preprint Ian J Goodfellow. 2014. On distinguishability crite-\n",
      "arXiv:1701.07875. riaforestimatinggenerativemodels. arXivpreprint\n",
      "arXiv:1412.6515.\n",
      "DavidBelangerandAndrewMcCallum.2016. Struc-\n",
      "tured prediction energy networks. In International Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff\n",
      "ConferenceonMachineLearning,pages983–992. Roeder,andDavidDuvenaud.2017. Backpropaga-\n",
      "tion through the void: Optimizing control variates\n",
      "Antoine Bordes, Xavier Glorot, Jason Weston, and for black-box gradient estimation. arXiv preprint\n",
      "YoshuaBengio.2014. Asemanticmatchingenergy arXiv:1711.00123.\n",
      "functionforlearningwithmulti-relationaldata. Ma-\n",
      "chineLearning,94(2):233–259. Edouard Grave, Armand Joulin, Moustapha Cisse´,\n",
      "David Grangier, and Herve´ Je´gou. 2016. Efficient\n",
      "Antoine Bordes, Nicolas Usunier, Alberto Garcia- softmax approximation for GPUs. arXiv preprint\n",
      "Duran, Jason Weston, and Oksana Yakhnenko. arXiv:1609.04309.\n",
      "2013. Translating embeddings for modeling multi-\n",
      "relational data. In Advances in neural information IshaanGulrajani,FarukAhmed,MartinArjovsky,Vin-\n",
      "processingsystems,pages2787–2795. cent Dumoulin, and Aaron C Courville. 2017. Im-\n",
      "proved training of wasserstein gans. In Advances\n",
      "LiweiCaiandWilliamYangWang.2017. Kbgan: Ad- in Neural Information Processing Systems, pages\n",
      "versariallearningforknowledgegraphembeddings. 5769–5779.\n",
      "arXivpreprintarXiv:1711.04071.\n",
      "MichaelGutmannandAapoHyva¨rinen.2010. Noise-\n",
      "Yanshuai Cao, Gavin Weiguang Ding, Kry Yik-Chau contrastive estimation: A new estimation principle\n",
      "Lui, and Ruitong Huang. 2018. Improving GAN forunnormalizedstatisticalmodels. InProceedings\n",
      "trainingviabinarizedrepresentationentropy(BRE) oftheThirteenthInternationalConferenceonArtifi-\n",
      "regularization. In International Conference on cialIntelligenceandStatistics,pages297–304.\n",
      "LearningRepresentations.\n",
      "Michael U Gutmann and Aapo Hyva¨rinen. 2012.\n",
      "BoDaiandDahuaLin.2017. Contrastivelearningfor Noise-contrastive estimation of unnormalized sta-\n",
      "image captioning. In Advances in Neural Informa- tistical models, with applications to natural image\n",
      "tionProcessingSystems,pages898–907. statistics. Journal of Machine Learning Research,\n",
      "13(Feb):307–361.\n",
      "Tim Dettmers, Pasquale Minervini, Pontus Stene-\n",
      "torp, and Sebastian Riedel. 2017. Convolutional EricJang,ShixiangGu,andBenPoole.2016. Categor-\n",
      "2d knowledge graph embeddings. arXiv preprint icalreparameterizationwithgumbel-softmax. arXiv\n",
      "arXiv:1707.01476. preprintarXiv:1611.01144.\n",
      "Chris Dyer. 2014. Notes on noise contrastive es- Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and\n",
      "timation and negative sampling. arXiv preprint Jun Zhao. 2015. Knowledge graph embedding via\n",
      "arXiv:1410.8251. dynamic mapping matrix. In Proceedings of the\n",
      "53rdAnnualMeetingoftheAssociationforCompu-\n",
      "William Fedus, Ian Goodfellow, and Andrew M Dai. tational Linguistics and the 7th International Joint\n",
      "2018. MaskGAN: Better text generation via filling Conference on Natural Language Processing (Vol-\n",
      "inthe. arXivpreprintarXiv:1801.07736. ume1: LongPapers),volume1,pages687–696.\n",
      "Diederik P Kingma and Jimmy Ba. 2014. Adam: A NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,\n",
      "method for stochastic optimization. arXiv preprint Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff\n",
      "arXiv:1412.6980. Dean. 2017. Outrageously large neural networks:\n",
      "The sparsely-gated mixture-of-experts layer. arXiv\n",
      "YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,and preprintarXiv:1701.06538.\n",
      "Xuan Zhu. 2015. Learning entity and relation em-\n",
      "beddingsforknowledgegraphcompletion. InAAAI, Abhinav Shrivastava, Abhinav Gupta, and Ross Gir-\n",
      "volume15,pages2181–2187. shick.2016. Trainingregion-basedobjectdetectors\n",
      "withonlinehardexamplemining. InProceedingsof\n",
      "Hao Liu, Yihao Feng, Yi Mao, Dengyong Zhou, Jian the IEEE Conference on Computer Vision and Pat-\n",
      "Peng,andQiangLiu.2018. Action-dependentcon- ternRecognition,pages761–769.\n",
      "trol variates for policy optimization via stein iden-\n",
      "NoahASmithandJasonEisner.2005. Contrastivees-\n",
      "tity. InInternationalConferenceonLearningRep-\n",
      "timation: Training log-linear models on unlabeled\n",
      "resentations.\n",
      "data. In Proceedings of the 43rd Annual Meeting\n",
      "onAssociationforComputationalLinguistics,pages\n",
      "Thang Luong, Richard Socher, and Christopher D\n",
      "354–362. Association for Computational Linguis-\n",
      "Manning. 2013. Better word representations with\n",
      "tics.\n",
      "recursive neural networks for morphology. In\n",
      "CoNLL,pages104–113.\n",
      "Ben Taskar, Vassil Chatalbashev, Daphne Koller, and\n",
      "Carlos Guestrin. 2005. Learning structured predic-\n",
      "Chris J Maddison, Andriy Mnih, and Yee Whye Teh.\n",
      "tionmodels: Alargemarginapproach. InProceed-\n",
      "2016. The concrete distribution: A continuous\n",
      "ings of the 22nd international conference on Ma-\n",
      "relaxation of discrete random variables. arXiv\n",
      "chinelearning,pages896–903.ACM.\n",
      "preprintarXiv:1611.00712.\n",
      "The´oTrouillon,JohannesWelbl,SebastianRiedel,E´ric\n",
      "TomasMikolov,IlyaSutskever,KaiChen,GregSCor- Gaussier, and Guillaume Bouchard. 2016. Com-\n",
      "rado, and Jeff Dean. 2013. Distributed representa- plex embeddings for simple link prediction. In In-\n",
      "tionsofwordsandphrasesandtheircompositional- ternationalConferenceonMachineLearning,pages\n",
      "ity. In Advances in neural information processing 2071–2080.\n",
      "systems,pages3111–3119.\n",
      "Ioannis Tsochantaridis, Thorsten Joachims, Thomas\n",
      "M.MirzaandS.Osindero.2014. ConditionalGenera- Hofmann, and Yasemin Altun. 2005. Large mar-\n",
      "tiveAdversarialNets. ArXive-prints. gin methods for structured and interdependent out-\n",
      "putvariables. Journalofmachinelearningresearch,\n",
      "AndriyMnihandKorayKavukcuoglu.2013. Learning 6(Sep):1453–1484.\n",
      "word embeddings efficiently with noise-contrastive\n",
      "estimation. InAdvancesinneuralinformationpro- Lifu Tu and Kevin Gimpel. 2018. Learning approx-\n",
      "cessingsystems,pages2265–2273. imate inference networks for structured prediction.\n",
      "InInternationalConferenceonLearningRepresen-\n",
      "AndriyMnihandYeeWhyeTeh.2012. Afastandsim- tations.\n",
      "ple algorithm for training neural probabilistic lan-\n",
      "GeorgeTucker,AndriyMnih,ChrisJMaddison,John\n",
      "guagemodels. arXivpreprintarXiv:1206.6426.\n",
      "Lawson, and Jascha Sohl-Dickstein. 2017. Rebar:\n",
      "Low-variance, unbiased gradient estimates for dis-\n",
      "Jeffrey Pennington, Richard Socher, and Christopher\n",
      "crete latent variable models. In I. Guyon, U. V.\n",
      "Manning. 2014. Glove: Global vectors for word\n",
      "Luxburg,S.Bengio,H.Wallach,R.Fergus,S.Vish-\n",
      "representation. In Proceedings of the 2014 confer-\n",
      "wanathan,andR.Garnett,editors,AdvancesinNeu-\n",
      "enceonempiricalmethodsinnaturallanguagepro-\n",
      "ralInformationProcessingSystems30,pages2627–\n",
      "cessing(EMNLP),pages1532–1543.\n",
      "2636.CurranAssociates,Inc.\n",
      "StevenJRennie,EtienneMarcheret,YoussefMroueh,\n",
      "Ashish Vaswani, Yinggong Zhao, Victoria Fossum,\n",
      "Jarret Ross, and Vaibhava Goel. 2016. Self-critical\n",
      "and David Chiang. 2013. Decoding with large-\n",
      "sequence training for image captioning. arXiv\n",
      "scale neural language models improves translation.\n",
      "preprintarXiv:1612.00563.\n",
      "In Proceedings of the 2013 Conference on Empiri-\n",
      "calMethodsinNaturalLanguageProcessing,pages\n",
      "Francisco JR Ruiz, Michalis K Titsias, Adji B Dieng,\n",
      "1387–1392.\n",
      "and David M Blei. 2018. Augment and reduce:\n",
      "Stochastic inference for large categorical distribu- Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel\n",
      "tions. arXivpreprintarXiv:1802.04220. Urtasun. 2016. Order-embeddings of images and\n",
      "language. InInternationalConferenceonLearning\n",
      "Florian Schroff, Dmitry Kalenichenko, and James Representations.\n",
      "Philbin. 2015. Facenet: A unified embedding for\n",
      "face recognition and clustering. In Proceedings of PeifengWang,ShuangyinLi,andRongPan.2018. In-\n",
      "the IEEE Conference on Computer Vision and Pat- corporating GAN for negative sampling in knowl-\n",
      "ternRecognition,pages815–823. edge representation learning. In The Thirty-Second\n",
      "AAAI Conference on Artificial Intelligence (AAAI-\n",
      "18).\n",
      "Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\n",
      "Chen.2014. Knowledgegraphembeddingbytrans-\n",
      "latingonhyperplanes. InProceedingsoftheTwenty-\n",
      "Eighth AAAI Conference on Artificial Intelligence,\n",
      "pages1112–1119.AAAIPress.\n",
      "Ronald J Williams. 1992. Simple statistical gradient-\n",
      "following algorithms for connectionist reinforce-\n",
      "mentlearning. Machinelearning,8(3-4):229–256.\n",
      "Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\n",
      "Gao, and Li Deng. 2014. Embedding entities and\n",
      "relations for learning and inference in knowledge\n",
      "bases. arXivpreprintarXiv:1412.6575.\n",
      "BengioYoshua,DucharmeRejean,VincentPascal,and\n",
      "Jauvin Christian.2003. Aneuralprobabilisticlan-\n",
      "guage model. Journal of Machine Learning Re-\n",
      "search.\n",
      "JunboZhao,MichaelMathieu,andYannLeCun.2016.\n",
      "Energy-basedgenerativeadversarialnetwork. arXiv\n",
      "preprintarXiv:1609.03126.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  17228,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['WN18', 'WordSim353', 'Rare Word', 'WordSim353']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Adversarial Contrastive Estimation\n",
      "AvishekJoeyBose1,2,∗† HuanLing1,2,∗† YanshuaiCao1,∗\n",
      "1BorealisAI 2UniversityofToronto\n",
      "{joey.bose,huan.ling}@mail.utoronto.ca\n",
      "{yanshuai.cao}@borealisai.com\n",
      "Abstract modelingneed,ascertainassumptionsarebestex-\n",
      "pressed as some score or energy in margin based\n",
      "Learning by contrasting positive and neg- or un-normalized probability models (Smith and\n",
      "ativesamplesisageneralstrategyadopted Eisner, 2005). For example, modeling entity re-\n",
      "by many methods. Noise contrastive lationsastranslationsorvariantsthereofinavec-\n",
      "estimation (NCE) for word embeddings torspacenaturallyleadstoadistance-basedscore\n",
      "andtranslatingembeddingsforknowledge tobeminimizedforobservedentity-relation-entity\n",
      "graphs are examples in NLP employing triplets(Bordesetal.,2013).\n",
      "this approach. In this work, we view Given a scoring function, the gradient of the\n",
      "contrastive learning as an abstraction of model’sparametersonobservedpositiveexamples\n",
      "all such methods and augment the neg- can be readily computed, but the negative phase\n",
      "ative sampler into a mixture distribution requiresadesigndecisiononhowtosampledata.\n",
      "containing an adversarially learned sam- In noise contrastive estimation for word embed-\n",
      "pler. The resulting adaptive sampler finds dings, a negative example is formed by replacing\n",
      "harder negative examples, which forces acomponentofapositivepairbyrandomlyselect-\n",
      "the main model to learn a better represen- ingasampledwordfromthevocabulary,resulting\n",
      "tation of the data. We evaluate our pro- in a fictitious word-context pair which would be\n",
      "posalonlearningwordembeddings,order unlikelytoactuallyexistinthedataset. Thisnega-\n",
      "embeddingsandknowledgegraphembed- tive sampling by corruption approach is also used\n",
      "dingsandobservebothfasterconvergence inlearningknowledgegraphembeddings(Bordes\n",
      "andimprovedresultsonmultiplemetrics. etal.,2013;Linetal.,2015;Jietal.,2015;Wang\n",
      "et al., 2014; Trouillon et al., 2016; Yang et al.,\n",
      "1 Introduction 2014; Dettmers et al., 2017), order embeddings\n",
      "(Vendrovetal.,2016),captiongeneration(Daiand\n",
      "Many models learn by contrasting losses on ob-\n",
      "Lin,2017),etc.\n",
      "served positive examples with those on some fic-\n",
      "Typicallythecorruptiondistributionisthesame\n",
      "titiousnegativeexamples,tryingtodecreasesome\n",
      "for all inputs like in skip-gram or CBOW NCE,\n",
      "score on positive ones while increasing it on neg-\n",
      "rather than being a conditional distribution that\n",
      "ative ones. There are multiple reasons why such\n",
      "takes into account information about the input\n",
      "contrastive learning approach is needed. Com-\n",
      "sampleunderconsideration. Furthermore,thecor-\n",
      "putational tractability is one. For instance, in-\n",
      "ruption process usually only encodes a human\n",
      "steadofusingsoftmaxtopredictawordforlearn-\n",
      "prior as to what constitutes a hard negative sam-\n",
      "ing word embeddings, noise contrastive estima-\n",
      "ple,ratherthanbeinglearnedfromdata. Forthese\n",
      "tion (NCE) (Dyer, 2014; Mnih and Teh, 2012)\n",
      "two reasons, the simple fixed corruption process\n",
      "can be used in skip-gram or CBOW word em-\n",
      "often yields only easy negative examples. Easy\n",
      "bedding models (Gutmann and Hyva¨rinen, 2012;\n",
      "negativesaresub-optimalforlearningdiscrimina-\n",
      "Mikolov et al., 2013; Mnih and Kavukcuoglu,\n",
      "tive representation as they do not force the model\n",
      "2013; Vaswani et al., 2013). Another reason is\n",
      "tofindcriticalcharacteristicsofobservedpositive\n",
      "∗authorscontributedequally data, which has been independently discovered in\n",
      "†WorkdonewhileauthorwasaninternatBorealisAI applications outside NLP previously (Shrivastava\n",
      "8102\n",
      "guA\n",
      "2\n",
      "]LC.sc[\n",
      "3v24630.5081:viXra\n",
      "etal.,2016). Evenifhardnegativesareoccasion- with respect to some joint distribution over pos-\n",
      "ally reached, the infrequency means slow conver- itive and negative samples. Furthermore, by\n",
      "gence. Designingamoresophisticatedcorruption the law of total expectation, and the fact that\n",
      "process could be fruitful, but requires costly trial- given x+, the negative sampling is not depen-\n",
      "and-errorbyahumanexpert. dent on the positive label, i.e. p(y+,y−|x+) =\n",
      "In this work, we propose to augment the sim- p(y+|x+)p(y−|x+),Eq.1canbere-writtenas\n",
      "plecorruptionnoiseprocessinvariousembedding\n",
      "models with an adversarially learned conditional E p(x+)[E p(y+|x+)p(y−|x+)l ω(x+,y+,y−)] (2)\n",
      "distribution, forming a mixture negative sampler\n",
      "that adapts to the underlying data and the em- Separableloss\n",
      "bedding model training progress. The resulting Inthecasewherethelossdecomposesintoasum\n",
      "methodisreferredtoasadversarialcontrastivees- of scores on positive and negative tuples such as\n",
      "timation (ACE). The adaptive conditional model l (x+,y+,y−) = s (x+,y+)−s˜ (x+,y−),then\n",
      "ω ω ω\n",
      "engagesinaminimaxgamewiththeprimaryem- Expression.2becomes\n",
      "beddingmodel,muchlikeinGenerativeAdversar-\n",
      "ial Networks (GANs) (Goodfellow et al., 2014a), E [E s (x,y)−E s˜ (x,y)]\n",
      "p+(x) p+(y|x) ω p−(y|x) ω\n",
      "where a discriminator net (D), tries to distinguish (3)\n",
      "samples produced by a generator (G) from real where we moved the + and − to p for notational\n",
      "data(Goodfellowetal.,2014b). InACE,themain brevity. Learning by stochastic gradient descent\n",
      "model learns to distinguish between a real posi- aims to adjust ω to pushing down s (x,y) on\n",
      "ω\n",
      "tive example and a negative sample selected by samples from p+ while pushing up s˜ (x,y) on\n",
      "ω\n",
      "themixtureofafixedNCEsamplerandanadver- samples from p−. Note that for generality, the\n",
      "sarial generator. The main model and the genera- scoringfunctionfornegativesamples,denotedby\n",
      "tor takes alternating turns to update their parame- s˜, could be slightly different from s. For in-\n",
      "ω ω\n",
      "ters. In fact, our method can be viewed as a con- stance, s˜could contain a margin as in the case of\n",
      "ditionalGAN(MirzaandOsindero,2014)ondis- OrderEmbeddingsinSec.4.2.\n",
      "creteinputs,withamixturegeneratorconsistingof\n",
      "Nonseparableloss\n",
      "a learned and a fixed distribution, with additional\n",
      "techniques introduced to achieve stable and con- Eq. 1 is the general form that we would like to\n",
      "vergenttrainingofembeddingmodels. consider because for certain problems, the loss\n",
      "InourproposedACEapproach,theconditional function cannot be separated into sums of terms\n",
      "sampler finds harder negatives than NCE, while containing only positive (x+,y+) and terms with\n",
      "being able to gracefully fall back to NCE when- negatives (x+,y−). An example of such a non-\n",
      "ever the generator cannot find hard negatives. We separable loss is the triplet ranking loss (Schroff\n",
      "demonstratetheefficacyandgeneralityofthepro- et al., 2015): l ω = max(0,η + s ω(x+,y+) −\n",
      "posed method on three different learning tasks, s ω(x+,y−)), which does not decompose due to\n",
      "word embeddings (Mikolov et al., 2013), order therectification.\n",
      "embeddings(Vendrovetal.,2016)andknowledge\n",
      "Noisecontrastiveestimation\n",
      "graphembeddings(Jietal.,2015).\n",
      "The typical NCE approach in tasks such as word\n",
      "2 Method embeddings (Mikolov et al., 2013), order embed-\n",
      "dings(Vendrovetal.,2016),andknowledgegraph\n",
      "2.1 Background: contrastivelearning\n",
      "embeddingscanbeviewedasaspecialcaseofEq.\n",
      "In the most general form, our method applies to 2 by taking p(y−|x+) to be some unconditional\n",
      "supervised learning problems with a contrastive\n",
      "p (y).\n",
      "nce\n",
      "objectiveofthefollowingform:\n",
      "Thisleadstoefficientcomputationduringtrain-\n",
      "L(ω) = E l (x+,y+,y−) (1) ing,however,p nce(y)sacrificesthesamplingeffi-\n",
      "p(x+,y+,y−) ω\n",
      "ciencyoflearningasthenegativesproducedusing\n",
      "where l (x+,y+,y−) captures both the model afixeddistributionarenottailoredtowardx+,and\n",
      "ω\n",
      "with parameters ω and the loss that scores a asaresultarenotnecessarilyhardnegativeexam-\n",
      "positive tuple (x+,y+) against a negative one ples. Thus, the model is not forced to discover\n",
      "(x+,y−). E (.) denotes expectation discriminative representation of observed positive\n",
      "p(x+,y+,y−)\n",
      "data. As training progresses, more and more neg- Instead, we use the REINFORCE (Williams,\n",
      "ativeexamplesarecorrectlylearned,theprobabil- 1992)gradientestimatorfor∇ L(θ,x):\n",
      "θ\n",
      "ityofdrawingahardnegativeexamplediminishes\n",
      "(1−λ)E(cid:2) −l (x,y+,y−)∇ log(g (y−|x))(cid:3) (6)\n",
      "further,causingslowconvergence. ω θ θ\n",
      "where the expectation E is with respect to\n",
      "2.2 Adversarialmixturenoise\n",
      "p(y+,y−|x) = p(y+|x)g (y−|x), and the dis-\n",
      "θ\n",
      "To remedy the above mentioned problem of a criminatorlossl (x,y+,y−)actsasthereward.\n",
      "ω\n",
      "fixed unconditional negative sampler, we propose\n",
      "With a separable loss, the (conditional) value\n",
      "toaugmentitintoamixtureone,λp (y)+(1−\n",
      "nce functionoftheminimaxgameis:\n",
      "λ)g (y|x), where g is a conditional distribution\n",
      "θ θ\n",
      "with a learnable parameter θ and λ is a hyper- L(ω,θ;x) = E s (x,y)\n",
      "p+(y|x) ω\n",
      "parameter. The objective in Expression. 2 can −E s˜ (x,y)−E s˜ (x,y) (7)\n",
      "thenbewrittenas(conditionedonxfornotational\n",
      "pnce(y) ω g θ(y|x) ω\n",
      "brevity): and only the last term depends on the generator\n",
      "parameterω. Hence,withaseparableloss,there-\n",
      "L(ω,θ;x) = λE l (x,y+,y−) wardis−s˜(x+,y−). Thisreductiondoesnothap-\n",
      "p(y+|x)pnce(y−) ω\n",
      "+(1−λ)E l (x,y+,y−) (4) penwithanon-separableloss,andwehavetouse\n",
      "p(y+|x)g (y−|x) ω\n",
      "θ l (x,y+,y−).\n",
      "ω\n",
      "Welearn(ω,θ)inaGAN-styleminimaxgame:\n",
      "2.4 Entropyandtrainingstability\n",
      "minmaxV(ω,θ) = minmaxE L(ω,θ;x) GAN training can suffer from instability and de-\n",
      "p+(x)\n",
      "ω θ ω θ generacy where the generator probability mass\n",
      "(5)\n",
      "collapses to a few modes or points. Much work\n",
      "The embedding model behind l (x,y+,y−) is\n",
      "ω has been done to stabilize GAN training in the\n",
      "similar to the discriminator in (conditional) GAN\n",
      "continuous case (Arjovsky et al., 2017; Gulrajani\n",
      "(or critic in Wasserstein (Arjovsky et al., 2017)\n",
      "et al., 2017; Cao et al., 2018). In ACE, if the\n",
      "or Energy-based GAN (Zhao et al., 2016), while\n",
      "generator g probability mass collapses to a few\n",
      "g (y|x) acts as the generator. Henceforth, we θ\n",
      "θ candidates, then after the discriminator success-\n",
      "willusethetermdiscriminator(D)andembedding\n",
      "fullylearnsaboutthesenegatives,g cannotadapt\n",
      "modelinterchangeably,andrefertog asthegen- θ\n",
      "θ to select new hard negatives, because the REIN-\n",
      "erator.\n",
      "FORCEgradientestimatorEq.6reliesong being\n",
      "θ\n",
      "able to explore other candidates during sampling.\n",
      "2.3 Learningthegenerator\n",
      "Therefore,iftheg probabilitymasscollapses,in-\n",
      "θ\n",
      "ThereisoneimportantdistinctiontotypicalGAN:\n",
      "stead of leading to oscillation as in typical GAN,\n",
      "g (y|x)definesacategoricaldistributionoverpos-\n",
      "θ themin-maxgameinACEreachesanequilibrium\n",
      "sibleyvalues,andsamplesaredrawnaccordingly;\n",
      "wherethediscriminatorwinsandg cannolonger\n",
      "θ\n",
      "in contrast to typical GAN over continuous data\n",
      "adapt,thenACEfallsbacktoNCEsincethenega-\n",
      "space such as images, where samples are gener-\n",
      "tivesamplerhasanothermixturecomponentfrom\n",
      "ated by an implicit generative model that warps\n",
      "NCE.\n",
      "noise vectors into data points. Due to the discrete\n",
      "ThisbehaviorofgracefullyfallingbacktoNCE\n",
      "samplingstep,g cannotlearnbyreceivinggradi-\n",
      "θ is more desirable than the alternative of stalled\n",
      "ent through the discriminator. One possible solu- training if p−(y|x) does not have a simple p\n",
      "nce\n",
      "tion is to use the Gumbel-softmax reparametriza-\n",
      "mixturecomponent. However, wewouldstilllike\n",
      "tion trick (Jang et al., 2016; Maddison et al.,\n",
      "to avoid such collapse, as the adversarial samples\n",
      "2016),whichgivesadifferentiableapproximation.\n",
      "provide greater learning signals than NCE sam-\n",
      "However,thisdifferentiabilitycomesatthecostof\n",
      "ples. To this end, we propose to use a regularizer\n",
      "drawing N Gumbel samples per each categorical\n",
      "to encourage the categorical distribution g (y|x)\n",
      "θ\n",
      "sample,whereN isthenumberofcategories. For\n",
      "tohavehighentropy. Inordertomakethethereg-\n",
      "word embeddings, N is the vocabulary size, and\n",
      "ularizerinterpretableanditshyperparameterseasy\n",
      "for knowledge graph embeddings, N is the num-\n",
      "totune,wedesignthefollowingform:\n",
      "berofentities,bothleadingtoinfeasiblecomputa-\n",
      "tionalrequirements. R (x) = max(0,c−H(g (y|x))) (8)\n",
      "ent θ\n",
      "whereH(g (y|x))istheentropyofthecategorical self-criticalbaselinemethod(Rennieetal.,2016),\n",
      "θ\n",
      "distributiong (y|x),andc = log(k)istheentropy where the baseline is b(x) = l (y+,y(cid:63),x), or\n",
      "θ ω\n",
      "of a uniform distribution over k choices, and k is b(x) = −s˜ (y(cid:63),x)intheseparablelosscase,and\n",
      "ω\n",
      "a hyper-parameter. Intuitively, R expresses the y(cid:63) = argmax g (y |x). In other words, the base-\n",
      "ent i θ i\n",
      "priorthatthegeneratorshouldspreaditsmassover lineistherewardofthemostlikelysampleaccord-\n",
      "morethank choicesforeachx. ingtothegenerator.\n",
      "2.5 Handlingfalsenegatives 2.7 Improvingexplorationing by\n",
      "θ\n",
      "Duringnegativesampling,p−(y|x)couldactually leveragingNCEsamples\n",
      "produce y that forms a positive pair that exists in\n",
      "In Sec. 2.4 we touched on the need for sufficient\n",
      "the training set, i.e., a false negative. This possi-\n",
      "exploration in g. It is possible to also leverage\n",
      "θ\n",
      "bility exists in NCE already, but since p is not\n",
      "nce negative samples from NCE to help the gener-\n",
      "adaptive,theprobabilityofsamplingafalsenega-\n",
      "ator learn. This is essentially off-policy explo-\n",
      "tive is low. Hence in NCE, the score on this false\n",
      "ration in reinforcement learning since NCE sam-\n",
      "negative (true observation) pair is pushed up less\n",
      "plesarenotdrawnaccordingtog (y|x). Thegen-\n",
      "θ\n",
      "inthenegativetermthaninthepositiveterm.\n",
      "erator learning can use importance re-weighting\n",
      "However, with the adaptive sampler, g (y|x),\n",
      "ω to leverage those samples. The resulting REIN-\n",
      "falsenegativesbecomeamuchmoresevereissue.\n",
      "FORCE gradient estimator is basically the same\n",
      "g (y|x)canlearntoconcentrateitsmassonafew\n",
      "ω asEq.6exceptthattherewardsarereweightedby\n",
      "false negatives, significantly canceling the learn- g (y−|x)/p (y−), and the expectation is with\n",
      "θ nce\n",
      "ing of those observations in the positive phase. respect to p(y+|x)p (y−). This additional off-\n",
      "nce\n",
      "Theentropyregularizationreducesthisproblemas\n",
      "policy learning term provides gradient informa-\n",
      "itforcesthegeneratortospreaditsmass,hencere- tionforgeneratorlearningifg (y−|x)isnotzero,\n",
      "θ\n",
      "ducingthechanceofafalsenegative.\n",
      "meaning that for it to be effective in helping ex-\n",
      "To further alleviate this problem, whenever\n",
      "ploration,thegeneratorcannotbecollapsedatthe\n",
      "computationally feasible, we apply an additional\n",
      "first place. Hence, in practice, this term is only\n",
      "two-steptechnique. First,wemaintainahashmap\n",
      "usedtofurtherhelpontopoftheentropyregular-\n",
      "of the training data in memory, and use it to effi-\n",
      "ization,butitdoesnotreplaceit.\n",
      "ciently detect if a negative sample (x+,y−) is an\n",
      "actual observation. If so, its contribution to the 3 RelatedWork\n",
      "lossisgivenazeroweightinωlearningstep. Sec-\n",
      "ond,toupdateθinthegeneratorlearningstep,the Smith and Eisner (2005) proposed contrastive es-\n",
      "reward for false negative samples are replaced by timation as a way for unsupervised learning of\n",
      "a large penalty, so that the REINFORCE gradient log-linearmodelsbytakingimplicitevidencefrom\n",
      "update would steer g away from those samples. user-defined neighborhoods around observed dat-\n",
      "θ\n",
      "The second step is needed to prevent null compu- apoints. Gutmann and Hyva¨rinen (2010) intro-\n",
      "tation where g learns to sample false negatives duced NCE as an alternative to the hierarchical\n",
      "θ\n",
      "which are subsequently ignored by the discrimi- softmax. IntheworksofMnihandTeh(2012)and\n",
      "natorupdateforω. MnihandKavukcuoglu(2013),NCEisappliedto\n",
      "log-bilinear models and Vaswani et al. (2013) ap-\n",
      "2.6 VarianceReduction\n",
      "pliedNCEtoneuralprobabilisticlanguagemodels\n",
      "The basic REINFORCE gradient estimator is (Yoshuaetal.,2003). Comparedtotheseprevious\n",
      "poised with high variance, so in practice one of- NCE methods that rely on simple fixed sampling\n",
      "ten needs to apply variance reduction techniques. heuristics,ACEusesanadaptivesamplerthatpro-\n",
      "The most basic form of variance reduction is to duceshardernegatives.\n",
      "subtract a baseline from the reward. As long as In the domain of max-margin estimation for\n",
      "thebaselineisnotafunctionofactions(i.e.,sam- structured prediction (Taskar et al., 2005), loss\n",
      "ples y− being drawn), the REINFORCE gradi- augmented MAP inference plays the role of find-\n",
      "ent estimator remains unbiased. More advanced inghardnegatives(thehardest). However,thisin-\n",
      "gradient estimators exist that also reduce vari- ferenceisonlytractableinalimitedclassofmod-\n",
      "ance (Grathwohl et al., 2017; Tucker et al., 2017; els such structured SVM (Tsochantaridis et al.,\n",
      "Liu et al., 2018), but for simplicity we use the 2005). Compared to those models that use exact\n",
      "maximization to find the hardest negative config- is that the negative context words are sampled in\n",
      "uration each time, the generator in ACE can be the same way, rather than tailored toward the ac-\n",
      "viewed as learning an approximate amortized in- tual target word. To apply ACE to this problem\n",
      "ference network. Concurrently to this work, Tu we first define the value function for the minimax\n",
      "andGimpel(2018)proposesaverysimilarframe- game,V(D,G),asfollows:\n",
      "work,usingalearnedinferencenetworkforStruc-\n",
      "tured prediction energy networks (SPEN) (Be- V(D,G) = E [logD(w,w )]\n",
      "p+(wc) c t\n",
      "langerandMcCallum,2016). −E [−log(1−D(w,w ))] (10)\n",
      "pnce(wc) c t\n",
      "Concurrent with our work, there have been\n",
      "−E [−log(1−D(w,w ))]\n",
      "otherinterestsinapplyingtheGANtoNLPprob- g θ(wc|wt) c t\n",
      "lems (Fedus et al., 2018; Wang et al., 2018; Cai\n",
      "withD = p(y = 1|w,w )andG = g (w |w ).\n",
      "andWang,2017). Knowledgegraphmodelsnatu- t c θ c t\n",
      "rally lend to a GAN setup, and has been the sub-\n",
      "Implementationdetails\n",
      "ject of study in Wang et al. (2018) and Cai and\n",
      "For our experiments, we train all our models on\n",
      "Wang (2017). These two concurrent works are\n",
      "a single pass of the May 2017 dump of the En-\n",
      "most closely related to one of the three tasks on\n",
      "glish Wikipedia with lowercased unigrams. The\n",
      "whichwestudyACEinthiswork. Besidesamore\n",
      "vocabulary size is restricted to the top 150k most\n",
      "general formulation that applies to problems be-\n",
      "frequent words when training from scratch while\n",
      "yond those considered in Wang et al. (2018) and\n",
      "forfinetuningweusethesamevocabularyasPen-\n",
      "Cai and Wang (2017), the techniques introduced\n",
      "nington et al. (2014), which is 400k of the most\n",
      "in our work on handling false negatives and en-\n",
      "frequent words. We use 5 NCE samples for each\n",
      "tropy regularization lead to improved experimen-\n",
      "positivesampleand1adversarialsampleinawin-\n",
      "talresultsasshowninSec.5.4.\n",
      "dowsizeof10andthesamepositivesubsampling\n",
      "4 ApplicationofACEonthreetasks schemeproposedbyMikolovetal.(2013). Learn-\n",
      "ing for both G and D uses Adam (Kingma and\n",
      "4.1 WordEmbeddings Ba, 2014) optimizer with its default parameters.\n",
      "Our conditional discriminator is modeled using\n",
      "Wordembeddingslearnavectorrepresentationof\n",
      "the Skip-Gram architecture, which is a two layer\n",
      "wordsfromco-occurrencesinatextcorpus. NCE\n",
      "neuralnetworkwithalinearmappingbetweenthe\n",
      "casts this learning problem as a binary classifica-\n",
      "layers. The generator network consists of an em-\n",
      "tion where the model tries to distinguish positive\n",
      "bedding layer followed by two small hidden lay-\n",
      "word and context pairs, from negative noise sam-\n",
      "ers,followedbyanoutputsoftmaxlayer. Thefirst\n",
      "ples composed of word and false context pairs.\n",
      "layer of the generator shares its weights with the\n",
      "The NCE objective in Skip-gram (Mikolov et al.,\n",
      "second embedding layer in the discriminator net-\n",
      "2013) for word embeddings is a separable loss of\n",
      "work,whichwefindreallyspeedsupconvergence\n",
      "theform:\n",
      "(cid:88) asthegeneratordoesnothavetorelearnitsownset\n",
      "L = − [logp(y = 1|w,w+)\n",
      "t c of embeddings. The difference between the dis-\n",
      "wt∈V criminatorandgeneratoristhatasigmoidnonlin-\n",
      "(9)\n",
      "K\n",
      "(cid:88) earityisusedafterthesecondlayerinthediscrim-\n",
      "+ logp(y = 0|w,w−)]\n",
      "t c inator, while in the generator, a softmax layer is\n",
      "c=1\n",
      "usedtodefineacategoricaldistributionovernega-\n",
      "Here, w+ is sampled from the set of true con- tive word candidates. We find that controlling the\n",
      "c\n",
      "texts and w− ∼ Q is sampled k times from a generator entropy is critical for finetuning exper-\n",
      "c\n",
      "iments as otherwise the generator collapses to its\n",
      "fixed noise distribution. Mikolov et al. (2013) in-\n",
      "favorite negative sample. The word embeddings\n",
      "troduced a further simplification of NCE, called\n",
      "are taken to be the first dense matrix in the dis-\n",
      "“Negative Sampling” (Dyer, 2014). With respect\n",
      "criminator.\n",
      "to our ACE framework, the difference between\n",
      "NCE and Negative Sampling is inconsequential,\n",
      "4.2 OrderEmbeddingsHypernymPrediction\n",
      "sowecontinuethediscussionusingNCE.Adraw-\n",
      "back of this sampling scheme is that it favors As introduced in Vendrov et al. (2016), ordered\n",
      "more common words as context. Another issue representations over hierarchy can be learned by\n",
      "order embeddings. An example task for such or- take TransD as an example, and modify its noise\n",
      "dered representation is hypernym prediction. A contrastivelearningtoACE,anddemonstratesig-\n",
      "hypernympairisapairofconceptswherethefirst nificantimprovementinsampleefficiencyandlink\n",
      "concept is a specialization or an instance of the predictionresults.\n",
      "second.\n",
      "Implementationdetails\n",
      "Forcompleteness,webrieflydescribeorderem-\n",
      "Let a positive entity-relation-entity triplet be de-\n",
      "beddings,thenanalyzeACEonthehypernympre-\n",
      "notedbyξ+ = (h+,r+,t+),andanegativetriplet\n",
      "diction task. In order embeddings, each entity is\n",
      "represented by a vector in RN, the score for a couldeitherhaveitsheadortailbeanegativesam-\n",
      "ple,i.e. ξ− = (h−,r+,t+)orξ− = (h+,r+,t−).\n",
      "positive ordered pair of entities (x,y) is defined\n",
      "by s (x,y) = ||max(0,y − x)||2 and, score for In either case, the general formulation in Sec. 2.1\n",
      "ω\n",
      "a negative ordered pair (x+,y−) is defined by stillapplies. Thenon-separablelossfunctiontakes\n",
      "s˜ (x+,y−) = max{0,η−s(x+,y−)},whereisη ontheform:\n",
      "ω\n",
      "isthemargin. Letf(u)betheembeddingfunction\n",
      "l = max(0,η+s (ξ+)−s (ξ−)) (12)\n",
      "ω ω\n",
      "which takes an entity as input and outputs an em-\n",
      "bedding vector. We define P as a set of positive Thescoringruleis:\n",
      "pairs and N as negative pairs, the separable loss\n",
      "functionfororderembeddingtaskisdefinedby: s = (cid:107)h ⊥+r−t ⊥(cid:107) (13)\n",
      "(cid:88) (cid:88) where r is the embedding vector for r, and h is\n",
      "L= s (f(u),f(v)))+ s˜(f(u),f(v)) ⊥\n",
      "ω\n",
      "projection of the embedding of h onto the space\n",
      "(u,v)∈P (u,v)∈N\n",
      "of r by h = h + r h(cid:62)h, where r and h are\n",
      "(11) ⊥ p p p p\n",
      "projection parameters of the model. t is defined\n",
      "⊥\n",
      "Implementationdetails inasimilarwaythroughparameterst,t andr.\n",
      "p p\n",
      "Ourgeneratorforthistaskisjustalinearfullycon- Theformofthegeneratorg θ(t−|r+,h+)ischo-\n",
      "nectedsoftmaxlayer,takinganembeddingvector sen to be f θ(h ⊥,h ⊥ +r), where f θ is a feedfor-\n",
      "fromdiscriminatorasinputandoutputtingacate- wardneuralnetthatconcatenatesitstwoinputar-\n",
      "goricaldistributionovertheentityset. Forthedis- guments,thenpropagatesthroughtwohiddenlay-\n",
      "criminator,weinheritallmodelsettingfromVen- ers,followedbyafinalsoftmaxoutputlayer. Asa\n",
      "drov et al. (2016): we use 50 dimensions hidden functionof(r+,h+),g θ sharesparameterwiththe\n",
      "state and bash size 1000, a learning rate of 0.01 discriminator, as the inputs to f θ are the embed-\n",
      "andtheAdamoptimizer. Forthegenerator,weuse dingvectors. Duringgeneratorlearning, onlyθ is\n",
      "a batch size of 1000, a learning rate 0.01 and the updatedandtheTransDmodelembeddingparam-\n",
      "Adamoptimizer. Weapplyweightdecaywithrate etersarefrozen.\n",
      "0.1andentropylossregularizationasdescribedin\n",
      "5 Experiments\n",
      "Sec. 2.4. Wehandlefalsenegativeasdescribedin\n",
      "Sec. 2.5. After cross validation, variance reduc- We evaluate ACE with experiments on word\n",
      "tionandleveragingNCEsamplesdoesnotgreatly embeddings, order embeddings, and knowledge\n",
      "affecttheorderembeddingtask. graph embeddings tasks. In short, whenever\n",
      "the original learning objective is contrastive (all\n",
      "4.3 KnowledgeGraphEmbeddings\n",
      "tasks except Glove fine-tuning) our results con-\n",
      "Knowledgegraphscontainentityandrelationdata sistently show that ACE improves over NCE. In\n",
      "of the form (head entity, relation, tail entity), and somecases,weincludeadditionalcomparisonsto\n",
      "the goal is to learn from observed positive entity the state-of-art results on the task to put the sig-\n",
      "relations and predict missing links (a.k.a. link nificance of such improvements in context: the\n",
      "prediction). There have been many works on generic ACE can often make a reasonable base-\n",
      "knowledge graph embeddings, e.g. TransE (Bor- line competitive with SOTA methods that are op-\n",
      "desetal.,2013),TransR(Linetal.,2015),TransH timizedforthetask.\n",
      "(Wangetal.,2014),TransD(Jietal.,2015),Com- For word embeddings, we evaluate models\n",
      "plex(Trouillonetal.,2016),DistMult(Yangetal., trained from scratch as well as fine-tuned Glove\n",
      "2014)andConvE(Dettmersetal.,2017). Manyof models(Penningtonetal.,2014)onwordsimilar-\n",
      "themuseacontrastivelearningobjective. Herewe ity tasks that consist of computing the similarity\n",
      "Figure 1: Left: Order embedding Accuracy plot. Figure 2: losscurveonNCEnegativepairsandACE\n",
      "Right:OrderembeddingdiscriminatorLossplotonNCE negativepairs. Left: withoutentropyandweightdecay.\n",
      "samplednegativepairsandpositivepairs. Right:withentropyandweightdecay\n",
      "Figure 3: Left: Rare Word, Right: WS353 similarity scores during the first Figure 4: Training from scratch losses\n",
      "epochoftraining. ontheDiscriminator\n",
      "between word pairs where the ground truth is an NCE by 40.4% and 45.7%. We also evaluate\n",
      "average of human scores. We choose the Rare our model qualitatively by inspecting the nearest\n",
      "word dataset (Luong et al., 2013) and WordSim- neighbors of selected words in Table. 1. We first\n",
      "353 (Finkelstein et al., 2001) by virtue of our hy- present the five nearest neighbors to each word to\n",
      "pothesisthatACElearnsbetterrepresentationsfor show that both NCE and ACE models learn sen-\n",
      "both rare and frequent words. We also qualita- sible embeddings. We then show that ACE em-\n",
      "tivelyevaluateACEwordembeddingsbyinspect- beddings have much better semantic relevance in\n",
      "ingthenearestneighborsofselectedwords. alargerneighborhood(nearestneighbor45-50).\n",
      "For the hypernym prediction task, following\n",
      "V<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  25624,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Word Embeddings', 'Order Embeddings', 'Knowledge Graph Embeddings']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: riminator\n",
      "between word pairs where the ground truth is an NCE by 40.4% and 45.7%. We also evaluate\n",
      "average of human scores. We choose the Rare our model qualitatively by inspecting the nearest\n",
      "word dataset (Luong et al., 2013) and WordSim- neighbors of selected words in Table. 1. We first\n",
      "353 (Finkelstein et al., 2001) by virtue of our hy- present the five nearest neighbors to each word to\n",
      "pothesisthatACElearnsbetterrepresentationsfor show that both NCE and ACE models learn sen-\n",
      "both rare and frequent words. We also qualita- sible embeddings. We then show that ACE em-\n",
      "tivelyevaluateACEwordembeddingsbyinspect- beddings have much better semantic relevance in\n",
      "ingthenearestneighborsofselectedwords. alargerneighborhood(nearestneighbor45-50).\n",
      "For the hypernym prediction task, following\n",
      "Vendrov et al. (2016), hypernym pairs are created 5.2 FinetuningWordEmbeddings\n",
      "from the WordNet hierarchy’s transitive closure.\n",
      "We take off-the-shelf pre-trained Glove embed-\n",
      "Weusethereleasedrandomdevelopmentsplitand\n",
      "dings which were trained using 6 billion tokens\n",
      "test split from Vendrov et al. (2016), which both\n",
      "(Pennington et al., 2014) and fine-tune them us-\n",
      "contain4000edges.\n",
      "ingouralgorithm. Itisinterestingtonotethatthe\n",
      "For knowledge graph embeddings, we use original Glove objective does not fit into the con-\n",
      "TransD (Ji et al., 2015) as our base model, and trastive learning framework, but nonetheless we\n",
      "perform ablation study to analyze the behavior of find that they benefit from ACE. In fact, we ob-\n",
      "ACE with various add-on features, and confirm servethattrainingsuchthat75%ofthewordsap-\n",
      "thatentropyregularizationiscrucialforgoodper- pear as positive contexts is sufficient to beat the\n",
      "formance in ACE. We also obtain link prediction largestdimensionalitypre-trainedGlovemodelon\n",
      "resultsthatarecompetitiveorsuperiortothestate- word similarity tasks. We evaluate our perfor-\n",
      "of-artsontheWN18dataset(Bordesetal.,2014). mance on the Rare Word and WordSim353 data.\n",
      "AscanbeseenfromourresultsinTable2,ACEon\n",
      "5.1 TrainingWordEmbeddingsfromscratch RWisnotalwaysbetterandforthe100dand300d\n",
      "Glove embeddings is marginally worse. How-\n",
      "In this experiment, we empirically observe that\n",
      "ever,onWordSim353ACEdoesconsiderablybet-\n",
      "training word embeddings using ACE converges\n",
      "ter across the board to the point where 50d Glove\n",
      "significantly faster than NCE after one epoch. As\n",
      "embeddings outperform the 300d baseline Glove\n",
      "shown in Fig. 3 both ACE (a mixture of p and\n",
      "nce\n",
      "model.\n",
      "g ) and just g (denoted by ADV) significantly\n",
      "θ θ\n",
      "outperforms the NCE baseline, with an absolute\n",
      "5.3 HypernymPrediction\n",
      "improvementof73.1%and58.5%respectivelyon\n",
      "RW score. We note similar results on WordSim- As shown in Table 3, with ACE training, our\n",
      "353 dataset where ACE and ADV outperforms method achieves a 1.5% improvement on accu-\n",
      "Queen King Computer Man Woman\n",
      "Skip-GramNCETop5 princess prince computers woman girl\n",
      "king queen computing boy man\n",
      "empress kings software girl prostitute\n",
      "pxqueen emperor microcomputer stranger person\n",
      "monarch monarch mainframe person divorcee\n",
      "Skip-GramNCETop45-50 sambiria eraric hypercard angiomata suitor\n",
      "phongsri mumbere neurotechnology someone nymphomaniac\n",
      "safrit empress lgp bespectacled barmaid\n",
      "mcelvoy saxonvm pcs hero redheaded\n",
      "tsarina pretender keystroke clown jew\n",
      "Skip-GramACETop5 princess prince software woman girl\n",
      "prince vi computers girl herself\n",
      "elizabeth kings applications tells man\n",
      "duke duke computing dead lover\n",
      "consort iii hardware boy tells\n",
      "Skip-GramACETop45-50 baron earl files kid aunt\n",
      "abbey holy information told maid\n",
      "throne cardinal device revenge wife\n",
      "marie aragon design magic lady\n",
      "victoria princes compatible angry bride\n",
      "Table1: Top5NearestNeighborsofWordsfollowedbyNeighbors45-50fordifferentModels.\n",
      "RW WS353 Method Accuracy(%)\n",
      "SkipgramOnlyNCEbaseline 18.90 31.35\n",
      "order-embeddings 90.6\n",
      "Skipgram+OnlyADV 29.96 58.05\n",
      "Skipgram+ACE 32.71 55.00 order-embeddings+OurACE 92.0\n",
      "Glove-50(Recomputedbasedon(Penningtonetal.,2014)) 34.02 49.51\n",
      "Glove-100(Recomputedbasedon(Penningtonetal.,2014)) 36.64 52.76\n",
      "Table3: OrderEmbeddingPerformance\n",
      "Glove-300(Recomputedbasedon(Penningtonetal.,2014)) 41.18 60.12\n",
      "Glove-50+ACE 35.60 60.46\n",
      "Glove-100+ACE 36.51 63.29\n",
      "Glove-300+ACE 40.57 66.50\n",
      "Table 2: Spearman score (ρ ∗ 100) on RW and\n",
      "model(discriminator)weapplyACEtoisTransD\n",
      "WS353 Datasets. We trained a skipgram model\n",
      "(Ji et al., 2015). Fig. 5 shows validation per-\n",
      "from scratch under various settings for only 1\n",
      "formance as training progresses. All variants of\n",
      "epoch on wikipedia. For finetuned models we re-\n",
      "ACE converges to better results than base NCE.\n",
      "computed the scores based on the publicly avail-\n",
      "AmongACEvariants,allmethodsthatincludeen-\n",
      "able6BtokensGlovemodelsandwefinetunedun-\n",
      "tropyregularizationsignificantlyoutperformwith-\n",
      "tilroughly75%ofthevocabularywasseen.\n",
      "out entropy regularization. Without the self crit-\n",
      "ical baseline variance reduction, learning could\n",
      "racy over Vendrov et al. (2016) without tunning progress faster at the beginning but the final per-\n",
      "any of the discriminator’s hyperparameters. We formancesuffersslightly. Thebestperformanceis\n",
      "further report training curve in Fig. 1, we report obtainedwithouttheadditionaloff-policylearning\n",
      "loss curve on randomly sampled pairs. We stress ofthegenerator.\n",
      "thatintheACEmodel,wetrainrandompairsand\n",
      "generatorgeneratedpairsjointly,asshowninFig. Table. 4 shows the final test results on WN18\n",
      "2,hardnegativeshelptheorderembeddingmodel link prediction task. It is interesting to note that\n",
      "convergesfaster. ACEimprovesMRRscoremoresignificantlythan\n",
      "hit@10. AsMRRisalotmoresensitivetothetop\n",
      "rankings, i.e., howthecorrectconfigurationranks\n",
      "5.4 AblationStudyandImprovingTransD\n",
      "amongthecompetitivealternatives,thisisconsis-\n",
      "To analyze different aspects of ACE, we perform tentwiththefactthatACEsampleshardnegatives\n",
      "an ablation study on the knowledge graph em- andforcesthebasemodeltolearnamorediscrim-\n",
      "bedding task. As described in Sec. 4.3, the base inativerepresentationofthepositiveexamples.\n",
      "faster than on the NCE negatives. After adding\n",
      "entropy regularization and weight decay, the gen-\n",
      "eratorworksasexpected.\n",
      "6 Limitations\n",
      "When the generator softmax is large, the current\n",
      "implementation of ACE training is computation-\n",
      "ally expensive. Although ACE converges faster\n",
      "per iteration, it may converge more slowly on\n",
      "wall-clocktimedependingonthecostofthesoft-\n",
      "max. However, embeddings are typically used as\n",
      "pre-trained building blocks for subsequent tasks.\n",
      "Thus,theirlearningisusuallythepre-computation\n",
      "step for the more complex downstream models\n",
      "Figure 5: Ablation study: measuring validation\n",
      "and spending more time is justified, especially\n",
      "Mean Reciprocal Rank (MRR) on WN18 dataset\n",
      "with GPU acceleration. We believe that the com-\n",
      "astrainingprogresses.\n",
      "putational cost could potentially be reduced via\n",
      "some existing techniques such as the “augment\n",
      "MRR hit@10\n",
      "and reduce” variational inference of (Ruiz et al.,\n",
      "ACE(Ent+SC) 0.792 0.945\n",
      "ACE(Ent+SC+IW) 0.768 0.949 2018), adaptive softmax (Grave et al., 2016), or\n",
      "NCETransD(ours) 0.527 0.947 the “sparsely-gated” softmax of Shazeer et al.\n",
      "NCETransD((Jietal.,2015)) - 0.925\n",
      "(2017),butleavethattofuturework.\n",
      "KBGAN(DISTMULT)((CaiandWang,2017)) 0.772 0.948\n",
      "KBGAN(COMPLEX)((CaiandWang,2017)) 0.779 0.948 Another limitation is on the theoretical front.\n",
      "Wangetal.((Wangetal.,2018)) - 0.93\n",
      "As noted in Goodfellow (2014), GAN learning\n",
      "COMPLEX((Trouillonetal.,2016)) 0.941 0.947\n",
      "does not implement maximum likelihood estima-\n",
      "tion (MLE), while NCE has MLE as an asymp-\n",
      "Table 4: WN18 experiments: the first portion of\n",
      "totic limit. To the best of our knowledge, more\n",
      "the table contains results where the base model is\n",
      "distantconnectionsbetweenGANandMLEtrain-\n",
      "TransD, the last separated line is the COMPLEX\n",
      "ing are not known, and tools for analyzing the\n",
      "embedding model (Trouillon et al., 2016), which\n",
      "equilibriumofamin-maxgamewhereplayersare\n",
      "achieves the SOTA on this dataset. Among all\n",
      "parametrizedbydeepneuralnetsarecurrentlynot\n",
      "TransDbasedmodels(thebestresultsinthisgroup\n",
      "availabletothebestofourknowledge.\n",
      "isunderlined),ACEimprovesoverbasicNCEand\n",
      "another GAN based approach KBGAN. The gap 7 Conclusion\n",
      "on MRR is likely due to the difference between\n",
      "In this paper, we propose Adversarial Contrastive\n",
      "TransDandCOMPLEXmodels.\n",
      "Estimation as a general technique for improving\n",
      "5.5 HardNegativeAnalysis\n",
      "supervised learning problems that learn by con-\n",
      "To better understand the effect of the adversarial trasting observed and fictitious samples. Specifi-\n",
      "samplesproposedbythegeneratorweplotthedis- cally, weuseageneratornetworkinaconditional\n",
      "criminator loss on both p and g samples. In GAN like setting to propose hard negative exam-\n",
      "nce θ\n",
      "this context, a harder sample means a higher loss ples for our discriminator model. We find that a\n",
      "assigned by the discriminator. Fig. 4 shows that mixture distribution of randomly sampling neg-\n",
      "discriminatorlossforthewordembeddingtaskon ative examples along with an adaptive negative\n",
      "g samples are always higher than on p sam- sampler leads to improved performances on a va-\n",
      "θ nce\n",
      "ples, confirming that the generator is indeed sam- rietyofembeddingtasks. Wevalidateourhypoth-\n",
      "plinghardernegatives. esisthathardnegativeexamplesarecriticaltoop-\n",
      "For Hypernym Prediction task, Fig.2 shows dis- timal learning and can be proposed via our ACE\n",
      "criminator loss on negative pairs sampled from framework. Finally, we find that controlling the\n",
      "NCE and ACE respectively. The higher the loss entropy of the generator through a regularization\n",
      "theharderthenegativepairis. Asindicatedinthe term and properly handling false negatives is cru-\n",
      "leftplot,lossontheACEnegativetermscollapses cialforsuccessfultraining.\n",
      "Acknowledgments Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,\n",
      "Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-\n",
      "We would like to thank Teng Long for providing tan Ruppin. 2001. Placing search in context: The\n",
      "the initial baseline code on knowledge graph em- conceptrevisited. InProceedingsofthe10thinter-\n",
      "nationalconferenceonWorldWideWeb,pages406–\n",
      "beddings,MatthewE.Taylorforproofreadingthe\n",
      "414.ACM.\n",
      "manuscript and Jackie Chi Kit Cheung for sug-\n",
      "gestions on preparing for the ACL oral presenta- Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\n",
      "tion. Additionally,wewouldalsoliketoacknowl- BingXu,DavidWarde-Farley,SherjilOzair,Aaron\n",
      "Courville, and Yoshua Bengio. 2014a. Generative\n",
      "edge Jordana Feldman for editing a blog post on\n",
      "adversarial nets. In Z. Ghahramani, M. Welling,\n",
      "this work and April Cooper for creating the art-\n",
      "C. Cortes, N. D. Lawrence, and K. Q. Weinberger,\n",
      "worksfortheblogpost. Finally,weappreciatethe editors,AdvancesinNeuralInformationProcessing\n",
      "broaderBorealisAIteamfordiscussionandemo- Systems27,pages2672–2680.\n",
      "tionalsupport.\n",
      "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\n",
      "BingXu,DavidWarde-Farley,SherjilOzair,Aaron\n",
      "Courville, and Yoshua Bengio. 2014b. Generative\n",
      "References adversarialnets. InAdvancesinneuralinformation\n",
      "processingsystems,pages2672–2680.\n",
      "Martin Arjovsky, Soumith Chintala, and Le´on Bot-\n",
      "tou. 2017. Wasserstein GAN. arXiv preprint Ian J Goodfellow. 2014. On distinguishability crite-\n",
      "arXiv:1701.07875. riaforestimatinggenerativemodels. arXivpreprint\n",
      "arXiv:1412.6515.\n",
      "DavidBelangerandAndrewMcCallum.2016. Struc-\n",
      "tured prediction energy networks. In International Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff\n",
      "ConferenceonMachineLearning,pages983–992. Roeder,andDavidDuvenaud.2017. Backpropaga-\n",
      "tion through the void: Optimizing control variates\n",
      "Antoine Bordes, Xavier Glorot, Jason Weston, and for black-box gradient estimation. arXiv preprint\n",
      "YoshuaBengio.2014. Asemanticmatchingenergy arXiv:1711.00123.\n",
      "functionforlearningwithmulti-relationaldata. Ma-\n",
      "chineLearning,94(2):233–259. Edouard Grave, Armand Joulin, Moustapha Cisse´,\n",
      "David Grangier, and Herve´ Je´gou. 2016. Efficient\n",
      "Antoine Bordes, Nicolas Usunier, Alberto Garcia- softmax approximation for GPUs. arXiv preprint\n",
      "Duran, Jason Weston, and Oksana Yakhnenko. arXiv:1609.04309.\n",
      "2013. Translating embeddings for modeling multi-\n",
      "relational data. In Advances in neural information IshaanGulrajani,FarukAhmed,MartinArjovsky,Vin-\n",
      "processingsystems,pages2787–2795. cent Dumoulin, and Aaron C Courville. 2017. Im-\n",
      "proved training of wasserstein gans. In Advances\n",
      "LiweiCaiandWilliamYangWang.2017. Kbgan: Ad- in Neural Information Processing Systems, pages\n",
      "versariallearningforknowledgegraphembeddings. 5769–5779.\n",
      "arXivpreprintarXiv:1711.04071.\n",
      "MichaelGutmannandAapoHyva¨rinen.2010. Noise-\n",
      "Yanshuai Cao, Gavin Weiguang Ding, Kry Yik-Chau contrastive estimation: A new estimation principle\n",
      "Lui, and Ruitong Huang. 2018. Improving GAN forunnormalizedstatisticalmodels. InProceedings\n",
      "trainingviabinarizedrepresentationentropy(BRE) oftheThirteenthInternationalConferenceonArtifi-\n",
      "regularization. In International Conference on cialIntelligenceandStatistics,pages297–304.\n",
      "LearningRepresentations.\n",
      "Michael U Gutmann and Aapo Hyva¨rinen. 2012.\n",
      "BoDaiandDahuaLin.2017. Contrastivelearningfor Noise-contrastive estimation of unnormalized sta-\n",
      "image captioning. In Advances in Neural Informa- tistical models, with applications to natural image\n",
      "tionProcessingSystems,pages898–907. statistics. Journal of Machine Learning Research,\n",
      "13(Feb):307–361.\n",
      "Tim Dettmers, Pasquale Minervini, Pontus Stene-\n",
      "torp, and Sebastian Riedel. 2017. Convolutional EricJang,ShixiangGu,andBenPoole.2016. Categor-\n",
      "2d knowledge graph embeddings. arXiv preprint icalreparameterizationwithgumbel-softmax. arXiv\n",
      "arXiv:1707.01476. preprintarXiv:1611.01144.\n",
      "Chris Dyer. 2014. Notes on noise contrastive es- Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and\n",
      "timation and negative sampling. arXiv preprint Jun Zhao. 2015. Knowledge graph embedding via\n",
      "arXiv:1410.8251. dynamic mapping matrix. In Proceedings of the\n",
      "53rdAnnualMeetingoftheAssociationforCompu-\n",
      "William Fedus, Ian Goodfellow, and Andrew M Dai. tational Linguistics and the 7th International Joint\n",
      "2018. MaskGAN: Better text generation via filling Conference on Natural Language Processing (Vol-\n",
      "inthe. arXivpreprintarXiv:1801.07736. ume1: LongPapers),volume1,pages687–696.\n",
      "Diederik P Kingma and Jimmy Ba. 2014. Adam: A NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,\n",
      "method for stochastic optimization. arXiv preprint Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff\n",
      "arXiv:1412.6980. Dean. 2017. Outrageously large neural networks:\n",
      "The sparsely-gated mixture-of-experts layer. arXiv\n",
      "YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,and preprintarXiv:1701.06538.\n",
      "Xuan Zhu. 2015. Learning entity and relation em-\n",
      "beddingsforknowledgegraphcompletion. InAAAI, Abhinav Shrivastava, Abhinav Gupta, and Ross Gir-\n",
      "volume15,pages2181–2187. shick.2016. Trainingregion-basedobjectdetectors\n",
      "withonlinehardexamplemining. InProceedingsof\n",
      "Hao Liu, Yihao Feng, Yi Mao, Dengyong Zhou, Jian the IEEE Conference on Computer Vision and Pat-\n",
      "Peng,andQiangLiu.2018. Action-dependentcon- ternRecognition,pages761–769.\n",
      "trol variates for policy optimization via stein iden-\n",
      "NoahASmithandJasonEisner.2005. Contrastivees-\n",
      "tity. InInternationalConferenceonLearningRep-\n",
      "timation: Training log-linear models on unlabeled\n",
      "resentations.\n",
      "data. In Proceedings of the 43rd Annual Meeting\n",
      "onAssociationforComputationalLinguistics,pages\n",
      "Thang Luong, Richard Socher, and Christopher D\n",
      "354–362. Association for Computational Linguis-\n",
      "Manning. 2013. Better word representations with\n",
      "tics.\n",
      "recursive neural networks for morphology. In\n",
      "CoNLL,pages104–113.\n",
      "Ben Taskar, Vassil Chatalbashev, Daphne Koller, and\n",
      "Carlos Guestrin. 2005. Learning structured predic-\n",
      "Chris J Maddison, Andriy Mnih, and Yee Whye Teh.\n",
      "tionmodels: Alargemarginapproach. InProceed-\n",
      "2016. The concrete distribution: A continuous\n",
      "ings of the 22nd international conference on Ma-\n",
      "relaxation of discrete random variables. arXiv\n",
      "chinelearning,pages896–903.ACM.\n",
      "preprintarXiv:1611.00712.\n",
      "The´oTrouillon,JohannesWelbl,SebastianRiedel,E´ric\n",
      "TomasMikolov,IlyaSutskever,KaiChen,GregSCor- Gaussier, and Guillaume Bouchard. 2016. Com-\n",
      "rado, and Jeff Dean. 2013. Distributed representa- plex embeddings for simple link prediction. In In-\n",
      "tionsofwordsandphrasesandtheircompositional- ternationalConferenceonMachineLearning,pages\n",
      "ity. In Advances in neural information processing 2071–2080.\n",
      "systems,pages3111–3119.\n",
      "Ioannis Tsochantaridis, Thorsten Joachims, Thomas\n",
      "M.MirzaandS.Osindero.2014. ConditionalGenera- Hofmann, and Yasemin Altun. 2005. Large mar-\n",
      "tiveAdversarialNets. ArXive-prints. gin methods for structured and interdependent out-\n",
      "putvariables. Journalofmachinelearningresearch,\n",
      "AndriyMnihandKorayKavukcuoglu.2013. Learning 6(Sep):1453–1484.\n",
      "word embeddings efficiently with noise-contrastive\n",
      "estimation. InAdvancesinneuralinformationpro- Lifu Tu and Kevin Gimpel. 2018. Learning approx-\n",
      "cessingsystems,pages2265–2273. imate inference networks for structured prediction.\n",
      "InInternationalConferenceonLearningRepresen-\n",
      "AndriyMnihandYeeWhyeTeh.2012. Afastandsim- tations.\n",
      "ple algorithm for training neural probabilistic lan-\n",
      "GeorgeTucker,AndriyMnih,ChrisJMaddison,John\n",
      "guagemodels. arXivpreprintarXiv:1206.6426.\n",
      "Lawson, and Jascha Sohl-Dickstein. 2017. Rebar:\n",
      "Low-variance, unbiased gradient estimates for dis-\n",
      "Jeffrey Pennington, Richard Socher, and Christopher\n",
      "crete latent variable models. In I. Guyon, U. V.\n",
      "Manning. 2014. Glove: Global vectors for word\n",
      "Luxburg,S.Bengio,H.Wallach,R.Fergus,S.Vish-\n",
      "representation. In Proceedings of the 2014 confer-\n",
      "wanathan,andR.Garnett,editors,AdvancesinNeu-\n",
      "enceonempiricalmethodsinnaturallanguagepro-\n",
      "ralInformationProcessingSystems30,pages2627–\n",
      "cessing(EMNLP),pages1532–1543.\n",
      "2636.CurranAssociates,Inc.\n",
      "StevenJRennie,EtienneMarcheret,YoussefMroueh,\n",
      "Ashish Vaswani, Yinggong Zhao, Victoria Fossum,\n",
      "Jarret Ross, and Vaibhava Goel. 2016. Self-critical\n",
      "and David Chiang. 2013. Decoding with large-\n",
      "sequence training for image captioning. arXiv\n",
      "scale neural language models improves translation.\n",
      "preprintarXiv:1612.00563.\n",
      "In Proceedings of the 2013 Conference on Empiri-\n",
      "calMethodsinNaturalLanguageProcessing,pages\n",
      "Francisco JR Ruiz, Michalis K Titsias, Adji B Dieng,\n",
      "1387–1392.\n",
      "and David M Blei. 2018. Augment and reduce:\n",
      "Stochastic inference for large categorical distribu- Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel\n",
      "tions. arXivpreprintarXiv:1802.04220. Urtasun. 2016. Order-embeddings of images and\n",
      "language. InInternationalConferenceonLearning\n",
      "Florian Schroff, Dmitry Kalenichenko, and James Representations.\n",
      "Philbin. 2015. Facenet: A unified embedding for\n",
      "face recognition and clustering. In Proceedings of PeifengWang,ShuangyinLi,andRongPan.2018. In-\n",
      "the IEEE Conference on Computer Vision and Pat- corporating GAN for negative sampling in knowl-\n",
      "ternRecognition,pages815–823. edge representation learning. In The Thirty-Second\n",
      "AAAI Conference on Artificial Intelligence (AAAI-\n",
      "18).\n",
      "Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\n",
      "Chen.2014. Knowledgegraphembeddingbytrans-\n",
      "latingonhyperplanes. InProceedingsoftheTwenty-\n",
      "Eighth AAAI Conference on Artificial Intelligence,\n",
      "pages1112–1119.AAAIPress.\n",
      "Ronald J Williams. 1992. Simple statistical gradient-\n",
      "following algorithms for connectionist reinforce-\n",
      "mentlearning. Machinelearning,8(3-4):229–256.\n",
      "Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\n",
      "Gao, and Li Deng. 2014. Embedding entities and\n",
      "relations for learning and inference in knowledge\n",
      "bases. arXivpreprintarXiv:1412.6575.\n",
      "BengioYoshua,DucharmeRejean,VincentPascal,and\n",
      "Jauvin Christian.2003. Aneuralprobabilisticlan-\n",
      "guage model. Journal of Machine Learning Re-\n",
      "search.\n",
      "JunboZhao,MichaelMathieu,andYannLeCun.2016.\n",
      "Energy-basedgenerativeadversarialnetwork. arXiv\n",
      "preprintarXiv:1609.03126.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  62965,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Word Embeddings', 'Hypernym Prediction', 'Knowledge Graph Embeddings', 'Link Prediction']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Adversarial Contrastive Estimation\n",
      "AvishekJoeyBose1,2,∗† HuanLing1,2,∗† YanshuaiCao1,∗\n",
      "1BorealisAI 2UniversityofToronto\n",
      "{joey.bose,huan.ling}@mail.utoronto.ca\n",
      "{yanshuai.cao}@borealisai.com\n",
      "Abstract modelingneed,ascertainassumptionsarebestex-\n",
      "pressed as some score or energy in margin based\n",
      "Learning by contrasting positive and neg- or un-normalized probability models (Smith and\n",
      "ativesamplesisageneralstrategyadopted Eisner, 2005). For example, modeling entity re-\n",
      "by many methods. Noise contrastive lationsastranslationsorvariantsthereofinavec-\n",
      "estimation (NCE) for word embeddings torspacenaturallyleadstoadistance-basedscore\n",
      "andtranslatingembeddingsforknowledge tobeminimizedforobservedentity-relation-entity\n",
      "graphs are examples in NLP employing triplets(Bordesetal.,2013).\n",
      "this approach. In this work, we view Given a scoring function, the gradient of the\n",
      "contrastive learning as an abstraction of model’sparametersonobservedpositiveexamples\n",
      "all such methods and augment the neg- can be readily computed, but the negative phase\n",
      "ative sampler into a mixture distribution requiresadesigndecisiononhowtosampledata.\n",
      "containing an adversarially learned sam- In noise contrastive estimation for word embed-\n",
      "pler. The resulting adaptive sampler finds dings, a negative example is formed by replacing\n",
      "harder negative examples, which forces acomponentofapositivepairbyrandomlyselect-\n",
      "the main model to learn a better represen- ingasampledwordfromthevocabulary,resulting\n",
      "tation of the data. We evaluate our pro- in a fictitious word-context pair which would be\n",
      "posalonlearningwordembeddings,order unlikelytoactuallyexistinthedataset. Thisnega-\n",
      "embeddingsandknowledgegraphembed- tive sampling by corruption approach is also used\n",
      "dingsandobservebothfasterconvergence inlearningknowledgegraphembeddings(Bordes\n",
      "andimprovedresultsonmultiplemetrics. etal.,2013;Linetal.,2015;Jietal.,2015;Wang\n",
      "et al., 2014; Trouillon et al., 2016; Yang et al.,\n",
      "1 Introduction 2014; Dettmers et al., 2017), order embeddings\n",
      "(Vendrovetal.,2016),captiongeneration(Daiand\n",
      "Many models learn by contrasting losses on ob-\n",
      "Lin,2017),etc.\n",
      "served positive examples with those on some fic-\n",
      "Typicallythecorruptiondistributionisthesame\n",
      "titiousnegativeexamples,tryingtodecreasesome\n",
      "for all inputs like in skip-gram or CBOW NCE,\n",
      "score on positive ones while increasing it on neg-\n",
      "rather than being a conditional distribution that\n",
      "ative ones. There are multiple reasons why such\n",
      "takes into account information about the input\n",
      "contrastive learning approach is needed. Com-\n",
      "sampleunderconsideration. Furthermore,thecor-\n",
      "putational tractability is one. For instance, in-\n",
      "ruption process usually only encodes a human\n",
      "steadofusingsoftmaxtopredictawordforlearn-\n",
      "prior as to what constitutes a hard negative sam-\n",
      "ing word embeddings, noise contrastive estima-\n",
      "ple,ratherthanbeinglearnedfromdata. Forthese\n",
      "tion (NCE) (Dyer, 2014; Mnih and Teh, 2012)\n",
      "two reasons, the simple fixed corruption process\n",
      "can be used in skip-gram or CBOW word em-\n",
      "often yields only easy negative examples. Easy\n",
      "bedding models (Gutmann and Hyva¨rinen, 2012;\n",
      "negativesaresub-optimalforlearningdiscrimina-\n",
      "Mikolov et al., 2013; Mnih and Kavukcuoglu,\n",
      "tive representation as they do not force the model\n",
      "2013; Vaswani et al., 2013). Another reason is\n",
      "tofindcriticalcharacteristicsofobservedpositive\n",
      "∗authorscontributedequally data, which has been independently discovered in\n",
      "†WorkdonewhileauthorwasaninternatBorealisAI applications outside NLP previously (Shrivastava\n",
      "8102\n",
      "guA\n",
      "2\n",
      "]LC.sc[\n",
      "3v24630.5081:viXra\n",
      "etal.,2016). Evenifhardnegativesareoccasion- with respect to some joint distribution over pos-\n",
      "ally reached, the infrequency means slow conver- itive and negative samples. Furthermore, by\n",
      "gence. Designingamoresophisticatedcorruption the law of total expectation, and the fact that\n",
      "process could be fruitful, but requires costly trial- given x+, the negative sampling is not depen-\n",
      "and-errorbyahumanexpert. dent on the positive label, i.e. p(y+,y−|x+) =\n",
      "In this work, we propose to augment the sim- p(y+|x+)p(y−|x+),Eq.1canbere-writtenas\n",
      "plecorruptionnoiseprocessinvariousembedding\n",
      "models with an adversarially learned conditional E p(x+)[E p(y+|x+)p(y−|x+)l ω(x+,y+,y−)] (2)\n",
      "distribution, forming a mixture negative sampler\n",
      "that adapts to the underlying data and the em- Separableloss\n",
      "bedding model training progress. The resulting Inthecasewherethelossdecomposesintoasum\n",
      "methodisreferredtoasadversarialcontrastivees- of scores on positive and negative tuples such as\n",
      "timation (ACE). The adaptive conditional model l (x+,y+,y−) = s (x+,y+)−s˜ (x+,y−),then\n",
      "ω ω ω\n",
      "engagesinaminimaxgamewiththeprimaryem- Expression.2becomes\n",
      "beddingmodel,muchlikeinGenerativeAdversar-\n",
      "ial Networks (GANs) (Goodfellow et al., 2014a), E [E s (x,y)−E s˜ (x,y)]\n",
      "p+(x) p+(y|x) ω p−(y|x) ω\n",
      "where a discriminator net (D), tries to distinguish (3)\n",
      "samples produced by a generator (G) from real where we moved the + and − to p for notational\n",
      "data(Goodfellowetal.,2014b). InACE,themain brevity. Learning by stochastic gradient descent\n",
      "model learns to distinguish between a real posi- aims to adjust ω to pushing down s (x,y) on\n",
      "ω\n",
      "tive example and a negative sample selected by samples from p+ while pushing up s˜ (x,y) on\n",
      "ω\n",
      "themixtureofafixedNCEsamplerandanadver- samples from p−. Note that for generality, the\n",
      "sarial generator. The main model and the genera- scoringfunctionfornegativesamples,denotedby\n",
      "tor takes alternating turns to update their parame- s˜, could be slightly different from s. For in-\n",
      "ω ω\n",
      "ters. In fact, our method can be viewed as a con- stance, s˜could contain a margin as in the case of\n",
      "ditionalGAN(MirzaandOsindero,2014)ondis- OrderEmbeddingsinSec.4.2.\n",
      "creteinputs,withamixturegeneratorconsistingof\n",
      "Nonseparableloss\n",
      "a learned and a fixed distribution, with additional\n",
      "techniques introduced to achieve stable and con- Eq. 1 is the general form that we would like to\n",
      "vergenttrainingofembeddingmodels. consider because for certain problems, the loss\n",
      "InourproposedACEapproach,theconditional function cannot be separated into sums of terms\n",
      "sampler finds harder negatives than NCE, while containing only positive (x+,y+) and terms with\n",
      "being able to gracefully fall back to NCE when- negatives (x+,y−). An example of such a non-\n",
      "ever the generator cannot find hard negatives. We separable loss is the triplet ranking loss (Schroff\n",
      "demonstratetheefficacyandgeneralityofthepro- et al., 2015): l ω = max(0,η + s ω(x+,y+) −\n",
      "posed method on three different learning tasks, s ω(x+,y−)), which does not decompose due to\n",
      "word embeddings (Mikolov et al., 2013), order therectification.\n",
      "embeddings(Vendrovetal.,2016)andknowledge\n",
      "Noisecontrastiveestimation\n",
      "graphembeddings(Jietal.,2015).\n",
      "The typical NCE approach in tasks such as word\n",
      "2 Method embeddings (Mikolov et al., 2013), order embed-\n",
      "dings(Vendrovetal.,2016),andknowledgegraph\n",
      "2.1 Background: contrastivelearning\n",
      "embeddingscanbeviewedasaspecialcaseofEq.\n",
      "In the most general form, our method applies to 2 by taking p(y−|x+) to be some unconditional\n",
      "supervised learning problems with a contrastive\n",
      "p (y).\n",
      "nce\n",
      "objectiveofthefollowingform:\n",
      "Thisleadstoefficientcomputationduringtrain-\n",
      "L(ω) = E l (x+,y+,y−) (1) ing,however,p nce(y)sacrificesthesamplingeffi-\n",
      "p(x+,y+,y−) ω\n",
      "ciencyoflearningasthenegativesproducedusing\n",
      "where l (x+,y+,y−) captures both the model afixeddistributionarenottailoredtowardx+,and\n",
      "ω\n",
      "with parameters ω and the loss that scores a asaresultarenotnecessarilyhardnegativeexam-\n",
      "positive tuple (x+,y+) against a negative one ples. Thus, the model is not forced to discover\n",
      "(x+,y−). E (.) denotes expectation discriminative representation of observed positive\n",
      "p(x+,y+,y−)\n",
      "data. As training progresses, more and more neg- Instead, we use the REINFORCE (Williams,\n",
      "ativeexamplesarecorrectlylearned,theprobabil- 1992)gradientestimatorfor∇ L(θ,x):\n",
      "θ\n",
      "ityofdrawingahardnegativeexamplediminishes\n",
      "(1−λ)E(cid:2) −l (x,y+,y−)∇ log(g (y−|x))(cid:3) (6)\n",
      "further,causingslowconvergence. ω θ θ\n",
      "where the expectation E is with respect to\n",
      "2.2 Adversarialmixturenoise\n",
      "p(y+,y−|x) = p(y+|x)g (y−|x), and the dis-\n",
      "θ\n",
      "To remedy the above mentioned problem of a criminatorlossl (x,y+,y−)actsasthereward.\n",
      "ω\n",
      "fixed unconditional negative sampler, we propose\n",
      "With a separable loss, the (conditional) value\n",
      "toaugmentitintoamixtureone,λp (y)+(1−\n",
      "nce functionoftheminimaxgameis:\n",
      "λ)g (y|x), where g is a conditional distribution\n",
      "θ θ\n",
      "with a learnable parameter θ and λ is a hyper- L(ω,θ;x) = E s (x,y)\n",
      "p+(y|x) ω\n",
      "parameter. The objective in Expression. 2 can −E s˜ (x,y)−E s˜ (x,y) (7)\n",
      "thenbewrittenas(conditionedonxfornotational\n",
      "pnce(y) ω g θ(y|x) ω\n",
      "brevity): and only the last term depends on the generator\n",
      "parameterω. Hence,withaseparableloss,there-\n",
      "L(ω,θ;x) = λE l (x,y+,y−) wardis−s˜(x+,y−). Thisreductiondoesnothap-\n",
      "p(y+|x)pnce(y−) ω\n",
      "+(1−λ)E l (x,y+,y−) (4) penwithanon-separableloss,andwehavetouse\n",
      "p(y+|x)g (y−|x) ω\n",
      "θ l (x,y+,y−).\n",
      "ω\n",
      "Welearn(ω,θ)inaGAN-styleminimaxgame:\n",
      "2.4 Entropyandtrainingstability\n",
      "minmaxV(ω,θ) = minmaxE L(ω,θ;x) GAN training can suffer from instability and de-\n",
      "p+(x)\n",
      "ω θ ω θ generacy where the generator probability mass\n",
      "(5)\n",
      "collapses to a few modes or points. Much work\n",
      "The embedding model behind l (x,y+,y−) is\n",
      "ω has been done to stabilize GAN training in the\n",
      "similar to the discriminator in (conditional) GAN\n",
      "continuous case (Arjovsky et al., 2017; Gulrajani\n",
      "(or critic in Wasserstein (Arjovsky et al., 2017)\n",
      "et al., 2017; Cao et al., 2018). In ACE, if the\n",
      "or Energy-based GAN (Zhao et al., 2016), while\n",
      "generator g probability mass collapses to a few\n",
      "g (y|x) acts as the generator. Henceforth, we θ\n",
      "θ candidates, then after the discriminator success-\n",
      "willusethetermdiscriminator(D)andembedding\n",
      "fullylearnsaboutthesenegatives,g cannotadapt\n",
      "modelinterchangeably,andrefertog asthegen- θ\n",
      "θ to select new hard negatives, because the REIN-\n",
      "erator.\n",
      "FORCEgradientestimatorEq.6reliesong being\n",
      "θ\n",
      "able to explore other candidates during sampling.\n",
      "2.3 Learningthegenerator\n",
      "Therefore,iftheg probabilitymasscollapses,in-\n",
      "θ\n",
      "ThereisoneimportantdistinctiontotypicalGAN:\n",
      "stead of leading to oscillation as in typical GAN,\n",
      "g (y|x)definesacategoricaldistributionoverpos-\n",
      "θ themin-maxgameinACEreachesanequilibrium\n",
      "sibleyvalues,andsamplesaredrawnaccordingly;\n",
      "wherethediscriminatorwinsandg cannolonger\n",
      "θ\n",
      "in contrast to typical GAN over continuous data\n",
      "adapt,thenACEfallsbacktoNCEsincethenega-\n",
      "space such as images, where samples are gener-\n",
      "tivesamplerhasanothermixturecomponentfrom\n",
      "ated by an implicit generative model that warps\n",
      "NCE.\n",
      "noise vectors into data points. Due to the discrete\n",
      "ThisbehaviorofgracefullyfallingbacktoNCE\n",
      "samplingstep,g cannotlearnbyreceivinggradi-\n",
      "θ is more desirable than the alternative of stalled\n",
      "ent through the discriminator. One possible solu- training if p−(y|x) does not have a simple p\n",
      "nce\n",
      "tion is to use the Gumbel-softmax reparametriza-\n",
      "mixturecomponent. However, wewouldstilllike\n",
      "tion trick (Jang et al., 2016; Maddison et al.,\n",
      "to avoid such collapse, as the adversarial samples\n",
      "2016),whichgivesadifferentiableapproximation.\n",
      "provide greater learning signals than NCE sam-\n",
      "However,thisdifferentiabilitycomesatthecostof\n",
      "ples. To this end, we propose to use a regularizer\n",
      "drawing N Gumbel samples per each categorical\n",
      "to encourage the categorical distribution g (y|x)\n",
      "θ\n",
      "sample,whereN isthenumberofcategories. For\n",
      "tohavehighentropy. Inordertomakethethereg-\n",
      "word embeddings, N is the vocabulary size, and\n",
      "ularizerinterpretableanditshyperparameterseasy\n",
      "for knowledge graph embeddings, N is the num-\n",
      "totune,wedesignthefollowingform:\n",
      "berofentities,bothleadingtoinfeasiblecomputa-\n",
      "tionalrequirements. R (x) = max(0,c−H(g (y|x))) (8)\n",
      "ent θ\n",
      "whereH(g (y|x))istheentropyofthecategorical self-criticalbaselinemethod(Rennieetal.,2016),\n",
      "θ\n",
      "distributiong (y|x),andc = log(k)istheentropy where the baseline is b(x) = l (y+,y(cid:63),x), or\n",
      "θ ω\n",
      "of a uniform distribution over k choices, and k is b(x) = −s˜ (y(cid:63),x)intheseparablelosscase,and\n",
      "ω\n",
      "a hyper-parameter. Intuitively, R expresses the y(cid:63) = argmax g (y |x). In other words, the base-\n",
      "ent i θ i\n",
      "priorthatthegeneratorshouldspreaditsmassover lineistherewardofthemostlikelysampleaccord-\n",
      "morethank choicesforeachx. ingtothegenerator.\n",
      "2.5 Handlingfalsenegatives 2.7 Improvingexplorationing by\n",
      "θ\n",
      "Duringnegativesampling,p−(y|x)couldactually leveragingNCEsamples\n",
      "produce y that forms a positive pair that exists in\n",
      "In Sec. 2.4 we touched on the need for sufficient\n",
      "the training set, i.e., a false negative. This possi-\n",
      "exploration in g. It is possible to also leverage\n",
      "θ\n",
      "bility exists in NCE already, but since p is not\n",
      "nce negative samples from NCE to help the gener-\n",
      "adaptive,theprobabilityofsamplingafalsenega-\n",
      "ator learn. This is essentially off-policy explo-\n",
      "tive is low. Hence in NCE, the score on this false\n",
      "ration in reinforcement learning since NCE sam-\n",
      "negative (true observation) pair is pushed up less\n",
      "plesarenotdrawnaccordingtog (y|x). Thegen-\n",
      "θ\n",
      "inthenegativetermthaninthepositiveterm.\n",
      "erator learning can use importance re-weighting\n",
      "However, with the adaptive sampler, g (y|x),\n",
      "ω to leverage those samples. The resulting REIN-\n",
      "falsenegativesbecomeamuchmoresevereissue.\n",
      "FORCE gradient estimator is basically the same\n",
      "g (y|x)canlearntoconcentrateitsmassonafew\n",
      "ω asEq.6exceptthattherewardsarereweightedby\n",
      "false negatives, significantly canceling the learn- g (y−|x)/p (y−), and the expectation is with\n",
      "θ nce\n",
      "ing of those observations in the positive phase. respect to p(y+|x)p (y−). This additional off-\n",
      "nce\n",
      "Theentropyregularizationreducesthisproblemas\n",
      "policy learning term provides gradient informa-\n",
      "itforcesthegeneratortospreaditsmass,hencere- tionforgeneratorlearningifg (y−|x)isnotzero,\n",
      "θ\n",
      "ducingthechanceofafalsenegative.\n",
      "meaning that for it to be effective in helping ex-\n",
      "To further alleviate this problem, whenever\n",
      "ploration,thegeneratorcannotbecollapsedatthe\n",
      "computationally feasible, we apply an additional\n",
      "first place. Hence, in practice, this term is only\n",
      "two-steptechnique. First,wemaintainahashmap\n",
      "usedtofurtherhelpontopoftheentropyregular-\n",
      "of the training data in memory, and use it to effi-\n",
      "ization,butitdoesnotreplaceit.\n",
      "ciently detect if a negative sample (x+,y−) is an\n",
      "actual observation. If so, its contribution to the 3 RelatedWork\n",
      "lossisgivenazeroweightinωlearningstep. Sec-\n",
      "ond,toupdateθinthegeneratorlearningstep,the Smith and Eisner (2005) proposed contrastive es-\n",
      "reward for false negative samples are replaced by timation as a way for unsupervised learning of\n",
      "a large penalty, so that the REINFORCE gradient log-linearmodelsbytakingimplicitevidencefrom\n",
      "update would steer g away from those samples. user-defined neighborhoods around observed dat-\n",
      "θ\n",
      "The second step is needed to prevent null compu- apoints. Gutmann and Hyva¨rinen (2010) intro-\n",
      "tation where g learns to sample false negatives duced NCE as an alternative to the hierarchical\n",
      "θ\n",
      "which are subsequently ignored by the discrimi- softmax. IntheworksofMnihandTeh(2012)and\n",
      "natorupdateforω. MnihandKavukcuoglu(2013),NCEisappliedto\n",
      "log-bilinear models and Vaswani et al. (2013) ap-\n",
      "2.6 VarianceReduction\n",
      "pliedNCEtoneuralprobabilisticlanguagemodels\n",
      "The basic REINFORCE gradient estimator is (Yoshuaetal.,2003). Comparedtotheseprevious\n",
      "poised with high variance, so in practice one of- NCE methods that rely on simple fixed sampling\n",
      "ten needs to apply variance reduction techniques. heuristics,ACEusesanadaptivesamplerthatpro-\n",
      "The most basic form of variance reduction is to duceshardernegatives.\n",
      "subtract a baseline from the reward. As long as In the domain of max-margin estimation for\n",
      "thebaselineisnotafunctionofactions(i.e.,sam- structured prediction (Taskar et al., 2005), loss\n",
      "ples y− being drawn), the REINFORCE gradi- augmented MAP inference plays the role of find-\n",
      "ent estimator remains unbiased. More advanced inghardnegatives(thehardest). However,thisin-\n",
      "gradient estimators exist that also reduce vari- ferenceisonlytractableinalimitedclassofmod-\n",
      "ance (Grathwohl et al., 2017; Tucker et al., 2017; els such structured SVM (Tsochantaridis et al.,\n",
      "Liu et al., 2018), but for simplicity we use the 2005). Compared to those models that use exact\n",
      "maximization to find the hardest negative config- is that the negative context words are sampled in\n",
      "uration each time, the generator in ACE can be the same way, rather than tailored toward the ac-\n",
      "viewed as learning an approximate amortized in- tual target word. To apply ACE to this problem\n",
      "ference network. Concurrently to this work, Tu we first define the value function for the minimax\n",
      "andGimpel(2018)proposesaverysimilarframe- game,V(D,G),asfollows:\n",
      "work,usingalearnedinferencenetworkforStruc-\n",
      "tured prediction energy networks (SPEN) (Be- V(D,G) = E [logD(w,w )]\n",
      "p+(wc) c t\n",
      "langerandMcCallum,2016). −E [−log(1−D(w,w ))] (10)\n",
      "pnce(wc) c t\n",
      "Concurrent with our work, there have been\n",
      "−E [−log(1−D(w,w ))]\n",
      "otherinterestsinapplyingtheGANtoNLPprob- g θ(wc|wt) c t\n",
      "lems (Fedus et al., 2018; Wang et al., 2018; Cai\n",
      "withD = p(y = 1|w,w )andG = g (w |w ).\n",
      "andWang,2017). Knowledgegraphmodelsnatu- t c θ c t\n",
      "rally lend to a GAN setup, and has been the sub-\n",
      "Implementationdetails\n",
      "ject of study in Wang et al. (2018) and Cai and\n",
      "For our experiments, we train all our models on\n",
      "Wang (2017). These two concurrent works are\n",
      "a single pass of the May 2017 dump of the En-\n",
      "most closely related to one of the three tasks on\n",
      "glish Wikipedia with lowercased unigrams. The\n",
      "whichwestudyACEinthiswork. Besidesamore\n",
      "vocabulary size is restricted to the top 150k most\n",
      "general formulation that applies to problems be-\n",
      "frequent words when training from scratch while\n",
      "yond those considered in Wang et al. (2018) and\n",
      "forfinetuningweusethesamevocabularyasPen-\n",
      "Cai and Wang (2017), the techniques introduced\n",
      "nington et al. (2014), which is 400k of the most\n",
      "in our work on handling false negatives and en-\n",
      "frequent words. We use 5 NCE samples for each\n",
      "tropy regularization lead to improved experimen-\n",
      "positivesampleand1adversarialsampleinawin-\n",
      "talresultsasshowninSec.5.4.\n",
      "dowsizeof10andthesamepositivesubsampling\n",
      "4 ApplicationofACEonthreetasks schemeproposedbyMikolovetal.(2013). Learn-\n",
      "ing for both G and D uses Adam (Kingma and\n",
      "4.1 WordEmbeddings Ba, 2014) optimizer with its default parameters.\n",
      "Our conditional discriminator is modeled using\n",
      "Wordembeddingslearnavectorrepresentationof\n",
      "the Skip-Gram architecture, which is a two layer\n",
      "wordsfromco-occurrencesinatextcorpus. NCE\n",
      "neuralnetworkwithalinearmappingbetweenthe\n",
      "casts this learning problem as a binary classifica-\n",
      "layers. The generator network consists of an em-\n",
      "tion where the model tries to distinguish positive\n",
      "bedding layer followed by two small hidden lay-\n",
      "word and context pairs, from negative noise sam-\n",
      "ers,followedbyanoutputsoftmaxlayer. Thefirst\n",
      "ples composed of word and false context pairs.\n",
      "layer of the generator shares its weights with the\n",
      "The NCE objective in Skip-gram (Mikolov et al.,\n",
      "second embedding layer in the discriminator net-\n",
      "2013) for word embeddings is a separable loss of\n",
      "work,whichwefindreallyspeedsupconvergence\n",
      "theform:\n",
      "(cid:88) asthegeneratordoesnothavetorelearnitsownset\n",
      "L = − [logp(y = 1|w,w+)\n",
      "t c of embeddings. The difference between the dis-\n",
      "wt∈V criminatorandgeneratoristhatasigmoidnonlin-\n",
      "(9)\n",
      "K\n",
      "(cid:88) earityisusedafterthesecondlayerinthediscrim-\n",
      "+ logp(y = 0|w,w−)]\n",
      "t c inator, while in the generator, a softmax layer is\n",
      "c=1\n",
      "usedtodefineacategoricaldistributionovernega-\n",
      "Here, w+ is sampled from the set of true con- tive word candidates. We find that controlling the\n",
      "c\n",
      "texts and w− ∼ Q is sampled k times from a generator entropy is critical for finetuning exper-\n",
      "c\n",
      "iments as otherwise the generator collapses to its\n",
      "fixed noise distribution. Mikolov et al. (2013) in-\n",
      "favorite negative sample. The word embeddings\n",
      "troduced a further simplification of NCE, called\n",
      "are taken to be the first dense matrix in the dis-\n",
      "“Negative Sampling” (Dyer, 2014). With respect\n",
      "criminator.\n",
      "to our ACE framework, the difference between\n",
      "NCE and Negative Sampling is inconsequential,\n",
      "4.2 OrderEmbeddingsHypernymPrediction\n",
      "sowecontinuethediscussionusingNCE.Adraw-\n",
      "back of this sampling scheme is that it favors As introduced in Vendrov et al. (2016), ordered\n",
      "more common words as context. Another issue representations over hierarchy can be learned by\n",
      "order embeddings. An example task for such or- take TransD as an example, and modify its noise\n",
      "dered representation is hypernym prediction. A contrastivelearningtoACE,anddemonstratesig-\n",
      "hypernympairisapairofconceptswherethefirst nificantimprovementinsampleefficiencyandlink\n",
      "concept is a specialization or an instance of the predictionresults.\n",
      "second.\n",
      "Implementationdetails\n",
      "Forcompleteness,webrieflydescribeorderem-\n",
      "Let a positive entity-relation-entity triplet be de-\n",
      "beddings,thenanalyzeACEonthehypernympre-\n",
      "notedbyξ+ = (h+,r+,t+),andanegativetriplet\n",
      "diction task. In order embeddings, each entity is\n",
      "represented by a vector in RN, the score for a couldeitherhaveitsheadortailbeanegativesam-\n",
      "ple,i.e. ξ− = (h−,r+,t+)orξ− = (h+,r+,t−).\n",
      "positive ordered pair of entities (x,y) is defined\n",
      "by s (x,y) = ||max(0,y − x)||2 and, score for In either case, the general formulation in Sec. 2.1\n",
      "ω\n",
      "a negative ordered pair (x+,y−) is defined by stillapplies. Thenon-separablelossfunctiontakes\n",
      "s˜ (x+,y−) = max{0,η−s(x+,y−)},whereisη ontheform:\n",
      "ω\n",
      "isthemargin. Letf(u)betheembeddingfunction\n",
      "l = max(0,η+s (ξ+)−s (ξ−)) (12)\n",
      "ω ω\n",
      "which takes an entity as input and outputs an em-\n",
      "bedding vector. We define P as a set of positive Thescoringruleis:\n",
      "pairs and N as negative pairs, the separable loss\n",
      "functionfororderembeddingtaskisdefinedby: s = (cid:107)h ⊥+r−t ⊥(cid:107) (13)\n",
      "(cid:88) (cid:88) where r is the embedding vector for r, and h is\n",
      "L= s (f(u),f(v)))+ s˜(f(u),f(v)) ⊥\n",
      "ω\n",
      "projection of the embedding of h onto the space\n",
      "(u,v)∈P (u,v)∈N\n",
      "of r by h = h + r h(cid:62)h, where r and h are\n",
      "(11) ⊥ p p p p\n",
      "projection parameters of the model. t is defined\n",
      "⊥\n",
      "Implementationdetails inasimilarwaythroughparameterst,t andr.\n",
      "p p\n",
      "Ourgeneratorforthistaskisjustalinearfullycon- Theformofthegeneratorg θ(t−|r+,h+)ischo-\n",
      "nectedsoftmaxlayer,takinganembeddingvector sen to be f θ(h ⊥,h ⊥ +r), where f θ is a feedfor-\n",
      "fromdiscriminatorasinputandoutputtingacate- wardneuralnetthatconcatenatesitstwoinputar-\n",
      "goricaldistributionovertheentityset. Forthedis- guments,thenpropagatesthroughtwohiddenlay-\n",
      "criminator,weinheritallmodelsettingfromVen- ers,followedbyafinalsoftmaxoutputlayer. Asa\n",
      "drov et al. (2016): we use 50 dimensions hidden functionof(r+,h+),g θ sharesparameterwiththe\n",
      "state and bash size 1000, a learning rate of 0.01 discriminator, as the inputs to f θ are the embed-\n",
      "andtheAdamoptimizer. Forthegenerator,weuse dingvectors. Duringgeneratorlearning, onlyθ is\n",
      "a batch size of 1000, a learning rate 0.01 and the updatedandtheTransDmodelembeddingparam-\n",
      "Adamoptimizer. Weapplyweightdecaywithrate etersarefrozen.\n",
      "0.1andentropylossregularizationasdescribedin\n",
      "5 Experiments\n",
      "Sec. 2.4. Wehandlefalsenegativeasdescribedin\n",
      "Sec. 2.5. After cross validation, variance reduc- We evaluate ACE with experiments on word\n",
      "tionandleveragingNCEsamplesdoesnotgreatly embeddings, order embeddings, and knowledge\n",
      "affecttheorderembeddingtask. graph embeddings tasks. In short, whenever\n",
      "the original learning objective is contrastive (all\n",
      "4.3 KnowledgeGraphEmbeddings\n",
      "tasks except Glove fine-tuning) our results con-\n",
      "Knowledgegraphscontainentityandrelationdata sistently show that ACE improves over NCE. In\n",
      "of the form (head entity, relation, tail entity), and somecases,weincludeadditionalcomparisonsto\n",
      "the goal is to learn from observed positive entity the state-of-art results on the task to put the sig-\n",
      "relations and predict missing links (a.k.a. link nificance of such improvements in context: the\n",
      "prediction). There have been many works on generic ACE can often make a reasonable base-\n",
      "knowledge graph embeddings, e.g. TransE (Bor- line competitive with SOTA methods that are op-\n",
      "desetal.,2013),TransR(Linetal.,2015),TransH timizedforthetask.\n",
      "(Wangetal.,2014),TransD(Jietal.,2015),Com- For word embeddings, we evaluate models\n",
      "plex(Trouillonetal.,2016),DistMult(Yangetal., trained from scratch as well as fine-tuned Glove\n",
      "2014)andConvE(Dettmersetal.,2017). Manyof models(Penningtonetal.,2014)onwordsimilar-\n",
      "themuseacontrastivelearningobjective. Herewe ity tasks that consist of computing the similarity\n",
      "Figure 1: Left: Order embedding Accuracy plot. Figure 2: losscurveonNCEnegativepairsandACE\n",
      "Right:OrderembeddingdiscriminatorLossplotonNCE negativepairs. Left: withoutentropyandweightdecay.\n",
      "samplednegativepairsandpositivepairs. Right:withentropyandweightdecay\n",
      "Figure 3: Left: Rare Word, Right: WS353 similarity scores during the first Figure 4: Training from scratch losses\n",
      "epochoftraining. ontheDiscriminator\n",
      "between word pairs where the ground truth is an NCE by 40.4% and 45.7%. We also evaluate\n",
      "average of human scores. We choose the Rare our model qualitatively by inspecting the nearest\n",
      "word dataset (Luong et al., 2013) and WordSim- neighbors of selected words in Table. 1. We first\n",
      "353 (Finkelstein et al., 2001) by virtue of our hy- present the five nearest neighbors to each word to\n",
      "pothesisthatACElearnsbetterrepresentationsfor show that both NCE and ACE models learn sen-\n",
      "both rare and frequent words. We also qualita- sible embeddings. We then show that ACE em-\n",
      "tivelyevaluateACEwordembeddingsbyinspect- beddings have much better semantic relevance in\n",
      "ingthenearestneighborsofselectedwords. alargerneighborhood(nearestneighbor45-50).\n",
      "For the hypernym prediction task, following\n",
      "V<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   3524,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['AvishekJoeyBose', 'HuanLing', 'YanshuaiCao']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: riminator\n",
      "between word pairs where the ground truth is an NCE by 40.4% and 45.7%. We also evaluate\n",
      "average of human scores. We choose the Rare our model qualitatively by inspecting the nearest\n",
      "word dataset (Luong et al., 2013) and WordSim- neighbors of selected words in Table. 1. We first\n",
      "353 (Finkelstein et al., 2001) by virtue of our hy- present the five nearest neighbors to each word to\n",
      "pothesisthatACElearnsbetterrepresentationsfor show that both NCE and ACE models learn sen-\n",
      "both rare and frequent words. We also qualita- sible embeddings. We then show that ACE em-\n",
      "tivelyevaluateACEwordembeddingsbyinspect- beddings have much better semantic relevance in\n",
      "ingthenearestneighborsofselectedwords. alargerneighborhood(nearestneighbor45-50).\n",
      "For the hypernym prediction task, following\n",
      "Vendrov et al. (2016), hypernym pairs are created 5.2 FinetuningWordEmbeddings\n",
      "from the WordNet hierarchy’s transitive closure.\n",
      "We take off-the-shelf pre-trained Glove embed-\n",
      "Weusethereleasedrandomdevelopmentsplitand\n",
      "dings which were trained using 6 billion tokens\n",
      "test split from Vendrov et al. (2016), which both\n",
      "(Pennington et al., 2014) and fine-tune them us-\n",
      "contain4000edges.\n",
      "ingouralgorithm. Itisinterestingtonotethatthe\n",
      "For knowledge graph embeddings, we use original Glove objective does not fit into the con-\n",
      "TransD (Ji et al., 2015) as our base model, and trastive learning framework, but nonetheless we\n",
      "perform ablation study to analyze the behavior of find that they benefit from ACE. In fact, we ob-\n",
      "ACE with various add-on features, and confirm servethattrainingsuchthat75%ofthewordsap-\n",
      "thatentropyregularizationiscrucialforgoodper- pear as positive contexts is sufficient to beat the\n",
      "formance in ACE. We also obtain link prediction largestdimensionalitypre-trainedGlovemodelon\n",
      "resultsthatarecompetitiveorsuperiortothestate- word similarity tasks. We evaluate our perfor-\n",
      "of-artsontheWN18dataset(Bordesetal.,2014). mance on the Rare Word and WordSim353 data.\n",
      "AscanbeseenfromourresultsinTable2,ACEon\n",
      "5.1 TrainingWordEmbeddingsfromscratch RWisnotalwaysbetterandforthe100dand300d\n",
      "Glove embeddings is marginally worse. How-\n",
      "In this experiment, we empirically observe that\n",
      "ever,onWordSim353ACEdoesconsiderablybet-\n",
      "training word embeddings using ACE converges\n",
      "ter across the board to the point where 50d Glove\n",
      "significantly faster than NCE after one epoch. As\n",
      "embeddings outperform the 300d baseline Glove\n",
      "shown in Fig. 3 both ACE (a mixture of p and\n",
      "nce\n",
      "model.\n",
      "g ) and just g (denoted by ADV) significantly\n",
      "θ θ\n",
      "outperforms the NCE baseline, with an absolute\n",
      "5.3 HypernymPrediction\n",
      "improvementof73.1%and58.5%respectivelyon\n",
      "RW score. We note similar results on WordSim- As shown in Table 3, with ACE training, our\n",
      "353 dataset where ACE and ADV outperforms method achieves a 1.5% improvement on accu-\n",
      "Queen King Computer Man Woman\n",
      "Skip-GramNCETop5 princess prince computers woman girl\n",
      "king queen computing boy man\n",
      "empress kings software girl prostitute\n",
      "pxqueen emperor microcomputer stranger person\n",
      "monarch monarch mainframe person divorcee\n",
      "Skip-GramNCETop45-50 sambiria eraric hypercard angiomata suitor\n",
      "phongsri mumbere neurotechnology someone nymphomaniac\n",
      "safrit empress lgp bespectacled barmaid\n",
      "mcelvoy saxonvm pcs hero redheaded\n",
      "tsarina pretender keystroke clown jew\n",
      "Skip-GramACETop5 princess prince software woman girl\n",
      "prince vi computers girl herself\n",
      "elizabeth kings applications tells man\n",
      "duke duke computing dead lover\n",
      "consort iii hardware boy tells\n",
      "Skip-GramACETop45-50 baron earl files kid aunt\n",
      "abbey holy information told maid\n",
      "throne cardinal device revenge wife\n",
      "marie aragon design magic lady\n",
      "victoria princes compatible angry bride\n",
      "Table1: Top5NearestNeighborsofWordsfollowedbyNeighbors45-50fordifferentModels.\n",
      "RW WS353 Method Accuracy(%)\n",
      "SkipgramOnlyNCEbaseline 18.90 31.35\n",
      "order-embeddings 90.6\n",
      "Skipgram+OnlyADV 29.96 58.05\n",
      "Skipgram+ACE 32.71 55.00 order-embeddings+OurACE 92.0\n",
      "Glove-50(Recomputedbasedon(Penningtonetal.,2014)) 34.02 49.51\n",
      "Glove-100(Recomputedbasedon(Penningtonetal.,2014)) 36.64 52.76\n",
      "Table3: OrderEmbeddingPerformance\n",
      "Glove-300(Recomputedbasedon(Penningtonetal.,2014)) 41.18 60.12\n",
      "Glove-50+ACE 35.60 60.46\n",
      "Glove-100+ACE 36.51 63.29\n",
      "Glove-300+ACE 40.57 66.50\n",
      "Table 2: Spearman score (ρ ∗ 100) on RW and\n",
      "model(discriminator)weapplyACEtoisTransD\n",
      "WS353 Datasets. We trained a skipgram model\n",
      "(Ji et al., 2015). Fig. 5 shows validation per-\n",
      "from scratch under various settings for only 1\n",
      "formance as training progresses. All variants of\n",
      "epoch on wikipedia. For finetuned models we re-\n",
      "ACE converges to better results than base NCE.\n",
      "computed the scores based on the publicly avail-\n",
      "AmongACEvariants,allmethodsthatincludeen-\n",
      "able6BtokensGlovemodelsandwefinetunedun-\n",
      "tropyregularizationsignificantlyoutperformwith-\n",
      "tilroughly75%ofthevocabularywasseen.\n",
      "out entropy regularization. Without the self crit-\n",
      "ical baseline variance reduction, learning could\n",
      "racy over Vendrov et al. (2016) without tunning progress faster at the beginning but the final per-\n",
      "any of the discriminator’s hyperparameters. We formancesuffersslightly. Thebestperformanceis\n",
      "further report training curve in Fig. 1, we report obtainedwithouttheadditionaloff-policylearning\n",
      "loss curve on randomly sampled pairs. We stress ofthegenerator.\n",
      "thatintheACEmodel,wetrainrandompairsand\n",
      "generatorgeneratedpairsjointly,asshowninFig. Table. 4 shows the final test results on WN18\n",
      "2,hardnegativeshelptheorderembeddingmodel link prediction task. It is interesting to note that\n",
      "convergesfaster. ACEimprovesMRRscoremoresignificantlythan\n",
      "hit@10. AsMRRisalotmoresensitivetothetop\n",
      "rankings, i.e., howthecorrectconfigurationranks\n",
      "5.4 AblationStudyandImprovingTransD\n",
      "amongthecompetitivealternatives,thisisconsis-\n",
      "To analyze different aspects of ACE, we perform tentwiththefactthatACEsampleshardnegatives\n",
      "an ablation study on the knowledge graph em- andforcesthebasemodeltolearnamorediscrim-\n",
      "bedding task. As described in Sec. 4.3, the base inativerepresentationofthepositiveexamples.\n",
      "faster than on the NCE negatives. After adding\n",
      "entropy regularization and weight decay, the gen-\n",
      "eratorworksasexpected.\n",
      "6 Limitations\n",
      "When the generator softmax is large, the current\n",
      "implementation of ACE training is computation-\n",
      "ally expensive. Although ACE converges faster\n",
      "per iteration, it may converge more slowly on\n",
      "wall-clocktimedependingonthecostofthesoft-\n",
      "max. However, embeddings are typically used as\n",
      "pre-trained building blocks for subsequent tasks.\n",
      "Thus,theirlearningisusuallythepre-computation\n",
      "step for the more complex downstream models\n",
      "Figure 5: Ablation study: measuring validation\n",
      "and spending more time is justified, especially\n",
      "Mean Reciprocal Rank (MRR) on WN18 dataset\n",
      "with GPU acceleration. We believe that the com-\n",
      "astrainingprogresses.\n",
      "putational cost could potentially be reduced via\n",
      "some existing techniques such as the “augment\n",
      "MRR hit@10\n",
      "and reduce” variational inference of (Ruiz et al.,\n",
      "ACE(Ent+SC) 0.792 0.945\n",
      "ACE(Ent+SC+IW) 0.768 0.949 2018), adaptive softmax (Grave et al., 2016), or\n",
      "NCETransD(ours) 0.527 0.947 the “sparsely-gated” softmax of Shazeer et al.\n",
      "NCETransD((Jietal.,2015)) - 0.925\n",
      "(2017),butleavethattofuturework.\n",
      "KBGAN(DISTMULT)((CaiandWang,2017)) 0.772 0.948\n",
      "KBGAN(COMPLEX)((CaiandWang,2017)) 0.779 0.948 Another limitation is on the theoretical front.\n",
      "Wangetal.((Wangetal.,2018)) - 0.93\n",
      "As noted in Goodfellow (2014), GAN learning\n",
      "COMPLEX((Trouillonetal.,2016)) 0.941 0.947\n",
      "does not implement maximum likelihood estima-\n",
      "tion (MLE), while NCE has MLE as an asymp-\n",
      "Table 4: WN18 experiments: the first portion of\n",
      "totic limit. To the best of our knowledge, more\n",
      "the table contains results where the base model is\n",
      "distantconnectionsbetweenGANandMLEtrain-\n",
      "TransD, the last separated line is the COMPLEX\n",
      "ing are not known, and tools for analyzing the\n",
      "embedding model (Trouillon et al., 2016), which\n",
      "equilibriumofamin-maxgamewhereplayersare\n",
      "achieves the SOTA on this dataset. Among all\n",
      "parametrizedbydeepneuralnetsarecurrentlynot\n",
      "TransDbasedmodels(thebestresultsinthisgroup\n",
      "availabletothebestofourknowledge.\n",
      "isunderlined),ACEimprovesoverbasicNCEand\n",
      "another GAN based approach KBGAN. The gap 7 Conclusion\n",
      "on MRR is likely due to the difference between\n",
      "In this paper, we propose Adversarial Contrastive\n",
      "TransDandCOMPLEXmodels.\n",
      "Estimation as a general technique for improving\n",
      "5.5 HardNegativeAnalysis\n",
      "supervised learning problems that learn by con-\n",
      "To better understand the effect of the adversarial trasting observed and fictitious samples. Specifi-\n",
      "samplesproposedbythegeneratorweplotthedis- cally, weuseageneratornetworkinaconditional\n",
      "criminator loss on both p and g samples. In GAN like setting to propose hard negative exam-\n",
      "nce θ\n",
      "this context, a harder sample means a higher loss ples for our discriminator model. We find that a\n",
      "assigned by the discriminator. Fig. 4 shows that mixture distribution of randomly sampling neg-\n",
      "discriminatorlossforthewordembeddingtaskon ative examples along with an adaptive negative\n",
      "g samples are always higher than on p sam- sampler leads to improved performances on a va-\n",
      "θ nce\n",
      "ples, confirming that the generator is indeed sam- rietyofembeddingtasks. Wevalidateourhypoth-\n",
      "plinghardernegatives. esisthathardnegativeexamplesarecriticaltoop-\n",
      "For Hypernym Prediction task, Fig.2 shows dis- timal learning and can be proposed via our ACE\n",
      "criminator loss on negative pairs sampled from framework. Finally, we find that controlling the\n",
      "NCE and ACE respectively. The higher the loss entropy of the generator through a regularization\n",
      "theharderthenegativepairis. Asindicatedinthe term and properly handling false negatives is cru-\n",
      "leftplot,lossontheACEnegativetermscollapses cialforsuccessfultraining.\n",
      "Acknowledgments Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,\n",
      "Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-\n",
      "We would like to thank Teng Long for providing tan Ruppin. 2001. Placing search in context: The\n",
      "the initial baseline code on knowledge graph em- conceptrevisited. InProceedingsofthe10thinter-\n",
      "nationalconferenceonWorldWideWeb,pages406–\n",
      "beddings,MatthewE.Taylorforproofreadingthe\n",
      "414.ACM.\n",
      "manuscript and Jackie Chi Kit Cheung for sug-\n",
      "gestions on preparing for the ACL oral presenta- Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\n",
      "tion. Additionally,wewouldalsoliketoacknowl- BingXu,DavidWarde-Farley,SherjilOzair,Aaron\n",
      "Courville, and Yoshua Bengio. 2014a. Generative\n",
      "edge Jordana Feldman for editing a blog post on\n",
      "adversarial nets. In Z. Ghahramani, M. Welling,\n",
      "this work and April Cooper for creating the art-\n",
      "C. Cortes, N. D. Lawrence, and K. Q. Weinberger,\n",
      "worksfortheblogpost. Finally,weappreciatethe editors,AdvancesinNeuralInformationProcessing\n",
      "broaderBorealisAIteamfordiscussionandemo- Systems27,pages2672–2680.\n",
      "tionalsupport.\n",
      "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\n",
      "BingXu,DavidWarde-Farley,SherjilOzair,Aaron\n",
      "Courville, and Yoshua Bengio. 2014b. Generative\n",
      "References adversarialnets. InAdvancesinneuralinformation\n",
      "processingsystems,pages2672–2680.\n",
      "Martin Arjovsky, Soumith Chintala, and Le´on Bot-\n",
      "tou. 2017. Wasserstein GAN. arXiv preprint Ian J Goodfellow. 2014. On distinguishability crite-\n",
      "arXiv:1701.07875. riaforestimatinggenerativemodels. arXivpreprint\n",
      "arXiv:1412.6515.\n",
      "DavidBelangerandAndrewMcCallum.2016. Struc-\n",
      "tured prediction energy networks. In International Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff\n",
      "ConferenceonMachineLearning,pages983–992. Roeder,andDavidDuvenaud.2017. Backpropaga-\n",
      "tion through the void: Optimizing control variates\n",
      "Antoine Bordes, Xavier Glorot, Jason Weston, and for black-box gradient estimation. arXiv preprint\n",
      "YoshuaBengio.2014. Asemanticmatchingenergy arXiv:1711.00123.\n",
      "functionforlearningwithmulti-relationaldata. Ma-\n",
      "chineLearning,94(2):233–259. Edouard Grave, Armand Joulin, Moustapha Cisse´,\n",
      "David Grangier, and Herve´ Je´gou. 2016. Efficient\n",
      "Antoine Bordes, Nicolas Usunier, Alberto Garcia- softmax approximation for GPUs. arXiv preprint\n",
      "Duran, Jason Weston, and Oksana Yakhnenko. arXiv:1609.04309.\n",
      "2013. Translating embeddings for modeling multi-\n",
      "relational data. In Advances in neural information IshaanGulrajani,FarukAhmed,MartinArjovsky,Vin-\n",
      "processingsystems,pages2787–2795. cent Dumoulin, and Aaron C Courville. 2017. Im-\n",
      "proved training of wasserstein gans. In Advances\n",
      "LiweiCaiandWilliamYangWang.2017. Kbgan: Ad- in Neural Information Processing Systems, pages\n",
      "versariallearningforknowledgegraphembeddings. 5769–5779.\n",
      "arXivpreprintarXiv:1711.04071.\n",
      "MichaelGutmannandAapoHyva¨rinen.2010. Noise-\n",
      "Yanshuai Cao, Gavin Weiguang Ding, Kry Yik-Chau contrastive estimation: A new estimation principle\n",
      "Lui, and Ruitong Huang. 2018. Improving GAN forunnormalizedstatisticalmodels. InProceedings\n",
      "trainingviabinarizedrepresentationentropy(BRE) oftheThirteenthInternationalConferenceonArtifi-\n",
      "regularization. In International Conference on cialIntelligenceandStatistics,pages297–304.\n",
      "LearningRepresentations.\n",
      "Michael U Gutmann and Aapo Hyva¨rinen. 2012.\n",
      "BoDaiandDahuaLin.2017. Contrastivelearningfor Noise-contrastive estimation of unnormalized sta-\n",
      "image captioning. In Advances in Neural Informa- tistical models, with applications to natural image\n",
      "tionProcessingSystems,pages898–907. statistics. Journal of Machine Learning Research,\n",
      "13(Feb):307–361.\n",
      "Tim Dettmers, Pasquale Minervini, Pontus Stene-\n",
      "torp, and Sebastian Riedel. 2017. Convolutional EricJang,ShixiangGu,andBenPoole.2016. Categor-\n",
      "2d knowledge graph embeddings. arXiv preprint icalreparameterizationwithgumbel-softmax. arXiv\n",
      "arXiv:1707.01476. preprintarXiv:1611.01144.\n",
      "Chris Dyer. 2014. Notes on noise contrastive es- Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and\n",
      "timation and negative sampling. arXiv preprint Jun Zhao. 2015. Knowledge graph embedding via\n",
      "arXiv:1410.8251. dynamic mapping matrix. In Proceedings of the\n",
      "53rdAnnualMeetingoftheAssociationforCompu-\n",
      "William Fedus, Ian Goodfellow, and Andrew M Dai. tational Linguistics and the 7th International Joint\n",
      "2018. MaskGAN: Better text generation via filling Conference on Natural Language Processing (Vol-\n",
      "inthe. arXivpreprintarXiv:1801.07736. ume1: LongPapers),volume1,pages687–696.\n",
      "Diederik P Kingma and Jimmy Ba. 2014. Adam: A NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,\n",
      "method for stochastic optimization. arXiv preprint Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff\n",
      "arXiv:1412.6980. Dean. 2017. Outrageously large neural networks:\n",
      "The sparsely-gated mixture-of-experts layer. arXiv\n",
      "YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,and preprintarXiv:1701.06538.\n",
      "Xuan Zhu. 2015. Learning entity and relation em-\n",
      "beddingsforknowledgegraphcompletion. InAAAI, Abhinav Shrivastava, Abhinav Gupta, and Ross Gir-\n",
      "volume15,pages2181–2187. shick.2016. Trainingregion-basedobjectdetectors\n",
      "withonlinehardexamplemining. InProceedingsof\n",
      "Hao Liu, Yihao Feng, Yi Mao, Dengyong Zhou, Jian the IEEE Conference on Computer Vision and Pat-\n",
      "Peng,andQiangLiu.2018. Action-dependentcon- ternRecognition,pages761–769.\n",
      "trol variates for policy optimization via stein iden-\n",
      "NoahASmithandJasonEisner.2005. Contrastivees-\n",
      "tity. InInternationalConferenceonLearningRep-\n",
      "timation: Training log-linear models on unlabeled\n",
      "resentations.\n",
      "data. In Proceedings of the 43rd Annual Meeting\n",
      "onAssociationforComputationalLinguistics,pages\n",
      "Thang Luong, Richard Socher, and Christopher D\n",
      "354–362. Association for Computational Linguis-\n",
      "Manning. 2013. Better word representations with\n",
      "tics.\n",
      "recursive neural networks for morphology. In\n",
      "CoNLL,pages104–113.\n",
      "Ben Taskar, Vassil Chatalbashev, Daphne Koller, and\n",
      "Carlos Guestrin. 2005. Learning structured predic-\n",
      "Chris J Maddison, Andriy Mnih, and Yee Whye Teh.\n",
      "tionmodels: Alargemarginapproach. InProceed-\n",
      "2016. The concrete distribution: A continuous\n",
      "ings of the 22nd international conference on Ma-\n",
      "relaxation of discrete random variables. arXiv\n",
      "chinelearning,pages896–903.ACM.\n",
      "preprintarXiv:1611.00712.\n",
      "The´oTrouillon,JohannesWelbl,SebastianRiedel,E´ric\n",
      "TomasMikolov,IlyaSutskever,KaiChen,GregSCor- Gaussier, and Guillaume Bouchard. 2016. Com-\n",
      "rado, and Jeff Dean. 2013. Distributed representa- plex embeddings for simple link prediction. In In-\n",
      "tionsofwordsandphrasesandtheircompositional- ternationalConferenceonMachineLearning,pages\n",
      "ity. In Advances in neural information processing 2071–2080.\n",
      "systems,pages3111–3119.\n",
      "Ioannis Tsochantaridis, Thorsten Joachims, Thomas\n",
      "M.MirzaandS.Osindero.2014. ConditionalGenera- Hofmann, and Yasemin Altun. 2005. Large mar-\n",
      "tiveAdversarialNets. ArXive-prints. gin methods for structured and interdependent out-\n",
      "putvariables. Journalofmachinelearningresearch,\n",
      "AndriyMnihandKorayKavukcuoglu.2013. Learning 6(Sep):1453–1484.\n",
      "word embeddings efficiently with noise-contrastive\n",
      "estimation. InAdvancesinneuralinformationpro- Lifu Tu and Kevin Gimpel. 2018. Learning approx-\n",
      "cessingsystems,pages2265–2273. imate inference networks for structured prediction.\n",
      "InInternationalConferenceonLearningRepresen-\n",
      "AndriyMnihandYeeWhyeTeh.2012. Afastandsim- tations.\n",
      "ple algorithm for training neural probabilistic lan-\n",
      "GeorgeTucker,AndriyMnih,ChrisJMaddison,John\n",
      "guagemodels. arXivpreprintarXiv:1206.6426.\n",
      "Lawson, and Jascha Sohl-Dickstein. 2017. Rebar:\n",
      "Low-variance, unbiased gradient estimates for dis-\n",
      "Jeffrey Pennington, Richard Socher, and Christopher\n",
      "crete latent variable models. In I. Guyon, U. V.\n",
      "Manning. 2014. Glove: Global vectors for word\n",
      "Luxburg,S.Bengio,H.Wallach,R.Fergus,S.Vish-\n",
      "representation. In Proceedings of the 2014 confer-\n",
      "wanathan,andR.Garnett,editors,AdvancesinNeu-\n",
      "enceonempiricalmethodsinnaturallanguagepro-\n",
      "ralInformationProcessingSystems30,pages2627–\n",
      "cessing(EMNLP),pages1532–1543.\n",
      "2636.CurranAssociates,Inc.\n",
      "StevenJRennie,EtienneMarcheret,YoussefMroueh,\n",
      "Ashish Vaswani, Yinggong Zhao, Victoria Fossum,\n",
      "Jarret Ross, and Vaibhava Goel. 2016. Self-critical\n",
      "and David Chiang. 2013. Decoding with large-\n",
      "sequence training for image captioning. arXiv\n",
      "scale neural language models improves translation.\n",
      "preprintarXiv:1612.00563.\n",
      "In Proceedings of the 2013 Conference on Empiri-\n",
      "calMethodsinNaturalLanguageProcessing,pages\n",
      "Francisco JR Ruiz, Michalis K Titsias, Adji B Dieng,\n",
      "1387–1392.\n",
      "and David M Blei. 2018. Augment and reduce:\n",
      "Stochastic inference for large categorical distribu- Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel\n",
      "tions. arXivpreprintarXiv:1802.04220. Urtasun. 2016. Order-embeddings of images and\n",
      "language. InInternationalConferenceonLearning\n",
      "Florian Schroff, Dmitry Kalenichenko, and James Representations.\n",
      "Philbin. 2015. Facenet: A unified embedding for\n",
      "face recognition and clustering. In Proceedings of PeifengWang,ShuangyinLi,andRongPan.2018. In-\n",
      "the IEEE Conference on Computer Vision and Pat- corporating GAN for negative sampling in knowl-\n",
      "ternRecognition,pages815–823. edge representation learning. In The Thirty-Second\n",
      "AAAI Conference on Artificial Intelligence (AAAI-\n",
      "18).\n",
      "Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\n",
      "Chen.2014. Knowledgegraphembeddingbytrans-\n",
      "latingonhyperplanes. InProceedingsoftheTwenty-\n",
      "Eighth AAAI Conference on Artificial Intelligence,\n",
      "pages1112–1119.AAAIPress.\n",
      "Ronald J Williams. 1992. Simple statistical gradient-\n",
      "following algorithms for connectionist reinforce-\n",
      "mentlearning. Machinelearning,8(3-4):229–256.\n",
      "Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\n",
      "Gao, and Li Deng. 2014. Embedding entities and\n",
      "relations for learning and inference in knowledge\n",
      "bases. arXivpreprintarXiv:1412.6575.\n",
      "BengioYoshua,DucharmeRejean,VincentPascal,and\n",
      "Jauvin Christian.2003. Aneuralprobabilisticlan-\n",
      "guage model. Journal of Machine Learning Re-\n",
      "search.\n",
      "JunboZhao,MichaelMathieu,andYannLeCun.2016.\n",
      "Energy-basedgenerativeadversarialnetwork. arXiv\n",
      "preprintarXiv:1609.03126.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,    647,  11233,    518]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Lev Finkelstein', 'Evgeniy Gabrilovich', 'Yossi Matias', 'Ehud Rivlin', 'Zach Solan', 'Gadi Wolfman', 'Eythan Ruppin', 'Teng Long', 'Matthew E. Taylor', 'Jackie Chi Kit Cheung', 'Jordana Feldman', 'April Cooper', 'Ian Goodfellow', 'Jean Pouget-Abadie', 'Mehdi Mirza', 'Bing Xu', 'David Warde-Farley', 'Sherjil Ozair', 'Aaron Courville', 'Yoshua Bengio', 'Martin Arjovsky', 'Soumith Chintala', 'Le´on Bottou', 'David Belanger', 'Andrew McCallum', 'Antoine Bordes', 'Xavier Glorot', 'Jason Weston', 'Yoshua Bengio', 'Nicolas Usunier', 'Alberto Garcia-Duran', 'Oksana Yakhnenko', 'Ishaan Gulrajani', 'Faruk Ahmed', 'Vincent Dumoulin', 'Aaron Courville', 'Noah A. Smith', 'Jason Eisner', 'Ben Taskar', 'Vassil Chatalbashev', 'Daphne Koller', 'Carlos Guestrin', 'Chris J. Maddison', 'Andriy Mnih', 'Yee Whye Teh', 'The´o Trouillon', 'Johannes Welbl', 'Sebastian Riedel', 'Eric Gaussier', 'Guillaume Bouchard', 'Ioannis Tsochantaridis', 'Thorsten Joachims', 'Thomas Hofmann', 'Yasemin Altun', 'Lifu Tu', 'Kevin Gimpel', 'George Tucker', 'John Lawson', 'Jascha Sohl-Dickstein', 'Jeffrey Pennington', 'Richard Socher', 'Christopher Manning', 'Steven J. Rennie', 'Etienne Marcheret', 'Youssef Mroueh', 'Jarret Ross', 'Vaibhava Goel', 'Francisco J. R. Ruiz', 'Michalis K. Titsias', 'Adji B. Dieng', 'David M. Blei', 'Ivan Vendrov', 'Ryan Kiros', 'Sanja Fidler', 'Raquel Urtasun', 'Peifeng Wang', 'Shuangyin Li', 'Rong Pan',\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: KBGAN: Adversarial Learning for Knowledge Graph Embeddings\n",
      "LiweiCai WilliamYangWang\n",
      "DepartmentofElectronicEngineering DepartmentofComputerScience\n",
      "TsinghuaUniversity UniversityofCalifornia,SantaBarbara\n",
      "Beijing100084China SantaBarbara,CA93106USA\n",
      "cai.lw123@gmail.com william@cs.ucsb.edu\n",
      "Abstract formofadiscreterelationaltriplesuchasLocate-\n",
      "dIn(NewOrleans,Louisiana).\n",
      "Weintroduce KBGAN,anadversariallearning A main challenge for using discrete represen-\n",
      "framework to improve the performances of a tation of knowledge graph is the lack of capa-\n",
      "wide range of existing knowledge graph em-\n",
      "bility of accessing the similarities among differ-\n",
      "bedding models. Because knowledge graphs\n",
      "ent entities and relations. Knowledge graph em-\n",
      "typicallyonlycontainpositivefacts,sampling\n",
      "bedding (KGE) techniques (e.g., RESCAL (Nickel\n",
      "useful negative training examples is a non-\n",
      "trivial task. Replacing the head or tail entity\n",
      "etal.,2011),TRANSE(Bordesetal.,2013),DIST-\n",
      "of a fact with a uniformly randomly selected MULT(Yangetal.,2015),andCOMPLEX(Trouil-\n",
      "entity is a conventional method for generat- lon et al., 2016)) have been proposed in recent\n",
      "ingnegativefacts,butthemajorityofthegen- years to deal with the issue. The main idea is\n",
      "erated negative facts can be easily discrimi- to represent the entities and relations in a vec-\n",
      "nated from positive facts, and will contribute\n",
      "torspace,andonecanusemachinelearningtech-\n",
      "littletowardsthetraining.Inspiredbygenera-\n",
      "niquetolearnthecontinuousrepresentationofthe\n",
      "tive adversarial networks (GANs), we use one\n",
      "knowledgegraphinthelatentspace.\n",
      "knowledge graph embedding model as a neg-\n",
      "ative sample generator to assist the training However, even steady progress has been made\n",
      "of our desired model, which acts as the dis- in developing novel algorithms for knowledge\n",
      "criminator in GANs. This framework is inde- graph embedding, there is still a common chal-\n",
      "pendentoftheconcreteformofgeneratorand lenge in this line of research. For space effi-\n",
      "discriminator,andthereforecanutilizeawide\n",
      "ciency, common knowledge graphs such as Free-\n",
      "variety of knowledge graph embedding mod-\n",
      "base (Bollacker et al., 2008), Yago (Suchanek\n",
      "els as its building blocks. In experiments, we\n",
      "et al., 2007), and NELL (Mitchell et al., 2015) by\n",
      "adversariallytraintwotranslation-basedmod-\n",
      "default only stores beliefs, rather than disbeliefs.\n",
      "els, TRANSE and TRANSD, each with assis-\n",
      "tance from one of the two probability-based Therefore, when training the embedding models,\n",
      "models,DISTMULTandCOMPLEX.Weeval- there is only the natural presence of the positive\n",
      "uate the performances of KBGAN on the link examples. To use negative examples, a common\n",
      "prediction task, using three knowledge base methodistoremovethecorrecttailentity,andran-\n",
      "completion datasets: FB15k-237, WN18 and\n",
      "domlysamplefromauniformdistribution(Bordes\n",
      "WN18RR.Experimentalresultsshowthatad-\n",
      "et al., 2013). Unfortunately, this approach is not\n",
      "versarial training substantially improves the\n",
      "ideal, because the sampled entity could be com-\n",
      "performancesoftargetembeddingmodelsun-\n",
      "dervarioussettings. pletely unrelated to the head and the target re-\n",
      "lation, and thus the quality of randomly gener-\n",
      "1 Introduction\n",
      "atednegativeexamplesisoftenpoor(e.g,Locate-\n",
      "dIn(NewOrleans,BarackObama)).Otherapproach\n",
      "Knowledge graph (Dong et al., 2014) is a pow- might leverage external ontological constraints\n",
      "erful graph structure that can provide direct ac- suchasentitytypes(Krompaßetal.,2015)togen-\n",
      "cess of knowledge to users via various applica- erate negative examples, but such resource does\n",
      "tions such as structured search, question answer- notalwaysexistoraccessible.\n",
      "ing, and intelligent virtual assistant. A common In this work, we provide a generic solution to\n",
      "representationofknowledgegraphbeliefsisinthe improve the training of a wide range of knowl-\n",
      "8102\n",
      "rpA\n",
      "61\n",
      "]LC.sc[\n",
      "3v17040.1171:viXra\n",
      "Model Scorefunctionf(h,r,t) Numberofparameters\n",
      "TRANSE ||h+r−t||\n",
      "1/2\n",
      "k|E|+k|R|\n",
      "TRANSD ||(I+r ph pT)h+r−(I+r pt pT)t||\n",
      "1/2\n",
      "2k|E|+2k|R|\n",
      "DISTMULT\n",
      "<h,r,t>(=(cid:80)k\n",
      "i=1h ir it i) k|E|+k|R|\n",
      "COMPLEX <h,r,¯t>(h,r,t∈Ck) 2k|E|+2k|R|\n",
      "TRANSH ||(I−r pr pT)h+r−(I+r pr pT)t||\n",
      "1/2\n",
      "k|E|+2k|R|\n",
      "TRANSR ||W rh+r−W rt||\n",
      "1/2\n",
      "k|E|+(k2+k)|R|\n",
      "MANIFOLDE(hyperplane) |(h+r head)T(t+r tail)−D r| k|E|+(2k+1)|R|\n",
      "RESCAL hTW rt k|E|+k2|R|\n",
      "HOLE rT(h(cid:63)t)((cid:63)iscircularcorrelation) k|E|+k|R|\n",
      "CONVE f(vec(f([h¯;¯r]∗ω))W)t k|E|+k|R|+kcmn\n",
      "Table1:Someselectedknowledgegraphembeddingmodels.Thefourmodelsabovethedoublelineare\n",
      "considered in this paper. Except for COMPLEX, all boldface lower case letters represent vectors in Rk,\n",
      "andboldfaceuppercaselettersrepresentmatricesinRk×k.Iistheidentitymatrix.\n",
      "edge graph embedding models. Inspired by the 2 RelatedWork\n",
      "recent advances of generative adversarial deep\n",
      "models (Goodfellow et al., 2014), we propose 2.1 KnowledgeGraphEmbeddings\n",
      "a novel adversarial learning framework, namely,\n",
      "A large number of knowledge graph embedding\n",
      "KBGAN, for generating better negative exam-\n",
      "models,whichrepresententitiesandrelationsina\n",
      "ples to train knowledge graph embedding mod-\n",
      "knowledge graph with vectors or matrices, have\n",
      "els. More specifically, we consider probability-\n",
      "been proposed in recent years. RESCAL (Nickel\n",
      "based, log-loss embedding models as the gener-\n",
      "et al., 2011) is one of the earliest studies on ma-\n",
      "ator to supply better quality negative examples,\n",
      "trix factorization based knowledge graph embed-\n",
      "and use distance-based, margin-loss embedding\n",
      "ding models, using a bilinear form as score func-\n",
      "models as the discriminator to generate the final\n",
      "tion. TRANSE (Bordes et al., 2013) is the first\n",
      "knowledge graph embeddings. Since the genera-\n",
      "model to introduce translation-based embedding.\n",
      "tor has a discrete generation step, we cannot di-\n",
      "Later variants, such as TRANSH (Wang et al.,\n",
      "rectly use the gradient-based approach to back-\n",
      "2014), TRANSR (Lin et al., 2015) and TRANSD\n",
      "propagate the errors. We then consider a one-\n",
      "(Jietal.,2015),extendTRANSEbyprojectingthe\n",
      "step reinforcement learning setting, and use a\n",
      "embeddingvectorsofentitiesintovariousspaces.\n",
      "variance-reductionREINFORCEmethodtoachieve\n",
      "DISTMULT(Yangetal.,2015)simplifiesRESCAL\n",
      "thisgoal.Empirically,weperformexperimentson\n",
      "by only using a diagonal matrix, and COMPLEX\n",
      "threecommonKGEdatasets(FB15K-237,WN18\n",
      "(Trouillon et al., 2016) extends DISTMULT into\n",
      "andWN18RR),andverifytheadversariallearning\n",
      "the complex number field. (Nickel et al., 2015) is\n",
      "approach with a set of KGE models. Our exper-\n",
      "acomprehensivesurveyonthesemodels.\n",
      "iments show that across various settings, this ad-\n",
      "Someofthemorerecentmodelsachievestrong\n",
      "versariallearningmechanismcansignificantlyim-\n",
      "performances. MANIFOLDE (Xiao et al., 2016)\n",
      "prove the performance of some of the most com-\n",
      "embeds a triple as a manifold rather than a point.\n",
      "monly used translation based KGE methods. Our\n",
      "HOLE (Nickel et al., 2016) employs circular cor-\n",
      "contributionsarethree-fold:\n",
      "relation to combine the two entities in a triple.\n",
      "CONVE (Dettmers et al., 2017) uses a convolu-\n",
      "• Wearethefirsttoconsideradversariallearn-\n",
      "tional neural network as the score function. How-\n",
      "ingtogenerateusefulnegativetrainingexam-\n",
      "ever, most of these studies use uniform sampling\n",
      "plestoimproveknowledgegraphembedding.\n",
      "to generate negative training examples (Bordes\n",
      "et al., 2013). Because our framework is indepen-\n",
      "• This adversarial learning framework applies dentoftheconcreteformofmodels,allthesemod-\n",
      "to a wide range of KGE models, without the elscanbepotentiallyincorporatedintoourframe-\n",
      "needofexternalontologiesconstraints. work, regardless of the complexity. As a proof of\n",
      "principle,ourworkfocusesonsimplermodels.Ta-\n",
      "• Our method shows consistent performance ble 1 summarizes the score functions and dimen-\n",
      "gainsonthreecommonlyusedKGEdatasets. sionsofallmodelsmentionedabove.\n",
      "2.2 GenerativeAdversarialNetworksandits KBGAN.\n",
      "Variants\n",
      "3.1 TypesofTrainingObjectives\n",
      "Generative Adversarial Networks (GANs) (Good-\n",
      "For a given knowledge graph, let E be the set of\n",
      "fellow et al., 2014) was originally proposed for\n",
      "entities, R be the set of relations, and T be the\n",
      "generating samples in a continuous space such as\n",
      "setofgroundtruthtriples.Ingeneral,aknowledge\n",
      "images. A GAN consists of two parts, the genera-\n",
      "graphembedding(KGE)modelcanbeformulated\n",
      "torandthediscriminator.Thegeneratoracceptsa\n",
      "as a score function f(h,r,t),h,t ∈ E,r ∈ R\n",
      "noiseinputandoutputsanimage.Thediscrimina-\n",
      "which assigns a score to every possible triple in\n",
      "torisaclassifierwhichclassifiesimagesas“true”\n",
      "theknowledgegraph.Theestimatedlikelihoodof\n",
      "(fromthegroundtruthset)or“fake”(generatedby\n",
      "a triple to be true depends only on its score given\n",
      "the generator). When training a GAN, the genera-\n",
      "bythescorefunction.\n",
      "torandthediscriminatorplayaminimaxgame,in\n",
      "Differentmodelsformulatetheirscorefunction\n",
      "whichthegeneratortriestogenerate“real”images\n",
      "basedondifferentdesigns,andthereforeinterpret\n",
      "todeceivethediscriminator,andthediscriminator\n",
      "scores differently, which further lead to various\n",
      "tries to tell them apart from ground truth images.\n",
      "training objectives. Two common forms of train-\n",
      "GANs are also capable of generating samples sat-\n",
      "ingobjectivesareparticularlyofourinterest:\n",
      "isfying certain requirements, such as conditional\n",
      "Marginal loss function is commonly used by\n",
      "GAN(MirzaandOsindero,2014).\n",
      "a large group of models called translation-based\n",
      "ItisnotpossibletouseGANsinitsoriginalform\n",
      "models, whose score function models distance\n",
      "for generating discrete samples like natural lan-\n",
      "between points or vectors, such as TRANSE,\n",
      "guage sentences or knowledge graph triples, be-\n",
      "TRANSH, TRANSR, TRANSD andsoon.Inthese\n",
      "cause the discrete sampling step prevents gradi-\n",
      "models, smaller distance indicates a higher likeli-\n",
      "ents from propagating back to the generator. SE-\n",
      "hoodoftruth,butonlyqualitatively.Themarginal\n",
      "QGAN (Yuetal.,2017)isoneofthefirstsuccess-\n",
      "lossfunctiontakesthefollowingform:\n",
      "ful solutions to this problem by using reinforce-\n",
      "(cid:88)\n",
      "ment learning—It trains the generator using pol- L = [f(h,r,t)−f(h(cid:48),r,t(cid:48))+γ] (1)\n",
      "m +\n",
      "icygradientandothertricks. IRGAN (Wangetal., (h,r,t)∈T\n",
      "2017) is a recent work which combines two cate-\n",
      "where γ is the margin, [·] = max(0,·) is the\n",
      "gories of information retrieval models into a dis- +\n",
      "hinge function, and (h(cid:48),r,t(cid:48)) is a negative triple.\n",
      "crete GAN framework. Likewise, our framework\n",
      "The negative triple is generated by replacing the\n",
      "relies on policy gradient to train the generator\n",
      "head entity or the tail entity of a positive triple\n",
      "whichprovidesdiscretenegativetriples.\n",
      "with a random entity in the knowledge graph,\n",
      "The discriminator in a GAN is not necessarily\n",
      "or formally (h(cid:48),r,t(cid:48)) ∈ {(h(cid:48),r,t)|h(cid:48) ∈ E} ∪\n",
      "aclassifier.Wasserstein GAN or WGAN (Arjovsky\n",
      "{(h,r,t(cid:48))|t(cid:48) ∈ E}.\n",
      "et al., 2017) uses a regressor with clipped param-\n",
      "Log-softmax loss function is commonly used by\n",
      "eters as its discriminator, based on solid analysis\n",
      "models whose score function has probabilistic in-\n",
      "about the mathematical nature of GANs. GOGAN\n",
      "terpretation.Somenotableexamplesare RESCAL,\n",
      "(Juefei-Xu et al., 2017) further replaces the loss\n",
      "DISTMULT, COMPLEX. Applying the softmax\n",
      "function in WGAN with marginal loss. Although\n",
      "function on scores of a given set of triples gives\n",
      "originating from very different fields, the form of\n",
      "theprobabilityofatripletobethebestoneamong\n",
      "loss function in our framework turns out to be\n",
      "expf(h,r,t)\n",
      "them: p(h,r,t) =. The loss\n",
      "morecloselyrelatedtotheonein GOGAN. (cid:80) (h(cid:48),r,t(cid:48))expf(h(cid:48),r,t(cid:48))\n",
      "functionisthenegativelog-likelihoodofthisprob-\n",
      "3 OurApproaches abilisticmodel:\n",
      "Inthissection,wefirstdefinetwotypesoftraining (cid:88) expf(h,r,t)\n",
      "L = −log\n",
      "objectives in knowledge graph embedding mod-\n",
      "l (cid:80) expf(h(cid:48),r,t(cid:48))\n",
      "(h,r,t)∈T\n",
      "els to show how KBGAN can be applied. Then, (h(cid:48),r,t(cid:48)) ∈ {(h,r,t)}∪Neg(h,r,t) (2)\n",
      "we demonstrate a long overlooked problem about\n",
      "negative sampling which motivates us to propose where Neg(h,r,t) ⊂ {(h(cid:48),r,t)|h(cid:48) ∈ E} ∪\n",
      "KBGAN to address the problem. Finally, we dive {(h,r,t(cid:48))|t(cid:48) ∈ E} is a set of sampled corrupted\n",
      "into the mathematical, and algorithmic details of triples.\n",
      "Figure1:Anoverviewofthe KBGAN framework.Thegenerator(G)calculatesaprobabilitydistribution\n",
      "over a set of candidate negative triples, then sample one triples from the distribution as the output. The\n",
      "discriminator (D) receives the generated negative triple as well as the ground truth triple (in the hexag-\n",
      "onal box), and calculates their scores. G minimizes the score of the generated negative triple by policy\n",
      "gradient,andDminimizesthemarginallossbetweenpositiveandnegativetriplesbygradientdescent.\n",
      "Other forms of loss functions exist, for exam- edge of American geography. If a KGE model is\n",
      "ple CONVE uses a triple-wise logistic function to fed with mostly “too easy” negative examples, it\n",
      "model how likely the triple is true, but by far the would probably only learn to represent types, not\n",
      "two described above are the most common. Also, theunderlyingsemantics.\n",
      "softmax function gives an probabilistic distribu- Theproblemislessseveretomodelsusinglog-\n",
      "tion over a set of triples, which is necessary for softmaxlossfunction,becausetheytypicallysam-\n",
      "ageneratortosamplefromthem. ples tens or hundreds of negative triples for one\n",
      "positive triple in each iteration, and it is likely to\n",
      "3.2 WeaknessofUniformNegativeSampling\n",
      "have a few useful negatives among them. For in-\n",
      "Most previous KGE models use uniform negative stance, (Trouillon et al., 2016) found that a 100:1\n",
      "samplingforgeneratingnegativetriples,thatis,re- negative-to-positive ratio results in the best per-\n",
      "placing the head or tail entity of a positive triple formance for COMPLEX. However, for marginal\n",
      "with any of the entities in E, all with equal prob- loss function, whose negative-to-positive ratio is\n",
      "ability. Most of the negative triples generated in always 1:1, the low quality of uniformly sampled\n",
      "this way contribute little to learning an effective negativescanseriouslydamagetheirperformance.\n",
      "embedding,becausetheyaretooobviouslyfalse.\n",
      "3.3 GenerativeAdversarialTrainingfor\n",
      "To demonstrate this issue, let us consider the\n",
      "KnowledgeGraphEmbeddingModels\n",
      "following example. Suppose we have a ground\n",
      "truth triple LocatedIn(NewOrleans,Louisiana), Inspired by GANs, we propose an adversarial\n",
      "and corrupt it by replacing its tail entity. training framework named KBGAN which uses a\n",
      "First, we remove the tail entity, leaving Lo- KGE model with softmax probabilities to pro-\n",
      "catedIn(NewOrleans,?). Because the relation Lo- vide high-quality negative samples for the train-\n",
      "catedIn constraints types of its entities, “?” ing of a KGE model whose training objective is\n",
      "must be a geographical region. If we fill “?” marginal loss function. This framework is inde-\n",
      "with a random entity e ∈ E, the prob- pendent of the score functions of these two mod-\n",
      "ability of e having a wrong type is very els,andthereforepossessessomeextentofuniver-\n",
      "high, resulting in ridiculous triples like Lo- sality. Figure 1 illustrates the overall structure of\n",
      "catedIn(NewOrleans,BarackObama) or Locate- KBGAN.\n",
      "dIn(NewOrleans,StarTrek). Such triples are con- In parallel to terminologies used in GAN liter-\n",
      "sidered “too easy”, because they can be elim- ature, we will simply call these two models gen-\n",
      "inated solely by types. In contrast, Locate- erator and discriminator respectively in the rest\n",
      "dIn(NewOrleans,Florida)isaveryusefulnegative of this paper. We use softmax probabilistic mod-\n",
      "triple, because it satisfies type constraints, but it els as the generator because they can adequately\n",
      "cannot be proved wrong without detailed knowl- model the “sampling from a probability distribu-\n",
      "Algorithm1:TheKBGANalgorithm\n",
      "Data:trainingsetofpositivefacttriplesT ={(h,r,t)}\n",
      "Input:Pre-trainedgeneratorGwithparametersθ andscorefunctionf (h,r,t),andpre-traineddiscriminatorDwith\n",
      "G G\n",
      "parametersθ andscorefunctionf (h,r,t)\n",
      "D D\n",
      "Output:Adversariallytraineddiscriminator\n",
      "1 b←−0;// baseline for policy gradient\n",
      "2 repeat\n",
      "3 Sampleamini-batchofdataT batchfromT ;\n",
      "4 G G ←−0,G D ←−0;// gradients of parameters of G and D\n",
      "5 r sum ←−0;// for calculating the baseline\n",
      "6 for(h,r,t)∈T batchdo\n",
      "7 UniformlyrandomlysampleN snegativetriplesNeg(h,r,t)={(h(cid:48) i,r,t(cid:48) i)} i=1...Ns;\n",
      "8 Obtaintheirprobabilityofbeinggenerated:p i = (cid:80)N j=e sx 1p ef xG p( fh G(cid:48) i (,r h, (cid:48) jt,(cid:48) i r),t(cid:48) j);\n",
      "9 Sampleonenegativetriple(h(cid:48) s,r,t(cid:48) s)fromNeg(h,r,t)accordingto{p i} i=1...Ns.Assumeitsprobabilitytobe\n",
      "p ;\n",
      "s\n",
      "10 G D ←−G D+∇ θD[f D(h,r,t)−f D(h(cid:48) s,r,t(cid:48) s)+γ] +;// accumulate gradients for D\n",
      "11 r←−−f D(h(cid:48) s,r,t(cid:48) s),r sum ←−r sum+r;// r is the reward\n",
      "12 G G ←−G G+(r−b)∇ θGlogp s;// accumulate gradients for G\n",
      "13 end\n",
      "14 θ G ←−θ G+η GG G,θ D ←−θ D−η DG D;// update parameters\n",
      "15 b←r sum/|T batch|;// update baseline\n",
      "16 untilconvergence;\n",
      "tion” process of discrete GANs, and we aim at minimizingthefollowingmarginallossfunction:\n",
      "improving discriminators based on marginal loss\n",
      "(cid:88)\n",
      "because they can benefit more from high-quality L D = [f D(h,r,t)−f D(h(cid:48),r,t(cid:48))+γ] +\n",
      "negativesamples.Notethatamajordifferencebe- (h,r,t)∈T\n",
      "tween GAN andourworkisthat,theultimategoal (h(cid:48),r,t(cid:48)) ∼ p (h(cid:48),r,t(cid:48)|h,r,t) (3)\n",
      "G\n",
      "of our framework is to produce a good discrimi-\n",
      "Theonlydifferencebetweenthislossfunctionand\n",
      "nator,whereas GANS areaimedattrainingagood\n",
      "Equation1isthatitusesnegativesamplesfromthe\n",
      "generator.Inaddition,thediscriminatorhereisnot\n",
      "generator.\n",
      "aclassifierasitwouldbeinmostGANs.\n",
      "The objective of the generator can be formu-\n",
      "lated as maximizing the following expectation of\n",
      "negativedistances:\n",
      "Intuitively,thediscriminatorshouldassignarel-\n",
      "atively small distance to a high-quality negative (cid:88)\n",
      "R = E[−f (h(cid:48),r,t(cid:48))]\n",
      "sample.Inordertoencouragethegeneratortogen- G D\n",
      "erateusefulnegativesamples,theobjectiveofthe (h,r,t)∈T\n",
      "generatoristominimizethedistancegivenbydis- (h(cid:48),r,t(cid:48)) ∼ p G(h(cid:48),r,t(cid:48)|h,r,t) (4)\n",
      "criminator for its generated triples. And just like\n",
      "R involves a discrete sampling step, so we\n",
      "G\n",
      "the ordinary training process, the objective of the\n",
      "cannotfinditsgradientwithsimpledifferentiation.\n",
      "discriminatoristominimizethemarginallossbe-\n",
      "We use a simple special case of Policy Gradient\n",
      "tween the positive triple and the generated nega-\n",
      "Theorem1 (Suttonetal.,2000)toobtainthegradi-\n",
      "tive triple. In an adversarial training setting, the\n",
      "entofR withrespecttoparametersofthegener-\n",
      "G\n",
      "generator and the discriminator are alternatively\n",
      "ator:\n",
      "trainedtowardstheirrespectiveobjectives.\n",
      "(cid:88)\n",
      "∇ R = E\n",
      "G G (h(cid:48),r,t(cid:48))∼pG(h(cid:48),r,t(cid:48)|h,r,t)\n",
      "(h,r,t)∈T\n",
      "Suppose that the generator produces a\n",
      "[−f (h(cid:48),r,t(cid:48))∇ logp (h(cid:48),r,t(cid:48)|h,r,t)]\n",
      "probability distribution on negative triples D G G\n",
      "p G(h(cid:48),r,t(cid:48)|h,r,t) given a positive triple (h,r,t), (cid:39) (cid:88) 1 (cid:88)\n",
      "and generates negative triples (h(cid:48),r,t(cid:48)) by sam- N\n",
      "(h,r,t)∈T (h(cid:48) i,r,t(cid:48) i)∼pG(h(cid:48),r,t(cid:48)|h,r,t),i=1...N\n",
      "pling from this distribution. Let f (h,r,t) be\n",
      "D [−f (h(cid:48),r,t(cid:48))∇ logp (h(cid:48),r,t(cid:48)|h,r,t)] (5)\n",
      "the score function of the discriminator. The ob- D G G\n",
      "jective of the discriminator can be formulated as 1Aproofcanbefoundinthesupplementarymaterial\n",
      "Model Hyperparameters ConstraintsorRegularizations\n",
      "TRANSE L\n",
      "1\n",
      "distance,k = 50,γ = 3 ||e||\n",
      "2\n",
      "≤ 1,||r||\n",
      "2\n",
      "≤ 1\n",
      "TRANSD L\n",
      "1\n",
      "distance,k = 50,γ = 3 ||e||\n",
      "2\n",
      "≤ 1,||r||\n",
      "2\n",
      "≤ 1,||e p||\n",
      "2\n",
      "≤ 1,||r p||\n",
      "2\n",
      "≤ 1\n",
      "DISTMULT k = 50,λ = 1/0.1 L2regularization:L\n",
      "reg\n",
      "= L+λ||Θ||2\n",
      "2\n",
      "COMPLEX 2k = 50,λ = 1/0.1 L2regularization:L\n",
      "reg\n",
      "= L+λ||Θ||2\n",
      "2\n",
      "Table 2: Hyperparameter settings of the 4 models we used. For DISTMULT and COMPLEX, λ = 1 is\n",
      "usedforFB15k-237andλ = 0.1isusedforWN18andWN18RR.Allotherhyperparametersareshared\n",
      "amongalldatasets.ListhegloballossdefinedinEquation(2).Θrepresentsallparametersinthemodel.\n",
      "Dataset #r #ent. #train #val #test out affecting the expectation of gradients.2\n",
      "FB15k-237 237 14,541 272,115 17,535 20,466\n",
      "In our case, we replace −f (h(cid:48),r,t(cid:48)) with\n",
      "WN18 18 40,943 141,442 5,000 5,000 D\n",
      "WN18RR 11 40,943 86,835 3,034 3,134 −f D(h(cid:48),r,t(cid:48)) − b(h,r,t) in the equation above\n",
      "to introduce the baseline. To avoid introducing\n",
      "Table3:Statisticsofdatasetsweusedintheexper-\n",
      "new parameters, we simply let b be a constant,\n",
      "iments.“r”:relations.\n",
      "the average reward of the whole training set: b =\n",
      "(cid:80) E [−f (h(cid:48),r,t(cid:48))].\n",
      "(h,r,t)∈T (h(cid:48),r,t(cid:48))∼pG(h(cid:48),r,t(cid:48)|h,r,t) D\n",
      "In practice, b is approximated by the mean of\n",
      "where the second approximate equality means rewardsofrecentlygeneratednegativetriples.\n",
      "we approximate the expectation with sampling in Let the generator’s score function to be\n",
      "practice.NowwecancalculatethegradientofR G f G(h,r,t),givenasetofcandidatenegativetriples\n",
      "andoptimizeitwithgradient-basedalgorithms. Neg(h,r,t) ⊂ {(h(cid:48),r,t)|h(cid:48) ∈ E}∪{(h,r,t(cid:48))|t(cid:48) ∈\n",
      "PolicyGradientTheoremarisesfromreinforce- E},theprobabilitydistributionp G ismodeledas:\n",
      "ment learning (RL), so we would like to draw an\n",
      "expf (h(cid:48),r,t(cid:48))\n",
      "analogybetweenourmodelandanRLmodel.The p (h(cid:48),r,t(cid:48)|h,r,t) = G\n",
      "G (cid:80) expf (h∗,r,t∗)\n",
      "generator can be viewed as an agent which inter- G\n",
      "(h∗,r,t∗) ∈ Neg(h,r,t) (6)\n",
      "acts with the environment by performing actions\n",
      "and improves itself by maximizing the reward re-\n",
      "Ideally,Neg(h,r,t)shouldcontainallpossible\n",
      "turnedfromtheenvironmentinresponseofitsac-\n",
      "negatives. However, knowledge graphs are usu-\n",
      "tions. Correspondingly, the discriminator can be\n",
      "ally highly incomplete, so the ”hardest” negative\n",
      "viewed as the environment. Using RL terminolo-\n",
      "triples are very likely to be false negatives (true\n",
      "gies, (h,r,t) is the state (which determines what\n",
      "facts). To address this issue, we instead generate\n",
      "actions the actor can take), p (h(cid:48),r,t(cid:48)|h,r,t) is\n",
      "G Neg(h,r,t)byuniformlysamplingofN entities\n",
      "s\n",
      "thepolicy(howtheactorchooseactions),(h(cid:48),r,t(cid:48))\n",
      "(a small number compared to the number of all\n",
      "is the action, and −f (h(cid:48),r,t(cid:48)) is the reward.\n",
      "D possible negatives) from E to replace h or t. Be-\n",
      "The method of optimizing R described above\n",
      "G cause in real-world knowledge graphs, true neg-\n",
      "is called REINFORCE (Williams, 1992) algorithm atives are usually far more than false negatives,\n",
      "in RL. Our model is a simple special case of\n",
      "such set would be unlikely to contain any false\n",
      "RL, called one-step RL. In a typical RL setting,\n",
      "negative, and the negative selected by the gener-\n",
      "each action performed by the agent will change\n",
      "atorwouldlikelybeatruenegative.Usingasmall\n",
      "its state, and the agent will perform a series of\n",
      "Neg(h,r,t) can also significantly reduce compu-\n",
      "actions (called an epoch) until it reaches certain\n",
      "tationalcomplexity.\n",
      "states or the number of actions reaches a certain\n",
      "Besides, we adopt the “bern” sampling tech-\n",
      "limit.However,intheanalogyabove,actionsdoes\n",
      "nique (Wang et al., 2014) which replaces the\n",
      "notaffectthestate,andaftereachactionwerestart\n",
      "“1” side in “1-to-N” and “N-to-1” relations with\n",
      "with another unrelated state, so each epoch con-\n",
      "higher probability to further reduce false nega-\n",
      "sistsofonlyoneaction.\n",
      "tives.\n",
      "To reduce the variance of REINFORCE al- Algorithm 1 summarizes the whole adversarial\n",
      "gorithm, it is common to subtract a base- training process. Both the generator and the dis-\n",
      "line from the reward, which is an arbitrary\n",
      "2Aproofofsuchfactcanalsobefoundinthesupplemen-\n",
      "number that only depends on the state, with- tarymaterial\n",
      "criminator require pre-training, which is the same 4.1.3 ImplementationDetails\n",
      "as conventionally training a single KBE model 3 In the pre-training stage, we train every model\n",
      "with uniform negative sampling. Formally speak-\n",
      "to convergence for 1000 epochs, and divide ev-\n",
      "ing, one can pre-train the generator by minimiz-\n",
      "eryepochinto100mini-batches.Toavoidoverfit-\n",
      "ing the loss function defined in Equation (1), and\n",
      "ting, we adopt early stopping by evaluating MRR\n",
      "pre-trainthediscriminatorbyminimizingtheloss\n",
      "on the validation set every 50 epochs. We tried\n",
      "function defined in Equation (2). Line 14 in the\n",
      "γ = 0.5,1,2,3,4,5 and L,L distances for\n",
      "1 2\n",
      "algorithm assumes that we are using the vanilla\n",
      "TRANSE and TRANSD, and λ = 0.01,0.1,1,10\n",
      "gradient descent as the optimization method, but\n",
      "for DISTMULT and COMPLEX, and determined\n",
      "obviously one can substitute it with any gradient-\n",
      "the best hyperparameters listed on table 2, based\n",
      "basedoptimizationalgorithm.\n",
      "on their performances on the validation set af-\n",
      "ter pre-training. Due to limited computation re-\n",
      "4 Experiments\n",
      "sources, we deliberately limit the dimensions of\n",
      "embeddings to k = 50, similar to the one used\n",
      "To evaluate our proposed framework, we test its\n",
      "in earlier works, to save time. We also apply cer-\n",
      "performance for the link prediction task with dif-\n",
      "tainconstraintsorregularizationstothesemodels,\n",
      "ferent generators and discriminators. For the gen-\n",
      "which are mostly the same as those described in\n",
      "erator, we choose two classical probability-based\n",
      "theiroriginalpublications,andalsolistedontable\n",
      "KGE model, DISTMULT and COMPLEX, and\n",
      "2.\n",
      "for the discriminator, we also choose two classi-\n",
      "In the adversarial training stage, we keep all\n",
      "cal translation-based KGE model, TRANSE and\n",
      "thehyperparamtersdeterminedinthepre-training\n",
      "TRANSD, resulting in four possible combinations\n",
      "stage unchanged. The number of candidate neg-\n",
      "of generator and discriminator in total. See Table<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   8268,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['FB15k-237', 'WN18', 'WN18RR']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  limited computation re-\n",
      "4 Experiments\n",
      "sources, we deliberately limit the dimensions of\n",
      "embeddings to k = 50, similar to the one used\n",
      "To evaluate our proposed framework, we test its\n",
      "in earlier works, to save time. We also apply cer-\n",
      "performance for the link prediction task with dif-\n",
      "tainconstraintsorregularizationstothesemodels,\n",
      "ferent generators and discriminators. For the gen-\n",
      "which are mostly the same as those described in\n",
      "erator, we choose two classical probability-based\n",
      "theiroriginalpublications,andalsolistedontable\n",
      "KGE model, DISTMULT and COMPLEX, and\n",
      "2.\n",
      "for the discriminator, we also choose two classi-\n",
      "In the adversarial training stage, we keep all\n",
      "cal translation-based KGE model, TRANSE and\n",
      "thehyperparamtersdeterminedinthepre-training\n",
      "TRANSD, resulting in four possible combinations\n",
      "stage unchanged. The number of candidate neg-\n",
      "of generator and discriminator in total. See Table\n",
      "ative triples, N, is set to 20 in all cases, which\n",
      "1forabriefsummaryofthesemodels. s\n",
      "is proven to be optimal among the candidate set\n",
      "4.1 ExperimentalSettings of {5,10,20,30,50}. We train for 5000 epochs,\n",
      "with100mini-batchesforeachepoch.Wealsouse\n",
      "4.1.1 Datasets\n",
      "earlystoppinginadversarialtrainingbyevaluating\n",
      "We use three common knowledge base com-\n",
      "MRRonthevalidationsetevery100epochs.\n",
      "pletion datasets for our experiment: FB15k-237,\n",
      "We use the self-adaptive optimization method\n",
      "WN18 and WN18RR. FB15k-237 is a subset\n",
      "Adam (Kingma and Ba, 2015) for all trainings,\n",
      "of FB15k introduced by (Toutanova and Chen,\n",
      "and always use the recommended default setting\n",
      "2015), which removed redundant relations in\n",
      "α = 0.001,β = 0.9,β = 0.999,(cid:15) = 10−8.\n",
      "1 2\n",
      "FB15k and greatly reduced the number of rela-\n",
      "tions.Likewise,WN18RRisasubsetofWN18in- 4.2 Results\n",
      "troducedby(Dettmersetal.,2017)whichremoves\n",
      "Results of our experiments as well as baselines\n",
      "reversing relations and dramatically increases the\n",
      "are shown in Table 4. All settings of adversarial\n",
      "difficulty of reasoning. Both FB15k and WN18\n",
      "training bring a pronounced improvement to the\n",
      "are first introduced by (Bordes et al., 2013) and\n",
      "model, which indicates that our method is con-\n",
      "havebeencommonlyusedinknowledgegraphre-\n",
      "sistently effective in various cases. TRANSE per-\n",
      "searches. Statistics of datasets we used are shown\n",
      "formsslightlyworsethanTRANSDonFB15k-237\n",
      "inTable3.\n",
      "and WN18, but better on WN18RR. Using DIST-\n",
      "4.1.2 EvaluationProtocols MULT or COMPLEX asthegeneratordoesnotaf-\n",
      "fectperformancegreatly.\n",
      "Followingpreviousworkslike(Yangetal.,2015)\n",
      "and (Trouillon et al., 2016), for each run, we re- TRANSE and TRANSD enhanced by KBGAN\n",
      "cansignificantlybeattheircorrespondingbaseline\n",
      "port two common metrics, mean reciprocal rank-\n",
      "implementations, and outperform stronger base-\n",
      "ing (MRR) and hits at 10 (H@10). We only re-\n",
      "lines in some cases. As a prototypical and proof-\n",
      "portscoresunderthefilteredsetting(Bordesetal.,\n",
      "of-principle experiment, we have never expected\n",
      "2013),whichremovesalltriplesappearedintrain-\n",
      "state-of-the-art results. Being simple models pro-\n",
      "ing, validating, and testing sets from candidate\n",
      "triples before obtaining the rank of the ground\n",
      "3The KBGAN source code is available at https://\n",
      "truthtriple. github.com/cai-lw/KBGAN\n",
      "FB15k-237 WN18 WN18RR\n",
      "Method MRR H@10 MRR H@10 MRR H@10\n",
      "TRANSE - 42.8† - 89.2 - 43.2†\n",
      "TRANSD - 45.3† - 92.2 - 42.8†\n",
      "DISTMULT 24.1‡ 41.9‡ 82.2 93.6 42.5‡ 49.1‡\n",
      "COMPLEX 24.0‡ 41.9‡ 94.1 94.7 44.4‡ 50.7‡\n",
      "TRANSE (pre-trained) 24.2 42.2 43.3 91.5 18.6 45.9\n",
      "KBGAN(TRANSE + DISTMULT) 27.4 45.0 71.0 94.9 21.3 48.1\n",
      "KBGAN(TRANSE + COMPLEX) 27.8 45.3 70.5 94.9 21.0 47.9\n",
      "TRANSD (pre-trained) 24.5 42.7 49.4 92.8 19.2 46.5\n",
      "KBGAN(TRANSD + DISTMULT) 27.8 45.8 77.2 94.8 21.4 47.2\n",
      "KBGAN(TRANSD + COMPLEX) 27.7 45.8 77.9 94.8 21.5 46.9\n",
      "Table 4: Experimental results. Results of KBGAN are results of its discriminator (on the left of the “+”\n",
      "sign). Underlined results are the best ones among our implementations. Results marked with † are pro-\n",
      "duced by running Fast-TransX (Lin et al., 2015) with its default parameters. Results marked with ‡ are\n",
      "copiedfrom(Dettmersetal.,2017).Allotherbaselineresultsarecopiedfromtheiroriginalpapers.\n",
      "Figure2:LearningcurvesofKBGAN.Allmetricsimprovesteadilyastrainingproceeds.\n",
      "posed several years ago, TRANSE and TRANSD imum as training proceeds, which indicates that\n",
      "hastheirlimitationsinexpressivenessthatareun- KBGANisarobustGANthatcanconvergetogood\n",
      "likely to be fully compensated by better training resultsinvarioussettings,althoughGANsarewell-\n",
      "technique. In future researches, people may try known for difficulty in convergence. Fluctuations\n",
      "employing more advanced models into KBGAN, in these graphs may seem more prominent than\n",
      "andwebelieveithasthepotentialtobecomestate- other KGE models, but is considered normal for\n",
      "of-the-art. an adversially trained model. Note that in some\n",
      "casesthecurvestilltendstoriseafter5000epochs.\n",
      "To illustrate our training progress, we plot per- Wedonothavesufficientcomputationresourceto\n",
      "formances of the discriminator on validation set trainformoreepochs,butwebelievethattheywill\n",
      "over epochs, which are displayed in Figure 2. As alsoeventuallyconverge.\n",
      "all these graphs show, our performances are al-\n",
      "ways in increasing trends, converging to its max-\n",
      "Positivefact Uniformrandomsample Trainedgenerator\n",
      "(condensation NN 2, family arcidae NN 1 revivification NN 1\n",
      "derivationally related form, repast NN 1 mouthpiece NN 3\n",
      "distill VB 4) beater NN 2 liquid body substance NN 1\n",
      "coverall NN 1 stiffen VB 2\n",
      "cash advance NN 1 hot up VB 1\n",
      "(colorado river NN 2, lunar calendar NN 1 idaho NN 1\n",
      "instance hypernym, umbellularia californica NN 1 sayan mountains NN 1\n",
      "river NN 1) tonality NN 1 lower saxony NN 1\n",
      "creepy-crawly NN 1 order ciconiiformes NN 1\n",
      "moor VB 3 jab NN 3\n",
      "(meeting NN 2, cellular JJ 1 attach VB 1\n",
      "hypernym, commercial activity NN 1 bond NN 6\n",
      "social gathering NN 1) giant cane NN 1 heavy spar NN 1\n",
      "streptomyces NN 1 satellite NN 1\n",
      "tranquillize VB 1 peep VB 3\n",
      "Table 5: Examples of negative samples in WN18 dataset. The first column is the positive fact, and the\n",
      "terminboldistheonetobereplacedbyanentityinthenexttwocolumns.Thesecondcolumnconsists\n",
      "ofrandomentitiesdrawnfromthewholedataset.Thethirdcolumncontainsnegativesamplesgenerated\n",
      "bythegeneratorinthelast5epochsoftraining.Entitiesinitalicareconsideredtohavesemanticrelation\n",
      "tothepositiveone\n",
      "4.3 Casestudy 5 Conclusions\n",
      "We propose a novel adversarial learning method\n",
      "To demonstrate that our approach does generate for improving a wide range of knowledge graph\n",
      "betternegativesamples,welistsomeexamplesof embedding models—We designed a generator-\n",
      "them in Table 5, using the KBGAN (TRANSE + discriminator framework with dual KGE compo-\n",
      "DISTMULT)modelandtheWN18dataset.Allhy- nents. Unlike random uniform sampling, the gen-\n",
      "perparameters are the same as those described in eratormodelgenerateshigherqualitynegativeex-\n",
      "Section4.1.3. amples, which allow the discriminator model to\n",
      "learn better. To enable backpropagation of error,\n",
      "Compared to uniform random negatives which\n",
      "we introduced a one-step REINFORCE method to\n",
      "are almost always totally unrelated, the genera-\n",
      "seamlesslyintegratethetwomodules.Experimen-\n",
      "tor generates more semantically related negative\n",
      "tally,wetestedtheproposedideaswithfourcom-\n",
      "samples, which is different from type relatedness\n",
      "monlyusedKGEmodelsonthreedatasets,andthe\n",
      "we used as example in Section 3.2, but also helps\n",
      "resultsshowedthattheadversariallearningframe-\n",
      "training.Inthefirstexample,twoofthefiveterms\n",
      "work brought consistent improvements to various\n",
      "are physically related to the process of distilling\n",
      "KGEmodelsunderdifferentsettings.\n",
      "liquids. In the second example, three of the five\n",
      "entities are geographical objects. In the third ex-\n",
      "ample,twoofthefiveentitiesexpresstheconcept\n",
      "of“gather”.\n",
      "Because we deliberately limited the strength of\n",
      "generated negatives by using a small N as de-\n",
      "s\n",
      "scribed in Section 3.3, the semantic relation is\n",
      "prettyweak,andtherearestillmanyunrelateden-\n",
      "tities. However, empirical results (when selecting\n",
      "the optimal N ) shows that such situation is more\n",
      "s\n",
      "beneficial for training the discriminator than gen-\n",
      "eratingevenstrongernegatives.\n",
      "References Tom M Mitchell, William Cohen, Estevam Hruschka,\n",
      "ParthaTalukdar,JustinBetteridge,AndrewCarlson,\n",
      "MartinArjovsky, SoumithChintala, andLeon Bottou.\n",
      "Bhavana Dalvi Mishra, Matthew Gardner, Bryan\n",
      "2017. Wasserstein gan. In International Confer-\n",
      "Kisiel, Jayant Krishnamurthy, et al. 2015. Never-\n",
      "renceonMachineLearning.\n",
      "endinglearning. InTheTwenty-ninthAAAIConfer-\n",
      "enceonArtificialIntelligence.\n",
      "Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim\n",
      "Sturge,andJamieTaylor.2008. Freebase:acollab-\n",
      "oratively created graph database for structuring hu- Maximilian Nickel, Kevin Murphy, Volker Tresp, and\n",
      "man knowledge. In Proceedings of the 2008 ACM Evgeniy Gabrilovich. 2015. A review of relational\n",
      "SIGMOD international conference on Management machine learning for knowledge graphs. arXiv\n",
      "ofdata.ACM,pages1247–1250. preprintarXiv:1503.00759.\n",
      "Antoine Bordes, Nicolas Usunier, Alberto Garcia- Maximilian Nickel, Lorenzo Rosasco, and\n",
      "Duran, Jason Weston, and Oksana Yakhnenko. Tomaso Poggio Poggio. 2016. Holographic\n",
      "2013. Translating embeddings for modeling multi- embeddings of knowledgegraphs. In The Thirtieth\n",
      "relational data. In Advances in Neural Information AAAI Conference on Artificial Intelligence. pages\n",
      "ProcessingSystems.pages2787–2795. 1955–1961.\n",
      "Tim Dettmers, Pasquale Minervini, Pontus Stene-\n",
      "Maximilian Nickel, Volker Tresp, and Hans-Peter\n",
      "torp, and Sebastian Riedel. 2017. Convolutional\n",
      "Kriegel. 2011. A three-way model for collective\n",
      "2d knowledge graph embeddings. arXiv preprint\n",
      "learning on multi-relational data. In Proceedings\n",
      "arXiv:1707.01476.\n",
      "of the 28th International Conference on Machine\n",
      "Learning.pages809–816.\n",
      "XinDong,EvgeniyGabrilovich,GeremyHeitz,Wilko\n",
      "Horn, Ni Lao, Kevin Murphy, Thomas Strohmann,\n",
      "Fabian M Suchanek, Gjergji Kasneci, and Gerhard\n",
      "Shaohua Sun, and Wei Zhang. 2014. Knowledge\n",
      "Weikum. 2007. Yago: a core of semantic knowl-\n",
      "vault:Aweb-scaleapproachtoprobabilisticknowl-\n",
      "edge fusion. In Proceedings of the 20th ACM edge. InProceedingsofthe16thinternationalcon-\n",
      "SIGKDD international conference on Knowledge\n",
      "ferenceonWorldWideWeb.ACM,pages697–706.\n",
      "discoveryanddatamining.ACM,pages601–610.\n",
      "Richard S Sutton, David A McAllester, Satinder P\n",
      "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Singh, and Yishay Mansour. 2000. Policy gradi-\n",
      "BingXu,DavidWarde-Farley,SherjilOzair,Aaron ent methods for reinforcement learning with func-\n",
      "Courville,andYoshuaBengio.2014. Generativead- tionapproximation. InAdvancesinneuralinforma-\n",
      "versarial nets. In Advances in Neural Information tionprocessingsystems.pages1057–1063.\n",
      "ProcessingSystems.pages2672–2680.\n",
      "Kristina Toutanova and Danqi Chen. 2015. Observed\n",
      "GuoliangJi,ShizhuHe,LihengXu,KangLiu,andJun\n",
      "versus latent features for knowledge base and text\n",
      "Zhao. 2015. Knowledge graph embedding via dy-\n",
      "inference. In Proceedings of the 3rd Workshop on\n",
      "namicmappingmatrix. InThe53rdAnnualMeeting\n",
      "ContinuousVectorSpaceModelsandtheirCompo-\n",
      "oftheAssociationforComputationalLinguistics.\n",
      "sitionality.pages57–66.\n",
      "Felix Juefei-Xu, Vishnu Naresh Boddeti, and Marios\n",
      "Savvides.2017. Gangofgans:Generativeadversar-\n",
      "The´oTrouillon,JohannesWelbl,SebastianRiedel,E´ric\n",
      "ialnetworkswithmaximummarginranking. arXiv Gaussier, and Guillaume Bouchard. 2016. Com-\n",
      "preprintarXiv:1704.04865. plex embeddings for simple link prediction. In In-\n",
      "ternationalConferenceonMachineLearning.pages\n",
      "Diederik P. Kingma and Jimmy Lei Ba. 2015. Adam: 2071–2080.\n",
      "A method for stochastic optimization. In The 3rd\n",
      "International Conference on Learning Representa- Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong,\n",
      "tions. Yinghui Xu, Benyou Wang, Peng Zhang, and Dell\n",
      "Zhang. 2017. Irgan: A minimax game for unifying\n",
      "Denis Krompaß, Stephan Baier, and Volker Tresp.\n",
      "generative and discriminative information retrieval\n",
      "2015. Type-constrained representation learning in\n",
      "models. InThe40thInternationalACMSIGIRCon-\n",
      "knowledge graphs. In International Semantic Web\n",
      "ference.\n",
      "Conference.Springer,pages640–655.\n",
      "Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\n",
      "YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,and\n",
      "Chen.2014. Knowledgegraphembeddingbytrans-\n",
      "Xuan Zhu. 2015. Learning entity and relation em-\n",
      "lating on hyperplanes. In The Twenty-eighth AAAI\n",
      "beddings for knowledge graph completion. In The\n",
      "Conference on Artificial Intelligence. pages 1112–\n",
      "Twenty-ninth AAAI Conference on Artificial Intelli-\n",
      "1119.\n",
      "gence.pages2181–2187.\n",
      "Mehdi Mirza and Simon Osindero. 2014. Condi- Ronald J Williams. 1992. Simple statistical gradient-\n",
      "tional generative adversarial nets. arXiv preprint following algorithms for connectionist reinforce-\n",
      "arXiv:1411.01784. mentlearning. Machinelearning8(3-4):229–256.\n",
      "Han Xiao, Minlie Huang, and Xiaoyan Zhu. 2016.\n",
      "Fromonepointtoamanifold:Knowledgegraphem-\n",
      "beddingforpreciselinkprediction. InTheTwenty-\n",
      "FifthInternationalJointConferenceonArtificialIn-\n",
      "telligence.\n",
      "Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\n",
      "Gao, and Li Deng. 2015. Embedding entities and\n",
      "relations for learning and inference in knowledge\n",
      "bases. The3rdInternationalConferenceonLearn-\n",
      "ingRepresentations.\n",
      "Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.\n",
      "2017. Seqgan:Sequencegenerativeadversarialnets\n",
      "withpolicygradient. InTheThirty-FirstAAAICon-\n",
      "ferenceonArtificialIntelligence.pages2852–2858.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   8268,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['FB15k-237', 'WN18', 'WN18RR']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: KBGAN: Adversarial Learning for Knowledge Graph Embeddings\n",
      "LiweiCai WilliamYangWang\n",
      "DepartmentofElectronicEngineering DepartmentofComputerScience\n",
      "TsinghuaUniversity UniversityofCalifornia,SantaBarbara\n",
      "Beijing100084China SantaBarbara,CA93106USA\n",
      "cai.lw123@gmail.com william@cs.ucsb.edu\n",
      "Abstract formofadiscreterelationaltriplesuchasLocate-\n",
      "dIn(NewOrleans,Louisiana).\n",
      "Weintroduce KBGAN,anadversariallearning A main challenge for using discrete represen-\n",
      "framework to improve the performances of a tation of knowledge graph is the lack of capa-\n",
      "wide range of existing knowledge graph em-\n",
      "bility of accessing the similarities among differ-\n",
      "bedding models. Because knowledge graphs\n",
      "ent entities and relations. Knowledge graph em-\n",
      "typicallyonlycontainpositivefacts,sampling\n",
      "bedding (KGE) techniques (e.g., RESCAL (Nickel\n",
      "useful negative training examples is a non-\n",
      "trivial task. Replacing the head or tail entity\n",
      "etal.,2011),TRANSE(Bordesetal.,2013),DIST-\n",
      "of a fact with a uniformly randomly selected MULT(Yangetal.,2015),andCOMPLEX(Trouil-\n",
      "entity is a conventional method for generat- lon et al., 2016)) have been proposed in recent\n",
      "ingnegativefacts,butthemajorityofthegen- years to deal with the issue. The main idea is\n",
      "erated negative facts can be easily discrimi- to represent the entities and relations in a vec-\n",
      "nated from positive facts, and will contribute\n",
      "torspace,andonecanusemachinelearningtech-\n",
      "littletowardsthetraining.Inspiredbygenera-\n",
      "niquetolearnthecontinuousrepresentationofthe\n",
      "tive adversarial networks (GANs), we use one\n",
      "knowledgegraphinthelatentspace.\n",
      "knowledge graph embedding model as a neg-\n",
      "ative sample generator to assist the training However, even steady progress has been made\n",
      "of our desired model, which acts as the dis- in developing novel algorithms for knowledge\n",
      "criminator in GANs. This framework is inde- graph embedding, there is still a common chal-\n",
      "pendentoftheconcreteformofgeneratorand lenge in this line of research. For space effi-\n",
      "discriminator,andthereforecanutilizeawide\n",
      "ciency, common knowledge graphs such as Free-\n",
      "variety of knowledge graph embedding mod-\n",
      "base (Bollacker et al., 2008), Yago (Suchanek\n",
      "els as its building blocks. In experiments, we\n",
      "et al., 2007), and NELL (Mitchell et al., 2015) by\n",
      "adversariallytraintwotranslation-basedmod-\n",
      "default only stores beliefs, rather than disbeliefs.\n",
      "els, TRANSE and TRANSD, each with assis-\n",
      "tance from one of the two probability-based Therefore, when training the embedding models,\n",
      "models,DISTMULTandCOMPLEX.Weeval- there is only the natural presence of the positive\n",
      "uate the performances of KBGAN on the link examples. To use negative examples, a common\n",
      "prediction task, using three knowledge base methodistoremovethecorrecttailentity,andran-\n",
      "completion datasets: FB15k-237, WN18 and\n",
      "domlysamplefromauniformdistribution(Bordes\n",
      "WN18RR.Experimentalresultsshowthatad-\n",
      "et al., 2013). Unfortunately, this approach is not\n",
      "versarial training substantially improves the\n",
      "ideal, because the sampled entity could be com-\n",
      "performancesoftargetembeddingmodelsun-\n",
      "dervarioussettings. pletely unrelated to the head and the target re-\n",
      "lation, and thus the quality of randomly gener-\n",
      "1 Introduction\n",
      "atednegativeexamplesisoftenpoor(e.g,Locate-\n",
      "dIn(NewOrleans,BarackObama)).Otherapproach\n",
      "Knowledge graph (Dong et al., 2014) is a pow- might leverage external ontological constraints\n",
      "erful graph structure that can provide direct ac- suchasentitytypes(Krompaßetal.,2015)togen-\n",
      "cess of knowledge to users via various applica- erate negative examples, but such resource does\n",
      "tions such as structured search, question answer- notalwaysexistoraccessible.\n",
      "ing, and intelligent virtual assistant. A common In this work, we provide a generic solution to\n",
      "representationofknowledgegraphbeliefsisinthe improve the training of a wide range of knowl-\n",
      "8102\n",
      "rpA\n",
      "61\n",
      "]LC.sc[\n",
      "3v17040.1171:viXra\n",
      "Model Scorefunctionf(h,r,t) Numberofparameters\n",
      "TRANSE ||h+r−t||\n",
      "1/2\n",
      "k|E|+k|R|\n",
      "TRANSD ||(I+r ph pT)h+r−(I+r pt pT)t||\n",
      "1/2\n",
      "2k|E|+2k|R|\n",
      "DISTMULT\n",
      "<h,r,t>(=(cid:80)k\n",
      "i=1h ir it i) k|E|+k|R|\n",
      "COMPLEX <h,r,¯t>(h,r,t∈Ck) 2k|E|+2k|R|\n",
      "TRANSH ||(I−r pr pT)h+r−(I+r pr pT)t||\n",
      "1/2\n",
      "k|E|+2k|R|\n",
      "TRANSR ||W rh+r−W rt||\n",
      "1/2\n",
      "k|E|+(k2+k)|R|\n",
      "MANIFOLDE(hyperplane) |(h+r head)T(t+r tail)−D r| k|E|+(2k+1)|R|\n",
      "RESCAL hTW rt k|E|+k2|R|\n",
      "HOLE rT(h(cid:63)t)((cid:63)iscircularcorrelation) k|E|+k|R|\n",
      "CONVE f(vec(f([h¯;¯r]∗ω))W)t k|E|+k|R|+kcmn\n",
      "Table1:Someselectedknowledgegraphembeddingmodels.Thefourmodelsabovethedoublelineare\n",
      "considered in this paper. Except for COMPLEX, all boldface lower case letters represent vectors in Rk,\n",
      "andboldfaceuppercaselettersrepresentmatricesinRk×k.Iistheidentitymatrix.\n",
      "edge graph embedding models. Inspired by the 2 RelatedWork\n",
      "recent advances of generative adversarial deep\n",
      "models (Goodfellow et al., 2014), we propose 2.1 KnowledgeGraphEmbeddings\n",
      "a novel adversarial learning framework, namely,\n",
      "A large number of knowledge graph embedding\n",
      "KBGAN, for generating better negative exam-\n",
      "models,whichrepresententitiesandrelationsina\n",
      "ples to train knowledge graph embedding mod-\n",
      "knowledge graph with vectors or matrices, have\n",
      "els. More specifically, we consider probability-\n",
      "been proposed in recent years. RESCAL (Nickel\n",
      "based, log-loss embedding models as the gener-\n",
      "et al., 2011) is one of the earliest studies on ma-\n",
      "ator to supply better quality negative examples,\n",
      "trix factorization based knowledge graph embed-\n",
      "and use distance-based, margin-loss embedding\n",
      "ding models, using a bilinear form as score func-\n",
      "models as the discriminator to generate the final\n",
      "tion. TRANSE (Bordes et al., 2013) is the first\n",
      "knowledge graph embeddings. Since the genera-\n",
      "model to introduce translation-based embedding.\n",
      "tor has a discrete generation step, we cannot di-\n",
      "Later variants, such as TRANSH (Wang et al.,\n",
      "rectly use the gradient-based approach to back-\n",
      "2014), TRANSR (Lin et al., 2015) and TRANSD\n",
      "propagate the errors. We then consider a one-\n",
      "(Jietal.,2015),extendTRANSEbyprojectingthe\n",
      "step reinforcement learning setting, and use a\n",
      "embeddingvectorsofentitiesintovariousspaces.\n",
      "variance-reductionREINFORCEmethodtoachieve\n",
      "DISTMULT(Yangetal.,2015)simplifiesRESCAL\n",
      "thisgoal.Empirically,weperformexperimentson\n",
      "by only using a diagonal matrix, and COMPLEX\n",
      "threecommonKGEdatasets(FB15K-237,WN18\n",
      "(Trouillon et al., 2016) extends DISTMULT into\n",
      "andWN18RR),andverifytheadversariallearning\n",
      "the complex number field. (Nickel et al., 2015) is\n",
      "approach with a set of KGE models. Our exper-\n",
      "acomprehensivesurveyonthesemodels.\n",
      "iments show that across various settings, this ad-\n",
      "Someofthemorerecentmodelsachievestrong\n",
      "versariallearningmechanismcansignificantlyim-\n",
      "performances. MANIFOLDE (Xiao et al., 2016)\n",
      "prove the performance of some of the most com-\n",
      "embeds a triple as a manifold rather than a point.\n",
      "monly used translation based KGE methods. Our\n",
      "HOLE (Nickel et al., 2016) employs circular cor-\n",
      "contributionsarethree-fold:\n",
      "relation to combine the two entities in a triple.\n",
      "CONVE (Dettmers et al., 2017) uses a convolu-\n",
      "• Wearethefirsttoconsideradversariallearn-\n",
      "tional neural network as the score function. How-\n",
      "ingtogenerateusefulnegativetrainingexam-\n",
      "ever, most of these studies use uniform sampling\n",
      "plestoimproveknowledgegraphembedding.\n",
      "to generate negative training examples (Bordes\n",
      "et al., 2013). Because our framework is indepen-\n",
      "• This adversarial learning framework applies dentoftheconcreteformofmodels,allthesemod-\n",
      "to a wide range of KGE models, without the elscanbepotentiallyincorporatedintoourframe-\n",
      "needofexternalontologiesconstraints. work, regardless of the complexity. As a proof of\n",
      "principle,ourworkfocusesonsimplermodels.Ta-\n",
      "• Our method shows consistent performance ble 1 summarizes the score functions and dimen-\n",
      "gainsonthreecommonlyusedKGEdatasets. sionsofallmodelsmentionedabove.\n",
      "2.2 GenerativeAdversarialNetworksandits KBGAN.\n",
      "Variants\n",
      "3.1 TypesofTrainingObjectives\n",
      "Generative Adversarial Networks (GANs) (Good-\n",
      "For a given knowledge graph, let E be the set of\n",
      "fellow et al., 2014) was originally proposed for\n",
      "entities, R be the set of relations, and T be the\n",
      "generating samples in a continuous space such as\n",
      "setofgroundtruthtriples.Ingeneral,aknowledge\n",
      "images. A GAN consists of two parts, the genera-\n",
      "graphembedding(KGE)modelcanbeformulated\n",
      "torandthediscriminator.Thegeneratoracceptsa\n",
      "as a score function f(h,r,t),h,t ∈ E,r ∈ R\n",
      "noiseinputandoutputsanimage.Thediscrimina-\n",
      "which assigns a score to every possible triple in\n",
      "torisaclassifierwhichclassifiesimagesas“true”\n",
      "theknowledgegraph.Theestimatedlikelihoodof\n",
      "(fromthegroundtruthset)or“fake”(generatedby\n",
      "a triple to be true depends only on its score given\n",
      "the generator). When training a GAN, the genera-\n",
      "bythescorefunction.\n",
      "torandthediscriminatorplayaminimaxgame,in\n",
      "Differentmodelsformulatetheirscorefunction\n",
      "whichthegeneratortriestogenerate“real”images\n",
      "basedondifferentdesigns,andthereforeinterpret\n",
      "todeceivethediscriminator,andthediscriminator\n",
      "scores differently, which further lead to various\n",
      "tries to tell them apart from ground truth images.\n",
      "training objectives. Two common forms of train-\n",
      "GANs are also capable of generating samples sat-\n",
      "ingobjectivesareparticularlyofourinterest:\n",
      "isfying certain requirements, such as conditional\n",
      "Marginal loss function is commonly used by\n",
      "GAN(MirzaandOsindero,2014).\n",
      "a large group of models called translation-based\n",
      "ItisnotpossibletouseGANsinitsoriginalform\n",
      "models, whose score function models distance\n",
      "for generating discrete samples like natural lan-\n",
      "between points or vectors, such as TRANSE,\n",
      "guage sentences or knowledge graph triples, be-\n",
      "TRANSH, TRANSR, TRANSD andsoon.Inthese\n",
      "cause the discrete sampling step prevents gradi-\n",
      "models, smaller distance indicates a higher likeli-\n",
      "ents from propagating back to the generator. SE-\n",
      "hoodoftruth,butonlyqualitatively.Themarginal\n",
      "QGAN (Yuetal.,2017)isoneofthefirstsuccess-\n",
      "lossfunctiontakesthefollowingform:\n",
      "ful solutions to this problem by using reinforce-\n",
      "(cid:88)\n",
      "ment learning—It trains the generator using pol- L = [f(h,r,t)−f(h(cid:48),r,t(cid:48))+γ] (1)\n",
      "m +\n",
      "icygradientandothertricks. IRGAN (Wangetal., (h,r,t)∈T\n",
      "2017) is a recent work which combines two cate-\n",
      "where γ is the margin, [·] = max(0,·) is the\n",
      "gories of information retrieval models into a dis- +\n",
      "hinge function, and (h(cid:48),r,t(cid:48)) is a negative triple.\n",
      "crete GAN framework. Likewise, our framework\n",
      "The negative triple is generated by replacing the\n",
      "relies on policy gradient to train the generator\n",
      "head entity or the tail entity of a positive triple\n",
      "whichprovidesdiscretenegativetriples.\n",
      "with a random entity in the knowledge graph,\n",
      "The discriminator in a GAN is not necessarily\n",
      "or formally (h(cid:48),r,t(cid:48)) ∈ {(h(cid:48),r,t)|h(cid:48) ∈ E} ∪\n",
      "aclassifier.Wasserstein GAN or WGAN (Arjovsky\n",
      "{(h,r,t(cid:48))|t(cid:48) ∈ E}.\n",
      "et al., 2017) uses a regressor with clipped param-\n",
      "Log-softmax loss function is commonly used by\n",
      "eters as its discriminator, based on solid analysis\n",
      "models whose score function has probabilistic in-\n",
      "about the mathematical nature of GANs. GOGAN\n",
      "terpretation.Somenotableexamplesare RESCAL,\n",
      "(Juefei-Xu et al., 2017) further replaces the loss\n",
      "DISTMULT, COMPLEX. Applying the softmax\n",
      "function in WGAN with marginal loss. Although\n",
      "function on scores of a given set of triples gives\n",
      "originating from very different fields, the form of\n",
      "theprobabilityofatripletobethebestoneamong\n",
      "loss function in our framework turns out to be\n",
      "expf(h,r,t)\n",
      "them: p(h,r,t) =. The loss\n",
      "morecloselyrelatedtotheonein GOGAN. (cid:80) (h(cid:48),r,t(cid:48))expf(h(cid:48),r,t(cid:48))\n",
      "functionisthenegativelog-likelihoodofthisprob-\n",
      "3 OurApproaches abilisticmodel:\n",
      "Inthissection,wefirstdefinetwotypesoftraining (cid:88) expf(h,r,t)\n",
      "L = −log\n",
      "objectives in knowledge graph embedding mod-\n",
      "l (cid:80) expf(h(cid:48),r,t(cid:48))\n",
      "(h,r,t)∈T\n",
      "els to show how KBGAN can be applied. Then, (h(cid:48),r,t(cid:48)) ∈ {(h,r,t)}∪Neg(h,r,t) (2)\n",
      "we demonstrate a long overlooked problem about\n",
      "negative sampling which motivates us to propose where Neg(h,r,t) ⊂ {(h(cid:48),r,t)|h(cid:48) ∈ E} ∪\n",
      "KBGAN to address the problem. Finally, we dive {(h,r,t(cid:48))|t(cid:48) ∈ E} is a set of sampled corrupted\n",
      "into the mathematical, and algorithmic details of triples.\n",
      "Figure1:Anoverviewofthe KBGAN framework.Thegenerator(G)calculatesaprobabilitydistribution\n",
      "over a set of candidate negative triples, then sample one triples from the distribution as the output. The\n",
      "discriminator (D) receives the generated negative triple as well as the ground truth triple (in the hexag-\n",
      "onal box), and calculates their scores. G minimizes the score of the generated negative triple by policy\n",
      "gradient,andDminimizesthemarginallossbetweenpositiveandnegativetriplesbygradientdescent.\n",
      "Other forms of loss functions exist, for exam- edge of American geography. If a KGE model is\n",
      "ple CONVE uses a triple-wise logistic function to fed with mostly “too easy” negative examples, it\n",
      "model how likely the triple is true, but by far the would probably only learn to represent types, not\n",
      "two described above are the most common. Also, theunderlyingsemantics.\n",
      "softmax function gives an probabilistic distribu- Theproblemislessseveretomodelsusinglog-\n",
      "tion over a set of triples, which is necessary for softmaxlossfunction,becausetheytypicallysam-\n",
      "ageneratortosamplefromthem. ples tens or hundreds of negative triples for one\n",
      "positive triple in each iteration, and it is likely to\n",
      "3.2 WeaknessofUniformNegativeSampling\n",
      "have a few useful negatives among them. For in-\n",
      "Most previous KGE models use uniform negative stance, (Trouillon et al., 2016) found that a 100:1\n",
      "samplingforgeneratingnegativetriples,thatis,re- negative-to-positive ratio results in the best per-\n",
      "placing the head or tail entity of a positive triple formance for COMPLEX. However, for marginal\n",
      "with any of the entities in E, all with equal prob- loss function, whose negative-to-positive ratio is\n",
      "ability. Most of the negative triples generated in always 1:1, the low quality of uniformly sampled\n",
      "this way contribute little to learning an effective negativescanseriouslydamagetheirperformance.\n",
      "embedding,becausetheyaretooobviouslyfalse.\n",
      "3.3 GenerativeAdversarialTrainingfor\n",
      "To demonstrate this issue, let us consider the\n",
      "KnowledgeGraphEmbeddingModels\n",
      "following example. Suppose we have a ground\n",
      "truth triple LocatedIn(NewOrleans,Louisiana), Inspired by GANs, we propose an adversarial\n",
      "and corrupt it by replacing its tail entity. training framework named KBGAN which uses a\n",
      "First, we remove the tail entity, leaving Lo- KGE model with softmax probabilities to pro-\n",
      "catedIn(NewOrleans,?). Because the relation Lo- vide high-quality negative samples for the train-\n",
      "catedIn constraints types of its entities, “?” ing of a KGE model whose training objective is\n",
      "must be a geographical region. If we fill “?” marginal loss function. This framework is inde-\n",
      "with a random entity e ∈ E, the prob- pendent of the score functions of these two mod-\n",
      "ability of e having a wrong type is very els,andthereforepossessessomeextentofuniver-\n",
      "high, resulting in ridiculous triples like Lo- sality. Figure 1 illustrates the overall structure of\n",
      "catedIn(NewOrleans,BarackObama) or Locate- KBGAN.\n",
      "dIn(NewOrleans,StarTrek). Such triples are con- In parallel to terminologies used in GAN liter-\n",
      "sidered “too easy”, because they can be elim- ature, we will simply call these two models gen-\n",
      "inated solely by types. In contrast, Locate- erator and discriminator respectively in the rest\n",
      "dIn(NewOrleans,Florida)isaveryusefulnegative of this paper. We use softmax probabilistic mod-\n",
      "triple, because it satisfies type constraints, but it els as the generator because they can adequately\n",
      "cannot be proved wrong without detailed knowl- model the “sampling from a probability distribu-\n",
      "Algorithm1:TheKBGANalgorithm\n",
      "Data:trainingsetofpositivefacttriplesT ={(h,r,t)}\n",
      "Input:Pre-trainedgeneratorGwithparametersθ andscorefunctionf (h,r,t),andpre-traineddiscriminatorDwith\n",
      "G G\n",
      "parametersθ andscorefunctionf (h,r,t)\n",
      "D D\n",
      "Output:Adversariallytraineddiscriminator\n",
      "1 b←−0;// baseline for policy gradient\n",
      "2 repeat\n",
      "3 Sampleamini-batchofdataT batchfromT ;\n",
      "4 G G ←−0,G D ←−0;// gradients of parameters of G and D\n",
      "5 r sum ←−0;// for calculating the baseline\n",
      "6 for(h,r,t)∈T batchdo\n",
      "7 UniformlyrandomlysampleN snegativetriplesNeg(h,r,t)={(h(cid:48) i,r,t(cid:48) i)} i=1...Ns;\n",
      "8 Obtaintheirprobabilityofbeinggenerated:p i = (cid:80)N j=e sx 1p ef xG p( fh G(cid:48) i (,r h, (cid:48) jt,(cid:48) i r),t(cid:48) j);\n",
      "9 Sampleonenegativetriple(h(cid:48) s,r,t(cid:48) s)fromNeg(h,r,t)accordingto{p i} i=1...Ns.Assumeitsprobabilitytobe\n",
      "p ;\n",
      "s\n",
      "10 G D ←−G D+∇ θD[f D(h,r,t)−f D(h(cid:48) s,r,t(cid:48) s)+γ] +;// accumulate gradients for D\n",
      "11 r←−−f D(h(cid:48) s,r,t(cid:48) s),r sum ←−r sum+r;// r is the reward\n",
      "12 G G ←−G G+(r−b)∇ θGlogp s;// accumulate gradients for G\n",
      "13 end\n",
      "14 θ G ←−θ G+η GG G,θ D ←−θ D−η DG D;// update parameters\n",
      "15 b←r sum/|T batch|;// update baseline\n",
      "16 untilconvergence;\n",
      "tion” process of discrete GANs, and we aim at minimizingthefollowingmarginallossfunction:\n",
      "improving discriminators based on marginal loss\n",
      "(cid:88)\n",
      "because they can benefit more from high-quality L D = [f D(h,r,t)−f D(h(cid:48),r,t(cid:48))+γ] +\n",
      "negativesamples.Notethatamajordifferencebe- (h,r,t)∈T\n",
      "tween GAN andourworkisthat,theultimategoal (h(cid:48),r,t(cid:48)) ∼ p (h(cid:48),r,t(cid:48)|h,r,t) (3)\n",
      "G\n",
      "of our framework is to produce a good discrimi-\n",
      "Theonlydifferencebetweenthislossfunctionand\n",
      "nator,whereas GANS areaimedattrainingagood\n",
      "Equation1isthatitusesnegativesamplesfromthe\n",
      "generator.Inaddition,thediscriminatorhereisnot\n",
      "generator.\n",
      "aclassifierasitwouldbeinmostGANs.\n",
      "The objective of the generator can be formu-\n",
      "lated as maximizing the following expectation of\n",
      "negativedistances:\n",
      "Intuitively,thediscriminatorshouldassignarel-\n",
      "atively small distance to a high-quality negative (cid:88)\n",
      "R = E[−f (h(cid:48),r,t(cid:48))]\n",
      "sample.Inordertoencouragethegeneratortogen- G D\n",
      "erateusefulnegativesamples,theobjectiveofthe (h,r,t)∈T\n",
      "generatoristominimizethedistancegivenbydis- (h(cid:48),r,t(cid:48)) ∼ p G(h(cid:48),r,t(cid:48)|h,r,t) (4)\n",
      "criminator for its generated triples. And just like\n",
      "R involves a discrete sampling step, so we\n",
      "G\n",
      "the ordinary training process, the objective of the\n",
      "cannotfinditsgradientwithsimpledifferentiation.\n",
      "discriminatoristominimizethemarginallossbe-\n",
      "We use a simple special case of Policy Gradient\n",
      "tween the positive triple and the generated nega-\n",
      "Theorem1 (Suttonetal.,2000)toobtainthegradi-\n",
      "tive triple. In an adversarial training setting, the\n",
      "entofR withrespecttoparametersofthegener-\n",
      "G\n",
      "generator and the discriminator are alternatively\n",
      "ator:\n",
      "trainedtowardstheirrespectiveobjectives.\n",
      "(cid:88)\n",
      "∇ R = E\n",
      "G G (h(cid:48),r,t(cid:48))∼pG(h(cid:48),r,t(cid:48)|h,r,t)\n",
      "(h,r,t)∈T\n",
      "Suppose that the generator produces a\n",
      "[−f (h(cid:48),r,t(cid:48))∇ logp (h(cid:48),r,t(cid:48)|h,r,t)]\n",
      "probability distribution on negative triples D G G\n",
      "p G(h(cid:48),r,t(cid:48)|h,r,t) given a positive triple (h,r,t), (cid:39) (cid:88) 1 (cid:88)\n",
      "and generates negative triples (h(cid:48),r,t(cid:48)) by sam- N\n",
      "(h,r,t)∈T (h(cid:48) i,r,t(cid:48) i)∼pG(h(cid:48),r,t(cid:48)|h,r,t),i=1...N\n",
      "pling from this distribution. Let f (h,r,t) be\n",
      "D [−f (h(cid:48),r,t(cid:48))∇ logp (h(cid:48),r,t(cid:48)|h,r,t)] (5)\n",
      "the score function of the discriminator. The ob- D G G\n",
      "jective of the discriminator can be formulated as 1Aproofcanbefoundinthesupplementarymaterial\n",
      "Model Hyperparameters ConstraintsorRegularizations\n",
      "TRANSE L\n",
      "1\n",
      "distance,k = 50,γ = 3 ||e||\n",
      "2\n",
      "≤ 1,||r||\n",
      "2\n",
      "≤ 1\n",
      "TRANSD L\n",
      "1\n",
      "distance,k = 50,γ = 3 ||e||\n",
      "2\n",
      "≤ 1,||r||\n",
      "2\n",
      "≤ 1,||e p||\n",
      "2\n",
      "≤ 1,||r p||\n",
      "2\n",
      "≤ 1\n",
      "DISTMULT k = 50,λ = 1/0.1 L2regularization:L\n",
      "reg\n",
      "= L+λ||Θ||2\n",
      "2\n",
      "COMPLEX 2k = 50,λ = 1/0.1 L2regularization:L\n",
      "reg\n",
      "= L+λ||Θ||2\n",
      "2\n",
      "Table 2: Hyperparameter settings of the 4 models we used. For DISTMULT and COMPLEX, λ = 1 is\n",
      "usedforFB15k-237andλ = 0.1isusedforWN18andWN18RR.Allotherhyperparametersareshared\n",
      "amongalldatasets.ListhegloballossdefinedinEquation(2).Θrepresentsallparametersinthemodel.\n",
      "Dataset #r #ent. #train #val #test out affecting the expectation of gradients.2\n",
      "FB15k-237 237 14,541 272,115 17,535 20,466\n",
      "In our case, we replace −f (h(cid:48),r,t(cid:48)) with\n",
      "WN18 18 40,943 141,442 5,000 5,000 D\n",
      "WN18RR 11 40,943 86,835 3,034 3,134 −f D(h(cid:48),r,t(cid:48)) − b(h,r,t) in the equation above\n",
      "to introduce the baseline. To avoid introducing\n",
      "Table3:Statisticsofdatasetsweusedintheexper-\n",
      "new parameters, we simply let b be a constant,\n",
      "iments.“r”:relations.\n",
      "the average reward of the whole training set: b =\n",
      "(cid:80) E [−f (h(cid:48),r,t(cid:48))].\n",
      "(h,r,t)∈T (h(cid:48),r,t(cid:48))∼pG(h(cid:48),r,t(cid:48)|h,r,t) D\n",
      "In practice, b is approximated by the mean of\n",
      "where the second approximate equality means rewardsofrecentlygeneratednegativetriples.\n",
      "we approximate the expectation with sampling in Let the generator’s score function to be\n",
      "practice.NowwecancalculatethegradientofR G f G(h,r,t),givenasetofcandidatenegativetriples\n",
      "andoptimizeitwithgradient-basedalgorithms. Neg(h,r,t) ⊂ {(h(cid:48),r,t)|h(cid:48) ∈ E}∪{(h,r,t(cid:48))|t(cid:48) ∈\n",
      "PolicyGradientTheoremarisesfromreinforce- E},theprobabilitydistributionp G ismodeledas:\n",
      "ment learning (RL), so we would like to draw an\n",
      "expf (h(cid:48),r,t(cid:48))\n",
      "analogybetweenourmodelandanRLmodel.The p (h(cid:48),r,t(cid:48)|h,r,t) = G\n",
      "G (cid:80) expf (h∗,r,t∗)\n",
      "generator can be viewed as an agent which inter- G\n",
      "(h∗,r,t∗) ∈ Neg(h,r,t) (6)\n",
      "acts with the environment by performing actions\n",
      "and improves itself by maximizing the reward re-\n",
      "Ideally,Neg(h,r,t)shouldcontainallpossible\n",
      "turnedfromtheenvironmentinresponseofitsac-\n",
      "negatives. However, knowledge graphs are usu-\n",
      "tions. Correspondingly, the discriminator can be\n",
      "ally highly incomplete, so the ”hardest” negative\n",
      "viewed as the environment. Using RL terminolo-\n",
      "triples are very likely to be false negatives (true\n",
      "gies, (h,r,t) is the state (which determines what\n",
      "facts). To address this issue, we instead generate\n",
      "actions the actor can take), p (h(cid:48),r,t(cid:48)|h,r,t) is\n",
      "G Neg(h,r,t)byuniformlysamplingofN entities\n",
      "s\n",
      "thepolicy(howtheactorchooseactions),(h(cid:48),r,t(cid:48))\n",
      "(a small number compared to the number of all\n",
      "is the action, and −f (h(cid:48),r,t(cid:48)) is the reward.\n",
      "D possible negatives) from E to replace h or t. Be-\n",
      "The method of optimizing R described above\n",
      "G cause in real-world knowledge graphs, true neg-\n",
      "is called REINFORCE (Williams, 1992) algorithm atives are usually far more than false negatives,\n",
      "in RL. Our model is a simple special case of\n",
      "such set would be unlikely to contain any false\n",
      "RL, called one-step RL. In a typical RL setting,\n",
      "negative, and the negative selected by the gener-\n",
      "each action performed by the agent will change\n",
      "atorwouldlikelybeatruenegative.Usingasmall\n",
      "its state, and the agent will perform a series of\n",
      "Neg(h,r,t) can also significantly reduce compu-\n",
      "actions (called an epoch) until it reaches certain\n",
      "tationalcomplexity.\n",
      "states or the number of actions reaches a certain\n",
      "Besides, we adopt the “bern” sampling tech-\n",
      "limit.However,intheanalogyabove,actionsdoes\n",
      "nique (Wang et al., 2014) which replaces the\n",
      "notaffectthestate,andaftereachactionwerestart\n",
      "“1” side in “1-to-N” and “N-to-1” relations with\n",
      "with another unrelated state, so each epoch con-\n",
      "higher probability to further reduce false nega-\n",
      "sistsofonlyoneaction.\n",
      "tives.\n",
      "To reduce the variance of REINFORCE al- Algorithm 1 summarizes the whole adversarial\n",
      "gorithm, it is common to subtract a base- training process. Both the generator and the dis-\n",
      "line from the reward, which is an arbitrary\n",
      "2Aproofofsuchfactcanalsobefoundinthesupplemen-\n",
      "number that only depends on the state, with- tarymaterial\n",
      "criminator require pre-training, which is the same 4.1.3 ImplementationDetails\n",
      "as conventionally training a single KBE model 3 In the pre-training stage, we train every model\n",
      "with uniform negative sampling. Formally speak-\n",
      "to convergence for 1000 epochs, and divide ev-\n",
      "ing, one can pre-train the generator by minimiz-\n",
      "eryepochinto100mini-batches.Toavoidoverfit-\n",
      "ing the loss function defined in Equation (1), and\n",
      "ting, we adopt early stopping by evaluating MRR\n",
      "pre-trainthediscriminatorbyminimizingtheloss\n",
      "on the validation set every 50 epochs. We tried\n",
      "function defined in Equation (2). Line 14 in the\n",
      "γ = 0.5,1,2,3,4,5 and L,L distances for\n",
      "1 2\n",
      "algorithm assumes that we are using the vanilla\n",
      "TRANSE and TRANSD, and λ = 0.01,0.1,1,10\n",
      "gradient descent as the optimization method, but\n",
      "for DISTMULT and COMPLEX, and determined\n",
      "obviously one can substitute it with any gradient-\n",
      "the best hyperparameters listed on table 2, based\n",
      "basedoptimizationalgorithm.\n",
      "on their performances on the validation set af-\n",
      "ter pre-training. Due to limited computation re-\n",
      "4 Experiments\n",
      "sources, we deliberately limit the dimensions of\n",
      "embeddings to k = 50, similar to the one used\n",
      "To evaluate our proposed framework, we test its\n",
      "in earlier works, to save time. We also apply cer-\n",
      "performance for the link prediction task with dif-\n",
      "tainconstraintsorregularizationstothesemodels,\n",
      "ferent generators and discriminators. For the gen-\n",
      "which are mostly the same as those described in\n",
      "erator, we choose two classical probability-based\n",
      "theiroriginalpublications,andalsolistedontable\n",
      "KGE model, DISTMULT and COMPLEX, and\n",
      "2.\n",
      "for the discriminator, we also choose two classi-\n",
      "In the adversarial training stage, we keep all\n",
      "cal translation-based KGE model, TRANSE and\n",
      "thehyperparamtersdeterminedinthepre-training\n",
      "TRANSD, resulting in four possible combinations\n",
      "stage unchanged. The number of candidate neg-\n",
      "of generator and discriminator in total. See Table<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  20212,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Link prediction']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  limited computation re-\n",
      "4 Experiments\n",
      "sources, we deliberately limit the dimensions of\n",
      "embeddings to k = 50, similar to the one used\n",
      "To evaluate our proposed framework, we test its\n",
      "in earlier works, to save time. We also apply cer-\n",
      "performance for the link prediction task with dif-\n",
      "tainconstraintsorregularizationstothesemodels,\n",
      "ferent generators and discriminators. For the gen-\n",
      "which are mostly the same as those described in\n",
      "erator, we choose two classical probability-based\n",
      "theiroriginalpublications,andalsolistedontable\n",
      "KGE model, DISTMULT and COMPLEX, and\n",
      "2.\n",
      "for the discriminator, we also choose two classi-\n",
      "In the adversarial training stage, we keep all\n",
      "cal translation-based KGE model, TRANSE and\n",
      "thehyperparamtersdeterminedinthepre-training\n",
      "TRANSD, resulting in four possible combinations\n",
      "stage unchanged. The number of candidate neg-\n",
      "of generator and discriminator in total. See Table\n",
      "ative triples, N, is set to 20 in all cases, which\n",
      "1forabriefsummaryofthesemodels. s\n",
      "is proven to be optimal among the candidate set\n",
      "4.1 ExperimentalSettings of {5,10,20,30,50}. We train for 5000 epochs,\n",
      "with100mini-batchesforeachepoch.Wealsouse\n",
      "4.1.1 Datasets\n",
      "earlystoppinginadversarialtrainingbyevaluating\n",
      "We use three common knowledge base com-\n",
      "MRRonthevalidationsetevery100epochs.\n",
      "pletion datasets for our experiment: FB15k-237,\n",
      "We use the self-adaptive optimization method\n",
      "WN18 and WN18RR. FB15k-237 is a subset\n",
      "Adam (Kingma and Ba, 2015) for all trainings,\n",
      "of FB15k introduced by (Toutanova and Chen,\n",
      "and always use the recommended default setting\n",
      "2015), which removed redundant relations in\n",
      "α = 0.001,β = 0.9,β = 0.999,(cid:15) = 10−8.\n",
      "1 2\n",
      "FB15k and greatly reduced the number of rela-\n",
      "tions.Likewise,WN18RRisasubsetofWN18in- 4.2 Results\n",
      "troducedby(Dettmersetal.,2017)whichremoves\n",
      "Results of our experiments as well as baselines\n",
      "reversing relations and dramatically increases the\n",
      "are shown in Table 4. All settings of adversarial\n",
      "difficulty of reasoning. Both FB15k and WN18\n",
      "training bring a pronounced improvement to the\n",
      "are first introduced by (Bordes et al., 2013) and\n",
      "model, which indicates that our method is con-\n",
      "havebeencommonlyusedinknowledgegraphre-\n",
      "sistently effective in various cases. TRANSE per-\n",
      "searches. Statistics of datasets we used are shown\n",
      "formsslightlyworsethanTRANSDonFB15k-237\n",
      "inTable3.\n",
      "and WN18, but better on WN18RR. Using DIST-\n",
      "4.1.2 EvaluationProtocols MULT or COMPLEX asthegeneratordoesnotaf-\n",
      "fectperformancegreatly.\n",
      "Followingpreviousworkslike(Yangetal.,2015)\n",
      "and (Trouillon et al., 2016), for each run, we re- TRANSE and TRANSD enhanced by KBGAN\n",
      "cansignificantlybeattheircorrespondingbaseline\n",
      "port two common metrics, mean reciprocal rank-\n",
      "implementations, and outperform stronger base-\n",
      "ing (MRR) and hits at 10 (H@10). We only re-\n",
      "lines in some cases. As a prototypical and proof-\n",
      "portscoresunderthefilteredsetting(Bordesetal.,\n",
      "of-principle experiment, we have never expected\n",
      "2013),whichremovesalltriplesappearedintrain-\n",
      "state-of-the-art results. Being simple models pro-\n",
      "ing, validating, and testing sets from candidate\n",
      "triples before obtaining the rank of the ground\n",
      "3The KBGAN source code is available at https://\n",
      "truthtriple. github.com/cai-lw/KBGAN\n",
      "FB15k-237 WN18 WN18RR\n",
      "Method MRR H@10 MRR H@10 MRR H@10\n",
      "TRANSE - 42.8† - 89.2 - 43.2†\n",
      "TRANSD - 45.3† - 92.2 - 42.8†\n",
      "DISTMULT 24.1‡ 41.9‡ 82.2 93.6 42.5‡ 49.1‡\n",
      "COMPLEX 24.0‡ 41.9‡ 94.1 94.7 44.4‡ 50.7‡\n",
      "TRANSE (pre-trained) 24.2 42.2 43.3 91.5 18.6 45.9\n",
      "KBGAN(TRANSE + DISTMULT) 27.4 45.0 71.0 94.9 21.3 48.1\n",
      "KBGAN(TRANSE + COMPLEX) 27.8 45.3 70.5 94.9 21.0 47.9\n",
      "TRANSD (pre-trained) 24.5 42.7 49.4 92.8 19.2 46.5\n",
      "KBGAN(TRANSD + DISTMULT) 27.8 45.8 77.2 94.8 21.4 47.2\n",
      "KBGAN(TRANSD + COMPLEX) 27.7 45.8 77.9 94.8 21.5 46.9\n",
      "Table 4: Experimental results. Results of KBGAN are results of its discriminator (on the left of the “+”\n",
      "sign). Underlined results are the best ones among our implementations. Results marked with † are pro-\n",
      "duced by running Fast-TransX (Lin et al., 2015) with its default parameters. Results marked with ‡ are\n",
      "copiedfrom(Dettmersetal.,2017).Allotherbaselineresultsarecopiedfromtheiroriginalpapers.\n",
      "Figure2:LearningcurvesofKBGAN.Allmetricsimprovesteadilyastrainingproceeds.\n",
      "posed several years ago, TRANSE and TRANSD imum as training proceeds, which indicates that\n",
      "hastheirlimitationsinexpressivenessthatareun- KBGANisarobustGANthatcanconvergetogood\n",
      "likely to be fully compensated by better training resultsinvarioussettings,althoughGANsarewell-\n",
      "technique. In future researches, people may try known for difficulty in convergence. Fluctuations\n",
      "employing more advanced models into KBGAN, in these graphs may seem more prominent than\n",
      "andwebelieveithasthepotentialtobecomestate- other KGE models, but is considered normal for\n",
      "of-the-art. an adversially trained model. Note that in some\n",
      "casesthecurvestilltendstoriseafter5000epochs.\n",
      "To illustrate our training progress, we plot per- Wedonothavesufficientcomputationresourceto\n",
      "formances of the discriminator on validation set trainformoreepochs,butwebelievethattheywill\n",
      "over epochs, which are displayed in Figure 2. As alsoeventuallyconverge.\n",
      "all these graphs show, our performances are al-\n",
      "ways in increasing trends, converging to its max-\n",
      "Positivefact Uniformrandomsample Trainedgenerator\n",
      "(condensation NN 2, family arcidae NN 1 revivification NN 1\n",
      "derivationally related form, repast NN 1 mouthpiece NN 3\n",
      "distill VB 4) beater NN 2 liquid body substance NN 1\n",
      "coverall NN 1 stiffen VB 2\n",
      "cash advance NN 1 hot up VB 1\n",
      "(colorado river NN 2, lunar calendar NN 1 idaho NN 1\n",
      "instance hypernym, umbellularia californica NN 1 sayan mountains NN 1\n",
      "river NN 1) tonality NN 1 lower saxony NN 1\n",
      "creepy-crawly NN 1 order ciconiiformes NN 1\n",
      "moor VB 3 jab NN 3\n",
      "(meeting NN 2, cellular JJ 1 attach VB 1\n",
      "hypernym, commercial activity NN 1 bond NN 6\n",
      "social gathering NN 1) giant cane NN 1 heavy spar NN 1\n",
      "streptomyces NN 1 satellite NN 1\n",
      "tranquillize VB 1 peep VB 3\n",
      "Table 5: Examples of negative samples in WN18 dataset. The first column is the positive fact, and the\n",
      "terminboldistheonetobereplacedbyanentityinthenexttwocolumns.Thesecondcolumnconsists\n",
      "ofrandomentitiesdrawnfromthewholedataset.Thethirdcolumncontainsnegativesamplesgenerated\n",
      "bythegeneratorinthelast5epochsoftraining.Entitiesinitalicareconsideredtohavesemanticrelation\n",
      "tothepositiveone\n",
      "4.3 Casestudy 5 Conclusions\n",
      "We propose a novel adversarial learning method\n",
      "To demonstrate that our approach does generate for improving a wide range of knowledge graph\n",
      "betternegativesamples,welistsomeexamplesof embedding models—We designed a generator-\n",
      "them in Table 5, using the KBGAN (TRANSE + discriminator framework with dual KGE compo-\n",
      "DISTMULT)modelandtheWN18dataset.Allhy- nents. Unlike random uniform sampling, the gen-\n",
      "perparameters are the same as those described in eratormodelgenerateshigherqualitynegativeex-\n",
      "Section4.1.3. amples, which allow the discriminator model to\n",
      "learn better. To enable backpropagation of error,\n",
      "Compared to uniform random negatives which\n",
      "we introduced a one-step REINFORCE method to\n",
      "are almost always totally unrelated, the genera-\n",
      "seamlesslyintegratethetwomodules.Experimen-\n",
      "tor generates more semantically related negative\n",
      "tally,wetestedtheproposedideaswithfourcom-\n",
      "samples, which is different from type relatedness\n",
      "monlyusedKGEmodelsonthreedatasets,andthe\n",
      "we used as example in Section 3.2, but also helps\n",
      "resultsshowedthattheadversariallearningframe-\n",
      "training.Inthefirstexample,twoofthefiveterms\n",
      "work brought consistent improvements to various\n",
      "are physically related to the process of distilling\n",
      "KGEmodelsunderdifferentsettings.\n",
      "liquids. In the second example, three of the five\n",
      "entities are geographical objects. In the third ex-\n",
      "ample,twoofthefiveentitiesexpresstheconcept\n",
      "of“gather”.\n",
      "Because we deliberately limited the strength of\n",
      "generated negatives by using a small N as de-\n",
      "s\n",
      "scribed in Section 3.3, the semantic relation is\n",
      "prettyweak,andtherearestillmanyunrelateden-\n",
      "tities. However, empirical results (when selecting\n",
      "the optimal N ) shows that such situation is more\n",
      "s\n",
      "beneficial for training the discriminator than gen-\n",
      "eratingevenstrongernegatives.\n",
      "References Tom M Mitchell, William Cohen, Estevam Hruschka,\n",
      "ParthaTalukdar,JustinBetteridge,AndrewCarlson,\n",
      "MartinArjovsky, SoumithChintala, andLeon Bottou.\n",
      "Bhavana Dalvi Mishra, Matthew Gardner, Bryan\n",
      "2017. Wasserstein gan. In International Confer-\n",
      "Kisiel, Jayant Krishnamurthy, et al. 2015. Never-\n",
      "renceonMachineLearning.\n",
      "endinglearning. InTheTwenty-ninthAAAIConfer-\n",
      "enceonArtificialIntelligence.\n",
      "Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim\n",
      "Sturge,andJamieTaylor.2008. Freebase:acollab-\n",
      "oratively created graph database for structuring hu- Maximilian Nickel, Kevin Murphy, Volker Tresp, and\n",
      "man knowledge. In Proceedings of the 2008 ACM Evgeniy Gabrilovich. 2015. A review of relational\n",
      "SIGMOD international conference on Management machine learning for knowledge graphs. arXiv\n",
      "ofdata.ACM,pages1247–1250. preprintarXiv:1503.00759.\n",
      "Antoine Bordes, Nicolas Usunier, Alberto Garcia- Maximilian Nickel, Lorenzo Rosasco, and\n",
      "Duran, Jason Weston, and Oksana Yakhnenko. Tomaso Poggio Poggio. 2016. Holographic\n",
      "2013. Translating embeddings for modeling multi- embeddings of knowledgegraphs. In The Thirtieth\n",
      "relational data. In Advances in Neural Information AAAI Conference on Artificial Intelligence. pages\n",
      "ProcessingSystems.pages2787–2795. 1955–1961.\n",
      "Tim Dettmers, Pasquale Minervini, Pontus Stene-\n",
      "Maximilian Nickel, Volker Tresp, and Hans-Peter\n",
      "torp, and Sebastian Riedel. 2017. Convolutional\n",
      "Kriegel. 2011. A three-way model for collective\n",
      "2d knowledge graph embeddings. arXiv preprint\n",
      "learning on multi-relational data. In Proceedings\n",
      "arXiv:1707.01476.\n",
      "of the 28th International Conference on Machine\n",
      "Learning.pages809–816.\n",
      "XinDong,EvgeniyGabrilovich,GeremyHeitz,Wilko\n",
      "Horn, Ni Lao, Kevin Murphy, Thomas Strohmann,\n",
      "Fabian M Suchanek, Gjergji Kasneci, and Gerhard\n",
      "Shaohua Sun, and Wei Zhang. 2014. Knowledge\n",
      "Weikum. 2007. Yago: a core of semantic knowl-\n",
      "vault:Aweb-scaleapproachtoprobabilisticknowl-\n",
      "edge fusion. In Proceedings of the 20th ACM edge. InProceedingsofthe16thinternationalcon-\n",
      "SIGKDD international conference on Knowledge\n",
      "ferenceonWorldWideWeb.ACM,pages697–706.\n",
      "discoveryanddatamining.ACM,pages601–610.\n",
      "Richard S Sutton, David A McAllester, Satinder P\n",
      "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Singh, and Yishay Mansour. 2000. Policy gradi-\n",
      "BingXu,DavidWarde-Farley,SherjilOzair,Aaron ent methods for reinforcement learning with func-\n",
      "Courville,andYoshuaBengio.2014. Generativead- tionapproximation. InAdvancesinneuralinforma-\n",
      "versarial nets. In Advances in Neural Information tionprocessingsystems.pages1057–1063.\n",
      "ProcessingSystems.pages2672–2680.\n",
      "Kristina Toutanova and Danqi Chen. 2015. Observed\n",
      "GuoliangJi,ShizhuHe,LihengXu,KangLiu,andJun\n",
      "versus latent features for knowledge base and text\n",
      "Zhao. 2015. Knowledge graph embedding via dy-\n",
      "inference. In Proceedings of the 3rd Workshop on\n",
      "namicmappingmatrix. InThe53rdAnnualMeeting\n",
      "ContinuousVectorSpaceModelsandtheirCompo-\n",
      "oftheAssociationforComputationalLinguistics.\n",
      "sitionality.pages57–66.\n",
      "Felix Juefei-Xu, Vishnu Naresh Boddeti, and Marios\n",
      "Savvides.2017. Gangofgans:Generativeadversar-\n",
      "The´oTrouillon,JohannesWelbl,SebastianRiedel,E´ric\n",
      "ialnetworkswithmaximummarginranking. arXiv Gaussier, and Guillaume Bouchard. 2016. Com-\n",
      "preprintarXiv:1704.04865. plex embeddings for simple link prediction. In In-\n",
      "ternationalConferenceonMachineLearning.pages\n",
      "Diederik P. Kingma and Jimmy Lei Ba. 2015. Adam: 2071–2080.\n",
      "A method for stochastic optimization. In The 3rd\n",
      "International Conference on Learning Representa- Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong,\n",
      "tions. Yinghui Xu, Benyou Wang, Peng Zhang, and Dell\n",
      "Zhang. 2017. Irgan: A minimax game for unifying\n",
      "Denis Krompaß, Stephan Baier, and Volker Tresp.\n",
      "generative and discriminative information retrieval\n",
      "2015. Type-constrained representation learning in\n",
      "models. InThe40thInternationalACMSIGIRCon-\n",
      "knowledge graphs. In International Semantic Web\n",
      "ference.\n",
      "Conference.Springer,pages640–655.\n",
      "Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\n",
      "YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,and\n",
      "Chen.2014. Knowledgegraphembeddingbytrans-\n",
      "Xuan Zhu. 2015. Learning entity and relation em-\n",
      "lating on hyperplanes. In The Twenty-eighth AAAI\n",
      "beddings for knowledge graph completion. In The\n",
      "Conference on Artificial Intelligence. pages 1112–\n",
      "Twenty-ninth AAAI Conference on Artificial Intelli-\n",
      "1119.\n",
      "gence.pages2181–2187.\n",
      "Mehdi Mirza and Simon Osindero. 2014. Condi- Ronald J Williams. 1992. Simple statistical gradient-\n",
      "tional generative adversarial nets. arXiv preprint following algorithms for connectionist reinforce-\n",
      "arXiv:1411.01784. mentlearning. Machinelearning8(3-4):229–256.\n",
      "Han Xiao, Minlie Huang, and Xiaoyan Zhu. 2016.\n",
      "Fromonepointtoamanifold:Knowledgegraphem-\n",
      "beddingforpreciselinkprediction. InTheTwenty-\n",
      "FifthInternationalJointConferenceonArtificialIn-\n",
      "telligence.\n",
      "Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\n",
      "Gao, and Li Deng. 2015. Embedding entities and\n",
      "relations for learning and inference in knowledge\n",
      "bases. The3rdInternationalConferenceonLearn-\n",
      "ingRepresentations.\n",
      "Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.\n",
      "2017. Seqgan:Sequencegenerativeadversarialnets\n",
      "withpolicygradient. InTheThirty-FirstAAAICon-\n",
      "ferenceonArtificialIntelligence.pages2852–2858.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  40188,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Link prediction', 'Knowledge graph embedding']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: KBGAN: Adversarial Learning for Knowledge Graph Embeddings\n",
      "LiweiCai WilliamYangWang\n",
      "DepartmentofElectronicEngineering DepartmentofComputerScience\n",
      "TsinghuaUniversity UniversityofCalifornia,SantaBarbara\n",
      "Beijing100084China SantaBarbara,CA93106USA\n",
      "cai.lw123@gmail.com william@cs.ucsb.edu\n",
      "Abstract formofadiscreterelationaltriplesuchasLocate-\n",
      "dIn(NewOrleans,Louisiana).\n",
      "Weintroduce KBGAN,anadversariallearning A main challenge for using discrete represen-\n",
      "framework to improve the performances of a tation of knowledge graph is the lack of capa-\n",
      "wide range of existing knowledge graph em-\n",
      "bility of accessing the similarities among differ-\n",
      "bedding models. Because knowledge graphs\n",
      "ent entities and relations. Knowledge graph em-\n",
      "typicallyonlycontainpositivefacts,sampling\n",
      "bedding (KGE) techniques (e.g., RESCAL (Nickel\n",
      "useful negative training examples is a non-\n",
      "trivial task. Replacing the head or tail entity\n",
      "etal.,2011),TRANSE(Bordesetal.,2013),DIST-\n",
      "of a fact with a uniformly randomly selected MULT(Yangetal.,2015),andCOMPLEX(Trouil-\n",
      "entity is a conventional method for generat- lon et al., 2016)) have been proposed in recent\n",
      "ingnegativefacts,butthemajorityofthegen- years to deal with the issue. The main idea is\n",
      "erated negative facts can be easily discrimi- to represent the entities and relations in a vec-\n",
      "nated from positive facts, and will contribute\n",
      "torspace,andonecanusemachinelearningtech-\n",
      "littletowardsthetraining.Inspiredbygenera-\n",
      "niquetolearnthecontinuousrepresentationofthe\n",
      "tive adversarial networks (GANs), we use one\n",
      "knowledgegraphinthelatentspace.\n",
      "knowledge graph embedding model as a neg-\n",
      "ative sample generator to assist the training However, even steady progress has been made\n",
      "of our desired model, which acts as the dis- in developing novel algorithms for knowledge\n",
      "criminator in GANs. This framework is inde- graph embedding, there is still a common chal-\n",
      "pendentoftheconcreteformofgeneratorand lenge in this line of research. For space effi-\n",
      "discriminator,andthereforecanutilizeawide\n",
      "ciency, common knowledge graphs such as Free-\n",
      "variety of knowledge graph embedding mod-\n",
      "base (Bollacker et al., 2008), Yago (Suchanek\n",
      "els as its building blocks. In experiments, we\n",
      "et al., 2007), and NELL (Mitchell et al., 2015) by\n",
      "adversariallytraintwotranslation-basedmod-\n",
      "default only stores beliefs, rather than disbeliefs.\n",
      "els, TRANSE and TRANSD, each with assis-\n",
      "tance from one of the two probability-based Therefore, when training the embedding models,\n",
      "models,DISTMULTandCOMPLEX.Weeval- there is only the natural presence of the positive\n",
      "uate the performances of KBGAN on the link examples. To use negative examples, a common\n",
      "prediction task, using three knowledge base methodistoremovethecorrecttailentity,andran-\n",
      "completion datasets: FB15k-237, WN18 and\n",
      "domlysamplefromauniformdistribution(Bordes\n",
      "WN18RR.Experimentalresultsshowthatad-\n",
      "et al., 2013). Unfortunately, this approach is not\n",
      "versarial training substantially improves the\n",
      "ideal, because the sampled entity could be com-\n",
      "performancesoftargetembeddingmodelsun-\n",
      "dervarioussettings. pletely unrelated to the head and the target re-\n",
      "lation, and thus the quality of randomly gener-\n",
      "1 Introduction\n",
      "atednegativeexamplesisoftenpoor(e.g,Locate-\n",
      "dIn(NewOrleans,BarackObama)).Otherapproach\n",
      "Knowledge graph (Dong et al., 2014) is a pow- might leverage external ontological constraints\n",
      "erful graph structure that can provide direct ac- suchasentitytypes(Krompaßetal.,2015)togen-\n",
      "cess of knowledge to users via various applica- erate negative examples, but such resource does\n",
      "tions such as structured search, question answer- notalwaysexistoraccessible.\n",
      "ing, and intelligent virtual assistant. A common In this work, we provide a generic solution to\n",
      "representationofknowledgegraphbeliefsisinthe improve the training of a wide range of knowl-\n",
      "8102\n",
      "rpA\n",
      "61\n",
      "]LC.sc[\n",
      "3v17040.1171:viXra\n",
      "Model Scorefunctionf(h,r,t) Numberofparameters\n",
      "TRANSE ||h+r−t||\n",
      "1/2\n",
      "k|E|+k|R|\n",
      "TRANSD ||(I+r ph pT)h+r−(I+r pt pT)t||\n",
      "1/2\n",
      "2k|E|+2k|R|\n",
      "DISTMULT\n",
      "<h,r,t>(=(cid:80)k\n",
      "i=1h ir it i) k|E|+k|R|\n",
      "COMPLEX <h,r,¯t>(h,r,t∈Ck) 2k|E|+2k|R|\n",
      "TRANSH ||(I−r pr pT)h+r−(I+r pr pT)t||\n",
      "1/2\n",
      "k|E|+2k|R|\n",
      "TRANSR ||W rh+r−W rt||\n",
      "1/2\n",
      "k|E|+(k2+k)|R|\n",
      "MANIFOLDE(hyperplane) |(h+r head)T(t+r tail)−D r| k|E|+(2k+1)|R|\n",
      "RESCAL hTW rt k|E|+k2|R|\n",
      "HOLE rT(h(cid:63)t)((cid:63)iscircularcorrelation) k|E|+k|R|\n",
      "CONVE f(vec(f([h¯;¯r]∗ω))W)t k|E|+k|R|+kcmn\n",
      "Table1:Someselectedknowledgegraphembeddingmodels.Thefourmodelsabovethedoublelineare\n",
      "considered in this paper. Except for COMPLEX, all boldface lower case letters represent vectors in Rk,\n",
      "andboldfaceuppercaselettersrepresentmatricesinRk×k.Iistheidentitymatrix.\n",
      "edge graph embedding models. Inspired by the 2 RelatedWork\n",
      "recent advances of generative adversarial deep\n",
      "models (Goodfellow et al., 2014), we propose 2.1 KnowledgeGraphEmbeddings\n",
      "a novel adversarial learning framework, namely,\n",
      "A large number of knowledge graph embedding\n",
      "KBGAN, for generating better negative exam-\n",
      "models,whichrepresententitiesandrelationsina\n",
      "ples to train knowledge graph embedding mod-\n",
      "knowledge graph with vectors or matrices, have\n",
      "els. More specifically, we consider probability-\n",
      "been proposed in recent years. RESCAL (Nickel\n",
      "based, log-loss embedding models as the gener-\n",
      "et al., 2011) is one of the earliest studies on ma-\n",
      "ator to supply better quality negative examples,\n",
      "trix factorization based knowledge graph embed-\n",
      "and use distance-based, margin-loss embedding\n",
      "ding models, using a bilinear form as score func-\n",
      "models as the discriminator to generate the final\n",
      "tion. TRANSE (Bordes et al., 2013) is the first\n",
      "knowledge graph embeddings. Since the genera-\n",
      "model to introduce translation-based embedding.\n",
      "tor has a discrete generation step, we cannot di-\n",
      "Later variants, such as TRANSH (Wang et al.,\n",
      "rectly use the gradient-based approach to back-\n",
      "2014), TRANSR (Lin et al., 2015) and TRANSD\n",
      "propagate the errors. We then consider a one-\n",
      "(Jietal.,2015),extendTRANSEbyprojectingthe\n",
      "step reinforcement learning setting, and use a\n",
      "embeddingvectorsofentitiesintovariousspaces.\n",
      "variance-reductionREINFORCEmethodtoachieve\n",
      "DISTMULT(Yangetal.,2015)simplifiesRESCAL\n",
      "thisgoal.Empirically,weperformexperimentson\n",
      "by only using a diagonal matrix, and COMPLEX\n",
      "threecommonKGEdatasets(FB15K-237,WN18\n",
      "(Trouillon et al., 2016) extends DISTMULT into\n",
      "andWN18RR),andverifytheadversariallearning\n",
      "the complex number field. (Nickel et al., 2015) is\n",
      "approach with a set of KGE models. Our exper-\n",
      "acomprehensivesurveyonthesemodels.\n",
      "iments show that across various settings, this ad-\n",
      "Someofthemorerecentmodelsachievestrong\n",
      "versariallearningmechanismcansignificantlyim-\n",
      "performances. MANIFOLDE (Xiao et al., 2016)\n",
      "prove the performance of some of the most com-\n",
      "embeds a triple as a manifold rather than a point.\n",
      "monly used translation based KGE methods. Our\n",
      "HOLE (Nickel et al., 2016) employs circular cor-\n",
      "contributionsarethree-fold:\n",
      "relation to combine the two entities in a triple.\n",
      "CONVE (Dettmers et al., 2017) uses a convolu-\n",
      "• Wearethefirsttoconsideradversariallearn-\n",
      "tional neural network as the score function. How-\n",
      "ingtogenerateusefulnegativetrainingexam-\n",
      "ever, most of these studies use uniform sampling\n",
      "plestoimproveknowledgegraphembedding.\n",
      "to generate negative training examples (Bordes\n",
      "et al., 2013). Because our framework is indepen-\n",
      "• This adversarial learning framework applies dentoftheconcreteformofmodels,allthesemod-\n",
      "to a wide range of KGE models, without the elscanbepotentiallyincorporatedintoourframe-\n",
      "needofexternalontologiesconstraints. work, regardless of the complexity. As a proof of\n",
      "principle,ourworkfocusesonsimplermodels.Ta-\n",
      "• Our method shows consistent performance ble 1 summarizes the score functions and dimen-\n",
      "gainsonthreecommonlyusedKGEdatasets. sionsofallmodelsmentionedabove.\n",
      "2.2 GenerativeAdversarialNetworksandits KBGAN.\n",
      "Variants\n",
      "3.1 TypesofTrainingObjectives\n",
      "Generative Adversarial Networks (GANs) (Good-\n",
      "For a given knowledge graph, let E be the set of\n",
      "fellow et al., 2014) was originally proposed for\n",
      "entities, R be the set of relations, and T be the\n",
      "generating samples in a continuous space such as\n",
      "setofgroundtruthtriples.Ingeneral,aknowledge\n",
      "images. A GAN consists of two parts, the genera-\n",
      "graphembedding(KGE)modelcanbeformulated\n",
      "torandthediscriminator.Thegeneratoracceptsa\n",
      "as a score function f(h,r,t),h,t ∈ E,r ∈ R\n",
      "noiseinputandoutputsanimage.Thediscrimina-\n",
      "which assigns a score to every possible triple in\n",
      "torisaclassifierwhichclassifiesimagesas“true”\n",
      "theknowledgegraph.Theestimatedlikelihoodof\n",
      "(fromthegroundtruthset)or“fake”(generatedby\n",
      "a triple to be true depends only on its score given\n",
      "the generator). When training a GAN, the genera-\n",
      "bythescorefunction.\n",
      "torandthediscriminatorplayaminimaxgame,in\n",
      "Differentmodelsformulatetheirscorefunction\n",
      "whichthegeneratortriestogenerate“real”images\n",
      "basedondifferentdesigns,andthereforeinterpret\n",
      "todeceivethediscriminator,andthediscriminator\n",
      "scores differently, which further lead to various\n",
      "tries to tell them apart from ground truth images.\n",
      "training objectives. Two common forms of train-\n",
      "GANs are also capable of generating samples sat-\n",
      "ingobjectivesareparticularlyofourinterest:\n",
      "isfying certain requirements, such as conditional\n",
      "Marginal loss function is commonly used by\n",
      "GAN(MirzaandOsindero,2014).\n",
      "a large group of models called translation-based\n",
      "ItisnotpossibletouseGANsinitsoriginalform\n",
      "models, whose score function models distance\n",
      "for generating discrete samples like natural lan-\n",
      "between points or vectors, such as TRANSE,\n",
      "guage sentences or knowledge graph triples, be-\n",
      "TRANSH, TRANSR, TRANSD andsoon.Inthese\n",
      "cause the discrete sampling step prevents gradi-\n",
      "models, smaller distance indicates a higher likeli-\n",
      "ents from propagating back to the generator. SE-\n",
      "hoodoftruth,butonlyqualitatively.Themarginal\n",
      "QGAN (Yuetal.,2017)isoneofthefirstsuccess-\n",
      "lossfunctiontakesthefollowingform:\n",
      "ful solutions to this problem by using reinforce-\n",
      "(cid:88)\n",
      "ment learning—It trains the generator using pol- L = [f(h,r,t)−f(h(cid:48),r,t(cid:48))+γ] (1)\n",
      "m +\n",
      "icygradientandothertricks. IRGAN (Wangetal., (h,r,t)∈T\n",
      "2017) is a recent work which combines two cate-\n",
      "where γ is the margin, [·] = max(0,·) is the\n",
      "gories of information retrieval models into a dis- +\n",
      "hinge function, and (h(cid:48),r,t(cid:48)) is a negative triple.\n",
      "crete GAN framework. Likewise, our framework\n",
      "The negative triple is generated by replacing the\n",
      "relies on policy gradient to train the generator\n",
      "head entity or the tail entity of a positive triple\n",
      "whichprovidesdiscretenegativetriples.\n",
      "with a random entity in the knowledge graph,\n",
      "The discriminator in a GAN is not necessarily\n",
      "or formally (h(cid:48),r,t(cid:48)) ∈ {(h(cid:48),r,t)|h(cid:48) ∈ E} ∪\n",
      "aclassifier.Wasserstein GAN or WGAN (Arjovsky\n",
      "{(h,r,t(cid:48))|t(cid:48) ∈ E}.\n",
      "et al., 2017) uses a regressor with clipped param-\n",
      "Log-softmax loss function is commonly used by\n",
      "eters as its discriminator, based on solid analysis\n",
      "models whose score function has probabilistic in-\n",
      "about the mathematical nature of GANs. GOGAN\n",
      "terpretation.Somenotableexamplesare RESCAL,\n",
      "(Juefei-Xu et al., 2017) further replaces the loss\n",
      "DISTMULT, COMPLEX. Applying the softmax\n",
      "function in WGAN with marginal loss. Although\n",
      "function on scores of a given set of triples gives\n",
      "originating from very different fields, the form of\n",
      "theprobabilityofatripletobethebestoneamong\n",
      "loss function in our framework turns out to be\n",
      "expf(h,r,t)\n",
      "them: p(h,r,t) =. The loss\n",
      "morecloselyrelatedtotheonein GOGAN. (cid:80) (h(cid:48),r,t(cid:48))expf(h(cid:48),r,t(cid:48))\n",
      "functionisthenegativelog-likelihoodofthisprob-\n",
      "3 OurApproaches abilisticmodel:\n",
      "Inthissection,wefirstdefinetwotypesoftraining (cid:88) expf(h,r,t)\n",
      "L = −log\n",
      "objectives in knowledge graph embedding mod-\n",
      "l (cid:80) expf(h(cid:48),r,t(cid:48))\n",
      "(h,r,t)∈T\n",
      "els to show how KBGAN can be applied. Then, (h(cid:48),r,t(cid:48)) ∈ {(h,r,t)}∪Neg(h,r,t) (2)\n",
      "we demonstrate a long overlooked problem about\n",
      "negative sampling which motivates us to propose where Neg(h,r,t) ⊂ {(h(cid:48),r,t)|h(cid:48) ∈ E} ∪\n",
      "KBGAN to address the problem. Finally, we dive {(h,r,t(cid:48))|t(cid:48) ∈ E} is a set of sampled corrupted\n",
      "into the mathematical, and algorithmic details of triples.\n",
      "Figure1:Anoverviewofthe KBGAN framework.Thegenerator(G)calculatesaprobabilitydistribution\n",
      "over a set of candidate negative triples, then sample one triples from the distribution as the output. The\n",
      "discriminator (D) receives the generated negative triple as well as the ground truth triple (in the hexag-\n",
      "onal box), and calculates their scores. G minimizes the score of the generated negative triple by policy\n",
      "gradient,andDminimizesthemarginallossbetweenpositiveandnegativetriplesbygradientdescent.\n",
      "Other forms of loss functions exist, for exam- edge of American geography. If a KGE model is\n",
      "ple CONVE uses a triple-wise logistic function to fed with mostly “too easy” negative examples, it\n",
      "model how likely the triple is true, but by far the would probably only learn to represent types, not\n",
      "two described above are the most common. Also, theunderlyingsemantics.\n",
      "softmax function gives an probabilistic distribu- Theproblemislessseveretomodelsusinglog-\n",
      "tion over a set of triples, which is necessary for softmaxlossfunction,becausetheytypicallysam-\n",
      "ageneratortosamplefromthem. ples tens or hundreds of negative triples for one\n",
      "positive triple in each iteration, and it is likely to\n",
      "3.2 WeaknessofUniformNegativeSampling\n",
      "have a few useful negatives among them. For in-\n",
      "Most previous KGE models use uniform negative stance, (Trouillon et al., 2016) found that a 100:1\n",
      "samplingforgeneratingnegativetriples,thatis,re- negative-to-positive ratio results in the best per-\n",
      "placing the head or tail entity of a positive triple formance for COMPLEX. However, for marginal\n",
      "with any of the entities in E, all with equal prob- loss function, whose negative-to-positive ratio is\n",
      "ability. Most of the negative triples generated in always 1:1, the low quality of uniformly sampled\n",
      "this way contribute little to learning an effective negativescanseriouslydamagetheirperformance.\n",
      "embedding,becausetheyaretooobviouslyfalse.\n",
      "3.3 GenerativeAdversarialTrainingfor\n",
      "To demonstrate this issue, let us consider the\n",
      "KnowledgeGraphEmbeddingModels\n",
      "following example. Suppose we have a ground\n",
      "truth triple LocatedIn(NewOrleans,Louisiana), Inspired by GANs, we propose an adversarial\n",
      "and corrupt it by replacing its tail entity. training framework named KBGAN which uses a\n",
      "First, we remove the tail entity, leaving Lo- KGE model with softmax probabilities to pro-\n",
      "catedIn(NewOrleans,?). Because the relation Lo- vide high-quality negative samples for the train-\n",
      "catedIn constraints types of its entities, “?” ing of a KGE model whose training objective is\n",
      "must be a geographical region. If we fill “?” marginal loss function. This framework is inde-\n",
      "with a random entity e ∈ E, the prob- pendent of the score functions of these two mod-\n",
      "ability of e having a wrong type is very els,andthereforepossessessomeextentofuniver-\n",
      "high, resulting in ridiculous triples like Lo- sality. Figure 1 illustrates the overall structure of\n",
      "catedIn(NewOrleans,BarackObama) or Locate- KBGAN.\n",
      "dIn(NewOrleans,StarTrek). Such triples are con- In parallel to terminologies used in GAN liter-\n",
      "sidered “too easy”, because they can be elim- ature, we will simply call these two models gen-\n",
      "inated solely by types. In contrast, Locate- erator and discriminator respectively in the rest\n",
      "dIn(NewOrleans,Florida)isaveryusefulnegative of this paper. We use softmax probabilistic mod-\n",
      "triple, because it satisfies type constraints, but it els as the generator because they can adequately\n",
      "cannot be proved wrong without detailed knowl- model the “sampling from a probability distribu-\n",
      "Algorithm1:TheKBGANalgorithm\n",
      "Data:trainingsetofpositivefacttriplesT ={(h,r,t)}\n",
      "Input:Pre-trainedgeneratorGwithparametersθ andscorefunctionf (h,r,t),andpre-traineddiscriminatorDwith\n",
      "G G\n",
      "parametersθ andscorefunctionf (h,r,t)\n",
      "D D\n",
      "Output:Adversariallytraineddiscriminator\n",
      "1 b←−0;// baseline for policy gradient\n",
      "2 repeat\n",
      "3 Sampleamini-batchofdataT batchfromT ;\n",
      "4 G G ←−0,G D ←−0;// gradients of parameters of G and D\n",
      "5 r sum ←−0;// for calculating the baseline\n",
      "6 for(h,r,t)∈T batchdo\n",
      "7 UniformlyrandomlysampleN snegativetriplesNeg(h,r,t)={(h(cid:48) i,r,t(cid:48) i)} i=1...Ns;\n",
      "8 Obtaintheirprobabilityofbeinggenerated:p i = (cid:80)N j=e sx 1p ef xG p( fh G(cid:48) i (,r h, (cid:48) jt,(cid:48) i r),t(cid:48) j);\n",
      "9 Sampleonenegativetriple(h(cid:48) s,r,t(cid:48) s)fromNeg(h,r,t)accordingto{p i} i=1...Ns.Assumeitsprobabilitytobe\n",
      "p ;\n",
      "s\n",
      "10 G D ←−G D+∇ θD[f D(h,r,t)−f D(h(cid:48) s,r,t(cid:48) s)+γ] +;// accumulate gradients for D\n",
      "11 r←−−f D(h(cid:48) s,r,t(cid:48) s),r sum ←−r sum+r;// r is the reward\n",
      "12 G G ←−G G+(r−b)∇ θGlogp s;// accumulate gradients for G\n",
      "13 end\n",
      "14 θ G ←−θ G+η GG G,θ D ←−θ D−η DG D;// update parameters\n",
      "15 b←r sum/|T batch|;// update baseline\n",
      "16 untilconvergence;\n",
      "tion” process of discrete GANs, and we aim at minimizingthefollowingmarginallossfunction:\n",
      "improving discriminators based on marginal loss\n",
      "(cid:88)\n",
      "because they can benefit more from high-quality L D = [f D(h,r,t)−f D(h(cid:48),r,t(cid:48))+γ] +\n",
      "negativesamples.Notethatamajordifferencebe- (h,r,t)∈T\n",
      "tween GAN andourworkisthat,theultimategoal (h(cid:48),r,t(cid:48)) ∼ p (h(cid:48),r,t(cid:48)|h,r,t) (3)\n",
      "G\n",
      "of our framework is to produce a good discrimi-\n",
      "Theonlydifferencebetweenthislossfunctionand\n",
      "nator,whereas GANS areaimedattrainingagood\n",
      "Equation1isthatitusesnegativesamplesfromthe\n",
      "generator.Inaddition,thediscriminatorhereisnot\n",
      "generator.\n",
      "aclassifierasitwouldbeinmostGANs.\n",
      "The objective of the generator can be formu-\n",
      "lated as maximizing the following expectation of\n",
      "negativedistances:\n",
      "Intuitively,thediscriminatorshouldassignarel-\n",
      "atively small distance to a high-quality negative (cid:88)\n",
      "R = E[−f (h(cid:48),r,t(cid:48))]\n",
      "sample.Inordertoencouragethegeneratortogen- G D\n",
      "erateusefulnegativesamples,theobjectiveofthe (h,r,t)∈T\n",
      "generatoristominimizethedistancegivenbydis- (h(cid:48),r,t(cid:48)) ∼ p G(h(cid:48),r,t(cid:48)|h,r,t) (4)\n",
      "criminator for its generated triples. And just like\n",
      "R involves a discrete sampling step, so we\n",
      "G\n",
      "the ordinary training process, the objective of the\n",
      "cannotfinditsgradientwithsimpledifferentiation.\n",
      "discriminatoristominimizethemarginallossbe-\n",
      "We use a simple special case of Policy Gradient\n",
      "tween the positive triple and the generated nega-\n",
      "Theorem1 (Suttonetal.,2000)toobtainthegradi-\n",
      "tive triple. In an adversarial training setting, the\n",
      "entofR withrespecttoparametersofthegener-\n",
      "G\n",
      "generator and the discriminator are alternatively\n",
      "ator:\n",
      "trainedtowardstheirrespectiveobjectives.\n",
      "(cid:88)\n",
      "∇ R = E\n",
      "G G (h(cid:48),r,t(cid:48))∼pG(h(cid:48),r,t(cid:48)|h,r,t)\n",
      "(h,r,t)∈T\n",
      "Suppose that the generator produces a\n",
      "[−f (h(cid:48),r,t(cid:48))∇ logp (h(cid:48),r,t(cid:48)|h,r,t)]\n",
      "probability distribution on negative triples D G G\n",
      "p G(h(cid:48),r,t(cid:48)|h,r,t) given a positive triple (h,r,t), (cid:39) (cid:88) 1 (cid:88)\n",
      "and generates negative triples (h(cid:48),r,t(cid:48)) by sam- N\n",
      "(h,r,t)∈T (h(cid:48) i,r,t(cid:48) i)∼pG(h(cid:48),r,t(cid:48)|h,r,t),i=1...N\n",
      "pling from this distribution. Let f (h,r,t) be\n",
      "D [−f (h(cid:48),r,t(cid:48))∇ logp (h(cid:48),r,t(cid:48)|h,r,t)] (5)\n",
      "the score function of the discriminator. The ob- D G G\n",
      "jective of the discriminator can be formulated as 1Aproofcanbefoundinthesupplementarymaterial\n",
      "Model Hyperparameters ConstraintsorRegularizations\n",
      "TRANSE L\n",
      "1\n",
      "distance,k = 50,γ = 3 ||e||\n",
      "2\n",
      "≤ 1,||r||\n",
      "2\n",
      "≤ 1\n",
      "TRANSD L\n",
      "1\n",
      "distance,k = 50,γ = 3 ||e||\n",
      "2\n",
      "≤ 1,||r||\n",
      "2\n",
      "≤ 1,||e p||\n",
      "2\n",
      "≤ 1,||r p||\n",
      "2\n",
      "≤ 1\n",
      "DISTMULT k = 50,λ = 1/0.1 L2regularization:L\n",
      "reg\n",
      "= L+λ||Θ||2\n",
      "2\n",
      "COMPLEX 2k = 50,λ = 1/0.1 L2regularization:L\n",
      "reg\n",
      "= L+λ||Θ||2\n",
      "2\n",
      "Table 2: Hyperparameter settings of the 4 models we used. For DISTMULT and COMPLEX, λ = 1 is\n",
      "usedforFB15k-237andλ = 0.1isusedforWN18andWN18RR.Allotherhyperparametersareshared\n",
      "amongalldatasets.ListhegloballossdefinedinEquation(2).Θrepresentsallparametersinthemodel.\n",
      "Dataset #r #ent. #train #val #test out affecting the expectation of gradients.2\n",
      "FB15k-237 237 14,541 272,115 17,535 20,466\n",
      "In our case, we replace −f (h(cid:48),r,t(cid:48)) with\n",
      "WN18 18 40,943 141,442 5,000 5,000 D\n",
      "WN18RR 11 40,943 86,835 3,034 3,134 −f D(h(cid:48),r,t(cid:48)) − b(h,r,t) in the equation above\n",
      "to introduce the baseline. To avoid introducing\n",
      "Table3:Statisticsofdatasetsweusedintheexper-\n",
      "new parameters, we simply let b be a constant,\n",
      "iments.“r”:relations.\n",
      "the average reward of the whole training set: b =\n",
      "(cid:80) E [−f (h(cid:48),r,t(cid:48))].\n",
      "(h,r,t)∈T (h(cid:48),r,t(cid:48))∼pG(h(cid:48),r,t(cid:48)|h,r,t) D\n",
      "In practice, b is approximated by the mean of\n",
      "where the second approximate equality means rewardsofrecentlygeneratednegativetriples.\n",
      "we approximate the expectation with sampling in Let the generator’s score function to be\n",
      "practice.NowwecancalculatethegradientofR G f G(h,r,t),givenasetofcandidatenegativetriples\n",
      "andoptimizeitwithgradient-basedalgorithms. Neg(h,r,t) ⊂ {(h(cid:48),r,t)|h(cid:48) ∈ E}∪{(h,r,t(cid:48))|t(cid:48) ∈\n",
      "PolicyGradientTheoremarisesfromreinforce- E},theprobabilitydistributionp G ismodeledas:\n",
      "ment learning (RL), so we would like to draw an\n",
      "expf (h(cid:48),r,t(cid:48))\n",
      "analogybetweenourmodelandanRLmodel.The p (h(cid:48),r,t(cid:48)|h,r,t) = G\n",
      "G (cid:80) expf (h∗,r,t∗)\n",
      "generator can be viewed as an agent which inter- G\n",
      "(h∗,r,t∗) ∈ Neg(h,r,t) (6)\n",
      "acts with the environment by performing actions\n",
      "and improves itself by maximizing the reward re-\n",
      "Ideally,Neg(h,r,t)shouldcontainallpossible\n",
      "turnedfromtheenvironmentinresponseofitsac-\n",
      "negatives. However, knowledge graphs are usu-\n",
      "tions. Correspondingly, the discriminator can be\n",
      "ally highly incomplete, so the ”hardest” negative\n",
      "viewed as the environment. Using RL terminolo-\n",
      "triples are very likely to be false negatives (true\n",
      "gies, (h,r,t) is the state (which determines what\n",
      "facts). To address this issue, we instead generate\n",
      "actions the actor can take), p (h(cid:48),r,t(cid:48)|h,r,t) is\n",
      "G Neg(h,r,t)byuniformlysamplingofN entities\n",
      "s\n",
      "thepolicy(howtheactorchooseactions),(h(cid:48),r,t(cid:48))\n",
      "(a small number compared to the number of all\n",
      "is the action, and −f (h(cid:48),r,t(cid:48)) is the reward.\n",
      "D possible negatives) from E to replace h or t. Be-\n",
      "The method of optimizing R described above\n",
      "G cause in real-world knowledge graphs, true neg-\n",
      "is called REINFORCE (Williams, 1992) algorithm atives are usually far more than false negatives,\n",
      "in RL. Our model is a simple special case of\n",
      "such set would be unlikely to contain any false\n",
      "RL, called one-step RL. In a typical RL setting,\n",
      "negative, and the negative selected by the gener-\n",
      "each action performed by the agent will change\n",
      "atorwouldlikelybeatruenegative.Usingasmall\n",
      "its state, and the agent will perform a series of\n",
      "Neg(h,r,t) can also significantly reduce compu-\n",
      "actions (called an epoch) until it reaches certain\n",
      "tationalcomplexity.\n",
      "states or the number of actions reaches a certain\n",
      "Besides, we adopt the “bern” sampling tech-\n",
      "limit.However,intheanalogyabove,actionsdoes\n",
      "nique (Wang et al., 2014) which replaces the\n",
      "notaffectthestate,andaftereachactionwerestart\n",
      "“1” side in “1-to-N” and “N-to-1” relations with\n",
      "with another unrelated state, so each epoch con-\n",
      "higher probability to further reduce false nega-\n",
      "sistsofonlyoneaction.\n",
      "tives.\n",
      "To reduce the variance of REINFORCE al- Algorithm 1 summarizes the whole adversarial\n",
      "gorithm, it is common to subtract a base- training process. Both the generator and the dis-\n",
      "line from the reward, which is an arbitrary\n",
      "2Aproofofsuchfactcanalsobefoundinthesupplemen-\n",
      "number that only depends on the state, with- tarymaterial\n",
      "criminator require pre-training, which is the same 4.1.3 ImplementationDetails\n",
      "as conventionally training a single KBE model 3 In the pre-training stage, we train every model\n",
      "with uniform negative sampling. Formally speak-\n",
      "to convergence for 1000 epochs, and divide ev-\n",
      "ing, one can pre-train the generator by minimiz-\n",
      "eryepochinto100mini-batches.Toavoidoverfit-\n",
      "ing the loss function defined in Equation (1), and\n",
      "ting, we adopt early stopping by evaluating MRR\n",
      "pre-trainthediscriminatorbyminimizingtheloss\n",
      "on the validation set every 50 epochs. We tried\n",
      "function defined in Equation (2). Line 14 in the\n",
      "γ = 0.5,1,2,3,4,5 and L,L distances for\n",
      "1 2\n",
      "algorithm assumes that we are using the vanilla\n",
      "TRANSE and TRANSD, and λ = 0.01,0.1,1,10\n",
      "gradient descent as the optimization method, but\n",
      "for DISTMULT and COMPLEX, and determined\n",
      "obviously one can substitute it with any gradient-\n",
      "the best hyperparameters listed on table 2, based\n",
      "basedoptimizationalgorithm.\n",
      "on their performances on the validation set af-\n",
      "ter pre-training. Due to limited computation re-\n",
      "4 Experiments\n",
      "sources, we deliberately limit the dimensions of\n",
      "embeddings to k = 50, similar to the one used\n",
      "To evaluate our proposed framework, we test its\n",
      "in earlier works, to save time. We also apply cer-\n",
      "performance for the link prediction task with dif-\n",
      "tainconstraintsorregularizationstothesemodels,\n",
      "ferent generators and discriminators. For the gen-\n",
      "which are mostly the same as those described in\n",
      "erator, we choose two classical probability-based\n",
      "theiroriginalpublications,andalsolistedontable\n",
      "KGE model, DISTMULT and COMPLEX, and\n",
      "2.\n",
      "for the discriminator, we also choose two classi-\n",
      "In the adversarial training stage, we keep all\n",
      "cal translation-based KGE model, TRANSE and\n",
      "thehyperparamtersdeterminedinthepre-training\n",
      "TRANSD, resulting in four possible combinations\n",
      "stage unchanged. The number of candidate neg-\n",
      "of generator and discriminator in total. See Table<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  29346,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Liwei Cai', 'William Yang Wang']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  limited computation re-\n",
      "4 Experiments\n",
      "sources, we deliberately limit the dimensions of\n",
      "embeddings to k = 50, similar to the one used\n",
      "To evaluate our proposed framework, we test its\n",
      "in earlier works, to save time. We also apply cer-\n",
      "performance for the link prediction task with dif-\n",
      "tainconstraintsorregularizationstothesemodels,\n",
      "ferent generators and discriminators. For the gen-\n",
      "which are mostly the same as those described in\n",
      "erator, we choose two classical probability-based\n",
      "theiroriginalpublications,andalsolistedontable\n",
      "KGE model, DISTMULT and COMPLEX, and\n",
      "2.\n",
      "for the discriminator, we also choose two classi-\n",
      "In the adversarial training stage, we keep all\n",
      "cal translation-based KGE model, TRANSE and\n",
      "thehyperparamtersdeterminedinthepre-training\n",
      "TRANSD, resulting in four possible combinations\n",
      "stage unchanged. The number of candidate neg-\n",
      "of generator and discriminator in total. See Table\n",
      "ative triples, N, is set to 20 in all cases, which\n",
      "1forabriefsummaryofthesemodels. s\n",
      "is proven to be optimal among the candidate set\n",
      "4.1 ExperimentalSettings of {5,10,20,30,50}. We train for 5000 epochs,\n",
      "with100mini-batchesforeachepoch.Wealsouse\n",
      "4.1.1 Datasets\n",
      "earlystoppinginadversarialtrainingbyevaluating\n",
      "We use three common knowledge base com-\n",
      "MRRonthevalidationsetevery100epochs.\n",
      "pletion datasets for our experiment: FB15k-237,\n",
      "We use the self-adaptive optimization method\n",
      "WN18 and WN18RR. FB15k-237 is a subset\n",
      "Adam (Kingma and Ba, 2015) for all trainings,\n",
      "of FB15k introduced by (Toutanova and Chen,\n",
      "and always use the recommended default setting\n",
      "2015), which removed redundant relations in\n",
      "α = 0.001,β = 0.9,β = 0.999,(cid:15) = 10−8.\n",
      "1 2\n",
      "FB15k and greatly reduced the number of rela-\n",
      "tions.Likewise,WN18RRisasubsetofWN18in- 4.2 Results\n",
      "troducedby(Dettmersetal.,2017)whichremoves\n",
      "Results of our experiments as well as baselines\n",
      "reversing relations and dramatically increases the\n",
      "are shown in Table 4. All settings of adversarial\n",
      "difficulty of reasoning. Both FB15k and WN18\n",
      "training bring a pronounced improvement to the\n",
      "are first introduced by (Bordes et al., 2013) and\n",
      "model, which indicates that our method is con-\n",
      "havebeencommonlyusedinknowledgegraphre-\n",
      "sistently effective in various cases. TRANSE per-\n",
      "searches. Statistics of datasets we used are shown\n",
      "formsslightlyworsethanTRANSDonFB15k-237\n",
      "inTable3.\n",
      "and WN18, but better on WN18RR. Using DIST-\n",
      "4.1.2 EvaluationProtocols MULT or COMPLEX asthegeneratordoesnotaf-\n",
      "fectperformancegreatly.\n",
      "Followingpreviousworkslike(Yangetal.,2015)\n",
      "and (Trouillon et al., 2016), for each run, we re- TRANSE and TRANSD enhanced by KBGAN\n",
      "cansignificantlybeattheircorrespondingbaseline\n",
      "port two common metrics, mean reciprocal rank-\n",
      "implementations, and outperform stronger base-\n",
      "ing (MRR) and hits at 10 (H@10). We only re-\n",
      "lines in some cases. As a prototypical and proof-\n",
      "portscoresunderthefilteredsetting(Bordesetal.,\n",
      "of-principle experiment, we have never expected\n",
      "2013),whichremovesalltriplesappearedintrain-\n",
      "state-of-the-art results. Being simple models pro-\n",
      "ing, validating, and testing sets from candidate\n",
      "triples before obtaining the rank of the ground\n",
      "3The KBGAN source code is available at https://\n",
      "truthtriple. github.com/cai-lw/KBGAN\n",
      "FB15k-237 WN18 WN18RR\n",
      "Method MRR H@10 MRR H@10 MRR H@10\n",
      "TRANSE - 42.8† - 89.2 - 43.2†\n",
      "TRANSD - 45.3† - 92.2 - 42.8†\n",
      "DISTMULT 24.1‡ 41.9‡ 82.2 93.6 42.5‡ 49.1‡\n",
      "COMPLEX 24.0‡ 41.9‡ 94.1 94.7 44.4‡ 50.7‡\n",
      "TRANSE (pre-trained) 24.2 42.2 43.3 91.5 18.6 45.9\n",
      "KBGAN(TRANSE + DISTMULT) 27.4 45.0 71.0 94.9 21.3 48.1\n",
      "KBGAN(TRANSE + COMPLEX) 27.8 45.3 70.5 94.9 21.0 47.9\n",
      "TRANSD (pre-trained) 24.5 42.7 49.4 92.8 19.2 46.5\n",
      "KBGAN(TRANSD + DISTMULT) 27.8 45.8 77.2 94.8 21.4 47.2\n",
      "KBGAN(TRANSD + COMPLEX) 27.7 45.8 77.9 94.8 21.5 46.9\n",
      "Table 4: Experimental results. Results of KBGAN are results of its discriminator (on the left of the “+”\n",
      "sign). Underlined results are the best ones among our implementations. Results marked with † are pro-\n",
      "duced by running Fast-TransX (Lin et al., 2015) with its default parameters. Results marked with ‡ are\n",
      "copiedfrom(Dettmersetal.,2017).Allotherbaselineresultsarecopiedfromtheiroriginalpapers.\n",
      "Figure2:LearningcurvesofKBGAN.Allmetricsimprovesteadilyastrainingproceeds.\n",
      "posed several years ago, TRANSE and TRANSD imum as training proceeds, which indicates that\n",
      "hastheirlimitationsinexpressivenessthatareun- KBGANisarobustGANthatcanconvergetogood\n",
      "likely to be fully compensated by better training resultsinvarioussettings,althoughGANsarewell-\n",
      "technique. In future researches, people may try known for difficulty in convergence. Fluctuations\n",
      "employing more advanced models into KBGAN, in these graphs may seem more prominent than\n",
      "andwebelieveithasthepotentialtobecomestate- other KGE models, but is considered normal for\n",
      "of-the-art. an adversially trained model. Note that in some\n",
      "casesthecurvestilltendstoriseafter5000epochs.\n",
      "To illustrate our training progress, we plot per- Wedonothavesufficientcomputationresourceto\n",
      "formances of the discriminator on validation set trainformoreepochs,butwebelievethattheywill\n",
      "over epochs, which are displayed in Figure 2. As alsoeventuallyconverge.\n",
      "all these graphs show, our performances are al-\n",
      "ways in increasing trends, converging to its max-\n",
      "Positivefact Uniformrandomsample Trainedgenerator\n",
      "(condensation NN 2, family arcidae NN 1 revivification NN 1\n",
      "derivationally related form, repast NN 1 mouthpiece NN 3\n",
      "distill VB 4) beater NN 2 liquid body substance NN 1\n",
      "coverall NN 1 stiffen VB 2\n",
      "cash advance NN 1 hot up VB 1\n",
      "(colorado river NN 2, lunar calendar NN 1 idaho NN 1\n",
      "instance hypernym, umbellularia californica NN 1 sayan mountains NN 1\n",
      "river NN 1) tonality NN 1 lower saxony NN 1\n",
      "creepy-crawly NN 1 order ciconiiformes NN 1\n",
      "moor VB 3 jab NN 3\n",
      "(meeting NN 2, cellular JJ 1 attach VB 1\n",
      "hypernym, commercial activity NN 1 bond NN 6\n",
      "social gathering NN 1) giant cane NN 1 heavy spar NN 1\n",
      "streptomyces NN 1 satellite NN 1\n",
      "tranquillize VB 1 peep VB 3\n",
      "Table 5: Examples of negative samples in WN18 dataset. The first column is the positive fact, and the\n",
      "terminboldistheonetobereplacedbyanentityinthenexttwocolumns.Thesecondcolumnconsists\n",
      "ofrandomentitiesdrawnfromthewholedataset.Thethirdcolumncontainsnegativesamplesgenerated\n",
      "bythegeneratorinthelast5epochsoftraining.Entitiesinitalicareconsideredtohavesemanticrelation\n",
      "tothepositiveone\n",
      "4.3 Casestudy 5 Conclusions\n",
      "We propose a novel adversarial learning method\n",
      "To demonstrate that our approach does generate for improving a wide range of knowledge graph\n",
      "betternegativesamples,welistsomeexamplesof embedding models—We designed a generator-\n",
      "them in Table 5, using the KBGAN (TRANSE + discriminator framework with dual KGE compo-\n",
      "DISTMULT)modelandtheWN18dataset.Allhy- nents. Unlike random uniform sampling, the gen-\n",
      "perparameters are the same as those described in eratormodelgenerateshigherqualitynegativeex-\n",
      "Section4.1.3. amples, which allow the discriminator model to\n",
      "learn better. To enable backpropagation of error,\n",
      "Compared to uniform random negatives which\n",
      "we introduced a one-step REINFORCE method to\n",
      "are almost always totally unrelated, the genera-\n",
      "seamlesslyintegratethetwomodules.Experimen-\n",
      "tor generates more semantically related negative\n",
      "tally,wetestedtheproposedideaswithfourcom-\n",
      "samples, which is different from type relatedness\n",
      "monlyusedKGEmodelsonthreedatasets,andthe\n",
      "we used as example in Section 3.2, but also helps\n",
      "resultsshowedthattheadversariallearningframe-\n",
      "training.Inthefirstexample,twoofthefiveterms\n",
      "work brought consistent improvements to various\n",
      "are physically related to the process of distilling\n",
      "KGEmodelsunderdifferentsettings.\n",
      "liquids. In the second example, three of the five\n",
      "entities are geographical objects. In the third ex-\n",
      "ample,twoofthefiveentitiesexpresstheconcept\n",
      "of“gather”.\n",
      "Because we deliberately limited the strength of\n",
      "generated negatives by using a small N as de-\n",
      "s\n",
      "scribed in Section 3.3, the semantic relation is\n",
      "prettyweak,andtherearestillmanyunrelateden-\n",
      "tities. However, empirical results (when selecting\n",
      "the optimal N ) shows that such situation is more\n",
      "s\n",
      "beneficial for training the discriminator than gen-\n",
      "eratingevenstrongernegatives.\n",
      "References Tom M Mitchell, William Cohen, Estevam Hruschka,\n",
      "ParthaTalukdar,JustinBetteridge,AndrewCarlson,\n",
      "MartinArjovsky, SoumithChintala, andLeon Bottou.\n",
      "Bhavana Dalvi Mishra, Matthew Gardner, Bryan\n",
      "2017. Wasserstein gan. In International Confer-\n",
      "Kisiel, Jayant Krishnamurthy, et al. 2015. Never-\n",
      "renceonMachineLearning.\n",
      "endinglearning. InTheTwenty-ninthAAAIConfer-\n",
      "enceonArtificialIntelligence.\n",
      "Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim\n",
      "Sturge,andJamieTaylor.2008. Freebase:acollab-\n",
      "oratively created graph database for structuring hu- Maximilian Nickel, Kevin Murphy, Volker Tresp, and\n",
      "man knowledge. In Proceedings of the 2008 ACM Evgeniy Gabrilovich. 2015. A review of relational\n",
      "SIGMOD international conference on Management machine learning for knowledge graphs. arXiv\n",
      "ofdata.ACM,pages1247–1250. preprintarXiv:1503.00759.\n",
      "Antoine Bordes, Nicolas Usunier, Alberto Garcia- Maximilian Nickel, Lorenzo Rosasco, and\n",
      "Duran, Jason Weston, and Oksana Yakhnenko. Tomaso Poggio Poggio. 2016. Holographic\n",
      "2013. Translating embeddings for modeling multi- embeddings of knowledgegraphs. In The Thirtieth\n",
      "relational data. In Advances in Neural Information AAAI Conference on Artificial Intelligence. pages\n",
      "ProcessingSystems.pages2787–2795. 1955–1961.\n",
      "Tim Dettmers, Pasquale Minervini, Pontus Stene-\n",
      "Maximilian Nickel, Volker Tresp, and Hans-Peter\n",
      "torp, and Sebastian Riedel. 2017. Convolutional\n",
      "Kriegel. 2011. A three-way model for collective\n",
      "2d knowledge graph embeddings. arXiv preprint\n",
      "learning on multi-relational data. In Proceedings\n",
      "arXiv:1707.01476.\n",
      "of the 28th International Conference on Machine\n",
      "Learning.pages809–816.\n",
      "XinDong,EvgeniyGabrilovich,GeremyHeitz,Wilko\n",
      "Horn, Ni Lao, Kevin Murphy, Thomas Strohmann,\n",
      "Fabian M Suchanek, Gjergji Kasneci, and Gerhard\n",
      "Shaohua Sun, and Wei Zhang. 2014. Knowledge\n",
      "Weikum. 2007. Yago: a core of semantic knowl-\n",
      "vault:Aweb-scaleapproachtoprobabilisticknowl-\n",
      "edge fusion. In Proceedings of the 20th ACM edge. InProceedingsofthe16thinternationalcon-\n",
      "SIGKDD international conference on Knowledge\n",
      "ferenceonWorldWideWeb.ACM,pages697–706.\n",
      "discoveryanddatamining.ACM,pages601–610.\n",
      "Richard S Sutton, David A McAllester, Satinder P\n",
      "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Singh, and Yishay Mansour. 2000. Policy gradi-\n",
      "BingXu,DavidWarde-Farley,SherjilOzair,Aaron ent methods for reinforcement learning with func-\n",
      "Courville,andYoshuaBengio.2014. Generativead- tionapproximation. InAdvancesinneuralinforma-\n",
      "versarial nets. In Advances in Neural Information tionprocessingsystems.pages1057–1063.\n",
      "ProcessingSystems.pages2672–2680.\n",
      "Kristina Toutanova and Danqi Chen. 2015. Observed\n",
      "GuoliangJi,ShizhuHe,LihengXu,KangLiu,andJun\n",
      "versus latent features for knowledge base and text\n",
      "Zhao. 2015. Knowledge graph embedding via dy-\n",
      "inference. In Proceedings of the 3rd Workshop on\n",
      "namicmappingmatrix. InThe53rdAnnualMeeting\n",
      "ContinuousVectorSpaceModelsandtheirCompo-\n",
      "oftheAssociationforComputationalLinguistics.\n",
      "sitionality.pages57–66.\n",
      "Felix Juefei-Xu, Vishnu Naresh Boddeti, and Marios\n",
      "Savvides.2017. Gangofgans:Generativeadversar-\n",
      "The´oTrouillon,JohannesWelbl,SebastianRiedel,E´ric\n",
      "ialnetworkswithmaximummarginranking. arXiv Gaussier, and Guillaume Bouchard. 2016. Com-\n",
      "preprintarXiv:1704.04865. plex embeddings for simple link prediction. In In-\n",
      "ternationalConferenceonMachineLearning.pages\n",
      "Diederik P. Kingma and Jimmy Lei Ba. 2015. Adam: 2071–2080.\n",
      "A method for stochastic optimization. In The 3rd\n",
      "International Conference on Learning Representa- Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong,\n",
      "tions. Yinghui Xu, Benyou Wang, Peng Zhang, and Dell\n",
      "Zhang. 2017. Irgan: A minimax game for unifying\n",
      "Denis Krompaß, Stephan Baier, and Volker Tresp.\n",
      "generative and discriminative information retrieval\n",
      "2015. Type-constrained representation learning in\n",
      "models. InThe40thInternationalACMSIGIRCon-\n",
      "knowledge graphs. In International Semantic Web\n",
      "ference.\n",
      "Conference.Springer,pages640–655.\n",
      "Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\n",
      "YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,and\n",
      "Chen.2014. Knowledgegraphembeddingbytrans-\n",
      "Xuan Zhu. 2015. Learning entity and relation em-\n",
      "lating on hyperplanes. In The Twenty-eighth AAAI\n",
      "beddings for knowledge graph completion. In The\n",
      "Conference on Artificial Intelligence. pages 1112–\n",
      "Twenty-ninth AAAI Conference on Artificial Intelli-\n",
      "1119.\n",
      "gence.pages2181–2187.\n",
      "Mehdi Mirza and Simon Osindero. 2014. Condi- Ronald J Williams. 1992. Simple statistical gradient-\n",
      "tional generative adversarial nets. arXiv preprint following algorithms for connectionist reinforce-\n",
      "arXiv:1411.01784. mentlearning. Machinelearning8(3-4):229–256.\n",
      "Han Xiao, Minlie Huang, and Xiaoyan Zhu. 2016.\n",
      "Fromonepointtoamanifold:Knowledgegraphem-\n",
      "beddingforpreciselinkprediction. InTheTwenty-\n",
      "FifthInternationalJointConferenceonArtificialIn-\n",
      "telligence.\n",
      "Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\n",
      "Gao, and Li Deng. 2015. Embedding entities and\n",
      "relations for learning and inference in knowledge\n",
      "bases. The3rdInternationalConferenceonLearn-\n",
      "ingRepresentations.\n",
      "Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.\n",
      "2017. Seqgan:Sequencegenerativeadversarialnets\n",
      "withpolicygradient. InTheThirty-FirstAAAICon-\n",
      "ferenceonArtificialIntelligence.pages2852–2858.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,     88,  10602,  38805]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Tom M Mitchell', 'William Cohen', 'Estevam Hruschka', 'Partha Talukdar', 'Justin Betteridge', 'Andrew Carlson', 'Martin Arjovsky', 'Soumith Chintala', 'Leon Bottou', 'Bhavana Dalvi Mishra', 'Matthew Gardner', 'Bryan Kisiel', 'Jayant Krishnamurthy', 'Kurt Bollacker', 'Colin Evans', 'Praveen Paritosh', 'Tim Sturge', 'Jamie Taylor', 'Antoine Bordes', 'Nicolas Usunier', 'Alberto Garcia-Duran', 'Jason Weston', 'Oksana Yakhnenko', 'Tim Dettmers', 'Pasquale Minervini', 'Pontus Stenborg', 'Sebastian Riedel', 'Xin Dong', 'Evgeniy Gabrilovich', 'Geremy Heitz', 'Wilko Horn', 'Ni Lao', 'Kevin Murphy', 'Thomas Strohmann', 'Fabian M Suchanek', 'Gjergji Kasneci', 'Gerhard Weikum', 'Richard S Sutton', 'David A McAllester', 'Satinder P Singh', 'Yishay Mansour', 'Ian Goodfellow', 'Jean Pouget-Abadie', 'Mehdi Mirza', 'Bing Xu', 'David Warde-Farley', 'Sherjil Ozair', 'Aaron Courville', 'Yoshua Bengio', 'Kristina Toutanova', 'Danqi Chen', 'Guoliang Ji', 'Shizhu He', 'Liheng Xu', 'Kang Liu', 'Jun Zhao', 'Felix Juefei-Xu', 'Vishnu Naresh Boddeti', 'Marios Savvides', 'The´o Trouillon', 'Johannes Welbl', 'Sebastian Riedel', 'E´ric Gaussier', 'Guillaume Bouchard', 'Diederik P Kingma', 'Jimmy Lei Ba', 'Jun Wang', 'Lantao Yu', 'Weinan Zhang', 'Yu Gong', 'Benyou Wang', 'Peng Zhang', 'Dell Zhang', 'Denis Krompaß', 'Stephan Baier', 'Volker Tresp', 'Zhen Wang', 'Jianwen Zhang', 'Jianlin Feng', 'Zheng Chen', 'Yankai Lin', 'Zhiyuan Liu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Convolutional 2D Knowledge Graph Embeddings\n",
      "TimDettmers∗\n",
      "UniversitàdellaSvizzeraitaliana\n",
      "tim.dettmers@gmail.com\n",
      "PasqualeMinervini PontusStenetorp SebastianRiedel\n",
      "UniversityCollegeLondon\n",
      "{p.minervini,p.stenetorp,s.riedel}@cs.ucl.ac.uk\n",
      "Abstract can contain millions of facts; as a consequence, link pre-\n",
      "dictors should scale in a manageable way with respect to\n",
      "Linkpredictionforknowledgegraphsisthetaskofpredict- boththenumberofparametersandcomputationalcoststobe\n",
      "ingmissingrelationshipsbetweenentities.Previousworkon applicableinreal-worldscenarios.\n",
      "link prediction has focused on shallow, fast models which\n",
      "Forsolvingsuchscalingproblems,linkpredictionmodels\n",
      "canscaletolargeknowledgegraphs.However,thesemodels\n",
      "areoftencomposedofsimpleoperations,likeinnerproducts\n",
      "learnlessexpressivefeaturesthandeep,multi-layermodels–\n",
      "and matrix multiplications over an embedding space, and\n",
      "whichpotentiallylimitsperformance.Inthisworkweintro-\n",
      "duceConvE,amulti-layerconvolutionalnetworkmodelfor use a limited number of parameters (Nickel et al. 2016).\n",
      "linkprediction,andreportstate-of-the-artresultsforseveral DistMult(Yangetal.2015)issuchamodel,characterised\n",
      "establisheddatasets.Wealsoshowthatthemodelishighlypa- by three-way interactions between embedding parameters,\n",
      "rameterefficient,yieldingthesameperformanceasDistMult whichproduceonefeatureperparameter. Usingsuchsimple,\n",
      "andR-GCNwith8xand17xfewerparameters. Analysisof fast,shallowmodelsallowsonetoscaletolargeknowledge\n",
      "ourmodelsuggeststhatitisparticularlyeffectiveatmodelling graphs,atthecostoflearninglessexpressivefeatures.\n",
      "nodes with high indegree – which are common in highly-\n",
      "Theonlywaytoincreasethenumberoffeaturesinshallow\n",
      "connected,complexknowledgegraphssuchasFreebaseand\n",
      "models–andthustheirexpressiveness–istoincreasethe\n",
      "YAGO3. Inaddition,ithasbeennotedthattheWN18and\n",
      "FB15kdatasetssufferfromtestsetleakage, duetoinverse embeddingsize. However,doingsodoesnotscaletolarger\n",
      "relationsfromthetrainingsetbeingpresentinthetestset– knowledgegraphs,sincethetotalnumberofembeddingpa-\n",
      "however,theextentofthisissuehassofarnotbeenquantified. rameters is proportional to the the number of entities and\n",
      "Wefindthisproblemtobesevere:asimplerule-basedmodel relations in the graph. For example, a shallow model like\n",
      "canachievestate-of-the-artresultsonbothWN18andFB15k. DistMult with an embedding size of 200, applied to Free-\n",
      "Toensurethatmodelsareevaluatedondatasetswheresimply base, will need 33 GB of memory for its parameters. To\n",
      "exploitinginverserelationscannotyieldcompetitiveresults,\n",
      "increase the number of features independently of the em-\n",
      "weinvestigateandvalidateseveralcommonlyuseddatasets\n",
      "beddingsizerequirestheuseofmultiplelayersoffeatures.\n",
      "–derivingrobustvariantswherenecessary.Wethenperform\n",
      "However,previousmulti-layerknowledgegraphembedding\n",
      "experimentsontheserobustdatasetsforourownandseveral\n",
      "architectures,thatfeaturefullyconnectedlayers,areprone\n",
      "previouslyproposedmodels, andfindthatConvEachieves\n",
      "state-of-the-artMeanReciprocalRankacrossmostdatasets. tooverfit(Nickeletal.2016). Onewaytosolvethescaling\n",
      "problemofshallowarchitectures,andtheoverfittingproblem\n",
      "of fully connected deep architectures, is to use parameter\n",
      "Introduction efficient, fast operators which can be composed into deep\n",
      "networks.\n",
      "Knowledge graphs are graph-structured knowledge bases,\n",
      "The convolution operator, commonly used in computer\n",
      "where facts are represented in the form of relationships\n",
      "vision,hasexactlytheseproperties: itisparameterefficient\n",
      "(edges)betweenentities(nodes). Theyhaveimportantap-\n",
      "and fast to compute, due to highly optimised GPU imple-\n",
      "plications in search, analytics, recommendation, and data\n",
      "mentations. Furthermore,duetoitsubiquitoususe,robust\n",
      "integration – however, they tend to suffer from incom-\n",
      "methodologieshavebeenestablishedtocontroloverfitting\n",
      "pleteness, that is, missing links in the graph. For exam-\n",
      "whentrainingmulti-layerconvolutionalnetworks(Szegedy\n",
      "ple, in Freebase and DBpedia more than 66% of the per-\n",
      "etal.2015;IoffeandSzegedy2015;Srivastavaetal.2014;\n",
      "son entries are missing a birthplace (Dong et al. 2014;\n",
      "Szegedyetal.2016).\n",
      "Krompaß, Baier, and Tresp 2015). Identifying such miss-\n",
      "InthispaperweintroduceConvE,amodelthatuses2D\n",
      "inglinksisreferredtoaslinkprediction. Knowledgegraphs\n",
      "convolutions over embeddings to predict missing links in\n",
      "knowledgegraphs. ConvEisthesimplestmulti-layercon-\n",
      "∗ThisworkwasconductedduringaresearchvisittoUniversity\n",
      "volutionalarchitectureforlinkprediction: itisdefinedbya\n",
      "CollegeLondon.\n",
      "Copyright(cid:13)c 2018,AssociationfortheAdvancementofArtificial singleconvolutionlayer,aprojectionlayertotheembedding\n",
      "Intelligence(www.aaai.org).Allrightsreserved. dimension,andaninnerproductlayer.\n",
      "8102\n",
      "luJ\n",
      "4\n",
      "]GL.sc[\n",
      "6v67410.7071:viXra\n",
      "Specifically,ourcontributionsareasfollows: space. Inthiswork,weuse2D-convolutionswhichoperate\n",
      "spatiallyoverembeddings.\n",
      "• Introducingasimple,competitive2Dconvolutionallink\n",
      "predictionmodel,ConvE.\n",
      "NumberofInteractionsfor1Dvs2DConvolutions\n",
      "• Developinga1-Nscoringprocedurethatspeedsuptraining\n",
      "Using2Dratherthan1Dconvolutionsincreasestheexpres-\n",
      "three-foldandevaluationby300x.\n",
      "sivenessofourmodelthroughadditionalpointsofinteraction\n",
      "• Establishingthatourmodelishighlyparameterefficient, betweenembeddings. Forexample,considerthecasewhere\n",
      "achieving better scores than DistMult and R-GCNs on weconcatenatetworowsof1Dembeddings, aandbwith\n",
      "FB15k-237with8xand17xfewerparameters. dimensionn=3:\n",
      "• Showingthatforincreasinglycomplexknowledgegraphs,\n",
      "([a a a];[b b b])=[a a a b b b].\n",
      "as measured by indegree and PageRank, the difference\n",
      "inperformancebetweenourmodelandashallowmodel A padded 1D convolution with filter size k = 3 will be\n",
      "increasesproportionallytothecomplexityofthegraph. abletomodeltheinteractionsbetweenthesetwoembeddings\n",
      "• Systematicallyinvestigatingreportedinverserelationstest aroundtheconcatenationpoint(withanumberofinteractions\n",
      "proportionaltok).\n",
      "setleakageacrosscommonlyusedlinkpredictiondatasets,\n",
      "Ifweconcatenate(i.e. stack)tworowsof2Dembeddings\n",
      "introducingrobustversionsofdatasetswherenecessary,so\n",
      "withdimensionm×n,wherem=2andn=3,weobtain\n",
      "thattheycannotbesolvedusingsimplerule-basedmodels.\n",
      "thefollowing:\n",
      "• EvaluatingConvEandseveralpreviouslyproposedmodels\n",
      "ontheserobustdatasets: ourmodelachievesstate-of-the- a a a\n",
      "(cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19)\n",
      "artMeanReciprocalRankacrossmostofthem. a a a b b b a a a\n",
      "; = .\n",
      "a a a b b b b b b\n",
      "RelatedWork b b b\n",
      "Several neural link prediction models have been proposed\n",
      "Apadded2Dconvolutionwithfiltersize3×3willbeable\n",
      "intheliterature,suchastheTranslatingEmbeddingsmodel\n",
      "tomodeltheinteractionsaroundtheentireconcatenationline\n",
      "(TransE)(Bordesetal.2013a),theBilinearDiagonalmodel\n",
      "(withanumberofinteractionsproportionaltonandk).\n",
      "(DistMult)(Yangetal.2015)anditsextensioninthecomplex\n",
      "Wecanextendthisprincipletoanalternatingpattern,such\n",
      "space(ComplEx)(Trouillonetal.2016);werefertoNickel\n",
      "asthefollowing:\n",
      "et al. (2016) for a recent survey. The model that is most a a a\n",
      "closelyrelatedtothisworkismostlikelytheHolographic\n",
      "b b b\n",
      "Embeddings model (HolE) (Nickel, Rosasco, and Poggio  .\n",
      "a a a\n",
      "2016),whichusescross-correlation–theinverseofcircular\n",
      "b b b\n",
      "convolution–formatchingentityembeddings;itisinspired\n",
      "Inthiscase,a2Dconvolutionoperationisabletomodeleven\n",
      "by holographic models of associative memory. However,\n",
      "moreinteractionsbetweenaandb(withanumberofinter-\n",
      "HolEdoesnotlearnmultiplelayersofnon-linearfeatures,\n",
      "actionsproportionaltom,n,andk). Thus,2Dconvolution\n",
      "anditisthustheoreticallylessexpressivethanourmodel.\n",
      "isabletoextractmorefeatureinteractionsbetweentwoem-\n",
      "To the best of our knowledge, our model is the first\n",
      "beddingscomparedto1Dconvolution. Thesameprinciple\n",
      "neural link prediction model to use 2D convolutional lay-\n",
      "ers. Graph Convolutional Networks (GCNs) (Duvenaud canbeextendingtohigherdimensionalconvolutions,butwe\n",
      "leavethisasfuturework.\n",
      "etal.2015;Defferrard, Bresson, andVandergheynst2016;\n",
      "KipfandWelling2016)arearelatedlineofresearch,where\n",
      "Background\n",
      "theconvolutionoperatorisgeneralisedtouselocalityinfor-\n",
      "mationingraphs. However,theGCNframeworkislimited\n",
      "A knowledge graph G = {(s,r,o)} ⊆ E ×R×E can\n",
      "toundirectedgraphs,whileknowledgegraphsarenaturally\n",
      "be formalised as a set of triples (facts), each consisting of\n",
      "directed, and suffers from potentially prohibitive memory\n",
      "arelationshipr ∈ Randtwoentitiess,o ∈ E, referredto\n",
      "requirements (Kipf and Welling 2016). Relational GCNs\n",
      "as the subject and object of the triple. Each triple (s,r,o)\n",
      "(R-GCNs) (Schlichtkrull et al. 2017) are a generalisation\n",
      "denotesarelationshipoftyperbetweentheentitiessando.\n",
      "ofGCNsdevelopedfordealingwithhighlymulti-relational\n",
      "Thelinkpredictionproblemcanbeformalisedasapoint-\n",
      "data such as knowledge graphs – we include them in our\n",
      "wiselearningtorankproblem,wheretheobjectiveislearning\n",
      "experimentalevaluations.\n",
      "ascoringfunctionψ :E×R×E (cid:55)→R. Givenaninputtriple\n",
      "Severalconvolutionalmodelshavebeenproposedinnatu-\n",
      "x=(s,r,o),itsscoreψ(x)∈Risproportionaltothelikeli-\n",
      "rallanguageprocessing(NLP)forsolvingavarietyoftasks,\n",
      "hoodthatthefactencodedbyxistrue.\n",
      "includingsemanticparsing(Yihetal.2011),sentenceclassi-\n",
      "NeuralLinkPredictors\n",
      "fication(Kim2014),searchqueryretrieval(Shenetal.2014),\n",
      "sentencemodelling(Kalchbrenner,Grefenstette,andBlun- Neural link prediction models (Nickel et al. 2016) can be\n",
      "som2014),aswellasotherNLPtasks(Collobertetal.2011). seenasmulti-layerneuralnetworksconsistingofanencoding\n",
      "However,mostworkinNLPuses1D-convolutions,thatis componentandascoringcomponent. Givenaninputtriple\n",
      "convolutionswhichoperateoveratemporalsequenceofem- (s,r,o),theencodingcomponentmapsentitiess,o ∈ E to\n",
      "beddings, for example a sequence of words in embedding theirdistributedembeddingrepresentationse,e ∈Rk. In\n",
      "s o\n",
      "Table1: Scoringfunctionsψ (e,e )fromneurallinkpredictorsintheliterature,theirrelation-dependentparametersandspace\n",
      "r s o\n",
      "complexity;n andn respectivelydenotethenumberofentitiesandrelationtypes,i.e. n =|E|andn =|R|.\n",
      "e r e r\n",
      "Model ScoringFunctionψ (e,e ) RelationParameters SpaceComplexity\n",
      "r s o\n",
      "SE(Bordesetal.2014) (cid:13) (cid:13)W rLe s−W rRe o(cid:13) (cid:13)\n",
      "p\n",
      "W rL,W rR ∈Rk×k O(n ek+n rk2)\n",
      "TransE(Bordesetal.2013a) (cid:107)e +r −e (cid:107) r ∈Rk O(n k+n k)\n",
      "s r o p r e r\n",
      "DistMult(Yangetal.2015) (cid:104)e,r,e (cid:105) r ∈Rk O(n k+n k)\n",
      "s r o r e r\n",
      "ComplEx(Trouillonetal.2016) (cid:104)e,r,e (cid:105) r ∈Ck O(n k+n k)\n",
      "s r o r e r\n",
      "ConvE f(vec(f([e ;r ]∗ω))W)e r ∈Rk(cid:48) O(n k+n k(cid:48))\n",
      "s r o r e r\n",
      "thescoringcomponent,thetwoentityembeddingse ande entropyloss:\n",
      "s o\n",
      "arescoredbyafunctionψ r. Thescoreofatriple(s,r,o)is 1 (cid:88)\n",
      "definedasψ(s,r,o)=ψ r(e s,e o)∈R. L(p,t)=−\n",
      "N\n",
      "(t i·log(p i)+(1−t i)·log(1−p i)), (2)\n",
      "InTable1wesummarisethescoringfunctionofseveral i\n",
      "linkpredictionmodelsfromtheliterature.Thevectorse and wheretisthelabelvectorwithdimensionR1x1for1-1scor-\n",
      "s\n",
      "e denotethesubjectandobjectembedding,wheree,e ∈ ing or R1xN for 1-N scoring (see the next section for 1-N\n",
      "o s o\n",
      "Ck in ComplEx and e,e ∈ Rk in all other models, and scoring);theelementsofvectortareonesforrelationships\n",
      "s o\n",
      "(cid:80)\n",
      "(cid:104)x,y,z(cid:105) = x y z denotes the tri-linear dot product; ∗ thatexistsandzerootherwise.\n",
      "i i i i\n",
      "denotes the convolution operator; f denotes a non-linear We use rectified linear units as the non-linearity f for\n",
      "function. fastertraining(Krizhevsky,Sutskever,andHinton2012),and\n",
      "batchnormalisationaftereachlayertostabilise,regularise\n",
      "andincreaserateofconvergence(IoffeandSzegedy2015).\n",
      "Convolutional2DKnowledgeGraphs\n",
      "Weregulariseourmodelbyusingdropout(Srivastavaetal.\n",
      "Embeddings\n",
      "2014) in several stages. In particular, we use dropout on\n",
      "theembeddings,onthefeaturemapsaftertheconvolution\n",
      "Inthisworkweproposeaneurallinkpredictionmodelwhere\n",
      "operation,andonthehiddenunitsafterthefullyconnected\n",
      "theinteractionsbetweeninputentitiesandrelationshipsare\n",
      "layer. We use Adam as optimiser (Kingma and Ba 2014),\n",
      "modelledbyconvolutionalandfully-connectedlayers. The\n",
      "andlabelsmoothingtolessenoverfittingduetosaturationof\n",
      "maincharacteristicofourmodelisthatthescoreisdefinedby\n",
      "outputnon-linearitiesatthelabels(Szegedyetal.2016).\n",
      "aconvolutionover2Dshapedembeddings. Thearchitecture\n",
      "issummarisedinFigure1;formally,thescoringfunctionis FastEvaluationforLinkPredictionTasks\n",
      "definedasfollows:\n",
      "Inourarchitectureconvolutionconsumesabout75-90%of\n",
      "thetotalcomputationtime,thusitisimportanttominimise\n",
      "ψ (e,e )= f(vec(f([e ;r ]∗ω))W)e, (1)\n",
      "r s o s r o the number of convolution operations to speed up compu-\n",
      "tationasmuchaspossible. Forlinkpredictionmodels,the\n",
      "wherer ∈ Rk isarelationparameterdependingonr, e\n",
      "r s batchsizeisusuallyincreasedtospeedupevaluation(Bordes\n",
      "andr denotea2Dreshapingofe andr,respectively: if\n",
      "r s r etal.2013b). However,thisisnotfeasibleforconvolutional\n",
      "e s,r\n",
      "r\n",
      "∈Rk,thene s,r\n",
      "r\n",
      "∈Rkw×kh,wherek =k wk h.\n",
      "modelssincethememoryrequirementsquicklyoutgrowthe\n",
      "Inthefeed-forwardpass,themodelperformsarow-vector\n",
      "GPUmemorycapacitywhenincreasingthebatchsize.\n",
      "look-upoperationontwoembeddingmatrices,oneforenti-\n",
      "Unlikeotherlinkpredictionmodelswhichtakeanentity\n",
      "ties,denotedE|E|×k andoneforrelations,denotedR|R|×k(cid:48), pairandarelationasatriple(s,r,o),andscoreit(1-1scor-\n",
      "wherekandk(cid:48)aretheentityandrelationembeddingdimen- ing),wetakeone(s,r)pairandscoreitagainstallentities\n",
      "sions, and |E| and |R| denote the number of entities and o ∈ E simultaneously(1-Nscoring). Ifwebenchmark1-1\n",
      "relations. Themodelthenconcatenatese sandr r,andusesit scoringonahigh-endGPUwithbatchsizeandembedding\n",
      "asaninputfora2Dconvolutionallayerwithfiltersω. Such size128,thenatrainingpassandanevaluationwithacon-\n",
      "alayerreturnsafeaturemaptensorT ∈Rc×m×n,wherec volution model on FB15k – one of the dataset used in the\n",
      "isthenumberof2Dfeaturemapswithdimensionsmandn. experiments–takes2.4minutesand3.34hours. Using1-N\n",
      "ThetensorT isthenreshapedintoavectorvec(T)∈Rcmn, scoring,therespectivenumbersare45and35seconds–a\n",
      "whichisthenprojectedintoak-dimensionalspaceusingalin- considerable improvement of over 300x in terms of evalu-\n",
      "eartransformationparametrisedbythematrixW∈Rcmn×k\n",
      "ationtime. Additionally, thisapproachisscalabletolarge\n",
      "andmatchedwiththeobjectembeddinge oviaaninnerprod- knowledgegraphsandincreasesconvergencespeed. Fora\n",
      "uct.Theparametersoftheconvolutionalfiltersandthematrix single forward-backward pass with batch size of 128, go-\n",
      "Wareindependentoftheparametersfortheentitiessando ing from N = 100,000 to N = 1,000,000 entities only\n",
      "andtherelationshipr. increases the computational time from 64ms to 80ms – in\n",
      "For training the model parameters, we apply the lo- otherwords,aten-foldincreaseinthenumberofentitiesonly\n",
      "gistic sigmoid function σ(·) to the scores, that is p = increasesthecomputationtimeby25%–whichatteststhe\n",
      "σ(ψ (e,e )), and minimise the following binary cross- scalabilityoftheapproach.\n",
      "r s o\n",
      "Figure1: IntheConvEmodel,theentityandrelationembeddingsarefirstreshapedandconcatenated(steps1,2);theresulting\n",
      "matrixisthenusedasinputtoaconvolutionallayer(step3);theresultingfeaturemaptensorisvectorisedandprojectedintoa\n",
      "k-dimensionalspace(step4)andmatchedwithallcandidateobjectembeddings(step5).\n",
      "0.9\n",
      "0.2\n",
      "0.1\n",
      "0.6\n",
      "0.2\n",
      "0.3\n",
      "0.0\n",
      "0.7\n",
      "0.1\n",
      "0.4\n",
      "0.4\n",
      "0.4\n",
      "Ifinsteadof1-Nscoring,weuse1-(0.1N)scoring–that relations: alargenumberoftesttriplescanbeobtainedsim-\n",
      "is,scoringagainst10%oftheentities–wecancomputea plybyinvertingtriplesinthetrainingset. Forexample,the\n",
      "forward-backwardpass25%faster. However,weconverge test set frequently contains triples such as (s,hyponym,o)\n",
      "roughly230%sloweronthetrainingset. Thus1-Nscoring while the training set contains its inverse (o,hypernym,s).\n",
      "has an additional effect which is akin to batch normalisa- To create a dataset without this property, Toutanova and\n",
      "tion (Ioffe and Szegedy 2015) – we trade some computa- Chen (2015) introduced FB15k-237 – a subset of FB15k\n",
      "tionalperformanceforgreatlyincreasedconvergencespeed where inverse relations are removed. However, they did\n",
      "andalsoachievebetterperformanceasshowninSection7. notexplicitlyinvestigatetheseverityofthisproblem,which\n",
      "Do note that the technique in general could by applied to mightexplainwhyresearchcontinuesusingthesedatasets\n",
      "any1-1scoringmodel. Thispracticaltrickinspeedingup forevaluationwithoutaddressingthisissue(e.g. Trouillonet\n",
      "training and evaluation can be applied to any 1-1 scoring al.(2016),Nickel,Rosasco,andPoggio(2016),Nguyenet\n",
      "model,suchasthegreatmajorityoflinkpredictionmodels. al.(2016),Liuetal.(2016)).\n",
      "Inthefollowingsection,weintroduceasimplerule-based\n",
      "Experiments modelwhichdemonstratestheseverityofthisbiasbyachiev-\n",
      "ing state-of-the-art results on both WN18 and FB15k. In\n",
      "KnowledgeGraphDatasets\n",
      "ordertoensurethatweevaluateondatasetsthatdonothave\n",
      "Forevaluatingourproposedmodel,weuseaselectionoflink inverserelationtestleakage,weapplyoursimplerule-based\n",
      "predictiondatasetsfromtheliterature. modeltoeachdataset. ApartfromFB15k, whichwascor-\n",
      "WN18(Bordesetal.2013a)isasubsetofWordNetwhich rectedbyFB15k-237, wealsofindflawswithWN18. We\n",
      "consists of 18 relations and 40,943 entities. Most of the thuscreateWN18RRtoreclaimWN18asadataset,which\n",
      "151,442triplesconsistofhyponymandhypernymrelations cannoteasilybecompletedusingasinglerule–butrequires\n",
      "and,forsuchareason,WN18tendstofollowastrictlyhier- modelling of the complete knowledge graph. WN18RR1\n",
      "archicalstructure. contains93,003tripleswith40,943entitiesand11relations.\n",
      "FB15k(Bordesetal.2013a)isasubsetofFreebasewhich Forfutureresearch,werecommendagainstusingFB15kand\n",
      "containsabout14,951entitieswith1,345differentrelations. WN18andinsteadrecommendFB15k-237,WN18RR,and\n",
      "Alargefractionofcontentinthisknowledgegraphdescribes YAGO3-10.\n",
      "factsaboutmovies,actors,awards,sports,andsportteams.\n",
      "ExperimentalSetup\n",
      "YAGO3-10(Mahdisoltani,Biega,andSuchanek2015)is\n",
      "asubsetofYAGO3whichconsistsofentitieswhichhavea We selected the hyperparameters of our ConvE model via\n",
      "minimumof10relationseach. Ithas123,182entitiesand37 grid search according to the mean reciprocal rank (MRR)\n",
      "relations. Mostofthetriplesdealwithdescriptiveattributes on the validation set. Hyperparameter ranges for the grid\n",
      "ofpeople,suchascitizenship,gender,andprofession. searchwereasfollows–embeddingdropout{0.0,0.1,0.2},\n",
      "Countries (Bouchard, Singh, and Trouillon 2015) is a feature map dropout {0.0,0.1,0.2,0.3}, projection layer\n",
      "benchmarkdatasetthatisusefultoevaluateamodel’sabil- dropout {0.0,0.1,0.3,0.5}, embedding size {100,200},\n",
      "ity to learn long-range dependencies between entities and batchsize{64,128,256},learningrate{0.001,0.003},and\n",
      "relations. It consists of three sub-tasks which increase in labelsmoothing{0.0,0.1,0.2,0.3}.\n",
      "difficultyinastep-wisefashion,wheretheminimumpath- Besidesthegridsearch,weinvestigatedmodificationsof\n",
      "lengthtofindasolutionincreasesfrom2to4. the2Dconvolutionlayerforourmodels. Inparticular,we\n",
      "It was first noted by Toutanova and Chen (2015) that\n",
      "WN18andFB15ksufferfromtestleakagethroughinverse 1https://github.com/TimDettmers/ConvE\n",
      "Table2: ParameterscalingofDistMultvsConvE. Togaugetheseverityofthisproblem,weconstructasim-\n",
      "ple, rule-based model that solely models inverse relations.\n",
      "Wecallthismodeltheinversemodel. Themodelextractsin-\n",
      "Param. Emb. Hits\n",
      "verserelationshipsautomaticallyfromthetrainingset: given\n",
      "Model count size MRR @10 @3 @1\n",
      "two relation pairs r,r ∈ R, we check whether (s,r,o)\n",
      "1 2 1\n",
      "DistMult 1.89M 128.23.41.25.15 implies(o,r,s),orvice-versa.\n",
      "2\n",
      "DistMult 0.95M 64.22.39.25.14 Weassumethatinverserelationsarerandomlydistributed\n",
      "DistMult 0.23M 16.16.31.17.09 amongthetraining,validationandtestsetsand,assuch,we\n",
      "expectthenumberofinverserelationstobeproportionalto\n",
      "ConvE 5.05M 200.32.49.35.23\n",
      "thesizeofthetrainingsetcomparedtothetotaldatasetsize.\n",
      "ConvE 1.89M 96.32.49.35.23\n",
      "Thus,wedetectinverserelationsifthepresenceof(s,r,o)\n",
      "ConvE 0.95M 54.30.46.33.22 1\n",
      "co-occurswiththepresenceof(o,r,s)withafrequencyof\n",
      "ConvE 0.46M 28.28.43.30.20 2\n",
      "atleast0.99−(f +f ),wheref andf isthefractionof\n",
      "ConvE 0.23M 14.26.40.28.19 v t v t\n",
      "thevalidationandtestsetcomparedtothetotalsizeofthe\n",
      "dataset. Relationsmatchingthiscriterionareassumedtobe\n",
      "theinverseofeachother.\n",
      "experimentedwithreplacingitwithfullyconnectedlayers Attesttime,wecheckifthetesttriplehasinversematches\n",
      "and 1D convolution; however, these modifications consis- outside the test set: if k matches are found, we sample a\n",
      "tentlyreducedthepredictiveaccuracyofthemodel. Wealso permutationofthetopkranksforthesematches;ifnomatch\n",
      "experimentedwithdifferentfiltersizes, andfoundthatwe isfound,weselectarandomrankforthetesttriple.\n",
      "onlyreceivegoodresultsifthefirstconvolutionallayeruses\n",
      "Results\n",
      "small(i.e. 3x3)filters.\n",
      "Wefoundthatthefollowingcombinationofparameters Similarlytopreviouswork(Yangetal.2015;Trouillonetal.\n",
      "works well on WN18, YAGO3-10 and FB15k: embed- 2016;Niepert2016),wereportresultsusingafilteredsetting,\n",
      "dingdropout0.2,featuremapdropout0.2,projectionlayer i.e. we rank test triples against all other candidate triples\n",
      "dropout0.3, embeddingsize200, batchsize128, learning notappearinginthetraining,validation,ortestset(Bordes\n",
      "rate0.001,andlabelsmoothing0.1.FortheCountriesdataset, etal.2013a). Candidatesareobtainedbypermutingeither\n",
      "weincreaseembeddingdropoutto0.3,hiddendropoutto0.5, the subject or the object of a test triple with all entities in\n",
      "andsetlabelsmoothingto0.Weuseearlystoppingaccording the knowledge graph. Our results on the standard bench-\n",
      "tothemeanreciprocalrank(WN18,FB15k,YAGO3-10)and marksFB15kandWN18areshowninTable3;resultsonthe\n",
      "AUC-PR(Countries)statisticsonthevalidationset,which datasetswithinverserelationsremovedareshowninTable4;\n",
      "we evaluate every three epochs. Unlike the other datasets, resultsonYAGO3-10andCountriesareshowninTable5.\n",
      "for Countries the results have a high variance, as such we Strikingly,theinversemodelachievesstate-of-the-arton\n",
      "average10runsandproduce95%confidenceintervals. For many different metrics for both FB15k and WN18. How-\n",
      "ourDistMultandComplExresultswith1-1training,weuse ever, it fails to pick up on inverse relations for YAGO3-\n",
      "an embedding size of 100, AdaGrad (Duchi, Hazan, and 10andFB15k-237. TheprocedureusedbyToutanovaand\n",
      "Singer2011)foroptimisation,andweregulariseourmodel Chen(2015)toderiveFB15k-237doesnotremovecertain\n",
      "byforcingtheentityembeddingstohaveaL2normof1after symmetricrelationships,forexample“similarto”. Thepres-\n",
      "eachparameterupdate. AsinBordesetal.(2013a),weusea ence of these relationships explains the good score of our\n",
      "pairwisemargin-basedrankingloss. inverse model on WN18RR, which was derived using the\n",
      "Thecodeforourmodelandexperimentsismadepublicly sameprocedure.\n",
      "available,2 aswellasthecodeforreplicatingtheDistMult Ourproposedmodel,ConvE,achievesstate-of-the-artper-\n",
      "results.3 formanceforallmetricsonYAGO3-10,forsomemetricson\n",
      "FB15k,anditdoeswellonWN18. OnCountries,itsolves\n",
      "InverseModel theS1andS2tasks,anddoeswellonS3,scoringbetterthan\n",
      "It has been noted by Toutanova and Chen (2015), that the othermodelslikeDistMultandComplEx.\n",
      "trainingdatasetsofWN18andFB15khave94%and81% For FB15k-237, we could not replicate the basic model\n",
      "test leakage as inverse relations, that is, 94% and 81% of results from Toutanova et al. (2015), where the models in\n",
      "thetriplesinthesedatasetshaveinverserelationswhichare generalhavebetterperformancethanwhatwecanachieve.\n",
      "linked to the test set. For instance, a test triple (feline, hy- ComparedtoSchlichtkrulletal.(2017),ourresultsforstan-\n",
      "ponym, cat) can easily be mapped to a training triple (cat, dard models are a slightly better then theirs, and on-a-par\n",
      "hypernym,feline)ifitisknownthathyponymistheinverse withtheirR-GCNmodel.\n",
      "ofhypernym. Thisishighlyproblematic,becauselinkpre-\n",
      "ParameterefficiencyofConvE\n",
      "dictorsthatdowellonthesedatasetsmaysimplylearnwhich\n",
      "relationsthataretheinverseofothers,ratherthantomodel From Table 2 we can see that ConvE for FB15k-237 with\n",
      "theactualknowledgegraph. 0.23MparametersperformsbetterthanDistMultwith1.89M\n",
      "parametersfor3metricsoutof5.\n",
      "2https://github.com/TimDettmers/ConvE ConvEwith0.46Mparametersstillachievesstate-of-the-\n",
      "3https://github.com/uclmr/inferbeddings artresultsonFB15k-237with0.425Hits@10. Compar<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   8268,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['WN18', 'FB15k', 'YAGO3-10', 'Countries', 'FB15k-237', 'WN18RR']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  dard models are a slightly better then theirs, and on-a-par\n",
      "hypernym,feline)ifitisknownthathyponymistheinverse withtheirR-GCNmodel.\n",
      "ofhypernym. Thisishighlyproblematic,becauselinkpre-\n",
      "ParameterefficiencyofConvE\n",
      "dictorsthatdowellonthesedatasetsmaysimplylearnwhich\n",
      "relationsthataretheinverseofothers,ratherthantomodel From Table 2 we can see that ConvE for FB15k-237 with\n",
      "theactualknowledgegraph. 0.23MparametersperformsbetterthanDistMultwith1.89M\n",
      "parametersfor3metricsoutof5.\n",
      "2https://github.com/TimDettmers/ConvE ConvEwith0.46Mparametersstillachievesstate-of-the-\n",
      "3https://github.com/uclmr/inferbeddings artresultsonFB15k-237with0.425Hits@10. Comparingto\n",
      "Table3: LinkpredictionresultsforWN18andFB15k\n",
      "WN18 FB15k\n",
      "Hits Hits\n",
      "MR MRR @10 @3 @1 MR MRR @10 @3 @1\n",
      "DistMult(Yangetal.2015) 902.822.936.914.728 97.654.824.733.546\n",
      "ComplEx(Trouillonetal.2016) –.941.947.936.936 –.692.840.759.599\n",
      "Gaifman(Niepert2016) 352 –.939 –.761 75 –.842 –.692\n",
      "ANALOGY(Liu,Wu,andYang2017) –.942.947.944.939 –.725.854.785.646\n",
      "R-GCN(Schlichtkrulletal.2017) –.814.964.929.697 –.696.842.760.601\n",
      "ConvE 374.943.956.946.935 51.657.831.723.558\n",
      "InverseModel 740.963.964.964.953 2501.660.660.659.658\n",
      "Table4: LinkpredictionresultsforWN18RRandFB15k-237\n",
      "WN18RR FB15k-237\n",
      "Hits Hits\n",
      "MR MRR @10 @3 @1 MR MRR @10 @3 @1\n",
      "DistMult(Yangetal.2015) 5110.43.49.44.39 254.241.419.263.155\n",
      "ComplEx(Trouillonetal.2016) 5261.44.51.46.41 339.247.428.275.158\n",
      "R-GCN(Schlichtkrulletal.2017) – – – – – –.248.417.258.153\n",
      "ConvE 4187.43.52.44.40 244.325.501.356.237\n",
      "InverseModel 13526.35.35.35.35 7030.010.014.011.007\n",
      "thepreviousbestmodel,R-GCN(Schlichtkrulletal.2017), business people) and successful modelling of such a high\n",
      "whichachieves0.417Hits@10withmorethan8Mparame- indegreenodesrequirescapturingallthesedifferences. Our\n",
      "ters. hypothesisisthatdeepermodels,thatis,modelsthatlearn\n",
      "Overall,ConvEismorethan17xparameterefficientthan multiplelayersoffeatures,likeConvE,haveanadvantage\n",
      "R-GCNs,and8xmoreparameterefficientthanDistMult. For overshallowmodels,likeDistMult,tocaptureallthesecon-\n",
      "theentiretyofFreebase,thesizeofthesemodelswouldbe straints.\n",
      "morethan82GBforR-GCNs,21GBforDistMult,compared However,deepermodelsaremoredifficulttooptimise,so\n",
      "to5.2GBforConvE. wehypothesisethatfordatasetswithlowaveragerelation-\n",
      "specific indegree (like WN18RR and WN18), a shallow\n",
      "Analysis modellikeDistMultmightsufficeforaccuratelyrepresenting\n",
      "thestructureofthenetwork.\n",
      "AblationStudy\n",
      "To test our two hypotheses, we take two datasets with\n",
      "Table 7 shows the results from our ablation study where low(low-WN18)andhigh(high-FB15k)relation-specificin-\n",
      "we evaluate different parameter initialisation (n = 2) to degreeandreversethemintohigh(high-WN18)andlow(low-\n",
      "calculateconfidenceintervals. Weseethathiddendropoutis FB15k)relation-specificindegreedatasetsbydeletinglow\n",
      "byfarthemostimportantcomponent,whichisunsurprising andhighindegreenodes. Wehypothesisethat,comparedto\n",
      "since it is our main regularisation technique. 1-N scoring DistMult,ConvEwillalwaysdobetteronthedatasetwith\n",
      "improvesperformance,asdoesinputdropout,featuremap highrelation-specificindegree,andvice-versa.\n",
      "dropouthasaminoreffect,whilelabelsmoothingseemsto Indeed,wefindthatbothhypotheseshold: forlow-FB15k\n",
      "beunimportant–asgoodresultscanbeachievedwithoutit. wehaveConvE0.586Hits@10vsDistMult0.728Hits@10;\n",
      "forhigh-WN18wehaveConvE0.952Hits@10vsDistMult\n",
      "0.938 Hits@10. This supports our hypothesis that deeper\n",
      "AnalysisofIndegreeandPageRank\n",
      "models such as ConvE have an advantage to model more\n",
      "Ourmainhypothesisforthegoodperformanceofourmodel complexgraphs(e.g. FB15kandFB15k-237),butthatshal-\n",
      "on datasets like YAGO3-10 and FB15k-237 compared to low models such as DistMult have an advantage to model\n",
      "WN18RR,isthatthesedatasetscontainnodeswithveryhigh lesscomplexgraphs(e.g. WN18WN18RR).\n",
      "relation-specific indegree. For example the node “United Toinvestigatethisfurther,welookatPageRank,ameasure\n",
      "States” with edges “was born in” has an indegree of over of centrality of a node. PageRank can also be seen as a\n",
      "10,000. Many of these 10,000 nodes will be very differ- measureoftherecursiveindegreeofanode: thePageRank\n",
      "entfromeachother(actors,writers,academics,politicians, valueofanodeisproportionaltotheindegreeofthisnode,its\n",
      "Table5: LinkpredictionresultsforYAGO3-10andCountries\n",
      "YAGO3-10 Countries\n",
      "Hits AUC-PR\n",
      "MR MRR @10 @3 @1 S1 S2 S3\n",
      "DistMult(Yangetal.2015) 5926.34.54.38.24 1.00±0.00 0.72±0.12 0.52±0.07\n",
      "ComplEx(Trouillonetal.2016) 6351.36.55.40.26 0.97±0.02 0.57±0.10 0.43±0.07\n",
      "ConvE 1676.44.62.49.35 1.00±0.00 0.99±0.01 0.86±0.05\n",
      "InverseModel 59448.01.02.02.01 – – –\n",
      "Table6: MeanPageRank×10−3 ofnodesinthetestsetvs error reduction of ConvE compared to DistMult is strong\n",
      "reductioninerrorintermsofAUC-PRorHits@10ofConvE withr = 0.56. Thisgivesadditionalevidencethatmodels\n",
      "wrt. DistMult. that are deeper have an advantage when modelling nodes\n",
      "withhigh(recursive)indegree.\n",
      "Dataset PageRank ErrorReduction Fromthisevidenceweconclude,thattheincreasedperfor-\n",
      "manceofourmodelcomparedtoastandardlinkpredictor,\n",
      "WN18RR 0.104 0.06\n",
      "DistMult, can be partially explained due to our it’s ability\n",
      "WN18 0.125 0.45\n",
      "tomodelnodeswithhighindegreewithgreaterprecision–\n",
      "FB15k 0.599 0.04\n",
      "whichispossiblyrelatedtoitsdepth.\n",
      "FB15-237 0.733 0.16\n",
      "YAGO3-10 0.988 0.21\n",
      "ConclusionandFutureWork\n",
      "CountriesS3 1.415 2.36\n",
      "CountriesS1 1.711 0.00 WeintroducedConvE,alinkpredictionmodelthatuses2D\n",
      "CountriesS2 1.796 17.6 convolution over embeddings and multiple layers of non-\n",
      "linearfeaturestomodelknowledgegraphs.ConvEusesfewer\n",
      "parameters; it is fast through 1-N scoring; it is expressive\n",
      "Table7: AblationstudyforFB15k-237.\n",
      "through multiple layers of non-linear features; it is robust\n",
      "to overfitting dueto batchnormalisation anddropout; and\n",
      "Ablation Hits@10 achievesstate-of-the-artresultsonseveraldatasets,whilestill\n",
      "FullConvE 0.491 scalingtolargeknowledgegraphs. Inouranalysis,weshow\n",
      "thattheperformanceofConvEcomparedtoacommonlink\n",
      "Hiddendropout -0.044±0.003\n",
      "predictor,DistMult,canpartiallybeexplainedbyitsability\n",
      "Inputdropout -0.022±0.000\n",
      "tomodelnodeswithhigh(recursive)indegree.\n",
      "1-Nscoring -0.019\n",
      "TestleakagethroughinverserelationsofWN18andFB15k\n",
      "Featuremapdropout -0.013±0.001\n",
      "wasfirstreportedbyToutanovaandChen(2015): weinvesti-\n",
      "Labelsmoothing -0.008±0.000\n",
      "gatetheseverityofthisproblemforcommonlyuseddatasets\n",
      "by introducing a simple rule-based model, and find that it\n",
      "canachievestate-of-the-artresultsonWN18andFB15k. To\n",
      "ensurerobustversionsofallinvestigateddatasetsexists,we\n",
      "neighboursindegrees,itsneighbours-neighboursindegrees\n",
      "deriveWN18RR.\n",
      "andsoforthscaledrelativetoallothernodesinthenetwork.\n",
      "Bythislineofreasoning,wealsoexpectConvEtobebetter Ourmodelisstillshallowcomparedtoconvolutionalarchi-\n",
      "thanDistMultondatasetswithhighaveragePageRank(high tecturefoundincomputervision,andfutureworkmightdeal\n",
      "connectivitygraphs),andvice-versa. withconvolutionalmodelsofincreasingdepth. Furtherwork\n",
      "mightalsolookattheinterpretationof2Dconvolution,or\n",
      "Totestthishypothesis,wecalculatethePageRankforeach\n",
      "howtoenforcelarge-scalestructureinembeddingspaceso\n",
      "dataset as a measure of centrality. We find that the most\n",
      "toincreasethenumberofinteractionsbetweenembeddings.\n",
      "central nodes in WN18 have a PageRank value more than\n",
      "oneorderofmagnitudesmallerthanthemostcentralnodes\n",
      "Acknowledgements\n",
      "inYAGO3-10andCountries,andabout4timessmallerthan\n",
      "themostcentralnodesinFB15k. Whenwelookatthemean We would like to thank Johannes Welbl, Peter Hayes, and\n",
      "PageRank of nodes contained in the test sets, we find that TakumaEbisufortheirfeedbackandhelpfuldiscussionsre-\n",
      "thedifferenceofperformanceintermsofHits@10between latedtothiswork. WethankTakumaEbisuforpointingout\n",
      "DistMult and ConvE is roughly proportional to the mean anerrorinourInverseModelscript–thecorrectedresults\n",
      "test set PageRank, that is, the higher the mean PageRank areslightlybetterforWN18andslightlyworseforFB15k.\n",
      "of the test set nodes the better ConvE does compared to WethankVictoriaLinforhelpingustounrootandfixabug\n",
      "DistMult, and vice-versa. See Table 6 for these statistics. where the exclusion of triples during inference worked in-\n",
      "ThecorrelationbetweenmeantestsetPageRankandrelative correctly–thechangesdidnotaffectthemainresultsinthis\n",
      "work,thoughsomeresultsintheappendixchanged(UMLS, [2014] Kingma,D.,andBa,J. 2014. Adam: Amethodfor\n",
      "Nations). This work was supported by a Marie Curie Ca- stochasticoptimization. arXivpreprintarXiv:1412.6980.\n",
      "reerIntegrationAward,anAllenDistinguishedInvestigator [2016] Kipf,T.N.,andWelling,M. 2016. Semi-Supervised\n",
      "Award,aGoogleEuropeScholarshipforStudentswithDis- ClassificationwithGraphConvolutionalNetworks. InPro-\n",
      "abilities,andtheH2020projectSUMMA. ceedingsofICLR2016.\n",
      "[2012] Krizhevsky,A.;Sutskever,I.;andHinton,G.E. 2012.\n",
      "References\n",
      "ImageNet Classification with Deep Convolutional Neural\n",
      "[2013a] Bordes,A.;Usunier,N.;García-Durán,A.;Weston, Networks. InProceedingsofNIPS2012,1097–1105.\n",
      "J.;andYakhnenko,O. 2013a. TranslatingEmbeddingsfor [2015] Krompaß, D.; Baier, S.; and Tresp, V. 2015. Type-\n",
      "Modeling Multi-relational Data. In Proceedings of NIPS, ConstrainedRepresentationLearninginKnowledgeGraphs.\n",
      "2787–2795. InProceedingsofISWC2015,640–655.\n",
      "[2013b] Bordes,A.;Usunier,N.;García-Durán,A.;Weston, [2016] Liu,Q.;Jiang,L.;Han,M.;Liu,Y.;andQin,Z. 2016.\n",
      "J.;andYakhnenko,O. 2013b. TranslatingEmbeddingsfor Hierarchicalrandomwalkinferenceinknowledgegraphs. In\n",
      "Modeling Multi-relational Data. In Proceedings of NIPS, Proceedingsofthe39thInternationalACMSIGIRconference\n",
      "2787–2795. onResearchandDevelopmentinInformationRetrieval,445–\n",
      "454. ACM.\n",
      "[2014] Bordes, A.; Glorot, X.; Weston, J.; and Bengio, Y.\n",
      "2014. Asemanticmatchingenergyfunctionforlearningwith [2017] Liu, H.; Wu, Y.; and Yang, Y. 2017. Analogical\n",
      "multi-relationaldata-applicationtoword-sensedisambigua- InferenceforMulti-RelationalEmbeddings. ArXive-prints.\n",
      "tion. MachineLearning94(2):233–259. [2015] Mahdisoltani,F.;Biega,J.;andSuchanek,F.M. 2015.\n",
      "[2015] Bouchard, G.; Singh, S.; and Trouillon, T. 2015. YAGO3: AKnowledgeBasefromMultilingualWikipedias.\n",
      "On approximate reasoning capabilities of low-rank vector InProceedingsofCIDR2015.\n",
      "spaces. AAAISpringSyposiumonKnowledgeRepresenta- [2016] Nguyen, D. Q.; Sirts, K.; Qu, L.; and Johnson,\n",
      "tionandReasoning(KRR):IntegratingSymbolicandNeural M. 2016. Stranse: a novel embedding model of enti-\n",
      "Approaches. ties and relationships in knowledge bases. arXiv preprint\n",
      "[2011] Collobert, R.; Weston, J.; Bottou, L.; Karlen, M.; arXiv:1606.08140.\n",
      "Kavukcuoglu, K.; and Kuksa, P. P. 2011. Natural Lan- [2016] Nickel,M.;Murphy,K.;Tresp,V.;andGabrilovich,E.\n",
      "guageProcessing(Almost)fromScratch.JournalofMachine 2016. Areviewofrelationalmachinelearningforknowledge\n",
      "LearningResearch12:2493–2537. graphs. ProceedingsoftheIEEE104(1):11–33.\n",
      "[2016] Defferrard, M.; Bresson, X.; and Vandergheynst, P. [2016] Nickel,M.;Rosasco,L.;andPoggio,T.A.2016.Holo-\n",
      "2016. ConvolutionalNeuralNetworksonGraphswithFast graphicEmbeddingsofKnowledgeGraphs. InProceedings\n",
      "LocalizedSpectralFiltering. InProceedingsofNIPS,3837– ofAAAI,1955–1961.\n",
      "3845.\n",
      "[2016] Niepert,M. 2016. DiscriminativeGaifmanModels.\n",
      "[2014] Dong,X.;Gabrilovich,E.;Heitz,G.;Horn,W.;Lao, InProceedingsofNIPS2016,3405–3413.\n",
      "N.;Murphy,K.;Strohmann,T.;Sun,S.;andZhang,W. 2014. [2017] Schlichtkrull, M.; Kipf, T. N.; Bloem, P.; Berg, R.\n",
      "KnowledgeVault: AWeb-ScaleApproachtoProbabilistic v.d.;Titov,I.;andWelling,M. 2017. ModelingRelational\n",
      "KnowledgeFusion. InProceedingsofKDD2014,601–610. Data with Graph Convolutional Networks. arXiv preprint\n",
      "[2011] Duchi,J.C.;Hazan,E.;andSinger,Y. 2011. Adap- arXiv:1703.06103.\n",
      "tive subgradient methods for online learning and stochas- [2014] Shen, Y.; He, X.; Gao, J.; Deng, L.; and Mesnil, G.\n",
      "tic optimization. Journal of Machine Learning Research 2014. LearningSemanticRepresentationsUsingConvolu-\n",
      "12:2121–2159. tionalNeuralNetworksforWebSearch. InProceedingsof\n",
      "[2015] Duvenaud, D. K.; Maclaurin, D.; Aguilera- WWW2014,373–374.\n",
      "Iparraguirre, J.; Bombarell, R.; Hirzel, T.; Aspuru-Guzik, [2014] Srivastava, N.; Hinton, G. E.; Krizhevsky, A.;\n",
      "A.; and Adams, R. P. 2015. Convolutional Networks on Sutskever, I.; and Salakhutdinov, R. 2014. Dropout: A\n",
      "GraphsforLearningMolecularFingerprints. InProceedings SimpleWaytoPreventNeuralNetworksfromOverfitting.\n",
      "ofNIPS2015,2224–2232. JournalofMachineLearningResearch15(1):1929–1958.\n",
      "[2015] Ioffe,S.,andSzegedy,C. 2015. BatchNormalization: [2015] Szegedy,C.;Liu,W.;Jia,Y.;Sermanet,P.;Reed,S.;\n",
      "AcceleratingDeepNetworkTrainingbyReducingInternal Anguelov,D.;Erhan,D.;Vanhoucke,V.;andRabinovich,A.\n",
      "CovariateShift. arXivpreprintarXiv:1502.03167. 2015. Goingdeeperwithconvolutions. InProceedingsof\n",
      "[2014] Kalchbrenner, N.; Grefenstette, E.; andBlunsom, P. IEEECVPR,1–9.\n",
      "2014. AConvolutionalNeuralNetworkforModellingSen- [2016] Szegedy,C.;Vanhoucke,V.;Ioffe,S.;Shlens,J.;and\n",
      "tences. InProceedingsofACL2014,Volume1: LongPapers, Wojna,Z. 2016. RethinkingtheInceptionArchitecturefor\n",
      "655–665. Computer Vision. In Proceedings of IEEE CVPR, 2818–\n",
      "[2014] Kim, Y. 2014. Convolutional Neural Networks for 2826.\n",
      "Sentence Classification. In Proceedings of EMNLP 2014, [2015] Toutanova,K.,andChen,D. 2015. ObservedVersus\n",
      "1746–1751. LatentFeaturesforKnowledgeBaseandTextInference. In\n",
      "Proceedingsofthe3rdWorkshoponContinuousVectorSpace Table8: ConvElinkpredictionresultsforUMLS,Nations,\n",
      "ModelsandtheirCompositionality,57–66. andKinship.\n",
      "[2015] Toutanova,K.;Chen,D.;Pantel,P.;Poon,H.;Choud-\n",
      "hury,P.;andGamon,M. 2015. RepresentingTextforJoint Hits\n",
      "EmbeddingofTextandKnowledgeBases. InProceedingsof Dataset Model MR MRR @10 @3 @1\n",
      "EMNLP2015,volume15,1499–1509.\n",
      "UMLS ConvE 1.94.99.96.92\n",
      "[2016] Trouillon,T.;Welbl,J.;Riedel,S.;Gaussier,É.;and\n",
      "Kinship ConvE 2.83.98.92.74\n",
      "Bouchard,G. 2016. ComplexEmbeddingsforSimpleLink\n",
      "Prediction. InProceedingsofICML2016,2071–2080.\n",
      "[2015] Yang,B.;Yih,W.;He,X.;Gao,J.;andDeng,L. 2015.\n",
      "the test set. Following Bordes et al. (2013a), for the i-th\n",
      "EmbeddingEntitiesandRelationsforLearningandInference\n",
      "testtriplex inT, wegenerateallitspossiblecorruptions\n",
      "inKnowledgeBases. InProceedingsofICLR2015. i\n",
      "Cs(x ) (resp. Co(x )) – obtained by replacing its subject\n",
      "i i\n",
      "[2011] Yih, W.; Toutanova, K.; Platt, J. C.; and Meek, C.\n",
      "(resp. object)withanyotherentityintheKnowledgeGraph\n",
      "2011.LearningDiscriminativeProjectionsforTextSimilarity\n",
      "–tocheckwhetherthemodelassignsanhigherscoretox\n",
      "i\n",
      "Measures. InProceedingsofCoNLL2011,247–256.\n",
      "and a lower score to its corruptions. Note that the set of\n",
      "corruptionscanalsocontainseveraltruetriples,anditisnot\n",
      "SUPPLEMENTAL MATERIAL\n",
      "amistaketorankthemwithanhigherscorethanx. Forsuch\n",
      "i\n",
      "areason,weremovealltriplesinthegraphfromthesetof\n",
      "Versions\n",
      "corruptions:thisisreferredtoasthefilteredsettinginBordes\n",
      "• 2018-07-04: etal.(2013a). Theleftandrightrankofthei-thtesttriple–\n",
      "– Added new YAGO3-10 results. The new results are eachassociatedtocorruptingeitherthesubjectortheobject–\n",
      "worseonmostmetrics,butstate-of-the-artresultsare accordingtoamodelwithscoringfunctionψ(·),aredefined\n",
      "retained. asfollows:\n",
      "– IwasunabletoreplicateFB15kscoresthatIinitially (cid:88)\n",
      "ranks =1+ I[ψ(x )<ψ(x˜ )],\n",
      "reported.4 i i i\n",
      "– IupdatethePageRanktableandthereportedPageRank-\n",
      "x˜i∈Cs(xi)\\G\n",
      "(cid:88)\n",
      "error-reductioncorrelationtoreflectthenewscores. ranko =1+ I[ψ(x )<ψ(x˜ )],\n",
      "i i i\n",
      "– I removed the Nations scores in the appendix. The x˜i∈Co(xi)\\G\n",
      "Nationsdatasethasahighproportionofinverserelation-\n",
      "shipsandisthusnotsuitablefortheuseinresearch. I\n",
      "whereI[P]is1ifftheconditionP istrue,and0otherwise.\n",
      "donotwanttoencourageitsuse. Formeasuringthequalityoftheranking,weusetheMean\n",
      "ReciprocalRank(MRR)andtheHits@kmetrics,whichare\n",
      "• 2018-04-06: VictoriaLinhelpedustofindandfixissues5\n",
      "definedasfollows:\n",
      "withtriplemasksduringevaluation. Wereportnewnum-\n",
      "bersforUMLSandNationsintheappendix. Theresults 1 (cid:88) 1 1\n",
      "MRR: +,\n",
      "wereunchangedonotherdatasetsthatwetestedthusfar 2|T| ranks ranko\n",
      "(Kinship,WN18,WN18RR,FB15k-237). 100\n",
      "x (cid:88)i∈T i i\n",
      "Hits@k(%): I[ranks ≤k]+I[ranko ≤k].\n",
      "• 2018-03-28: 2|T| i i\n",
      "– NewnumbersforInverseModelafterbugfixbyTakuma\n",
      "xi∈T\n",
      "Ebisu. MRRistheaverageinverserankforalltesttriples:thehigher,\n",
      "thebetter. Hits@kisthepercentageofrankslowerthanor\n",
      "– New numbersfor UMLS/Nations/Kinship datasets in\n",
      "equaltok: thehigher,thebetter.\n",
      "appendixusingthemostcommonlyreportedtestdata\n",
      "splits.\n",
      "• 2018-01-07: ExtendedAAAIcameraready(6/7/8).\n",
      "• 2017-07-08: Missinggrantinacknowledgements.\n",
      "• 2017-07-05: OriginalNIPSsubmission(6/7/6).\n",
      "FurtherConvEresults\n",
      "EvaluationMetrics\n",
      "Wenowdescribetheevaluationmetricsusedforassessingthe\n",
      "qualityofthemodels. LetT = {x,x,...,x }denote\n",
      "1 2 |T|\n",
      "4See https://github.com/TimDettmers/ConvE/\n",
      "issues/26fordetails.\n",
      "5See https://github.com/TimDettmers/ConvE/\n",
      "issues/18formoreinformation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   5383,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['YAGO3-10', 'WN18', 'WN18RR', 'FB15k', 'FB15k-237', 'Countries', 'UMLS', 'Kinship']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Convolutional 2D Knowledge Graph Embeddings\n",
      "TimDettmers∗\n",
      "UniversitàdellaSvizzeraitaliana\n",
      "tim.dettmers@gmail.com\n",
      "PasqualeMinervini PontusStenetorp SebastianRiedel\n",
      "UniversityCollegeLondon\n",
      "{p.minervini,p.stenetorp,s.riedel}@cs.ucl.ac.uk\n",
      "Abstract can contain millions of facts; as a consequence, link pre-\n",
      "dictors should scale in a manageable way with respect to\n",
      "Linkpredictionforknowledgegraphsisthetaskofpredict- boththenumberofparametersandcomputationalcoststobe\n",
      "ingmissingrelationshipsbetweenentities.Previousworkon applicableinreal-worldscenarios.\n",
      "link prediction has focused on shallow, fast models which\n",
      "Forsolvingsuchscalingproblems,linkpredictionmodels\n",
      "canscaletolargeknowledgegraphs.However,thesemodels\n",
      "areoftencomposedofsimpleoperations,likeinnerproducts\n",
      "learnlessexpressivefeaturesthandeep,multi-layermodels–\n",
      "and matrix multiplications over an embedding space, and\n",
      "whichpotentiallylimitsperformance.Inthisworkweintro-\n",
      "duceConvE,amulti-layerconvolutionalnetworkmodelfor use a limited number of parameters (Nickel et al. 2016).\n",
      "linkprediction,andreportstate-of-the-artresultsforseveral DistMult(Yangetal.2015)issuchamodel,characterised\n",
      "establisheddatasets.Wealsoshowthatthemodelishighlypa- by three-way interactions between embedding parameters,\n",
      "rameterefficient,yieldingthesameperformanceasDistMult whichproduceonefeatureperparameter. Usingsuchsimple,\n",
      "andR-GCNwith8xand17xfewerparameters. Analysisof fast,shallowmodelsallowsonetoscaletolargeknowledge\n",
      "ourmodelsuggeststhatitisparticularlyeffectiveatmodelling graphs,atthecostoflearninglessexpressivefeatures.\n",
      "nodes with high indegree – which are common in highly-\n",
      "Theonlywaytoincreasethenumberoffeaturesinshallow\n",
      "connected,complexknowledgegraphssuchasFreebaseand\n",
      "models–andthustheirexpressiveness–istoincreasethe\n",
      "YAGO3. Inaddition,ithasbeennotedthattheWN18and\n",
      "FB15kdatasetssufferfromtestsetleakage, duetoinverse embeddingsize. However,doingsodoesnotscaletolarger\n",
      "relationsfromthetrainingsetbeingpresentinthetestset– knowledgegraphs,sincethetotalnumberofembeddingpa-\n",
      "however,theextentofthisissuehassofarnotbeenquantified. rameters is proportional to the the number of entities and\n",
      "Wefindthisproblemtobesevere:asimplerule-basedmodel relations in the graph. For example, a shallow model like\n",
      "canachievestate-of-the-artresultsonbothWN18andFB15k. DistMult with an embedding size of 200, applied to Free-\n",
      "Toensurethatmodelsareevaluatedondatasetswheresimply base, will need 33 GB of memory for its parameters. To\n",
      "exploitinginverserelationscannotyieldcompetitiveresults,\n",
      "increase the number of features independently of the em-\n",
      "weinvestigateandvalidateseveralcommonlyuseddatasets\n",
      "beddingsizerequirestheuseofmultiplelayersoffeatures.\n",
      "–derivingrobustvariantswherenecessary.Wethenperform\n",
      "However,previousmulti-layerknowledgegraphembedding\n",
      "experimentsontheserobustdatasetsforourownandseveral\n",
      "architectures,thatfeaturefullyconnectedlayers,areprone\n",
      "previouslyproposedmodels, andfindthatConvEachieves\n",
      "state-of-the-artMeanReciprocalRankacrossmostdatasets. tooverfit(Nickeletal.2016). Onewaytosolvethescaling\n",
      "problemofshallowarchitectures,andtheoverfittingproblem\n",
      "of fully connected deep architectures, is to use parameter\n",
      "Introduction efficient, fast operators which can be composed into deep\n",
      "networks.\n",
      "Knowledge graphs are graph-structured knowledge bases,\n",
      "The convolution operator, commonly used in computer\n",
      "where facts are represented in the form of relationships\n",
      "vision,hasexactlytheseproperties: itisparameterefficient\n",
      "(edges)betweenentities(nodes). Theyhaveimportantap-\n",
      "and fast to compute, due to highly optimised GPU imple-\n",
      "plications in search, analytics, recommendation, and data\n",
      "mentations. Furthermore,duetoitsubiquitoususe,robust\n",
      "integration – however, they tend to suffer from incom-\n",
      "methodologieshavebeenestablishedtocontroloverfitting\n",
      "pleteness, that is, missing links in the graph. For exam-\n",
      "whentrainingmulti-layerconvolutionalnetworks(Szegedy\n",
      "ple, in Freebase and DBpedia more than 66% of the per-\n",
      "etal.2015;IoffeandSzegedy2015;Srivastavaetal.2014;\n",
      "son entries are missing a birthplace (Dong et al. 2014;\n",
      "Szegedyetal.2016).\n",
      "Krompaß, Baier, and Tresp 2015). Identifying such miss-\n",
      "InthispaperweintroduceConvE,amodelthatuses2D\n",
      "inglinksisreferredtoaslinkprediction. Knowledgegraphs\n",
      "convolutions over embeddings to predict missing links in\n",
      "knowledgegraphs. ConvEisthesimplestmulti-layercon-\n",
      "∗ThisworkwasconductedduringaresearchvisittoUniversity\n",
      "volutionalarchitectureforlinkprediction: itisdefinedbya\n",
      "CollegeLondon.\n",
      "Copyright(cid:13)c 2018,AssociationfortheAdvancementofArtificial singleconvolutionlayer,aprojectionlayertotheembedding\n",
      "Intelligence(www.aaai.org).Allrightsreserved. dimension,andaninnerproductlayer.\n",
      "8102\n",
      "luJ\n",
      "4\n",
      "]GL.sc[\n",
      "6v67410.7071:viXra\n",
      "Specifically,ourcontributionsareasfollows: space. Inthiswork,weuse2D-convolutionswhichoperate\n",
      "spatiallyoverembeddings.\n",
      "• Introducingasimple,competitive2Dconvolutionallink\n",
      "predictionmodel,ConvE.\n",
      "NumberofInteractionsfor1Dvs2DConvolutions\n",
      "• Developinga1-Nscoringprocedurethatspeedsuptraining\n",
      "Using2Dratherthan1Dconvolutionsincreasestheexpres-\n",
      "three-foldandevaluationby300x.\n",
      "sivenessofourmodelthroughadditionalpointsofinteraction\n",
      "• Establishingthatourmodelishighlyparameterefficient, betweenembeddings. Forexample,considerthecasewhere\n",
      "achieving better scores than DistMult and R-GCNs on weconcatenatetworowsof1Dembeddings, aandbwith\n",
      "FB15k-237with8xand17xfewerparameters. dimensionn=3:\n",
      "• Showingthatforincreasinglycomplexknowledgegraphs,\n",
      "([a a a];[b b b])=[a a a b b b].\n",
      "as measured by indegree and PageRank, the difference\n",
      "inperformancebetweenourmodelandashallowmodel A padded 1D convolution with filter size k = 3 will be\n",
      "increasesproportionallytothecomplexityofthegraph. abletomodeltheinteractionsbetweenthesetwoembeddings\n",
      "• Systematicallyinvestigatingreportedinverserelationstest aroundtheconcatenationpoint(withanumberofinteractions\n",
      "proportionaltok).\n",
      "setleakageacrosscommonlyusedlinkpredictiondatasets,\n",
      "Ifweconcatenate(i.e. stack)tworowsof2Dembeddings\n",
      "introducingrobustversionsofdatasetswherenecessary,so\n",
      "withdimensionm×n,wherem=2andn=3,weobtain\n",
      "thattheycannotbesolvedusingsimplerule-basedmodels.\n",
      "thefollowing:\n",
      "• EvaluatingConvEandseveralpreviouslyproposedmodels\n",
      "ontheserobustdatasets: ourmodelachievesstate-of-the- a a a\n",
      "(cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19)\n",
      "artMeanReciprocalRankacrossmostofthem. a a a b b b a a a\n",
      "; = .\n",
      "a a a b b b b b b\n",
      "RelatedWork b b b\n",
      "Several neural link prediction models have been proposed\n",
      "Apadded2Dconvolutionwithfiltersize3×3willbeable\n",
      "intheliterature,suchastheTranslatingEmbeddingsmodel\n",
      "tomodeltheinteractionsaroundtheentireconcatenationline\n",
      "(TransE)(Bordesetal.2013a),theBilinearDiagonalmodel\n",
      "(withanumberofinteractionsproportionaltonandk).\n",
      "(DistMult)(Yangetal.2015)anditsextensioninthecomplex\n",
      "Wecanextendthisprincipletoanalternatingpattern,such\n",
      "space(ComplEx)(Trouillonetal.2016);werefertoNickel\n",
      "asthefollowing:\n",
      "et al. (2016) for a recent survey. The model that is most a a a\n",
      "closelyrelatedtothisworkismostlikelytheHolographic\n",
      "b b b\n",
      "Embeddings model (HolE) (Nickel, Rosasco, and Poggio  .\n",
      "a a a\n",
      "2016),whichusescross-correlation–theinverseofcircular\n",
      "b b b\n",
      "convolution–formatchingentityembeddings;itisinspired\n",
      "Inthiscase,a2Dconvolutionoperationisabletomodeleven\n",
      "by holographic models of associative memory. However,\n",
      "moreinteractionsbetweenaandb(withanumberofinter-\n",
      "HolEdoesnotlearnmultiplelayersofnon-linearfeatures,\n",
      "actionsproportionaltom,n,andk). Thus,2Dconvolution\n",
      "anditisthustheoreticallylessexpressivethanourmodel.\n",
      "isabletoextractmorefeatureinteractionsbetweentwoem-\n",
      "To the best of our knowledge, our model is the first\n",
      "beddingscomparedto1Dconvolution. Thesameprinciple\n",
      "neural link prediction model to use 2D convolutional lay-\n",
      "ers. Graph Convolutional Networks (GCNs) (Duvenaud canbeextendingtohigherdimensionalconvolutions,butwe\n",
      "leavethisasfuturework.\n",
      "etal.2015;Defferrard, Bresson, andVandergheynst2016;\n",
      "KipfandWelling2016)arearelatedlineofresearch,where\n",
      "Background\n",
      "theconvolutionoperatorisgeneralisedtouselocalityinfor-\n",
      "mationingraphs. However,theGCNframeworkislimited\n",
      "A knowledge graph G = {(s,r,o)} ⊆ E ×R×E can\n",
      "toundirectedgraphs,whileknowledgegraphsarenaturally\n",
      "be formalised as a set of triples (facts), each consisting of\n",
      "directed, and suffers from potentially prohibitive memory\n",
      "arelationshipr ∈ Randtwoentitiess,o ∈ E, referredto\n",
      "requirements (Kipf and Welling 2016). Relational GCNs\n",
      "as the subject and object of the triple. Each triple (s,r,o)\n",
      "(R-GCNs) (Schlichtkrull et al. 2017) are a generalisation\n",
      "denotesarelationshipoftyperbetweentheentitiessando.\n",
      "ofGCNsdevelopedfordealingwithhighlymulti-relational\n",
      "Thelinkpredictionproblemcanbeformalisedasapoint-\n",
      "data such as knowledge graphs – we include them in our\n",
      "wiselearningtorankproblem,wheretheobjectiveislearning\n",
      "experimentalevaluations.\n",
      "ascoringfunctionψ :E×R×E (cid:55)→R. Givenaninputtriple\n",
      "Severalconvolutionalmodelshavebeenproposedinnatu-\n",
      "x=(s,r,o),itsscoreψ(x)∈Risproportionaltothelikeli-\n",
      "rallanguageprocessing(NLP)forsolvingavarietyoftasks,\n",
      "hoodthatthefactencodedbyxistrue.\n",
      "includingsemanticparsing(Yihetal.2011),sentenceclassi-\n",
      "NeuralLinkPredictors\n",
      "fication(Kim2014),searchqueryretrieval(Shenetal.2014),\n",
      "sentencemodelling(Kalchbrenner,Grefenstette,andBlun- Neural link prediction models (Nickel et al. 2016) can be\n",
      "som2014),aswellasotherNLPtasks(Collobertetal.2011). seenasmulti-layerneuralnetworksconsistingofanencoding\n",
      "However,mostworkinNLPuses1D-convolutions,thatis componentandascoringcomponent. Givenaninputtriple\n",
      "convolutionswhichoperateoveratemporalsequenceofem- (s,r,o),theencodingcomponentmapsentitiess,o ∈ E to\n",
      "beddings, for example a sequence of words in embedding theirdistributedembeddingrepresentationse,e ∈Rk. In\n",
      "s o\n",
      "Table1: Scoringfunctionsψ (e,e )fromneurallinkpredictorsintheliterature,theirrelation-dependentparametersandspace\n",
      "r s o\n",
      "complexity;n andn respectivelydenotethenumberofentitiesandrelationtypes,i.e. n =|E|andn =|R|.\n",
      "e r e r\n",
      "Model ScoringFunctionψ (e,e ) RelationParameters SpaceComplexity\n",
      "r s o\n",
      "SE(Bordesetal.2014) (cid:13) (cid:13)W rLe s−W rRe o(cid:13) (cid:13)\n",
      "p\n",
      "W rL,W rR ∈Rk×k O(n ek+n rk2)\n",
      "TransE(Bordesetal.2013a) (cid:107)e +r −e (cid:107) r ∈Rk O(n k+n k)\n",
      "s r o p r e r\n",
      "DistMult(Yangetal.2015) (cid:104)e,r,e (cid:105) r ∈Rk O(n k+n k)\n",
      "s r o r e r\n",
      "ComplEx(Trouillonetal.2016) (cid:104)e,r,e (cid:105) r ∈Ck O(n k+n k)\n",
      "s r o r e r\n",
      "ConvE f(vec(f([e ;r ]∗ω))W)e r ∈Rk(cid:48) O(n k+n k(cid:48))\n",
      "s r o r e r\n",
      "thescoringcomponent,thetwoentityembeddingse ande entropyloss:\n",
      "s o\n",
      "arescoredbyafunctionψ r. Thescoreofatriple(s,r,o)is 1 (cid:88)\n",
      "definedasψ(s,r,o)=ψ r(e s,e o)∈R. L(p,t)=−\n",
      "N\n",
      "(t i·log(p i)+(1−t i)·log(1−p i)), (2)\n",
      "InTable1wesummarisethescoringfunctionofseveral i\n",
      "linkpredictionmodelsfromtheliterature.Thevectorse and wheretisthelabelvectorwithdimensionR1x1for1-1scor-\n",
      "s\n",
      "e denotethesubjectandobjectembedding,wheree,e ∈ ing or R1xN for 1-N scoring (see the next section for 1-N\n",
      "o s o\n",
      "Ck in ComplEx and e,e ∈ Rk in all other models, and scoring);theelementsofvectortareonesforrelationships\n",
      "s o\n",
      "(cid:80)\n",
      "(cid:104)x,y,z(cid:105) = x y z denotes the tri-linear dot product; ∗ thatexistsandzerootherwise.\n",
      "i i i i\n",
      "denotes the convolution operator; f denotes a non-linear We use rectified linear units as the non-linearity f for\n",
      "function. fastertraining(Krizhevsky,Sutskever,andHinton2012),and\n",
      "batchnormalisationaftereachlayertostabilise,regularise\n",
      "andincreaserateofconvergence(IoffeandSzegedy2015).\n",
      "Convolutional2DKnowledgeGraphs\n",
      "Weregulariseourmodelbyusingdropout(Srivastavaetal.\n",
      "Embeddings\n",
      "2014) in several stages. In particular, we use dropout on\n",
      "theembeddings,onthefeaturemapsaftertheconvolution\n",
      "Inthisworkweproposeaneurallinkpredictionmodelwhere\n",
      "operation,andonthehiddenunitsafterthefullyconnected\n",
      "theinteractionsbetweeninputentitiesandrelationshipsare\n",
      "layer. We use Adam as optimiser (Kingma and Ba 2014),\n",
      "modelledbyconvolutionalandfully-connectedlayers. The\n",
      "andlabelsmoothingtolessenoverfittingduetosaturationof\n",
      "maincharacteristicofourmodelisthatthescoreisdefinedby\n",
      "outputnon-linearitiesatthelabels(Szegedyetal.2016).\n",
      "aconvolutionover2Dshapedembeddings. Thearchitecture\n",
      "issummarisedinFigure1;formally,thescoringfunctionis FastEvaluationforLinkPredictionTasks\n",
      "definedasfollows:\n",
      "Inourarchitectureconvolutionconsumesabout75-90%of\n",
      "thetotalcomputationtime,thusitisimportanttominimise\n",
      "ψ (e,e )= f(vec(f([e ;r ]∗ω))W)e, (1)\n",
      "r s o s r o the number of convolution operations to speed up compu-\n",
      "tationasmuchaspossible. Forlinkpredictionmodels,the\n",
      "wherer ∈ Rk isarelationparameterdependingonr, e\n",
      "r s batchsizeisusuallyincreasedtospeedupevaluation(Bordes\n",
      "andr denotea2Dreshapingofe andr,respectively: if\n",
      "r s r etal.2013b). However,thisisnotfeasibleforconvolutional\n",
      "e s,r\n",
      "r\n",
      "∈Rk,thene s,r\n",
      "r\n",
      "∈Rkw×kh,wherek =k wk h.\n",
      "modelssincethememoryrequirementsquicklyoutgrowthe\n",
      "Inthefeed-forwardpass,themodelperformsarow-vector\n",
      "GPUmemorycapacitywhenincreasingthebatchsize.\n",
      "look-upoperationontwoembeddingmatrices,oneforenti-\n",
      "Unlikeotherlinkpredictionmodelswhichtakeanentity\n",
      "ties,denotedE|E|×k andoneforrelations,denotedR|R|×k(cid:48), pairandarelationasatriple(s,r,o),andscoreit(1-1scor-\n",
      "wherekandk(cid:48)aretheentityandrelationembeddingdimen- ing),wetakeone(s,r)pairandscoreitagainstallentities\n",
      "sions, and |E| and |R| denote the number of entities and o ∈ E simultaneously(1-Nscoring). Ifwebenchmark1-1\n",
      "relations. Themodelthenconcatenatese sandr r,andusesit scoringonahigh-endGPUwithbatchsizeandembedding\n",
      "asaninputfora2Dconvolutionallayerwithfiltersω. Such size128,thenatrainingpassandanevaluationwithacon-\n",
      "alayerreturnsafeaturemaptensorT ∈Rc×m×n,wherec volution model on FB15k – one of the dataset used in the\n",
      "isthenumberof2Dfeaturemapswithdimensionsmandn. experiments–takes2.4minutesand3.34hours. Using1-N\n",
      "ThetensorT isthenreshapedintoavectorvec(T)∈Rcmn, scoring,therespectivenumbersare45and35seconds–a\n",
      "whichisthenprojectedintoak-dimensionalspaceusingalin- considerable improvement of over 300x in terms of evalu-\n",
      "eartransformationparametrisedbythematrixW∈Rcmn×k\n",
      "ationtime. Additionally, thisapproachisscalabletolarge\n",
      "andmatchedwiththeobjectembeddinge oviaaninnerprod- knowledgegraphsandincreasesconvergencespeed. Fora\n",
      "uct.Theparametersoftheconvolutionalfiltersandthematrix single forward-backward pass with batch size of 128, go-\n",
      "Wareindependentoftheparametersfortheentitiessando ing from N = 100,000 to N = 1,000,000 entities only\n",
      "andtherelationshipr. increases the computational time from 64ms to 80ms – in\n",
      "For training the model parameters, we apply the lo- otherwords,aten-foldincreaseinthenumberofentitiesonly\n",
      "gistic sigmoid function σ(·) to the scores, that is p = increasesthecomputationtimeby25%–whichatteststhe\n",
      "σ(ψ (e,e )), and minimise the following binary cross- scalabilityoftheapproach.\n",
      "r s o\n",
      "Figure1: IntheConvEmodel,theentityandrelationembeddingsarefirstreshapedandconcatenated(steps1,2);theresulting\n",
      "matrixisthenusedasinputtoaconvolutionallayer(step3);theresultingfeaturemaptensorisvectorisedandprojectedintoa\n",
      "k-dimensionalspace(step4)andmatchedwithallcandidateobjectembeddings(step5).\n",
      "0.9\n",
      "0.2\n",
      "0.1\n",
      "0.6\n",
      "0.2\n",
      "0.3\n",
      "0.0\n",
      "0.7\n",
      "0.1\n",
      "0.4\n",
      "0.4\n",
      "0.4\n",
      "Ifinsteadof1-Nscoring,weuse1-(0.1N)scoring–that relations: alargenumberoftesttriplescanbeobtainedsim-\n",
      "is,scoringagainst10%oftheentities–wecancomputea plybyinvertingtriplesinthetrainingset. Forexample,the\n",
      "forward-backwardpass25%faster. However,weconverge test set frequently contains triples such as (s,hyponym,o)\n",
      "roughly230%sloweronthetrainingset. Thus1-Nscoring while the training set contains its inverse (o,hypernym,s).\n",
      "has an additional effect which is akin to batch normalisa- To create a dataset without this property, Toutanova and\n",
      "tion (Ioffe and Szegedy 2015) – we trade some computa- Chen (2015) introduced FB15k-237 – a subset of FB15k\n",
      "tionalperformanceforgreatlyincreasedconvergencespeed where inverse relations are removed. However, they did\n",
      "andalsoachievebetterperformanceasshowninSection7. notexplicitlyinvestigatetheseverityofthisproblem,which\n",
      "Do note that the technique in general could by applied to mightexplainwhyresearchcontinuesusingthesedatasets\n",
      "any1-1scoringmodel. Thispracticaltrickinspeedingup forevaluationwithoutaddressingthisissue(e.g. Trouillonet\n",
      "training and evaluation can be applied to any 1-1 scoring al.(2016),Nickel,Rosasco,andPoggio(2016),Nguyenet\n",
      "model,suchasthegreatmajorityoflinkpredictionmodels. al.(2016),Liuetal.(2016)).\n",
      "Inthefollowingsection,weintroduceasimplerule-based\n",
      "Experiments modelwhichdemonstratestheseverityofthisbiasbyachiev-\n",
      "ing state-of-the-art results on both WN18 and FB15k. In\n",
      "KnowledgeGraphDatasets\n",
      "ordertoensurethatweevaluateondatasetsthatdonothave\n",
      "Forevaluatingourproposedmodel,weuseaselectionoflink inverserelationtestleakage,weapplyoursimplerule-based\n",
      "predictiondatasetsfromtheliterature. modeltoeachdataset. ApartfromFB15k, whichwascor-\n",
      "WN18(Bordesetal.2013a)isasubsetofWordNetwhich rectedbyFB15k-237, wealsofindflawswithWN18. We\n",
      "consists of 18 relations and 40,943 entities. Most of the thuscreateWN18RRtoreclaimWN18asadataset,which\n",
      "151,442triplesconsistofhyponymandhypernymrelations cannoteasilybecompletedusingasinglerule–butrequires\n",
      "and,forsuchareason,WN18tendstofollowastrictlyhier- modelling of the complete knowledge graph. WN18RR1\n",
      "archicalstructure. contains93,003tripleswith40,943entitiesand11relations.\n",
      "FB15k(Bordesetal.2013a)isasubsetofFreebasewhich Forfutureresearch,werecommendagainstusingFB15kand\n",
      "containsabout14,951entitieswith1,345differentrelations. WN18andinsteadrecommendFB15k-237,WN18RR,and\n",
      "Alargefractionofcontentinthisknowledgegraphdescribes YAGO3-10.\n",
      "factsaboutmovies,actors,awards,sports,andsportteams.\n",
      "ExperimentalSetup\n",
      "YAGO3-10(Mahdisoltani,Biega,andSuchanek2015)is\n",
      "asubsetofYAGO3whichconsistsofentitieswhichhavea We selected the hyperparameters of our ConvE model via\n",
      "minimumof10relationseach. Ithas123,182entitiesand37 grid search according to the mean reciprocal rank (MRR)\n",
      "relations. Mostofthetriplesdealwithdescriptiveattributes on the validation set. Hyperparameter ranges for the grid\n",
      "ofpeople,suchascitizenship,gender,andprofession. searchwereasfollows–embeddingdropout{0.0,0.1,0.2},\n",
      "Countries (Bouchard, Singh, and Trouillon 2015) is a feature map dropout {0.0,0.1,0.2,0.3}, projection layer\n",
      "benchmarkdatasetthatisusefultoevaluateamodel’sabil- dropout {0.0,0.1,0.3,0.5}, embedding size {100,200},\n",
      "ity to learn long-range dependencies between entities and batchsize{64,128,256},learningrate{0.001,0.003},and\n",
      "relations. It consists of three sub-tasks which increase in labelsmoothing{0.0,0.1,0.2,0.3}.\n",
      "difficultyinastep-wisefashion,wheretheminimumpath- Besidesthegridsearch,weinvestigatedmodificationsof\n",
      "lengthtofindasolutionincreasesfrom2to4. the2Dconvolutionlayerforourmodels. Inparticular,we\n",
      "It was first noted by Toutanova and Chen (2015) that\n",
      "WN18andFB15ksufferfromtestleakagethroughinverse 1https://github.com/TimDettmers/ConvE\n",
      "Table2: ParameterscalingofDistMultvsConvE. Togaugetheseverityofthisproblem,weconstructasim-\n",
      "ple, rule-based model that solely models inverse relations.\n",
      "Wecallthismodeltheinversemodel. Themodelextractsin-\n",
      "Param. Emb. Hits\n",
      "verserelationshipsautomaticallyfromthetrainingset: given\n",
      "Model count size MRR @10 @3 @1\n",
      "two relation pairs r,r ∈ R, we check whether (s,r,o)\n",
      "1 2 1\n",
      "DistMult 1.89M 128.23.41.25.15 implies(o,r,s),orvice-versa.\n",
      "2\n",
      "DistMult 0.95M 64.22.39.25.14 Weassumethatinverserelationsarerandomlydistributed\n",
      "DistMult 0.23M 16.16.31.17.09 amongthetraining,validationandtestsetsand,assuch,we\n",
      "expectthenumberofinverserelationstobeproportionalto\n",
      "ConvE 5.05M 200.32.49.35.23\n",
      "thesizeofthetrainingsetcomparedtothetotaldatasetsize.\n",
      "ConvE 1.89M 96.32.49.35.23\n",
      "Thus,wedetectinverserelationsifthepresenceof(s,r,o)\n",
      "ConvE 0.95M 54.30.46.33.22 1\n",
      "co-occurswiththepresenceof(o,r,s)withafrequencyof\n",
      "ConvE 0.46M 28.28.43.30.20 2\n",
      "atleast0.99−(f +f ),wheref andf isthefractionof\n",
      "ConvE 0.23M 14.26.40.28.19 v t v t\n",
      "thevalidationandtestsetcomparedtothetotalsizeofthe\n",
      "dataset. Relationsmatchingthiscriterionareassumedtobe\n",
      "theinverseofeachother.\n",
      "experimentedwithreplacingitwithfullyconnectedlayers Attesttime,wecheckifthetesttriplehasinversematches\n",
      "and 1D convolution; however, these modifications consis- outside the test set: if k matches are found, we sample a\n",
      "tentlyreducedthepredictiveaccuracyofthemodel. Wealso permutationofthetopkranksforthesematches;ifnomatch\n",
      "experimentedwithdifferentfiltersizes, andfoundthatwe isfound,weselectarandomrankforthetesttriple.\n",
      "onlyreceivegoodresultsifthefirstconvolutionallayeruses\n",
      "Results\n",
      "small(i.e. 3x3)filters.\n",
      "Wefoundthatthefollowingcombinationofparameters Similarlytopreviouswork(Yangetal.2015;Trouillonetal.\n",
      "works well on WN18, YAGO3-10 and FB15k: embed- 2016;Niepert2016),wereportresultsusingafilteredsetting,\n",
      "dingdropout0.2,featuremapdropout0.2,projectionlayer i.e. we rank test triples against all other candidate triples\n",
      "dropout0.3, embeddingsize200, batchsize128, learning notappearinginthetraining,validation,ortestset(Bordes\n",
      "rate0.001,andlabelsmoothing0.1.FortheCountriesdataset, etal.2013a). Candidatesareobtainedbypermutingeither\n",
      "weincreaseembeddingdropoutto0.3,hiddendropoutto0.5, the subject or the object of a test triple with all entities in\n",
      "andsetlabelsmoothingto0.Weuseearlystoppingaccording the knowledge graph. Our results on the standard bench-\n",
      "tothemeanreciprocalrank(WN18,FB15k,YAGO3-10)and marksFB15kandWN18areshowninTable3;resultsonthe\n",
      "AUC-PR(Countries)statisticsonthevalidationset,which datasetswithinverserelationsremovedareshowninTable4;\n",
      "we evaluate every three epochs. Unlike the other datasets, resultsonYAGO3-10andCountriesareshowninTable5.\n",
      "for Countries the results have a high variance, as such we Strikingly,theinversemodelachievesstate-of-the-arton\n",
      "average10runsandproduce95%confidenceintervals. For many different metrics for both FB15k and WN18. How-\n",
      "ourDistMultandComplExresultswith1-1training,weuse ever, it fails to pick up on inverse relations for YAGO3-\n",
      "an embedding size of 100, AdaGrad (Duchi, Hazan, and 10andFB15k-237. TheprocedureusedbyToutanovaand\n",
      "Singer2011)foroptimisation,andweregulariseourmodel Chen(2015)toderiveFB15k-237doesnotremovecertain\n",
      "byforcingtheentityembeddingstohaveaL2normof1after symmetricrelationships,forexample“similarto”. Thepres-\n",
      "eachparameterupdate. AsinBordesetal.(2013a),weusea ence of these relationships explains the good score of our\n",
      "pairwisemargin-basedrankingloss. inverse model on WN18RR, which was derived using the\n",
      "Thecodeforourmodelandexperimentsismadepublicly sameprocedure.\n",
      "available,2 aswellasthecodeforreplicatingtheDistMult Ourproposedmodel,ConvE,achievesstate-of-the-artper-\n",
      "results.3 formanceforallmetricsonYAGO3-10,forsomemetricson\n",
      "FB15k,anditdoeswellonWN18. OnCountries,itsolves\n",
      "InverseModel theS1andS2tasks,anddoeswellonS3,scoringbetterthan\n",
      "It has been noted by Toutanova and Chen (2015), that the othermodelslikeDistMultandComplEx.\n",
      "trainingdatasetsofWN18andFB15khave94%and81% For FB15k-237, we could not replicate the basic model\n",
      "test leakage as inverse relations, that is, 94% and 81% of results from Toutanova et al. (2015), where the models in\n",
      "thetriplesinthesedatasetshaveinverserelationswhichare generalhavebetterperformancethanwhatwecanachieve.\n",
      "linked to the test set. For instance, a test triple (feline, hy- ComparedtoSchlichtkrulletal.(2017),ourresultsforstan-\n",
      "ponym, cat) can easily be mapped to a training triple (cat, dard models are a slightly better then theirs, and on-a-par\n",
      "hypernym,feline)ifitisknownthathyponymistheinverse withtheirR-GCNmodel.\n",
      "ofhypernym. Thisishighlyproblematic,becauselinkpre-\n",
      "ParameterefficiencyofConvE\n",
      "dictorsthatdowellonthesedatasetsmaysimplylearnwhich\n",
      "relationsthataretheinverseofothers,ratherthantomodel From Table 2 we can see that ConvE for FB15k-237 with\n",
      "theactualknowledgegraph. 0.23MparametersperformsbetterthanDistMultwith1.89M\n",
      "parametersfor3metricsoutof5.\n",
      "2https://github.com/TimDettmers/ConvE ConvEwith0.46Mparametersstillachievesstate-of-the-\n",
      "3https://github.com/uclmr/inferbeddings artresultsonFB15k-237with0.425Hits@10. Compar<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  20413,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Link prediction', '1-N scoring']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  dard models are a slightly better then theirs, and on-a-par\n",
      "hypernym,feline)ifitisknownthathyponymistheinverse withtheirR-GCNmodel.\n",
      "ofhypernym. Thisishighlyproblematic,becauselinkpre-\n",
      "ParameterefficiencyofConvE\n",
      "dictorsthatdowellonthesedatasetsmaysimplylearnwhich\n",
      "relationsthataretheinverseofothers,ratherthantomodel From Table 2 we can see that ConvE for FB15k-237 with\n",
      "theactualknowledgegraph. 0.23MparametersperformsbetterthanDistMultwith1.89M\n",
      "parametersfor3metricsoutof5.\n",
      "2https://github.com/TimDettmers/ConvE ConvEwith0.46Mparametersstillachievesstate-of-the-\n",
      "3https://github.com/uclmr/inferbeddings artresultsonFB15k-237with0.425Hits@10. Comparingto\n",
      "Table3: LinkpredictionresultsforWN18andFB15k\n",
      "WN18 FB15k\n",
      "Hits Hits\n",
      "MR MRR @10 @3 @1 MR MRR @10 @3 @1\n",
      "DistMult(Yangetal.2015) 902.822.936.914.728 97.654.824.733.546\n",
      "ComplEx(Trouillonetal.2016) –.941.947.936.936 –.692.840.759.599\n",
      "Gaifman(Niepert2016) 352 –.939 –.761 75 –.842 –.692\n",
      "ANALOGY(Liu,Wu,andYang2017) –.942.947.944.939 –.725.854.785.646\n",
      "R-GCN(Schlichtkrulletal.2017) –.814.964.929.697 –.696.842.760.601\n",
      "ConvE 374.943.956.946.935 51.657.831.723.558\n",
      "InverseModel 740.963.964.964.953 2501.660.660.659.658\n",
      "Table4: LinkpredictionresultsforWN18RRandFB15k-237\n",
      "WN18RR FB15k-237\n",
      "Hits Hits\n",
      "MR MRR @10 @3 @1 MR MRR @10 @3 @1\n",
      "DistMult(Yangetal.2015) 5110.43.49.44.39 254.241.419.263.155\n",
      "ComplEx(Trouillonetal.2016) 5261.44.51.46.41 339.247.428.275.158\n",
      "R-GCN(Schlichtkrulletal.2017) – – – – – –.248.417.258.153\n",
      "ConvE 4187.43.52.44.40 244.325.501.356.237\n",
      "InverseModel 13526.35.35.35.35 7030.010.014.011.007\n",
      "thepreviousbestmodel,R-GCN(Schlichtkrulletal.2017), business people) and successful modelling of such a high\n",
      "whichachieves0.417Hits@10withmorethan8Mparame- indegreenodesrequirescapturingallthesedifferences. Our\n",
      "ters. hypothesisisthatdeepermodels,thatis,modelsthatlearn\n",
      "Overall,ConvEismorethan17xparameterefficientthan multiplelayersoffeatures,likeConvE,haveanadvantage\n",
      "R-GCNs,and8xmoreparameterefficientthanDistMult. For overshallowmodels,likeDistMult,tocaptureallthesecon-\n",
      "theentiretyofFreebase,thesizeofthesemodelswouldbe straints.\n",
      "morethan82GBforR-GCNs,21GBforDistMult,compared However,deepermodelsaremoredifficulttooptimise,so\n",
      "to5.2GBforConvE. wehypothesisethatfordatasetswithlowaveragerelation-\n",
      "specific indegree (like WN18RR and WN18), a shallow\n",
      "Analysis modellikeDistMultmightsufficeforaccuratelyrepresenting\n",
      "thestructureofthenetwork.\n",
      "AblationStudy\n",
      "To test our two hypotheses, we take two datasets with\n",
      "Table 7 shows the results from our ablation study where low(low-WN18)andhigh(high-FB15k)relation-specificin-\n",
      "we evaluate different parameter initialisation (n = 2) to degreeandreversethemintohigh(high-WN18)andlow(low-\n",
      "calculateconfidenceintervals. Weseethathiddendropoutis FB15k)relation-specificindegreedatasetsbydeletinglow\n",
      "byfarthemostimportantcomponent,whichisunsurprising andhighindegreenodes. Wehypothesisethat,comparedto\n",
      "since it is our main regularisation technique. 1-N scoring DistMult,ConvEwillalwaysdobetteronthedatasetwith\n",
      "improvesperformance,asdoesinputdropout,featuremap highrelation-specificindegree,andvice-versa.\n",
      "dropouthasaminoreffect,whilelabelsmoothingseemsto Indeed,wefindthatbothhypotheseshold: forlow-FB15k\n",
      "beunimportant–asgoodresultscanbeachievedwithoutit. wehaveConvE0.586Hits@10vsDistMult0.728Hits@10;\n",
      "forhigh-WN18wehaveConvE0.952Hits@10vsDistMult\n",
      "0.938 Hits@10. This supports our hypothesis that deeper\n",
      "AnalysisofIndegreeandPageRank\n",
      "models such as ConvE have an advantage to model more\n",
      "Ourmainhypothesisforthegoodperformanceofourmodel complexgraphs(e.g. FB15kandFB15k-237),butthatshal-\n",
      "on datasets like YAGO3-10 and FB15k-237 compared to low models such as DistMult have an advantage to model\n",
      "WN18RR,isthatthesedatasetscontainnodeswithveryhigh lesscomplexgraphs(e.g. WN18WN18RR).\n",
      "relation-specific indegree. For example the node “United Toinvestigatethisfurther,welookatPageRank,ameasure\n",
      "States” with edges “was born in” has an indegree of over of centrality of a node. PageRank can also be seen as a\n",
      "10,000. Many of these 10,000 nodes will be very differ- measureoftherecursiveindegreeofanode: thePageRank\n",
      "entfromeachother(actors,writers,academics,politicians, valueofanodeisproportionaltotheindegreeofthisnode,its\n",
      "Table5: LinkpredictionresultsforYAGO3-10andCountries\n",
      "YAGO3-10 Countries\n",
      "Hits AUC-PR\n",
      "MR MRR @10 @3 @1 S1 S2 S3\n",
      "DistMult(Yangetal.2015) 5926.34.54.38.24 1.00±0.00 0.72±0.12 0.52±0.07\n",
      "ComplEx(Trouillonetal.2016) 6351.36.55.40.26 0.97±0.02 0.57±0.10 0.43±0.07\n",
      "ConvE 1676.44.62.49.35 1.00±0.00 0.99±0.01 0.86±0.05\n",
      "InverseModel 59448.01.02.02.01 – – –\n",
      "Table6: MeanPageRank×10−3 ofnodesinthetestsetvs error reduction of ConvE compared to DistMult is strong\n",
      "reductioninerrorintermsofAUC-PRorHits@10ofConvE withr = 0.56. Thisgivesadditionalevidencethatmodels\n",
      "wrt. DistMult. that are deeper have an advantage when modelling nodes\n",
      "withhigh(recursive)indegree.\n",
      "Dataset PageRank ErrorReduction Fromthisevidenceweconclude,thattheincreasedperfor-\n",
      "manceofourmodelcomparedtoastandardlinkpredictor,\n",
      "WN18RR 0.104 0.06\n",
      "DistMult, can be partially explained due to our it’s ability\n",
      "WN18 0.125 0.45\n",
      "tomodelnodeswithhighindegreewithgreaterprecision–\n",
      "FB15k 0.599 0.04\n",
      "whichispossiblyrelatedtoitsdepth.\n",
      "FB15-237 0.733 0.16\n",
      "YAGO3-10 0.988 0.21\n",
      "ConclusionandFutureWork\n",
      "CountriesS3 1.415 2.36\n",
      "CountriesS1 1.711 0.00 WeintroducedConvE,alinkpredictionmodelthatuses2D\n",
      "CountriesS2 1.796 17.6 convolution over embeddings and multiple layers of non-\n",
      "linearfeaturestomodelknowledgegraphs.ConvEusesfewer\n",
      "parameters; it is fast through 1-N scoring; it is expressive\n",
      "Table7: AblationstudyforFB15k-237.\n",
      "through multiple layers of non-linear features; it is robust\n",
      "to overfitting dueto batchnormalisation anddropout; and\n",
      "Ablation Hits@10 achievesstate-of-the-artresultsonseveraldatasets,whilestill\n",
      "FullConvE 0.491 scalingtolargeknowledgegraphs. Inouranalysis,weshow\n",
      "thattheperformanceofConvEcomparedtoacommonlink\n",
      "Hiddendropout -0.044±0.003\n",
      "predictor,DistMult,canpartiallybeexplainedbyitsability\n",
      "Inputdropout -0.022±0.000\n",
      "tomodelnodeswithhigh(recursive)indegree.\n",
      "1-Nscoring -0.019\n",
      "TestleakagethroughinverserelationsofWN18andFB15k\n",
      "Featuremapdropout -0.013±0.001\n",
      "wasfirstreportedbyToutanovaandChen(2015): weinvesti-\n",
      "Labelsmoothing -0.008±0.000\n",
      "gatetheseverityofthisproblemforcommonlyuseddatasets\n",
      "by introducing a simple rule-based model, and find that it\n",
      "canachievestate-of-the-artresultsonWN18andFB15k. To\n",
      "ensurerobustversionsofallinvestigateddatasetsexists,we\n",
      "neighboursindegrees,itsneighbours-neighboursindegrees\n",
      "deriveWN18RR.\n",
      "andsoforthscaledrelativetoallothernodesinthenetwork.\n",
      "Bythislineofreasoning,wealsoexpectConvEtobebetter Ourmodelisstillshallowcomparedtoconvolutionalarchi-\n",
      "thanDistMultondatasetswithhighaveragePageRank(high tecturefoundincomputervision,andfutureworkmightdeal\n",
      "connectivitygraphs),andvice-versa. withconvolutionalmodelsofincreasingdepth. Furtherwork\n",
      "mightalsolookattheinterpretationof2Dconvolution,or\n",
      "Totestthishypothesis,wecalculatethePageRankforeach\n",
      "howtoenforcelarge-scalestructureinembeddingspaceso\n",
      "dataset as a measure of centrality. We find that the most\n",
      "toincreasethenumberofinteractionsbetweenembeddings.\n",
      "central nodes in WN18 have a PageRank value more than\n",
      "oneorderofmagnitudesmallerthanthemostcentralnodes\n",
      "Acknowledgements\n",
      "inYAGO3-10andCountries,andabout4timessmallerthan\n",
      "themostcentralnodesinFB15k. Whenwelookatthemean We would like to thank Johannes Welbl, Peter Hayes, and\n",
      "PageRank of nodes contained in the test sets, we find that TakumaEbisufortheirfeedbackandhelpfuldiscussionsre-\n",
      "thedifferenceofperformanceintermsofHits@10between latedtothiswork. WethankTakumaEbisuforpointingout\n",
      "DistMult and ConvE is roughly proportional to the mean anerrorinourInverseModelscript–thecorrectedresults\n",
      "test set PageRank, that is, the higher the mean PageRank areslightlybetterforWN18andslightlyworseforFB15k.\n",
      "of the test set nodes the better ConvE does compared to WethankVictoriaLinforhelpingustounrootandfixabug\n",
      "DistMult, and vice-versa. See Table 6 for these statistics. where the exclusion of triples during inference worked in-\n",
      "ThecorrelationbetweenmeantestsetPageRankandrelative correctly–thechangesdidnotaffectthemainresultsinthis\n",
      "work,thoughsomeresultsintheappendixchanged(UMLS, [2014] Kingma,D.,andBa,J. 2014. Adam: Amethodfor\n",
      "Nations). This work was supported by a Marie Curie Ca- stochasticoptimization. arXivpreprintarXiv:1412.6980.\n",
      "reerIntegrationAward,anAllenDistinguishedInvestigator [2016] Kipf,T.N.,andWelling,M. 2016. Semi-Supervised\n",
      "Award,aGoogleEuropeScholarshipforStudentswithDis- ClassificationwithGraphConvolutionalNetworks. InPro-\n",
      "abilities,andtheH2020projectSUMMA. ceedingsofICLR2016.\n",
      "[2012] Krizhevsky,A.;Sutskever,I.;andHinton,G.E. 2012.\n",
      "References\n",
      "ImageNet Classification with Deep Convolutional Neural\n",
      "[2013a] Bordes,A.;Usunier,N.;García-Durán,A.;Weston, Networks. InProceedingsofNIPS2012,1097–1105.\n",
      "J.;andYakhnenko,O. 2013a. TranslatingEmbeddingsfor [2015] Krompaß, D.; Baier, S.; and Tresp, V. 2015. Type-\n",
      "Modeling Multi-relational Data. In Proceedings of NIPS, ConstrainedRepresentationLearninginKnowledgeGraphs.\n",
      "2787–2795. InProceedingsofISWC2015,640–655.\n",
      "[2013b] Bordes,A.;Usunier,N.;García-Durán,A.;Weston, [2016] Liu,Q.;Jiang,L.;Han,M.;Liu,Y.;andQin,Z. 2016.\n",
      "J.;andYakhnenko,O. 2013b. TranslatingEmbeddingsfor Hierarchicalrandomwalkinferenceinknowledgegraphs. In\n",
      "Modeling Multi-relational Data. In Proceedings of NIPS, Proceedingsofthe39thInternationalACMSIGIRconference\n",
      "2787–2795. onResearchandDevelopmentinInformationRetrieval,445–\n",
      "454. ACM.\n",
      "[2014] Bordes, A.; Glorot, X.; Weston, J.; and Bengio, Y.\n",
      "2014. Asemanticmatchingenergyfunctionforlearningwith [2017] Liu, H.; Wu, Y.; and Yang, Y. 2017. Analogical\n",
      "multi-relationaldata-applicationtoword-sensedisambigua- InferenceforMulti-RelationalEmbeddings. ArXive-prints.\n",
      "tion. MachineLearning94(2):233–259. [2015] Mahdisoltani,F.;Biega,J.;andSuchanek,F.M. 2015.\n",
      "[2015] Bouchard, G.; Singh, S.; and Trouillon, T. 2015. YAGO3: AKnowledgeBasefromMultilingualWikipedias.\n",
      "On approximate reasoning capabilities of low-rank vector InProceedingsofCIDR2015.\n",
      "spaces. AAAISpringSyposiumonKnowledgeRepresenta- [2016] Nguyen, D. Q.; Sirts, K.; Qu, L.; and Johnson,\n",
      "tionandReasoning(KRR):IntegratingSymbolicandNeural M. 2016. Stranse: a novel embedding model of enti-\n",
      "Approaches. ties and relationships in knowledge bases. arXiv preprint\n",
      "[2011] Collobert, R.; Weston, J.; Bottou, L.; Karlen, M.; arXiv:1606.08140.\n",
      "Kavukcuoglu, K.; and Kuksa, P. P. 2011. Natural Lan- [2016] Nickel,M.;Murphy,K.;Tresp,V.;andGabrilovich,E.\n",
      "guageProcessing(Almost)fromScratch.JournalofMachine 2016. Areviewofrelationalmachinelearningforknowledge\n",
      "LearningResearch12:2493–2537. graphs. ProceedingsoftheIEEE104(1):11–33.\n",
      "[2016] Defferrard, M.; Bresson, X.; and Vandergheynst, P. [2016] Nickel,M.;Rosasco,L.;andPoggio,T.A.2016.Holo-\n",
      "2016. ConvolutionalNeuralNetworksonGraphswithFast graphicEmbeddingsofKnowledgeGraphs. InProceedings\n",
      "LocalizedSpectralFiltering. InProceedingsofNIPS,3837– ofAAAI,1955–1961.\n",
      "3845.\n",
      "[2016] Niepert,M. 2016. DiscriminativeGaifmanModels.\n",
      "[2014] Dong,X.;Gabrilovich,E.;Heitz,G.;Horn,W.;Lao, InProceedingsofNIPS2016,3405–3413.\n",
      "N.;Murphy,K.;Strohmann,T.;Sun,S.;andZhang,W. 2014. [2017] Schlichtkrull, M.; Kipf, T. N.; Bloem, P.; Berg, R.\n",
      "KnowledgeVault: AWeb-ScaleApproachtoProbabilistic v.d.;Titov,I.;andWelling,M. 2017. ModelingRelational\n",
      "KnowledgeFusion. InProceedingsofKDD2014,601–610. Data with Graph Convolutional Networks. arXiv preprint\n",
      "[2011] Duchi,J.C.;Hazan,E.;andSinger,Y. 2011. Adap- arXiv:1703.06103.\n",
      "tive subgradient methods for online learning and stochas- [2014] Shen, Y.; He, X.; Gao, J.; Deng, L.; and Mesnil, G.\n",
      "tic optimization. Journal of Machine Learning Research 2014. LearningSemanticRepresentationsUsingConvolu-\n",
      "12:2121–2159. tionalNeuralNetworksforWebSearch. InProceedingsof\n",
      "[2015] Duvenaud, D. K.; Maclaurin, D.; Aguilera- WWW2014,373–374.\n",
      "Iparraguirre, J.; Bombarell, R.; Hirzel, T.; Aspuru-Guzik, [2014] Srivastava, N.; Hinton, G. E.; Krizhevsky, A.;\n",
      "A.; and Adams, R. P. 2015. Convolutional Networks on Sutskever, I.; and Salakhutdinov, R. 2014. Dropout: A\n",
      "GraphsforLearningMolecularFingerprints. InProceedings SimpleWaytoPreventNeuralNetworksfromOverfitting.\n",
      "ofNIPS2015,2224–2232. JournalofMachineLearningResearch15(1):1929–1958.\n",
      "[2015] Ioffe,S.,andSzegedy,C. 2015. BatchNormalization: [2015] Szegedy,C.;Liu,W.;Jia,Y.;Sermanet,P.;Reed,S.;\n",
      "AcceleratingDeepNetworkTrainingbyReducingInternal Anguelov,D.;Erhan,D.;Vanhoucke,V.;andRabinovich,A.\n",
      "CovariateShift. arXivpreprintarXiv:1502.03167. 2015. Goingdeeperwithconvolutions. InProceedingsof\n",
      "[2014] Kalchbrenner, N.; Grefenstette, E.; andBlunsom, P. IEEECVPR,1–9.\n",
      "2014. AConvolutionalNeuralNetworkforModellingSen- [2016] Szegedy,C.;Vanhoucke,V.;Ioffe,S.;Shlens,J.;and\n",
      "tences. InProceedingsofACL2014,Volume1: LongPapers, Wojna,Z. 2016. RethinkingtheInceptionArchitecturefor\n",
      "655–665. Computer Vision. In Proceedings of IEEE CVPR, 2818–\n",
      "[2014] Kim, Y. 2014. Convolutional Neural Networks for 2826.\n",
      "Sentence Classification. In Proceedings of EMNLP 2014, [2015] Toutanova,K.,andChen,D. 2015. ObservedVersus\n",
      "1746–1751. LatentFeaturesforKnowledgeBaseandTextInference. In\n",
      "Proceedingsofthe3rdWorkshoponContinuousVectorSpace Table8: ConvElinkpredictionresultsforUMLS,Nations,\n",
      "ModelsandtheirCompositionality,57–66. andKinship.\n",
      "[2015] Toutanova,K.;Chen,D.;Pantel,P.;Poon,H.;Choud-\n",
      "hury,P.;andGamon,M. 2015. RepresentingTextforJoint Hits\n",
      "EmbeddingofTextandKnowledgeBases. InProceedingsof Dataset Model MR MRR @10 @3 @1\n",
      "EMNLP2015,volume15,1499–1509.\n",
      "UMLS ConvE 1.94.99.96.92\n",
      "[2016] Trouillon,T.;Welbl,J.;Riedel,S.;Gaussier,É.;and\n",
      "Kinship ConvE 2.83.98.92.74\n",
      "Bouchard,G. 2016. ComplexEmbeddingsforSimpleLink\n",
      "Prediction. InProceedingsofICML2016,2071–2080.\n",
      "[2015] Yang,B.;Yih,W.;He,X.;Gao,J.;andDeng,L. 2015.\n",
      "the test set. Following Bordes et al. (2013a), for the i-th\n",
      "EmbeddingEntitiesandRelationsforLearningandInference\n",
      "testtriplex inT, wegenerateallitspossiblecorruptions\n",
      "inKnowledgeBases. InProceedingsofICLR2015. i\n",
      "Cs(x ) (resp. Co(x )) – obtained by replacing its subject\n",
      "i i\n",
      "[2011] Yih, W.; Toutanova, K.; Platt, J. C.; and Meek, C.\n",
      "(resp. object)withanyotherentityintheKnowledgeGraph\n",
      "2011.LearningDiscriminativeProjectionsforTextSimilarity\n",
      "–tocheckwhetherthemodelassignsanhigherscoretox\n",
      "i\n",
      "Measures. InProceedingsofCoNLL2011,247–256.\n",
      "and a lower score to its corruptions. Note that the set of\n",
      "corruptionscanalsocontainseveraltruetriples,anditisnot\n",
      "SUPPLEMENTAL MATERIAL\n",
      "amistaketorankthemwithanhigherscorethanx. Forsuch\n",
      "i\n",
      "areason,weremovealltriplesinthegraphfromthesetof\n",
      "Versions\n",
      "corruptions:thisisreferredtoasthefilteredsettinginBordes\n",
      "• 2018-07-04: etal.(2013a). Theleftandrightrankofthei-thtesttriple–\n",
      "– Added new YAGO3-10 results. The new results are eachassociatedtocorruptingeitherthesubjectortheobject–\n",
      "worseonmostmetrics,butstate-of-the-artresultsare accordingtoamodelwithscoringfunctionψ(·),aredefined\n",
      "retained. asfollows:\n",
      "– IwasunabletoreplicateFB15kscoresthatIinitially (cid:88)\n",
      "ranks =1+ I[ψ(x )<ψ(x˜ )],\n",
      "reported.4 i i i\n",
      "– IupdatethePageRanktableandthereportedPageRank-\n",
      "x˜i∈Cs(xi)\\G\n",
      "(cid:88)\n",
      "error-reductioncorrelationtoreflectthenewscores. ranko =1+ I[ψ(x )<ψ(x˜ )],\n",
      "i i i\n",
      "– I removed the Nations scores in the appendix. The x˜i∈Co(xi)\\G\n",
      "Nationsdatasethasahighproportionofinverserelation-\n",
      "shipsandisthusnotsuitablefortheuseinresearch. I\n",
      "whereI[P]is1ifftheconditionP istrue,and0otherwise.\n",
      "donotwanttoencourageitsuse. Formeasuringthequalityoftheranking,weusetheMean\n",
      "ReciprocalRank(MRR)andtheHits@kmetrics,whichare\n",
      "• 2018-04-06: VictoriaLinhelpedustofindandfixissues5\n",
      "definedasfollows:\n",
      "withtriplemasksduringevaluation. Wereportnewnum-\n",
      "bersforUMLSandNationsintheappendix. Theresults 1 (cid:88) 1 1\n",
      "MRR: +,\n",
      "wereunchangedonotherdatasetsthatwetestedthusfar 2|T| ranks ranko\n",
      "(Kinship,WN18,WN18RR,FB15k-237). 100\n",
      "x (cid:88)i∈T i i\n",
      "Hits@k(%): I[ranks ≤k]+I[ranko ≤k].\n",
      "• 2018-03-28: 2|T| i i\n",
      "– NewnumbersforInverseModelafterbugfixbyTakuma\n",
      "xi∈T\n",
      "Ebisu. MRRistheaverageinverserankforalltesttriples:thehigher,\n",
      "thebetter. Hits@kisthepercentageofrankslowerthanor\n",
      "– New numbersfor UMLS/Nations/Kinship datasets in\n",
      "equaltok: thehigher,thebetter.\n",
      "appendixusingthemostcommonlyreportedtestdata\n",
      "splits.\n",
      "• 2018-01-07: ExtendedAAAIcameraready(6/7/8).\n",
      "• 2017-07-08: Missinggrantinacknowledgements.\n",
      "• 2017-07-05: OriginalNIPSsubmission(6/7/6).\n",
      "FurtherConvEresults\n",
      "EvaluationMetrics\n",
      "Wenowdescribetheevaluationmetricsusedforassessingthe\n",
      "qualityofthemodels. LetT = {x,x,...,x }denote\n",
      "1 2 |T|\n",
      "4See https://github.com/TimDettmers/ConvE/\n",
      "issues/26fordetails.\n",
      "5See https://github.com/TimDettmers/ConvE/\n",
      "issues/18formoreinformation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  20212,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Link prediction']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Convolutional 2D Knowledge Graph Embeddings\n",
      "TimDettmers∗\n",
      "UniversitàdellaSvizzeraitaliana\n",
      "tim.dettmers@gmail.com\n",
      "PasqualeMinervini PontusStenetorp SebastianRiedel\n",
      "UniversityCollegeLondon\n",
      "{p.minervini,p.stenetorp,s.riedel}@cs.ucl.ac.uk\n",
      "Abstract can contain millions of facts; as a consequence, link pre-\n",
      "dictors should scale in a manageable way with respect to\n",
      "Linkpredictionforknowledgegraphsisthetaskofpredict- boththenumberofparametersandcomputationalcoststobe\n",
      "ingmissingrelationshipsbetweenentities.Previousworkon applicableinreal-worldscenarios.\n",
      "link prediction has focused on shallow, fast models which\n",
      "Forsolvingsuchscalingproblems,linkpredictionmodels\n",
      "canscaletolargeknowledgegraphs.However,thesemodels\n",
      "areoftencomposedofsimpleoperations,likeinnerproducts\n",
      "learnlessexpressivefeaturesthandeep,multi-layermodels–\n",
      "and matrix multiplications over an embedding space, and\n",
      "whichpotentiallylimitsperformance.Inthisworkweintro-\n",
      "duceConvE,amulti-layerconvolutionalnetworkmodelfor use a limited number of parameters (Nickel et al. 2016).\n",
      "linkprediction,andreportstate-of-the-artresultsforseveral DistMult(Yangetal.2015)issuchamodel,characterised\n",
      "establisheddatasets.Wealsoshowthatthemodelishighlypa- by three-way interactions between embedding parameters,\n",
      "rameterefficient,yieldingthesameperformanceasDistMult whichproduceonefeatureperparameter. Usingsuchsimple,\n",
      "andR-GCNwith8xand17xfewerparameters. Analysisof fast,shallowmodelsallowsonetoscaletolargeknowledge\n",
      "ourmodelsuggeststhatitisparticularlyeffectiveatmodelling graphs,atthecostoflearninglessexpressivefeatures.\n",
      "nodes with high indegree – which are common in highly-\n",
      "Theonlywaytoincreasethenumberoffeaturesinshallow\n",
      "connected,complexknowledgegraphssuchasFreebaseand\n",
      "models–andthustheirexpressiveness–istoincreasethe\n",
      "YAGO3. Inaddition,ithasbeennotedthattheWN18and\n",
      "FB15kdatasetssufferfromtestsetleakage, duetoinverse embeddingsize. However,doingsodoesnotscaletolarger\n",
      "relationsfromthetrainingsetbeingpresentinthetestset– knowledgegraphs,sincethetotalnumberofembeddingpa-\n",
      "however,theextentofthisissuehassofarnotbeenquantified. rameters is proportional to the the number of entities and\n",
      "Wefindthisproblemtobesevere:asimplerule-basedmodel relations in the graph. For example, a shallow model like\n",
      "canachievestate-of-the-artresultsonbothWN18andFB15k. DistMult with an embedding size of 200, applied to Free-\n",
      "Toensurethatmodelsareevaluatedondatasetswheresimply base, will need 33 GB of memory for its parameters. To\n",
      "exploitinginverserelationscannotyieldcompetitiveresults,\n",
      "increase the number of features independently of the em-\n",
      "weinvestigateandvalidateseveralcommonlyuseddatasets\n",
      "beddingsizerequirestheuseofmultiplelayersoffeatures.\n",
      "–derivingrobustvariantswherenecessary.Wethenperform\n",
      "However,previousmulti-layerknowledgegraphembedding\n",
      "experimentsontheserobustdatasetsforourownandseveral\n",
      "architectures,thatfeaturefullyconnectedlayers,areprone\n",
      "previouslyproposedmodels, andfindthatConvEachieves\n",
      "state-of-the-artMeanReciprocalRankacrossmostdatasets. tooverfit(Nickeletal.2016). Onewaytosolvethescaling\n",
      "problemofshallowarchitectures,andtheoverfittingproblem\n",
      "of fully connected deep architectures, is to use parameter\n",
      "Introduction efficient, fast operators which can be composed into deep\n",
      "networks.\n",
      "Knowledge graphs are graph-structured knowledge bases,\n",
      "The convolution operator, commonly used in computer\n",
      "where facts are represented in the form of relationships\n",
      "vision,hasexactlytheseproperties: itisparameterefficient\n",
      "(edges)betweenentities(nodes). Theyhaveimportantap-\n",
      "and fast to compute, due to highly optimised GPU imple-\n",
      "plications in search, analytics, recommendation, and data\n",
      "mentations. Furthermore,duetoitsubiquitoususe,robust\n",
      "integration – however, they tend to suffer from incom-\n",
      "methodologieshavebeenestablishedtocontroloverfitting\n",
      "pleteness, that is, missing links in the graph. For exam-\n",
      "whentrainingmulti-layerconvolutionalnetworks(Szegedy\n",
      "ple, in Freebase and DBpedia more than 66% of the per-\n",
      "etal.2015;IoffeandSzegedy2015;Srivastavaetal.2014;\n",
      "son entries are missing a birthplace (Dong et al. 2014;\n",
      "Szegedyetal.2016).\n",
      "Krompaß, Baier, and Tresp 2015). Identifying such miss-\n",
      "InthispaperweintroduceConvE,amodelthatuses2D\n",
      "inglinksisreferredtoaslinkprediction. Knowledgegraphs\n",
      "convolutions over embeddings to predict missing links in\n",
      "knowledgegraphs. ConvEisthesimplestmulti-layercon-\n",
      "∗ThisworkwasconductedduringaresearchvisittoUniversity\n",
      "volutionalarchitectureforlinkprediction: itisdefinedbya\n",
      "CollegeLondon.\n",
      "Copyright(cid:13)c 2018,AssociationfortheAdvancementofArtificial singleconvolutionlayer,aprojectionlayertotheembedding\n",
      "Intelligence(www.aaai.org).Allrightsreserved. dimension,andaninnerproductlayer.\n",
      "8102\n",
      "luJ\n",
      "4\n",
      "]GL.sc[\n",
      "6v67410.7071:viXra\n",
      "Specifically,ourcontributionsareasfollows: space. Inthiswork,weuse2D-convolutionswhichoperate\n",
      "spatiallyoverembeddings.\n",
      "• Introducingasimple,competitive2Dconvolutionallink\n",
      "predictionmodel,ConvE.\n",
      "NumberofInteractionsfor1Dvs2DConvolutions\n",
      "• Developinga1-Nscoringprocedurethatspeedsuptraining\n",
      "Using2Dratherthan1Dconvolutionsincreasestheexpres-\n",
      "three-foldandevaluationby300x.\n",
      "sivenessofourmodelthroughadditionalpointsofinteraction\n",
      "• Establishingthatourmodelishighlyparameterefficient, betweenembeddings. Forexample,considerthecasewhere\n",
      "achieving better scores than DistMult and R-GCNs on weconcatenatetworowsof1Dembeddings, aandbwith\n",
      "FB15k-237with8xand17xfewerparameters. dimensionn=3:\n",
      "• Showingthatforincreasinglycomplexknowledgegraphs,\n",
      "([a a a];[b b b])=[a a a b b b].\n",
      "as measured by indegree and PageRank, the difference\n",
      "inperformancebetweenourmodelandashallowmodel A padded 1D convolution with filter size k = 3 will be\n",
      "increasesproportionallytothecomplexityofthegraph. abletomodeltheinteractionsbetweenthesetwoembeddings\n",
      "• Systematicallyinvestigatingreportedinverserelationstest aroundtheconcatenationpoint(withanumberofinteractions\n",
      "proportionaltok).\n",
      "setleakageacrosscommonlyusedlinkpredictiondatasets,\n",
      "Ifweconcatenate(i.e. stack)tworowsof2Dembeddings\n",
      "introducingrobustversionsofdatasetswherenecessary,so\n",
      "withdimensionm×n,wherem=2andn=3,weobtain\n",
      "thattheycannotbesolvedusingsimplerule-basedmodels.\n",
      "thefollowing:\n",
      "• EvaluatingConvEandseveralpreviouslyproposedmodels\n",
      "ontheserobustdatasets: ourmodelachievesstate-of-the- a a a\n",
      "(cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19)\n",
      "artMeanReciprocalRankacrossmostofthem. a a a b b b a a a\n",
      "; = .\n",
      "a a a b b b b b b\n",
      "RelatedWork b b b\n",
      "Several neural link prediction models have been proposed\n",
      "Apadded2Dconvolutionwithfiltersize3×3willbeable\n",
      "intheliterature,suchastheTranslatingEmbeddingsmodel\n",
      "tomodeltheinteractionsaroundtheentireconcatenationline\n",
      "(TransE)(Bordesetal.2013a),theBilinearDiagonalmodel\n",
      "(withanumberofinteractionsproportionaltonandk).\n",
      "(DistMult)(Yangetal.2015)anditsextensioninthecomplex\n",
      "Wecanextendthisprincipletoanalternatingpattern,such\n",
      "space(ComplEx)(Trouillonetal.2016);werefertoNickel\n",
      "asthefollowing:\n",
      "et al. (2016) for a recent survey. The model that is most a a a\n",
      "closelyrelatedtothisworkismostlikelytheHolographic\n",
      "b b b\n",
      "Embeddings model (HolE) (Nickel, Rosasco, and Poggio  .\n",
      "a a a\n",
      "2016),whichusescross-correlation–theinverseofcircular\n",
      "b b b\n",
      "convolution–formatchingentityembeddings;itisinspired\n",
      "Inthiscase,a2Dconvolutionoperationisabletomodeleven\n",
      "by holographic models of associative memory. However,\n",
      "moreinteractionsbetweenaandb(withanumberofinter-\n",
      "HolEdoesnotlearnmultiplelayersofnon-linearfeatures,\n",
      "actionsproportionaltom,n,andk). Thus,2Dconvolution\n",
      "anditisthustheoreticallylessexpressivethanourmodel.\n",
      "isabletoextractmorefeatureinteractionsbetweentwoem-\n",
      "To the best of our knowledge, our model is the first\n",
      "beddingscomparedto1Dconvolution. Thesameprinciple\n",
      "neural link prediction model to use 2D convolutional lay-\n",
      "ers. Graph Convolutional Networks (GCNs) (Duvenaud canbeextendingtohigherdimensionalconvolutions,butwe\n",
      "leavethisasfuturework.\n",
      "etal.2015;Defferrard, Bresson, andVandergheynst2016;\n",
      "KipfandWelling2016)arearelatedlineofresearch,where\n",
      "Background\n",
      "theconvolutionoperatorisgeneralisedtouselocalityinfor-\n",
      "mationingraphs. However,theGCNframeworkislimited\n",
      "A knowledge graph G = {(s,r,o)} ⊆ E ×R×E can\n",
      "toundirectedgraphs,whileknowledgegraphsarenaturally\n",
      "be formalised as a set of triples (facts), each consisting of\n",
      "directed, and suffers from potentially prohibitive memory\n",
      "arelationshipr ∈ Randtwoentitiess,o ∈ E, referredto\n",
      "requirements (Kipf and Welling 2016). Relational GCNs\n",
      "as the subject and object of the triple. Each triple (s,r,o)\n",
      "(R-GCNs) (Schlichtkrull et al. 2017) are a generalisation\n",
      "denotesarelationshipoftyperbetweentheentitiessando.\n",
      "ofGCNsdevelopedfordealingwithhighlymulti-relational\n",
      "Thelinkpredictionproblemcanbeformalisedasapoint-\n",
      "data such as knowledge graphs – we include them in our\n",
      "wiselearningtorankproblem,wheretheobjectiveislearning\n",
      "experimentalevaluations.\n",
      "ascoringfunctionψ :E×R×E (cid:55)→R. Givenaninputtriple\n",
      "Severalconvolutionalmodelshavebeenproposedinnatu-\n",
      "x=(s,r,o),itsscoreψ(x)∈Risproportionaltothelikeli-\n",
      "rallanguageprocessing(NLP)forsolvingavarietyoftasks,\n",
      "hoodthatthefactencodedbyxistrue.\n",
      "includingsemanticparsing(Yihetal.2011),sentenceclassi-\n",
      "NeuralLinkPredictors\n",
      "fication(Kim2014),searchqueryretrieval(Shenetal.2014),\n",
      "sentencemodelling(Kalchbrenner,Grefenstette,andBlun- Neural link prediction models (Nickel et al. 2016) can be\n",
      "som2014),aswellasotherNLPtasks(Collobertetal.2011). seenasmulti-layerneuralnetworksconsistingofanencoding\n",
      "However,mostworkinNLPuses1D-convolutions,thatis componentandascoringcomponent. Givenaninputtriple\n",
      "convolutionswhichoperateoveratemporalsequenceofem- (s,r,o),theencodingcomponentmapsentitiess,o ∈ E to\n",
      "beddings, for example a sequence of words in embedding theirdistributedembeddingrepresentationse,e ∈Rk. In\n",
      "s o\n",
      "Table1: Scoringfunctionsψ (e,e )fromneurallinkpredictorsintheliterature,theirrelation-dependentparametersandspace\n",
      "r s o\n",
      "complexity;n andn respectivelydenotethenumberofentitiesandrelationtypes,i.e. n =|E|andn =|R|.\n",
      "e r e r\n",
      "Model ScoringFunctionψ (e,e ) RelationParameters SpaceComplexity\n",
      "r s o\n",
      "SE(Bordesetal.2014) (cid:13) (cid:13)W rLe s−W rRe o(cid:13) (cid:13)\n",
      "p\n",
      "W rL,W rR ∈Rk×k O(n ek+n rk2)\n",
      "TransE(Bordesetal.2013a) (cid:107)e +r −e (cid:107) r ∈Rk O(n k+n k)\n",
      "s r o p r e r\n",
      "DistMult(Yangetal.2015) (cid:104)e,r,e (cid:105) r ∈Rk O(n k+n k)\n",
      "s r o r e r\n",
      "ComplEx(Trouillonetal.2016) (cid:104)e,r,e (cid:105) r ∈Ck O(n k+n k)\n",
      "s r o r e r\n",
      "ConvE f(vec(f([e ;r ]∗ω))W)e r ∈Rk(cid:48) O(n k+n k(cid:48))\n",
      "s r o r e r\n",
      "thescoringcomponent,thetwoentityembeddingse ande entropyloss:\n",
      "s o\n",
      "arescoredbyafunctionψ r. Thescoreofatriple(s,r,o)is 1 (cid:88)\n",
      "definedasψ(s,r,o)=ψ r(e s,e o)∈R. L(p,t)=−\n",
      "N\n",
      "(t i·log(p i)+(1−t i)·log(1−p i)), (2)\n",
      "InTable1wesummarisethescoringfunctionofseveral i\n",
      "linkpredictionmodelsfromtheliterature.Thevectorse and wheretisthelabelvectorwithdimensionR1x1for1-1scor-\n",
      "s\n",
      "e denotethesubjectandobjectembedding,wheree,e ∈ ing or R1xN for 1-N scoring (see the next section for 1-N\n",
      "o s o\n",
      "Ck in ComplEx and e,e ∈ Rk in all other models, and scoring);theelementsofvectortareonesforrelationships\n",
      "s o\n",
      "(cid:80)\n",
      "(cid:104)x,y,z(cid:105) = x y z denotes the tri-linear dot product; ∗ thatexistsandzerootherwise.\n",
      "i i i i\n",
      "denotes the convolution operator; f denotes a non-linear We use rectified linear units as the non-linearity f for\n",
      "function. fastertraining(Krizhevsky,Sutskever,andHinton2012),and\n",
      "batchnormalisationaftereachlayertostabilise,regularise\n",
      "andincreaserateofconvergence(IoffeandSzegedy2015).\n",
      "Convolutional2DKnowledgeGraphs\n",
      "Weregulariseourmodelbyusingdropout(Srivastavaetal.\n",
      "Embeddings\n",
      "2014) in several stages. In particular, we use dropout on\n",
      "theembeddings,onthefeaturemapsaftertheconvolution\n",
      "Inthisworkweproposeaneurallinkpredictionmodelwhere\n",
      "operation,andonthehiddenunitsafterthefullyconnected\n",
      "theinteractionsbetweeninputentitiesandrelationshipsare\n",
      "layer. We use Adam as optimiser (Kingma and Ba 2014),\n",
      "modelledbyconvolutionalandfully-connectedlayers. The\n",
      "andlabelsmoothingtolessenoverfittingduetosaturationof\n",
      "maincharacteristicofourmodelisthatthescoreisdefinedby\n",
      "outputnon-linearitiesatthelabels(Szegedyetal.2016).\n",
      "aconvolutionover2Dshapedembeddings. Thearchitecture\n",
      "issummarisedinFigure1;formally,thescoringfunctionis FastEvaluationforLinkPredictionTasks\n",
      "definedasfollows:\n",
      "Inourarchitectureconvolutionconsumesabout75-90%of\n",
      "thetotalcomputationtime,thusitisimportanttominimise\n",
      "ψ (e,e )= f(vec(f([e ;r ]∗ω))W)e, (1)\n",
      "r s o s r o the number of convolution operations to speed up compu-\n",
      "tationasmuchaspossible. Forlinkpredictionmodels,the\n",
      "wherer ∈ Rk isarelationparameterdependingonr, e\n",
      "r s batchsizeisusuallyincreasedtospeedupevaluation(Bordes\n",
      "andr denotea2Dreshapingofe andr,respectively: if\n",
      "r s r etal.2013b). However,thisisnotfeasibleforconvolutional\n",
      "e s,r\n",
      "r\n",
      "∈Rk,thene s,r\n",
      "r\n",
      "∈Rkw×kh,wherek =k wk h.\n",
      "modelssincethememoryrequirementsquicklyoutgrowthe\n",
      "Inthefeed-forwardpass,themodelperformsarow-vector\n",
      "GPUmemorycapacitywhenincreasingthebatchsize.\n",
      "look-upoperationontwoembeddingmatrices,oneforenti-\n",
      "Unlikeotherlinkpredictionmodelswhichtakeanentity\n",
      "ties,denotedE|E|×k andoneforrelations,denotedR|R|×k(cid:48), pairandarelationasatriple(s,r,o),andscoreit(1-1scor-\n",
      "wherekandk(cid:48)aretheentityandrelationembeddingdimen- ing),wetakeone(s,r)pairandscoreitagainstallentities\n",
      "sions, and |E| and |R| denote the number of entities and o ∈ E simultaneously(1-Nscoring). Ifwebenchmark1-1\n",
      "relations. Themodelthenconcatenatese sandr r,andusesit scoringonahigh-endGPUwithbatchsizeandembedding\n",
      "asaninputfora2Dconvolutionallayerwithfiltersω. Such size128,thenatrainingpassandanevaluationwithacon-\n",
      "alayerreturnsafeaturemaptensorT ∈Rc×m×n,wherec volution model on FB15k – one of the dataset used in the\n",
      "isthenumberof2Dfeaturemapswithdimensionsmandn. experiments–takes2.4minutesand3.34hours. Using1-N\n",
      "ThetensorT isthenreshapedintoavectorvec(T)∈Rcmn, scoring,therespectivenumbersare45and35seconds–a\n",
      "whichisthenprojectedintoak-dimensionalspaceusingalin- considerable improvement of over 300x in terms of evalu-\n",
      "eartransformationparametrisedbythematrixW∈Rcmn×k\n",
      "ationtime. Additionally, thisapproachisscalabletolarge\n",
      "andmatchedwiththeobjectembeddinge oviaaninnerprod- knowledgegraphsandincreasesconvergencespeed. Fora\n",
      "uct.Theparametersoftheconvolutionalfiltersandthematrix single forward-backward pass with batch size of 128, go-\n",
      "Wareindependentoftheparametersfortheentitiessando ing from N = 100,000 to N = 1,000,000 entities only\n",
      "andtherelationshipr. increases the computational time from 64ms to 80ms – in\n",
      "For training the model parameters, we apply the lo- otherwords,aten-foldincreaseinthenumberofentitiesonly\n",
      "gistic sigmoid function σ(·) to the scores, that is p = increasesthecomputationtimeby25%–whichatteststhe\n",
      "σ(ψ (e,e )), and minimise the following binary cross- scalabilityoftheapproach.\n",
      "r s o\n",
      "Figure1: IntheConvEmodel,theentityandrelationembeddingsarefirstreshapedandconcatenated(steps1,2);theresulting\n",
      "matrixisthenusedasinputtoaconvolutionallayer(step3);theresultingfeaturemaptensorisvectorisedandprojectedintoa\n",
      "k-dimensionalspace(step4)andmatchedwithallcandidateobjectembeddings(step5).\n",
      "0.9\n",
      "0.2\n",
      "0.1\n",
      "0.6\n",
      "0.2\n",
      "0.3\n",
      "0.0\n",
      "0.7\n",
      "0.1\n",
      "0.4\n",
      "0.4\n",
      "0.4\n",
      "Ifinsteadof1-Nscoring,weuse1-(0.1N)scoring–that relations: alargenumberoftesttriplescanbeobtainedsim-\n",
      "is,scoringagainst10%oftheentities–wecancomputea plybyinvertingtriplesinthetrainingset. Forexample,the\n",
      "forward-backwardpass25%faster. However,weconverge test set frequently contains triples such as (s,hyponym,o)\n",
      "roughly230%sloweronthetrainingset. Thus1-Nscoring while the training set contains its inverse (o,hypernym,s).\n",
      "has an additional effect which is akin to batch normalisa- To create a dataset without this property, Toutanova and\n",
      "tion (Ioffe and Szegedy 2015) – we trade some computa- Chen (2015) introduced FB15k-237 – a subset of FB15k\n",
      "tionalperformanceforgreatlyincreasedconvergencespeed where inverse relations are removed. However, they did\n",
      "andalsoachievebetterperformanceasshowninSection7. notexplicitlyinvestigatetheseverityofthisproblem,which\n",
      "Do note that the technique in general could by applied to mightexplainwhyresearchcontinuesusingthesedatasets\n",
      "any1-1scoringmodel. Thispracticaltrickinspeedingup forevaluationwithoutaddressingthisissue(e.g. Trouillonet\n",
      "training and evaluation can be applied to any 1-1 scoring al.(2016),Nickel,Rosasco,andPoggio(2016),Nguyenet\n",
      "model,suchasthegreatmajorityoflinkpredictionmodels. al.(2016),Liuetal.(2016)).\n",
      "Inthefollowingsection,weintroduceasimplerule-based\n",
      "Experiments modelwhichdemonstratestheseverityofthisbiasbyachiev-\n",
      "ing state-of-the-art results on both WN18 and FB15k. In\n",
      "KnowledgeGraphDatasets\n",
      "ordertoensurethatweevaluateondatasetsthatdonothave\n",
      "Forevaluatingourproposedmodel,weuseaselectionoflink inverserelationtestleakage,weapplyoursimplerule-based\n",
      "predictiondatasetsfromtheliterature. modeltoeachdataset. ApartfromFB15k, whichwascor-\n",
      "WN18(Bordesetal.2013a)isasubsetofWordNetwhich rectedbyFB15k-237, wealsofindflawswithWN18. We\n",
      "consists of 18 relations and 40,943 entities. Most of the thuscreateWN18RRtoreclaimWN18asadataset,which\n",
      "151,442triplesconsistofhyponymandhypernymrelations cannoteasilybecompletedusingasinglerule–butrequires\n",
      "and,forsuchareason,WN18tendstofollowastrictlyhier- modelling of the complete knowledge graph. WN18RR1\n",
      "archicalstructure. contains93,003tripleswith40,943entitiesand11relations.\n",
      "FB15k(Bordesetal.2013a)isasubsetofFreebasewhich Forfutureresearch,werecommendagainstusingFB15kand\n",
      "containsabout14,951entitieswith1,345differentrelations. WN18andinsteadrecommendFB15k-237,WN18RR,and\n",
      "Alargefractionofcontentinthisknowledgegraphdescribes YAGO3-10.\n",
      "factsaboutmovies,actors,awards,sports,andsportteams.\n",
      "ExperimentalSetup\n",
      "YAGO3-10(Mahdisoltani,Biega,andSuchanek2015)is\n",
      "asubsetofYAGO3whichconsistsofentitieswhichhavea We selected the hyperparameters of our ConvE model via\n",
      "minimumof10relationseach. Ithas123,182entitiesand37 grid search according to the mean reciprocal rank (MRR)\n",
      "relations. Mostofthetriplesdealwithdescriptiveattributes on the validation set. Hyperparameter ranges for the grid\n",
      "ofpeople,suchascitizenship,gender,andprofession. searchwereasfollows–embeddingdropout{0.0,0.1,0.2},\n",
      "Countries (Bouchard, Singh, and Trouillon 2015) is a feature map dropout {0.0,0.1,0.2,0.3}, projection layer\n",
      "benchmarkdatasetthatisusefultoevaluateamodel’sabil- dropout {0.0,0.1,0.3,0.5}, embedding size {100,200},\n",
      "ity to learn long-range dependencies between entities and batchsize{64,128,256},learningrate{0.001,0.003},and\n",
      "relations. It consists of three sub-tasks which increase in labelsmoothing{0.0,0.1,0.2,0.3}.\n",
      "difficultyinastep-wisefashion,wheretheminimumpath- Besidesthegridsearch,weinvestigatedmodificationsof\n",
      "lengthtofindasolutionincreasesfrom2to4. the2Dconvolutionlayerforourmodels. Inparticular,we\n",
      "It was first noted by Toutanova and Chen (2015) that\n",
      "WN18andFB15ksufferfromtestleakagethroughinverse 1https://github.com/TimDettmers/ConvE\n",
      "Table2: ParameterscalingofDistMultvsConvE. Togaugetheseverityofthisproblem,weconstructasim-\n",
      "ple, rule-based model that solely models inverse relations.\n",
      "Wecallthismodeltheinversemodel. Themodelextractsin-\n",
      "Param. Emb. Hits\n",
      "verserelationshipsautomaticallyfromthetrainingset: given\n",
      "Model count size MRR @10 @3 @1\n",
      "two relation pairs r,r ∈ R, we check whether (s,r,o)\n",
      "1 2 1\n",
      "DistMult 1.89M 128.23.41.25.15 implies(o,r,s),orvice-versa.\n",
      "2\n",
      "DistMult 0.95M 64.22.39.25.14 Weassumethatinverserelationsarerandomlydistributed\n",
      "DistMult 0.23M 16.16.31.17.09 amongthetraining,validationandtestsetsand,assuch,we\n",
      "expectthenumberofinverserelationstobeproportionalto\n",
      "ConvE 5.05M 200.32.49.35.23\n",
      "thesizeofthetrainingsetcomparedtothetotaldatasetsize.\n",
      "ConvE 1.89M 96.32.49.35.23\n",
      "Thus,wedetectinverserelationsifthepresenceof(s,r,o)\n",
      "ConvE 0.95M 54.30.46.33.22 1\n",
      "co-occurswiththepresenceof(o,r,s)withafrequencyof\n",
      "ConvE 0.46M 28.28.43.30.20 2\n",
      "atleast0.99−(f +f ),wheref andf isthefractionof\n",
      "ConvE 0.23M 14.26.40.28.19 v t v t\n",
      "thevalidationandtestsetcomparedtothetotalsizeofthe\n",
      "dataset. Relationsmatchingthiscriterionareassumedtobe\n",
      "theinverseofeachother.\n",
      "experimentedwithreplacingitwithfullyconnectedlayers Attesttime,wecheckifthetesttriplehasinversematches\n",
      "and 1D convolution; however, these modifications consis- outside the test set: if k matches are found, we sample a\n",
      "tentlyreducedthepredictiveaccuracyofthemodel. Wealso permutationofthetopkranksforthesematches;ifnomatch\n",
      "experimentedwithdifferentfiltersizes, andfoundthatwe isfound,weselectarandomrankforthetesttriple.\n",
      "onlyreceivegoodresultsifthefirstconvolutionallayeruses\n",
      "Results\n",
      "small(i.e. 3x3)filters.\n",
      "Wefoundthatthefollowingcombinationofparameters Similarlytopreviouswork(Yangetal.2015;Trouillonetal.\n",
      "works well on WN18, YAGO3-10 and FB15k: embed- 2016;Niepert2016),wereportresultsusingafilteredsetting,\n",
      "dingdropout0.2,featuremapdropout0.2,projectionlayer i.e. we rank test triples against all other candidate triples\n",
      "dropout0.3, embeddingsize200, batchsize128, learning notappearinginthetraining,validation,ortestset(Bordes\n",
      "rate0.001,andlabelsmoothing0.1.FortheCountriesdataset, etal.2013a). Candidatesareobtainedbypermutingeither\n",
      "weincreaseembeddingdropoutto0.3,hiddendropoutto0.5, the subject or the object of a test triple with all entities in\n",
      "andsetlabelsmoothingto0.Weuseearlystoppingaccording the knowledge graph. Our results on the standard bench-\n",
      "tothemeanreciprocalrank(WN18,FB15k,YAGO3-10)and marksFB15kandWN18areshowninTable3;resultsonthe\n",
      "AUC-PR(Countries)statisticsonthevalidationset,which datasetswithinverserelationsremovedareshowninTable4;\n",
      "we evaluate every three epochs. Unlike the other datasets, resultsonYAGO3-10andCountriesareshowninTable5.\n",
      "for Countries the results have a high variance, as such we Strikingly,theinversemodelachievesstate-of-the-arton\n",
      "average10runsandproduce95%confidenceintervals. For many different metrics for both FB15k and WN18. How-\n",
      "ourDistMultandComplExresultswith1-1training,weuse ever, it fails to pick up on inverse relations for YAGO3-\n",
      "an embedding size of 100, AdaGrad (Duchi, Hazan, and 10andFB15k-237. TheprocedureusedbyToutanovaand\n",
      "Singer2011)foroptimisation,andweregulariseourmodel Chen(2015)toderiveFB15k-237doesnotremovecertain\n",
      "byforcingtheentityembeddingstohaveaL2normof1after symmetricrelationships,forexample“similarto”. Thepres-\n",
      "eachparameterupdate. AsinBordesetal.(2013a),weusea ence of these relationships explains the good score of our\n",
      "pairwisemargin-basedrankingloss. inverse model on WN18RR, which was derived using the\n",
      "Thecodeforourmodelandexperimentsismadepublicly sameprocedure.\n",
      "available,2 aswellasthecodeforreplicatingtheDistMult Ourproposedmodel,ConvE,achievesstate-of-the-artper-\n",
      "results.3 formanceforallmetricsonYAGO3-10,forsomemetricson\n",
      "FB15k,anditdoeswellonWN18. OnCountries,itsolves\n",
      "InverseModel theS1andS2tasks,anddoeswellonS3,scoringbetterthan\n",
      "It has been noted by Toutanova and Chen (2015), that the othermodelslikeDistMultandComplEx.\n",
      "trainingdatasetsofWN18andFB15khave94%and81% For FB15k-237, we could not replicate the basic model\n",
      "test leakage as inverse relations, that is, 94% and 81% of results from Toutanova et al. (2015), where the models in\n",
      "thetriplesinthesedatasetshaveinverserelationswhichare generalhavebetterperformancethanwhatwecanachieve.\n",
      "linked to the test set. For instance, a test triple (feline, hy- ComparedtoSchlichtkrulletal.(2017),ourresultsforstan-\n",
      "ponym, cat) can easily be mapped to a training triple (cat, dard models are a slightly better then theirs, and on-a-par\n",
      "hypernym,feline)ifitisknownthathyponymistheinverse withtheirR-GCNmodel.\n",
      "ofhypernym. Thisishighlyproblematic,becauselinkpre-\n",
      "ParameterefficiencyofConvE\n",
      "dictorsthatdowellonthesedatasetsmaysimplylearnwhich\n",
      "relationsthataretheinverseofothers,ratherthantomodel From Table 2 we can see that ConvE for FB15k-237 with\n",
      "theactualknowledgegraph. 0.23MparametersperformsbetterthanDistMultwith1.89M\n",
      "parametersfor3metricsoutof5.\n",
      "2https://github.com/TimDettmers/ConvE ConvEwith0.46Mparametersstillachievesstate-of-the-\n",
      "3https://github.com/uclmr/inferbeddings artresultsonFB15k-237with0.425Hits@10. Compar<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,    301,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Tim Dettmers', 'Pasquale Minervini', 'Pontus Stenetorp', 'Sebastian Riedel']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  dard models are a slightly better then theirs, and on-a-par\n",
      "hypernym,feline)ifitisknownthathyponymistheinverse withtheirR-GCNmodel.\n",
      "ofhypernym. Thisishighlyproblematic,becauselinkpre-\n",
      "ParameterefficiencyofConvE\n",
      "dictorsthatdowellonthesedatasetsmaysimplylearnwhich\n",
      "relationsthataretheinverseofothers,ratherthantomodel From Table 2 we can see that ConvE for FB15k-237 with\n",
      "theactualknowledgegraph. 0.23MparametersperformsbetterthanDistMultwith1.89M\n",
      "parametersfor3metricsoutof5.\n",
      "2https://github.com/TimDettmers/ConvE ConvEwith0.46Mparametersstillachievesstate-of-the-\n",
      "3https://github.com/uclmr/inferbeddings artresultsonFB15k-237with0.425Hits@10. Comparingto\n",
      "Table3: LinkpredictionresultsforWN18andFB15k\n",
      "WN18 FB15k\n",
      "Hits Hits\n",
      "MR MRR @10 @3 @1 MR MRR @10 @3 @1\n",
      "DistMult(Yangetal.2015) 902.822.936.914.728 97.654.824.733.546\n",
      "ComplEx(Trouillonetal.2016) –.941.947.936.936 –.692.840.759.599\n",
      "Gaifman(Niepert2016) 352 –.939 –.761 75 –.842 –.692\n",
      "ANALOGY(Liu,Wu,andYang2017) –.942.947.944.939 –.725.854.785.646\n",
      "R-GCN(Schlichtkrulletal.2017) –.814.964.929.697 –.696.842.760.601\n",
      "ConvE 374.943.956.946.935 51.657.831.723.558\n",
      "InverseModel 740.963.964.964.953 2501.660.660.659.658\n",
      "Table4: LinkpredictionresultsforWN18RRandFB15k-237\n",
      "WN18RR FB15k-237\n",
      "Hits Hits\n",
      "MR MRR @10 @3 @1 MR MRR @10 @3 @1\n",
      "DistMult(Yangetal.2015) 5110.43.49.44.39 254.241.419.263.155\n",
      "ComplEx(Trouillonetal.2016) 5261.44.51.46.41 339.247.428.275.158\n",
      "R-GCN(Schlichtkrulletal.2017) – – – – – –.248.417.258.153\n",
      "ConvE 4187.43.52.44.40 244.325.501.356.237\n",
      "InverseModel 13526.35.35.35.35 7030.010.014.011.007\n",
      "thepreviousbestmodel,R-GCN(Schlichtkrulletal.2017), business people) and successful modelling of such a high\n",
      "whichachieves0.417Hits@10withmorethan8Mparame- indegreenodesrequirescapturingallthesedifferences. Our\n",
      "ters. hypothesisisthatdeepermodels,thatis,modelsthatlearn\n",
      "Overall,ConvEismorethan17xparameterefficientthan multiplelayersoffeatures,likeConvE,haveanadvantage\n",
      "R-GCNs,and8xmoreparameterefficientthanDistMult. For overshallowmodels,likeDistMult,tocaptureallthesecon-\n",
      "theentiretyofFreebase,thesizeofthesemodelswouldbe straints.\n",
      "morethan82GBforR-GCNs,21GBforDistMult,compared However,deepermodelsaremoredifficulttooptimise,so\n",
      "to5.2GBforConvE. wehypothesisethatfordatasetswithlowaveragerelation-\n",
      "specific indegree (like WN18RR and WN18), a shallow\n",
      "Analysis modellikeDistMultmightsufficeforaccuratelyrepresenting\n",
      "thestructureofthenetwork.\n",
      "AblationStudy\n",
      "To test our two hypotheses, we take two datasets with\n",
      "Table 7 shows the results from our ablation study where low(low-WN18)andhigh(high-FB15k)relation-specificin-\n",
      "we evaluate different parameter initialisation (n = 2) to degreeandreversethemintohigh(high-WN18)andlow(low-\n",
      "calculateconfidenceintervals. Weseethathiddendropoutis FB15k)relation-specificindegreedatasetsbydeletinglow\n",
      "byfarthemostimportantcomponent,whichisunsurprising andhighindegreenodes. Wehypothesisethat,comparedto\n",
      "since it is our main regularisation technique. 1-N scoring DistMult,ConvEwillalwaysdobetteronthedatasetwith\n",
      "improvesperformance,asdoesinputdropout,featuremap highrelation-specificindegree,andvice-versa.\n",
      "dropouthasaminoreffect,whilelabelsmoothingseemsto Indeed,wefindthatbothhypotheseshold: forlow-FB15k\n",
      "beunimportant–asgoodresultscanbeachievedwithoutit. wehaveConvE0.586Hits@10vsDistMult0.728Hits@10;\n",
      "forhigh-WN18wehaveConvE0.952Hits@10vsDistMult\n",
      "0.938 Hits@10. This supports our hypothesis that deeper\n",
      "AnalysisofIndegreeandPageRank\n",
      "models such as ConvE have an advantage to model more\n",
      "Ourmainhypothesisforthegoodperformanceofourmodel complexgraphs(e.g. FB15kandFB15k-237),butthatshal-\n",
      "on datasets like YAGO3-10 and FB15k-237 compared to low models such as DistMult have an advantage to model\n",
      "WN18RR,isthatthesedatasetscontainnodeswithveryhigh lesscomplexgraphs(e.g. WN18WN18RR).\n",
      "relation-specific indegree. For example the node “United Toinvestigatethisfurther,welookatPageRank,ameasure\n",
      "States” with edges “was born in” has an indegree of over of centrality of a node. PageRank can also be seen as a\n",
      "10,000. Many of these 10,000 nodes will be very differ- measureoftherecursiveindegreeofanode: thePageRank\n",
      "entfromeachother(actors,writers,academics,politicians, valueofanodeisproportionaltotheindegreeofthisnode,its\n",
      "Table5: LinkpredictionresultsforYAGO3-10andCountries\n",
      "YAGO3-10 Countries\n",
      "Hits AUC-PR\n",
      "MR MRR @10 @3 @1 S1 S2 S3\n",
      "DistMult(Yangetal.2015) 5926.34.54.38.24 1.00±0.00 0.72±0.12 0.52±0.07\n",
      "ComplEx(Trouillonetal.2016) 6351.36.55.40.26 0.97±0.02 0.57±0.10 0.43±0.07\n",
      "ConvE 1676.44.62.49.35 1.00±0.00 0.99±0.01 0.86±0.05\n",
      "InverseModel 59448.01.02.02.01 – – –\n",
      "Table6: MeanPageRank×10−3 ofnodesinthetestsetvs error reduction of ConvE compared to DistMult is strong\n",
      "reductioninerrorintermsofAUC-PRorHits@10ofConvE withr = 0.56. Thisgivesadditionalevidencethatmodels\n",
      "wrt. DistMult. that are deeper have an advantage when modelling nodes\n",
      "withhigh(recursive)indegree.\n",
      "Dataset PageRank ErrorReduction Fromthisevidenceweconclude,thattheincreasedperfor-\n",
      "manceofourmodelcomparedtoastandardlinkpredictor,\n",
      "WN18RR 0.104 0.06\n",
      "DistMult, can be partially explained due to our it’s ability\n",
      "WN18 0.125 0.45\n",
      "tomodelnodeswithhighindegreewithgreaterprecision–\n",
      "FB15k 0.599 0.04\n",
      "whichispossiblyrelatedtoitsdepth.\n",
      "FB15-237 0.733 0.16\n",
      "YAGO3-10 0.988 0.21\n",
      "ConclusionandFutureWork\n",
      "CountriesS3 1.415 2.36\n",
      "CountriesS1 1.711 0.00 WeintroducedConvE,alinkpredictionmodelthatuses2D\n",
      "CountriesS2 1.796 17.6 convolution over embeddings and multiple layers of non-\n",
      "linearfeaturestomodelknowledgegraphs.ConvEusesfewer\n",
      "parameters; it is fast through 1-N scoring; it is expressive\n",
      "Table7: AblationstudyforFB15k-237.\n",
      "through multiple layers of non-linear features; it is robust\n",
      "to overfitting dueto batchnormalisation anddropout; and\n",
      "Ablation Hits@10 achievesstate-of-the-artresultsonseveraldatasets,whilestill\n",
      "FullConvE 0.491 scalingtolargeknowledgegraphs. Inouranalysis,weshow\n",
      "thattheperformanceofConvEcomparedtoacommonlink\n",
      "Hiddendropout -0.044±0.003\n",
      "predictor,DistMult,canpartiallybeexplainedbyitsability\n",
      "Inputdropout -0.022±0.000\n",
      "tomodelnodeswithhigh(recursive)indegree.\n",
      "1-Nscoring -0.019\n",
      "TestleakagethroughinverserelationsofWN18andFB15k\n",
      "Featuremapdropout -0.013±0.001\n",
      "wasfirstreportedbyToutanovaandChen(2015): weinvesti-\n",
      "Labelsmoothing -0.008±0.000\n",
      "gatetheseverityofthisproblemforcommonlyuseddatasets\n",
      "by introducing a simple rule-based model, and find that it\n",
      "canachievestate-of-the-artresultsonWN18andFB15k. To\n",
      "ensurerobustversionsofallinvestigateddatasetsexists,we\n",
      "neighboursindegrees,itsneighbours-neighboursindegrees\n",
      "deriveWN18RR.\n",
      "andsoforthscaledrelativetoallothernodesinthenetwork.\n",
      "Bythislineofreasoning,wealsoexpectConvEtobebetter Ourmodelisstillshallowcomparedtoconvolutionalarchi-\n",
      "thanDistMultondatasetswithhighaveragePageRank(high tecturefoundincomputervision,andfutureworkmightdeal\n",
      "connectivitygraphs),andvice-versa. withconvolutionalmodelsofincreasingdepth. Furtherwork\n",
      "mightalsolookattheinterpretationof2Dconvolution,or\n",
      "Totestthishypothesis,wecalculatethePageRankforeach\n",
      "howtoenforcelarge-scalestructureinembeddingspaceso\n",
      "dataset as a measure of centrality. We find that the most\n",
      "toincreasethenumberofinteractionsbetweenembeddings.\n",
      "central nodes in WN18 have a PageRank value more than\n",
      "oneorderofmagnitudesmallerthanthemostcentralnodes\n",
      "Acknowledgements\n",
      "inYAGO3-10andCountries,andabout4timessmallerthan\n",
      "themostcentralnodesinFB15k. Whenwelookatthemean We would like to thank Johannes Welbl, Peter Hayes, and\n",
      "PageRank of nodes contained in the test sets, we find that TakumaEbisufortheirfeedbackandhelpfuldiscussionsre-\n",
      "thedifferenceofperformanceintermsofHits@10between latedtothiswork. WethankTakumaEbisuforpointingout\n",
      "DistMult and ConvE is roughly proportional to the mean anerrorinourInverseModelscript–thecorrectedresults\n",
      "test set PageRank, that is, the higher the mean PageRank areslightlybetterforWN18andslightlyworseforFB15k.\n",
      "of the test set nodes the better ConvE does compared to WethankVictoriaLinforhelpingustounrootandfixabug\n",
      "DistMult, and vice-versa. See Table 6 for these statistics. where the exclusion of triples during inference worked in-\n",
      "ThecorrelationbetweenmeantestsetPageRankandrelative correctly–thechangesdidnotaffectthemainresultsinthis\n",
      "work,thoughsomeresultsintheappendixchanged(UMLS, [2014] Kingma,D.,andBa,J. 2014. Adam: Amethodfor\n",
      "Nations). This work was supported by a Marie Curie Ca- stochasticoptimization. arXivpreprintarXiv:1412.6980.\n",
      "reerIntegrationAward,anAllenDistinguishedInvestigator [2016] Kipf,T.N.,andWelling,M. 2016. Semi-Supervised\n",
      "Award,aGoogleEuropeScholarshipforStudentswithDis- ClassificationwithGraphConvolutionalNetworks. InPro-\n",
      "abilities,andtheH2020projectSUMMA. ceedingsofICLR2016.\n",
      "[2012] Krizhevsky,A.;Sutskever,I.;andHinton,G.E. 2012.\n",
      "References\n",
      "ImageNet Classification with Deep Convolutional Neural\n",
      "[2013a] Bordes,A.;Usunier,N.;García-Durán,A.;Weston, Networks. InProceedingsofNIPS2012,1097–1105.\n",
      "J.;andYakhnenko,O. 2013a. TranslatingEmbeddingsfor [2015] Krompaß, D.; Baier, S.; and Tresp, V. 2015. Type-\n",
      "Modeling Multi-relational Data. In Proceedings of NIPS, ConstrainedRepresentationLearninginKnowledgeGraphs.\n",
      "2787–2795. InProceedingsofISWC2015,640–655.\n",
      "[2013b] Bordes,A.;Usunier,N.;García-Durán,A.;Weston, [2016] Liu,Q.;Jiang,L.;Han,M.;Liu,Y.;andQin,Z. 2016.\n",
      "J.;andYakhnenko,O. 2013b. TranslatingEmbeddingsfor Hierarchicalrandomwalkinferenceinknowledgegraphs. In\n",
      "Modeling Multi-relational Data. In Proceedings of NIPS, Proceedingsofthe39thInternationalACMSIGIRconference\n",
      "2787–2795. onResearchandDevelopmentinInformationRetrieval,445–\n",
      "454. ACM.\n",
      "[2014] Bordes, A.; Glorot, X.; Weston, J.; and Bengio, Y.\n",
      "2014. Asemanticmatchingenergyfunctionforlearningwith [2017] Liu, H.; Wu, Y.; and Yang, Y. 2017. Analogical\n",
      "multi-relationaldata-applicationtoword-sensedisambigua- InferenceforMulti-RelationalEmbeddings. ArXive-prints.\n",
      "tion. MachineLearning94(2):233–259. [2015] Mahdisoltani,F.;Biega,J.;andSuchanek,F.M. 2015.\n",
      "[2015] Bouchard, G.; Singh, S.; and Trouillon, T. 2015. YAGO3: AKnowledgeBasefromMultilingualWikipedias.\n",
      "On approximate reasoning capabilities of low-rank vector InProceedingsofCIDR2015.\n",
      "spaces. AAAISpringSyposiumonKnowledgeRepresenta- [2016] Nguyen, D. Q.; Sirts, K.; Qu, L.; and Johnson,\n",
      "tionandReasoning(KRR):IntegratingSymbolicandNeural M. 2016. Stranse: a novel embedding model of enti-\n",
      "Approaches. ties and relationships in knowledge bases. arXiv preprint\n",
      "[2011] Collobert, R.; Weston, J.; Bottou, L.; Karlen, M.; arXiv:1606.08140.\n",
      "Kavukcuoglu, K.; and Kuksa, P. P. 2011. Natural Lan- [2016] Nickel,M.;Murphy,K.;Tresp,V.;andGabrilovich,E.\n",
      "guageProcessing(Almost)fromScratch.JournalofMachine 2016. Areviewofrelationalmachinelearningforknowledge\n",
      "LearningResearch12:2493–2537. graphs. ProceedingsoftheIEEE104(1):11–33.\n",
      "[2016] Defferrard, M.; Bresson, X.; and Vandergheynst, P. [2016] Nickel,M.;Rosasco,L.;andPoggio,T.A.2016.Holo-\n",
      "2016. ConvolutionalNeuralNetworksonGraphswithFast graphicEmbeddingsofKnowledgeGraphs. InProceedings\n",
      "LocalizedSpectralFiltering. InProceedingsofNIPS,3837– ofAAAI,1955–1961.\n",
      "3845.\n",
      "[2016] Niepert,M. 2016. DiscriminativeGaifmanModels.\n",
      "[2014] Dong,X.;Gabrilovich,E.;Heitz,G.;Horn,W.;Lao, InProceedingsofNIPS2016,3405–3413.\n",
      "N.;Murphy,K.;Strohmann,T.;Sun,S.;andZhang,W. 2014. [2017] Schlichtkrull, M.; Kipf, T. N.; Bloem, P.; Berg, R.\n",
      "KnowledgeVault: AWeb-ScaleApproachtoProbabilistic v.d.;Titov,I.;andWelling,M. 2017. ModelingRelational\n",
      "KnowledgeFusion. InProceedingsofKDD2014,601–610. Data with Graph Convolutional Networks. arXiv preprint\n",
      "[2011] Duchi,J.C.;Hazan,E.;andSinger,Y. 2011. Adap- arXiv:1703.06103.\n",
      "tive subgradient methods for online learning and stochas- [2014] Shen, Y.; He, X.; Gao, J.; Deng, L.; and Mesnil, G.\n",
      "tic optimization. Journal of Machine Learning Research 2014. LearningSemanticRepresentationsUsingConvolu-\n",
      "12:2121–2159. tionalNeuralNetworksforWebSearch. InProceedingsof\n",
      "[2015] Duvenaud, D. K.; Maclaurin, D.; Aguilera- WWW2014,373–374.\n",
      "Iparraguirre, J.; Bombarell, R.; Hirzel, T.; Aspuru-Guzik, [2014] Srivastava, N.; Hinton, G. E.; Krizhevsky, A.;\n",
      "A.; and Adams, R. P. 2015. Convolutional Networks on Sutskever, I.; and Salakhutdinov, R. 2014. Dropout: A\n",
      "GraphsforLearningMolecularFingerprints. InProceedings SimpleWaytoPreventNeuralNetworksfromOverfitting.\n",
      "ofNIPS2015,2224–2232. JournalofMachineLearningResearch15(1):1929–1958.\n",
      "[2015] Ioffe,S.,andSzegedy,C. 2015. BatchNormalization: [2015] Szegedy,C.;Liu,W.;Jia,Y.;Sermanet,P.;Reed,S.;\n",
      "AcceleratingDeepNetworkTrainingbyReducingInternal Anguelov,D.;Erhan,D.;Vanhoucke,V.;andRabinovich,A.\n",
      "CovariateShift. arXivpreprintarXiv:1502.03167. 2015. Goingdeeperwithconvolutions. InProceedingsof\n",
      "[2014] Kalchbrenner, N.; Grefenstette, E.; andBlunsom, P. IEEECVPR,1–9.\n",
      "2014. AConvolutionalNeuralNetworkforModellingSen- [2016] Szegedy,C.;Vanhoucke,V.;Ioffe,S.;Shlens,J.;and\n",
      "tences. InProceedingsofACL2014,Volume1: LongPapers, Wojna,Z. 2016. RethinkingtheInceptionArchitecturefor\n",
      "655–665. Computer Vision. In Proceedings of IEEE CVPR, 2818–\n",
      "[2014] Kim, Y. 2014. Convolutional Neural Networks for 2826.\n",
      "Sentence Classification. In Proceedings of EMNLP 2014, [2015] Toutanova,K.,andChen,D. 2015. ObservedVersus\n",
      "1746–1751. LatentFeaturesforKnowledgeBaseandTextInference. In\n",
      "Proceedingsofthe3rdWorkshoponContinuousVectorSpace Table8: ConvElinkpredictionresultsforUMLS,Nations,\n",
      "ModelsandtheirCompositionality,57–66. andKinship.\n",
      "[2015] Toutanova,K.;Chen,D.;Pantel,P.;Poon,H.;Choud-\n",
      "hury,P.;andGamon,M. 2015. RepresentingTextforJoint Hits\n",
      "EmbeddingofTextandKnowledgeBases. InProceedingsof Dataset Model MR MRR @10 @3 @1\n",
      "EMNLP2015,volume15,1499–1509.\n",
      "UMLS ConvE 1.94.99.96.92\n",
      "[2016] Trouillon,T.;Welbl,J.;Riedel,S.;Gaussier,É.;and\n",
      "Kinship ConvE 2.83.98.92.74\n",
      "Bouchard,G. 2016. ComplexEmbeddingsforSimpleLink\n",
      "Prediction. InProceedingsofICML2016,2071–2080.\n",
      "[2015] Yang,B.;Yih,W.;He,X.;Gao,J.;andDeng,L. 2015.\n",
      "the test set. Following Bordes et al. (2013a), for the i-th\n",
      "EmbeddingEntitiesandRelationsforLearningandInference\n",
      "testtriplex inT, wegenerateallitspossiblecorruptions\n",
      "inKnowledgeBases. InProceedingsofICLR2015. i\n",
      "Cs(x ) (resp. Co(x )) – obtained by replacing its subject\n",
      "i i\n",
      "[2011] Yih, W.; Toutanova, K.; Platt, J. C.; and Meek, C.\n",
      "(resp. object)withanyotherentityintheKnowledgeGraph\n",
      "2011.LearningDiscriminativeProjectionsforTextSimilarity\n",
      "–tocheckwhetherthemodelassignsanhigherscoretox\n",
      "i\n",
      "Measures. InProceedingsofCoNLL2011,247–256.\n",
      "and a lower score to its corruptions. Note that the set of\n",
      "corruptionscanalsocontainseveraltruetriples,anditisnot\n",
      "SUPPLEMENTAL MATERIAL\n",
      "amistaketorankthemwithanhigherscorethanx. Forsuch\n",
      "i\n",
      "areason,weremovealltriplesinthegraphfromthesetof\n",
      "Versions\n",
      "corruptions:thisisreferredtoasthefilteredsettinginBordes\n",
      "• 2018-07-04: etal.(2013a). Theleftandrightrankofthei-thtesttriple–\n",
      "– Added new YAGO3-10 results. The new results are eachassociatedtocorruptingeitherthesubjectortheobject–\n",
      "worseonmostmetrics,butstate-of-the-artresultsare accordingtoamodelwithscoringfunctionψ(·),aredefined\n",
      "retained. asfollows:\n",
      "– IwasunabletoreplicateFB15kscoresthatIinitially (cid:88)\n",
      "ranks =1+ I[ψ(x )<ψ(x˜ )],\n",
      "reported.4 i i i\n",
      "– IupdatethePageRanktableandthereportedPageRank-\n",
      "x˜i∈Cs(xi)\\G\n",
      "(cid:88)\n",
      "error-reductioncorrelationtoreflectthenewscores. ranko =1+ I[ψ(x )<ψ(x˜ )],\n",
      "i i i\n",
      "– I removed the Nations scores in the appendix. The x˜i∈Co(xi)\\G\n",
      "Nationsdatasethasahighproportionofinverserelation-\n",
      "shipsandisthusnotsuitablefortheuseinresearch. I\n",
      "whereI[P]is1ifftheconditionP istrue,and0otherwise.\n",
      "donotwanttoencourageitsuse. Formeasuringthequalityoftheranking,weusetheMean\n",
      "ReciprocalRank(MRR)andtheHits@kmetrics,whichare\n",
      "• 2018-04-06: VictoriaLinhelpedustofindandfixissues5\n",
      "definedasfollows:\n",
      "withtriplemasksduringevaluation. Wereportnewnum-\n",
      "bersforUMLSandNationsintheappendix. Theresults 1 (cid:88) 1 1\n",
      "MRR: +,\n",
      "wereunchangedonotherdatasetsthatwetestedthusfar 2|T| ranks ranko\n",
      "(Kinship,WN18,WN18RR,FB15k-237). 100\n",
      "x (cid:88)i∈T i i\n",
      "Hits@k(%): I[ranks ≤k]+I[ranko ≤k].\n",
      "• 2018-03-28: 2|T| i i\n",
      "– NewnumbersforInverseModelafterbugfixbyTakuma\n",
      "xi∈T\n",
      "Ebisu. MRRistheaverageinverserankforalltesttriples:thehigher,\n",
      "thebetter. Hits@kisthepercentageofrankslowerthanor\n",
      "– New numbersfor UMLS/Nations/Kinship datasets in\n",
      "equaltok: thehigher,thebetter.\n",
      "appendixusingthemostcommonlyreportedtestdata\n",
      "splits.\n",
      "• 2018-01-07: ExtendedAAAIcameraready(6/7/8).\n",
      "• 2017-07-08: Missinggrantinacknowledgements.\n",
      "• 2017-07-05: OriginalNIPSsubmission(6/7/6).\n",
      "FurtherConvEresults\n",
      "EvaluationMetrics\n",
      "Wenowdescribetheevaluationmetricsusedforassessingthe\n",
      "qualityofthemodels. LetT = {x,x,...,x }denote\n",
      "1 2 |T|\n",
      "4See https://github.com/TimDettmers/ConvE/\n",
      "issues/26fordetails.\n",
      "5See https://github.com/TimDettmers/ConvE/\n",
      "issues/18formoreinformation.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   8732,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Tim Dettmers', 'Johannes Welbl', 'Peter Hayes', 'Takuma Ebisu', 'Victoria Lin']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: AutomatedKnowledgeBaseConstruction(2019) Conferencepaper\n",
      "Answering Visual-Relational Queries in\n",
      "Web-Extracted Knowledge Graphs\n",
      "Daniel Oñoro-Rubio daniel.onoro@neclab.eu\n",
      "Mathias Niepert mathias.niepert@neclab.eu\n",
      "Alberto García-Durán alberto.duran@neclab.eu\n",
      "Roberto González-Sánchez roberto.gonzalez@neclab.eu\n",
      "NEC Labs Europe\n",
      "Roberto J. López-Sastre robertoj.lopez@uah.es\n",
      "University of Alcalá\n",
      "Abstract\n",
      "A visual-relational knowledge graph (KG) is a multi-relational graph whose entities\n",
      "are associated with images. We explore novel machine learning approaches for answering\n",
      "visual-relational queries in web-extracted knowledge graphs. To this end, we have created\n",
      "ImageGraph, a KG with 1,330 relation types, 14,870 entities, and 829,931 images crawled\n",
      "from the web. With visual-relational KGs such as ImageGraph one can introduce novel\n",
      "probabilistic query types in which images are treated as first-class citizens. Both the\n",
      "prediction of relations between unseen images as well as multi-relational image retrieval\n",
      "can be expressed with specific families of visual-relational queries. We introduce novel\n",
      "combinationsofconvolutionalnetworksandknowledgegraphembeddingmethodstoanswer\n",
      "such queries. We also explore a zero-shot learning scenario where an image of an entirely\n",
      "new entity is linked with multiple relations to entities of an existing KG. The resulting\n",
      "multi-relational grounding of unseen entity images into a knowledge graph serves as a\n",
      "semantic entity representation. We conduct experiments to demonstrate that the proposed\n",
      "methods can answer these visual-relational queries efficiently and accurately.\n",
      "1. Introduction\n",
      "Numerous applications can be modeled with a knowledge graph representing entities with\n",
      "nodes, object attributes with node attributes, and relationships between entities by directed\n",
      "typed edges. For instance, a product recommendation system can be represented as a\n",
      "knowledge graph where nodes represent customers and products and where typed edges\n",
      "represent customer reviews and purchasing events. In the medical domain, there are several\n",
      "knowledge graphs that model diseases, symptoms, drugs, genes, and their interactions (cf.\n",
      "[Ashburner et al., 2000, Wishart et al., 2008]). Increasingly, entities in these knowledge\n",
      "graphs are associated with visual data. For instance, in the online retail domain, there are\n",
      "product and advertising images and in the medical domain, there are patient-associated\n",
      "imaging data sets (MRIs, CTs, and so on). In addition, visual data is a large part of social\n",
      "networks and, in general, the world wide web.\n",
      "Knowledge graphs facilitate the integration, organization, and retrieval of structured\n",
      "data and support various forms of search applications. In recent years KGs have been\n",
      "1Project URL: https://github.com/nle-ml/mmkb.git.\n",
      "2This paper is part of the proceedings of AKBC 2019.\n",
      "1\n",
      "9102\n",
      "yaM\n",
      "3\n",
      "]GL.sc[\n",
      "6v41320.9071:viXra\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "?\n",
      "(1)\n",
      "(2) locatedIn?\n",
      "Gotoh Museum Murasaki Shikibu\n",
      "n\n",
      "hasArtAbout\n",
      "catedI b\n",
      "o rn\n",
      "In\n",
      "(3)\n",
      "?\n",
      "lo captialOf\n",
      "Tokyo Japan New entity\n",
      "locatedIn\n",
      "?\n",
      "(4)\n",
      "Sensō-ji\n",
      "Japan\n",
      "New entity\n",
      "(a)\n",
      "(b)\n",
      "Figure 1: (a) a small part of a visual-relational knowledge graph and a set of query types;\n",
      "and (b) some visual-relational query types;\n",
      "playing an increasingly crucial role in fields such as question answering [Das et al., 2017],\n",
      "language modeling [Ahn et al., 2016], and text generation [Serban et al., 2016]. Even\n",
      "though there is a large body of work on constructing and maintaining KGs, the setting of\n",
      "visual-relational KGs, where entities are associated with visual data, has not received much\n",
      "attention. A visual-relational KG represents entities, relations between these entities, and a\n",
      "large number of images associated with the entities (see Figure 1a for an example). While\n",
      "ImageNet [Deng et al., 2009] and the VisualGenome [Krishna et al., 2016] datasets are\n",
      "based on KGs such as WordNet they are predominantly used as either an object classification\n",
      "data set as in the case of ImageNet or to facilitate scene understanding in a single image.\n",
      "With this work, we address the problem of reasoning about visual concepts across a large set\n",
      "of images organized in a knowledge graph. We want to explore to what extent web-extracted\n",
      "visual data can be used to enrich existing KGs so as to facilitate complex visual search\n",
      "applications going beyond basic image retrieval.\n",
      "The core idea of our work is to treat images as first-class citizens both in KGs and\n",
      "visual-relational queries. The main objective of our work is to understand to what extent\n",
      "visual data associated with entities of a KG can be used in conjunction with deep learning\n",
      "methods to answer these visual-relational queries. Allowing images to be arguments of\n",
      "queries facilitates numerous novel query types. In Figure 1b we list some of the query types\n",
      "we address in this paper. In order to answer these queries, we built on KG embedding\n",
      "methods as well as deep representation learning approaches for visual data. This allows us\n",
      "to answer these visual queries both accurately and efficiently.\n",
      "Therearenumerousapplicationdomainsthatcouldbenefitfromqueryansweringinvisual\n",
      "KGs. Forinstance,inonlineretail,visualrepresentationsofnovelproductscouldbeleveraged\n",
      "for zero-shot product recommendations. Crucially, instead of only being able to retrieve\n",
      "similar products, a visual-relational KG would support the prediction of product attributes\n",
      "and more specifically what attributes customers might be interested in. For instance, in\n",
      "2\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "Table 1: Statistics of the knowledge graphs used in this paper.\n",
      "Entities Relations Triples Images\n",
      "|E| |R| Train Valid Test Train Valid Test\n",
      "ImageNet[Dengetal.,2009] 21,841 18 - 14,197,122\n",
      "VisualGenome[Krishnaetal.,2016] 75,729 40,480 1,531,448 108,077\n",
      "FB15k[Bordesetal.,2013] 14,951 1,345 483,142 50,000 59,071 0 0 0\n",
      "ImageGraph 14,870 1,330 460,406 47,533 56,071 411,306 201,832 216,793\n",
      "Japan Football Michael Jackson Madrid The Simpsons Drummer\n",
      "Figure 2: Image samples for some entities of ImageGraph.\n",
      "the fashion industry visual attributes are crucial for product recommendations [Liu et al.,\n",
      "2016, Veit et al., 2015]. Being able to ground novel visual concepts into an existing KG with\n",
      "attributes and various relation types is a reasonable approach to zero-shot learning.\n",
      "We make the following contributions. First, we introduce ImageGraph, a visual-\n",
      "relational web-extracted KG with 1,330 relations where 829,931 images are associated with\n",
      "14,870 different entities. Second, we introduce a new set of visual-relational query types.\n",
      "Third, we propose a novel set of neural architectures and objectives that we use for answering\n",
      "these novel query types. These query types generalize image retrieval and link prediction\n",
      "queries. This is the first time that deep CNNs and KG embedding learning objectives\n",
      "are combined into a joint model. Fourth, we show that the proposed class of deep neural\n",
      "networks are also successful for zero-shot learning, that is, creating relations between entirely\n",
      "unseen entities and the KG using only visual data at query time.\n",
      "2. Related Work\n",
      "We discuss the relation of our contributions to previous work with an emphasis on relational\n",
      "learning, image retrieval, object detection, scene understanding, existing data sets, and\n",
      "zero-shot learning.\n",
      "Relational Learning\n",
      "There has been a flurry of approaches tailored to specific problems such as link prediction\n",
      "in multi-relational graphs. Examples are knowledge base factorization and embedding\n",
      "approaches [Bordes et al., 2013, Nickel et al., 2011, Guu et al., 2015] and random-walk based\n",
      "ML models [Lao et al., 2011, Gardner and Mitchell, 2015]. More recently, the focus has been\n",
      "on integrating additional attribute types such as text [Yahya et al., 2016, C. et al., 2017],\n",
      "temporal graph dynamics [Trivedi et al., 2017], and multiple modalities [Pezeshkpour et al.,\n",
      "2018]. Another line of research is concerned with extensions of the link prediction problem to\n",
      "multi-hopreasoning[Zhangetal.,2018]. Wecannotlistallpriorlinkpredictionmethodshere\n",
      "3\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "103\n",
      "101\n",
      "100 101 102 103\n",
      "Relation types\n",
      "tnuoc\n",
      "elpirT\n",
      "1200\n",
      "104\n",
      "award_nomineperofession\n",
      "disea ps re oduction_company\n",
      "tnuoC\n",
      "ytitnE\n",
      "1 24680 00000 00000\n",
      "11 00 23\n",
      "tv_actor 0\n",
      "ingredient 101\n",
      "100\n",
      "101 103\n",
      "Entities\n",
      "tnuoc\n",
      "elpirT\n",
      "United States of America\n",
      "English Language\n",
      "Executive Producer\n",
      "University of Oxford\n",
      "Parlophone\n",
      "Vigor Shipyards\n",
      "Figure 3: (Left) The distribution of relation types; (center) the 10 most frequent entity\n",
      "types; and (right) the distribution of entities in ImageGraph.\n",
      "and instead refer the reader to two survey papers [Nickel et al., 2016a, Al Hasan and Zaki,\n",
      "2011]. Contrarytoexistingapproaches, weaddresstheproblemofansweringvisual-relational\n",
      "queries in knowledge graphs where the entities are associated with web-extracted images.\n",
      "We also address the zero-shot learning scenario, a problem that has not been addressed in\n",
      "the context of link prediction in multi-relational graphs.\n",
      "Image ranking\n",
      "Image retrieval is a popular problem and has been addressed by several authors [Wang et al.,\n",
      "2014, Yang et al., 2016, Jiang et al., 2017, Niu et al., 2018, Guy et al., 2018]. In [Yang\n",
      "et al., 2016] a re-ranking of the output of a given search engine by learning a click-based\n",
      "multi-feature similarity is proposed. The authors performed spectral clustering and obtained\n",
      "the final ranked results by computing click-based clusters. In [Guy et al., 2018] the authors\n",
      "fine-tune a DNN to rank photos a user might like to share in social media as well as a\n",
      "mechanism to detect duplicates. In [Niu et al., 2018] a joint user-image embedding is learned\n",
      "to generate a ranking based on user preferences. Contrary to these previous approaches we\n",
      "introduce a set of novel visual query types in a web-extracted KG with images and provide\n",
      "methods to answer these queries efficiently.\n",
      "Relational and Visual Data\n",
      "Previous work on combining relational and visual data has focused on object detection\n",
      "[Felzenszwalb et al., 2010, Girshick et al., 2014, Russakovsky et al., 2013, Marino et al.,\n",
      "2017, Li et al., 2017] and scene recognition [Doersch et al., 2013, Pandey and Lazebnik, 2011,\n",
      "Sadeghi and Tappen, 2012, Xiao et al., 2010, Teney et al., 2017] which are required for more\n",
      "complex visual-relational reasoning. Recent years have witnessed a surge in reasoning about\n",
      "human-object, object-object, and object-attribute relationships [Gupta et al., 2009, Farhadi\n",
      "et al., 2009, Malisiewicz and Efros, 2009, Yao and Fei-Fei, 2010, Felzenszwalb et al., 2010,\n",
      "Chen et al., 2013, Izadinia et al., 2014, Zhu et al., 2014]. The VisualGenome project [Krishna\n",
      "et al., 2016] is a knowledge base that integrates language and vision modalities. The project\n",
      "provides a knowledge graph, based on WordNet, which provides annotations of categories,\n",
      "attributes, and relation types for each image. Recent work has used the dataset to focus on\n",
      "scene understanding in single images. For instance, Lu et al. [Lu et al., 2016] proposed a\n",
      "model to detect relation types between objects depicted in an image by inferring sentences\n",
      "4\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "such as “man riding bicycle.\" Veit et al. [Veit et al., 2015] propose a siamese CNN to learn a\n",
      "metric representation on pairs of textile products so as to learn which products have similar\n",
      "styles. There is a large body of work on metric learning where the objective is to generate\n",
      "image embeddings such that a pairwise distance-based loss is minimized [Schroff et al., 2015,\n",
      "Bell and Bala, 2015, Oh Song et al., 2016, Sohn, 2016, Wang et al., 2017]. Recent work has\n",
      "extended this idea to directly optimize a clustering quality metric [Song et al., 2017]. In\n",
      "Vincent et al. [Vincent et al., 2017] they proposed a mutual embedding space for images and\n",
      "knowledge graphs so the relationships between an image and known entities in a knowledge\n",
      "graph are jointly encoded. Zhou et al. [Zhou and Lin, 2016] propose a method based on a\n",
      "bipartite graph that links depictions of meals to its ingredients. Johnson et al. [Johnson\n",
      "et al., 2015] propose to use the VisualGenome data to recover images from text queries. In\n",
      "the work of Thoma et al. [Thoma et al., 2017], they merge in a joint representation the\n",
      "embeddings from images, text, and KG and use the representation to perform link prediction\n",
      "on DBpedia [Lehmann et al., 2015]. ImageGraph is different from these data sets in that\n",
      "the relation types hold between different images and image annotated entities. This defines\n",
      "a novel class of problems where one seeks to answer queries such as “How are these two\n",
      "images related?\" With this work, we address problems ranging from predicting the relation\n",
      "types for image pairs to multi-relational image retrieval.\n",
      "Zero-shot Learning\n",
      "We focus on exploring ways in which KGs can be used to find relationships between visual\n",
      "data of unseen entities, that is, entities not part of the KG during training, and visual data\n",
      "of known KG entities. This is a form of zero-shot learning (ZSL) where the objective is to\n",
      "generalize to novel visual concepts. Generally, ZSL methods (e.g. [Romera-Paredes and\n",
      "Torr, 2015, Zhang and Saligrama, 2015]) rely on an underlying embedding space, such as\n",
      "one based on visual attributes, to recognize unseen categories. With this paper, we do not\n",
      "assume the availability of such a common embedding space but we assume the existence of\n",
      "an external visual-relational KG. Similar to our approach, when this explicit knowledge is\n",
      "not encoded in the underlying embedding space, other works rely on finding the similarities\n",
      "through linguistic patterns (e.g. [Ba et al., 2015, Lu et al., 2016]), leveraging distributional\n",
      "word representations so as to capture a notion of similarity. These approaches, however,\n",
      "address scene understanding in a single image, i.e. these models are able to detect the\n",
      "visual relationships in one given image. Our approach, on the other hand, finds relationships\n",
      "between different images and entities.\n",
      "3. ImageGraph: A Web-Extracted Visual Knowledge Graph\n",
      "ImageGraphisavisual-relationalKGwhoserelationalstructureisbasedonFreebase[Bol-\n",
      "lacker et al., 2008] and, more specifically, on FB15k, a subset of FreeBase and a popular\n",
      "benchmark data set [Nickel et al., 2016a]. Since FB15k does not include visual data, we\n",
      "perform the following steps to enrich the KG entities with image data. We implemented a\n",
      "web crawler that is able to parse query results for the image search engines Google Images,\n",
      "Bing Images, and Yahoo Image Search. To minimize the amount of noise due to polysemous\n",
      "entity labels (for example, there are more than 100 Freebase entities with the text label\n",
      "“Springfield\") we extracted, for each entity in FB15k, all Wikipedia URIs from the 1.9 billion\n",
      "5\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "Relationtype Example(h,r,t) Symmetric Others\n",
      "Symmetric (EmmaThompson,sibling,SophieThompson) 4%8%\n",
      "(SophieThompson,sibling,EmmaThompson)\n",
      "(Non-profitorganization,company_type,ApacheSoftwareFoundation)\n",
      "Asymmetric\n",
      "(Statistics,students_majoring,PhD)\n",
      "(StarWars,film_series,StarWars)\n",
      "88%\n",
      "Others (StarWarsEpisodeI:ThePhantomMenace,film_series,StarWars)\n",
      "(StarWarsEpisodeII:AttackoftheClones,film_series,StarWars) Asymmetric\n",
      "Figure 4: (Left) Example triples for symmetric, asymmetric and others relation types.\n",
      "(Right) Fraction of symmetric, asymmetric, and other relation types among all\n",
      "relation types in ImageGraph.\n",
      "triple Freebase RDF dump. For instance, for Springfield, Massachusetts, we obtained such\n",
      "URIs as Springfield_(Massachusetts,United_States) and Springfield_(MA). These\n",
      "URIs were processed and used as search queries for disambiguation purposes. We used\n",
      "the crawler to download more than 2.4M images (more than 462Gb of data). We removed\n",
      "corrupted, low quality, and duplicate images and we used the 25 top images returned by\n",
      "each of the image search engines whenever there were more than 25 results. The images\n",
      "were scaled to have a maximum height or width of 500 pixels while maintaining their\n",
      "aspect ratio. This resulted in 829,931 images associated with 14,870 different entities (55.8\n",
      "images per entity). After filtering out triples where either the head or tail entity could\n",
      "not be associated with an image, the visual KG consists of 564,010 triples expressing 1,330\n",
      "different relation types between 14,870 entities. We provide three sets of triples for training,\n",
      "validation, and testing plus three more image splits also for training, validation and test.\n",
      "Table 1 lists the statistics of the resulting visual KG. Any KG derived from FB15k such\n",
      "as FB15k-237[Toutanova and Chen, 2015] can also be associated with the crawled images.\n",
      "Since providing the images themselves would violate copyright law, we provide the code\n",
      "for the distributed crawler and the list of image URLs crawled for the experiments in this\n",
      "paper2.\n",
      "The distribution of relation types is depicted in Figure 3 (left). It plots for each relation\n",
      "type the number of triples it occurs in. Some relation types such as award_nominee or\n",
      "profession occur quite frequently while others such as ingredient have only few instances.\n",
      "4% of the relation types are symmetric, 88% are asymmetric, and 8% are others (see\n",
      "Table 4 (left)). Table 4 (right) lists specific instances of some relation types. There are 585\n",
      "distinct entity types such as Person,Athlete, and City. Figure 3 (center) shows the most\n",
      "frequent entity types. Figure 3 (right) visualizes the distribution of entities in the triples of\n",
      "ImageGraph and some example entities.\n",
      "Table 1 lists some statistics of the ImageGraph KG and other KGs from related work.\n",
      "First, we would like to emphasize the differences between ImageGraph and the Visual\n",
      "Genome project (VG) [Krishna et al., 2016]. With ImageGraph we address the problem\n",
      "of learning a representation for a KG with canonical relation types and not for relation\n",
      "types expressed through text. On a high level, we focus on answering visual-relational\n",
      "queries in a web-extracted KG. This is related to information retrieval except that in our\n",
      "proposed work, images are first-class citizens and we introduce novel and more complex\n",
      "2ImageGraph crawler and URLs: https://github.com/robegs/imageDownloader.\n",
      "6\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "query types. In contrast, VGD is focused on modeling relations between objects in images\n",
      "and the relation types are expressed in natural language. Additional differences between\n",
      "ImageGraph and ImageNet are the following. ImageNet is based on WordNet a lexical\n",
      "database where synonymous words from the same lexical category are grouped into synsets.\n",
      "There are 18 relations expressing connections between synsets. In Freebase, on the other\n",
      "hand, there are two orders of magnitudes more relations. In FB15k, the subset we focus on,\n",
      "there are 1,345 relations expressing location of places, positions of basketball players, and\n",
      "gender of entities. Moreover, entities in ImageNet exclusively represent entity types such\n",
      "as Cats and Cars whereas entities in FB15k are either entity types or instances of entity\n",
      "types such as Albert Einstein and Paris. This renders the computer vision problems\n",
      "associated with ImageGraph more challenging than those for existing datasets. Moreover,\n",
      "with ImageGraph the focus is on learning relational ML models that incorporate visual\n",
      "data both during learning and at query time.\n",
      "4. Representation Learning for Visual-Relational Graphs\n",
      "A knowledge graph (KG) K is given by a set of triples T, that is, statements of the form\n",
      "(h,r,t), where h,t ∈ E are the head and tail entities, respectively, and r ∈ R is a relation\n",
      "type. Figure 1a depicts a small fragment of a KG with relations between entities and images\n",
      "associated with the entities. Prior work has not included image data and has, therefore,\n",
      "focused on the following two types of queries. First, the query type (h,r?,t) asks for the\n",
      "relations between a given pair of head and tail entities. Second, the query types (h,r,t?)\n",
      "and (h?,r,t), asks for entities correctly completing the triple. The latter query type is often\n",
      "referred to as knowledge base completion. Here, we focus on queries that involve visual data\n",
      "as query objects, that is, objects that are either contained in the queries, the answers to the\n",
      "queries, or both.\n",
      "4.1 Visual-Relational Query Answering\n",
      "When entities are associated with image data, several completely novel query types are\n",
      "possible. Figure 1b lists the query types we focus on in this paper. We refer to images used\n",
      "during training as seen and all other images as unseen.\n",
      "(1) Given a pair of unseen images for which we do not know their KG entities, determine\n",
      "the unknown relations between the underlying entities.\n",
      "(2) Given an unseen image, for which we do not know the underlying KG entity, and a\n",
      "relation type, determine the seen images that complete the query.\n",
      "(3) Given an unseen image of an entirely new entity that is not part of the KG, and an\n",
      "unseen image for which we do not know the underlying KG entity, determine the\n",
      "unknown relations between the two underlying entities.\n",
      "(4) Given an unseen image of an entirely new entity that is not part of the KG, and a\n",
      "known KG entity, determine the unknown relations between the two entities.\n",
      "For each of these query types, the sought-after relations between the underlying entities\n",
      "have never been observed during training. Query types (3) and (4) are a form of zero-shot\n",
      "7\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "Sensō-ji Japan\n",
      "DistMult\n",
      "2\n",
      "VGG16 5\n",
      "6 r?\n",
      "g\n",
      "op VGG16 VGG16\n",
      "r?\n",
      "2\n",
      "VGG16 5 r\n",
      "6\n",
      "g\n",
      "f\n",
      "(a) (b)\n",
      "Figure 5: (a) the proposed architecture for query answering; and (b) an illustration of two\n",
      "possible approaches to visual-relational query answering. One can predict relation\n",
      "types between two images directly (green arrow; our approach) or combine an\n",
      "entity classifier with a KB embedding model for relation prediction (red arrows;\n",
      "baseline VGG16+DistMult).\n",
      "learning since neither the new entity’s relationships with other entities nor its images have\n",
      "been observed during training. These considerations illustrate the novel nature of the visual\n",
      "query types. The machine learning models have to be able to learn the relational semantics\n",
      "of the KG and not simply a classifier that assigns images to entities. These query types are\n",
      "also motivated by the fact that for typical KGs the number of entities is orders of magnitude\n",
      "greater than the number of relations.\n",
      "4.2 Deep Representation Learning for Visual-Relational Query Answering\n",
      "We first discuss KG completion methods and translate the concepts to query answering in\n",
      "visual-relational KGs. Let raw be the raw feature representation for entity i ∈ E and let\n",
      "i\n",
      "f and g be differentiable functions. Most KG completion methods learn an embedding of\n",
      "the entities in a vector space via some scoring function that is trained to assign high scores\n",
      "to correct triples and low scores to incorrect triples. Scoring functions have often the form\n",
      "f (e,e ) where r is a relation type, e and e are d-dimensional vectors (the embeddings of\n",
      "r h t h t\n",
      "the head and tail entities, respectively), and where e = g(raw ) is an embedding function\n",
      "i i\n",
      "that maps the raw input representation of entities to the embedding space. In the case of\n",
      "KGs without visual data, the raw representation of an entity is simply its one-hot encoding.\n",
      "(cid:124)\n",
      "Existing KG completion methods use the embedding function g(raw ) = raw W where\n",
      "i i\n",
      "W is a |E|×d matrix, and differ only in their scoring function, that is, in the way the\n",
      "embeddings of the head and tail entities are combined with the parameter vector φ :\n",
      "r\n",
      "• Difference (TransE[Bordes et al., 2013]): f (e,e ) = −||e +φ −e || where φ is\n",
      "r h t h r t 2 r\n",
      "a d-dimensional vector;\n",
      "• Multiplication (DistMult[Yang et al., 2014]): f (e,e ) = (e ∗e )·φ where ∗ is\n",
      "r h t h t r\n",
      "the element-wise product and φ a d-dimensional vector;\n",
      "r\n",
      "• Circular correlation (HolE[Nickel et al., 2016b]): f (e,e ) = (e?e )·φ where\n",
      "r h t h t r\n",
      "[a?b] = Pd−1a b and φ a d-dimensional vector; and\n",
      "k i=0 i (i+k) mod d r\n",
      "8\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "• Concatenation: f (e,e ) = (e (cid:12)e )·φ where (cid:12) is the concatenation operator and\n",
      "r h t h t r\n",
      "φ a 2d-dimensional vector.\n",
      "r\n",
      "For each of these instances, the matrix W (storing the entity embeddings) and the\n",
      "vectors φ are learned during training. In general, the parameters are trained such that\n",
      "r\n",
      "f (e,e ) is high for true triples and low for triples assumed not to hold in the KG. The\n",
      "r h t\n",
      "training objective is often based on the logistic loss, which has been shown to be superior\n",
      "for most of the composition functions [Trouillon et al., 2016],\n",
      "min X log(1+exp(−f (e,e ))+ X log(1+exp(f (e,e )))+λ||Θ||2, (1)\n",
      "r h t r h t 2\n",
      "Θ\n",
      "(h,r,t)∈Tpos (h,r,t)∈Tneg\n",
      "where T and T are the set of positive and negative training triples, respectively, Θ are\n",
      "pos neg\n",
      "the parameters trained during learning and λ is a regularization hyperparameter. For the\n",
      "above objective, a process for creating corrupted triples T is required. This often involves\n",
      "neg\n",
      "sampling a random entity for either the head or tail entity. To answer queries of the types\n",
      "(h,r,t?) and (h?,r,t) after training, we form all possible completions of the queries and\n",
      "compute a ranking based on the scores assigned by the trained model to these completions.\n",
      "For the queries of type (h,r?,t) one typically uses the softmax activation in conjunction\n",
      "with the categorical cross-entropy loss, which does not require negative triples\n",
      "!\n",
      "min X −log exp(f r(e h,e t)) +λ||Θ||2, (2)\n",
      "Θ\n",
      "(h,r,t)∈Tpos\n",
      "P r∈Rexp(f r(e h,e t)) 2\n",
      "where Θ are the parameters trained during learning.\n",
      "For visual-relational KGs, the input consists of raw image data instead of the one-hot\n",
      "encodings of entities. The approach we propose builds on the ideas and methods developed\n",
      "for KG completion. Instead of having a simple embedding function g that multiplies the\n",
      "input with a weight matrix, however, we use deep convolutional neural networks to extract\n",
      "meaningfulvisualfeaturesfromtheinputimages. Forthecompositionfunctionfweevaluate\n",
      "thefouroperationsthatwereusedintheKGcompletionliterature: difference, multiplication,\n",
      "concatenation, and circular correlation. Figure 5a depicts the basic architecture we trained\n",
      "forqueryanswering. Theweightsofthepartsoftheneuralnetworkresponsibleforembedding\n",
      "the raw image input, denoted by g, are tied. We also experimented with additional hidden\n",
      "layers indicated by the dashed dense layer. The composition operation op is either difference,\n",
      "multiplication, concatenation, or circular correlation. To the best of our knowledge, this\n",
      "is the first time that KG embedding learning and deep CNNs have been combined for\n",
      "visual-relationsl query answering.\n",
      "5. Experiments\n",
      "We conduct a series of experiments to evaluate the proposed approach. First, we describe\n",
      "the experimental set-up that applies to all experiments. Second, we report and interpret\n",
      "results for the different types of visual-relational queries.\n",
      "9\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "5.1 General Set-up\n",
      "We used Caffe, a deep learning framework [Jia et al., 2014] for designing, training, and\n",
      "evaluating the proposed models. The embedding function g is based on the VGG16 model in-\n",
      "troducedin[SimonyanandZisserman,2014]. Wepre-trainedtheVGG16ontheILSVRC2012\n",
      "data set derived from ImageNet [Deng et al., 2009] and removed the softmax layer of\n",
      "the original VGG16. We added a 256-dimensional layer after the last dense layer of the\n",
      "VGG16. The output of this layer serves as the embedding of the input images. The reason\n",
      "for reducing the embedding dimensionality from 4096 to 256 is motivated by the objective\n",
      "to obtain an efficient and compact latent representation that is feasible for KGs with billion\n",
      "of entities. For the composition function f, we performed either of the four operations\n",
      "difference, multiplication, concatenation, and circular correlation. We also experimented\n",
      "with an additional hidden layer with ReLu activation. Figure 5a depicts the generic network\n",
      "architecture. The output layer of the architecture has a softmax or sigmoid activation with\n",
      "cross-entropy loss. We initialized the weights of the newly added layers with the Xavier\n",
      "method [Glorot and Bengio, 2010].\n",
      "We used a batch size of 45 which was the maximal possible fitting into GPU memory.\n",
      "To create the training batches, we sample a random triple uniformly at random from the\n",
      "training triples. For the given triple, we randomly sample one image for the head and one\n",
      "for the tail from the set of training images. We applied SGD with a learning rate of 10−5\n",
      "for the parameters of the VGG16 and a learning rate of 10−3 for the remaining parameters.\n",
      "It is crucial to use two different learning rates since the large gradients in the newly added\n",
      "layers would lead to unreasonable changes in the pretrained part of the network. We set\n",
      "the weight decay to 5×10−4. We reduced the learning rate by a factor of 0.1 every 40,000\n",
      "iterations. Each of the models was trained for 100,000 iterations.\n",
      "Since the answers to all query types are either rankings of images or rankings of relations,\n",
      "we utilize metrics measuring the quality of rankings. In particular, we report results for\n",
      "hits@1 (hits@10, hits@100) measuring the percentage of times the correct relation was\n",
      "ranked highest (ranked in the top 10, top 100). We also compute the median of the ranks\n",
      "of the correct entities or relations and the Mean Reciprocal Rank (MRR) for entity and\n",
      "relation rankings, respectively, defined as follows:\n",
      "!\n",
      "1 X 1 1\n",
      "MRR = + (3)\n",
      "2|T| rank rank<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   7099,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['FB15k', 'ImageGraph', 'VisualGenome', 'ImageNet']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  gradients in the newly added\n",
      "layers would lead to unreasonable changes in the pretrained part of the network. We set\n",
      "the weight decay to 5×10−4. We reduced the learning rate by a factor of 0.1 every 40,000\n",
      "iterations. Each of the models was trained for 100,000 iterations.\n",
      "Since the answers to all query types are either rankings of images or rankings of relations,\n",
      "we utilize metrics measuring the quality of rankings. In particular, we report results for\n",
      "hits@1 (hits@10, hits@100) measuring the percentage of times the correct relation was\n",
      "ranked highest (ranked in the top 10, top 100). We also compute the median of the ranks\n",
      "of the correct entities or relations and the Mean Reciprocal Rank (MRR) for entity and\n",
      "relation rankings, respectively, defined as follows:\n",
      "!\n",
      "1 X 1 1\n",
      "MRR = + (3)\n",
      "2|T| rank rank\n",
      "(h,r,t)∈T\n",
      "img(h) img(t)\n",
      "1 X 1\n",
      "MRR =, (4)\n",
      "|T| rank\n",
      "r\n",
      "(h,r,t)∈T\n",
      "where T is the set of all test triples, rank is the rank of the correct relation, and rank\n",
      "r img(h)\n",
      "is the rank of the highest ranked image of entity h. For each query, we remove all triples\n",
      "that are also correct answers to the query from the ranking. All experiments were run on\n",
      "commodity hardware with 128GB RAM, a single 2.8 GHz CPU, and a NVIDIA 1080 Ti.\n",
      "5.2 Visual Relation Prediction\n",
      "Given a pair of unseen images we want to determine the relations between their underlying\n",
      "unknown entities. This can be expressed with (img,r?,img ). Figure 1b illustrates this\n",
      "h t\n",
      "10\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "Table 2: Results for the relation prediction problem.\n",
      "Model Median Hits@1 Hits@10 MRR\n",
      "VGG16+DistMult 94 6.0 11.4 0.087\n",
      "Prob. Baseline 35 3.7 26.5 0.104\n",
      "DIFF 11 21.1 50.0 0.307\n",
      "MULT 8 15.5 54.3 0.282\n",
      "CAT 6 26.7 61.0 0.378\n",
      "DIFF+1HL 8 22.6 55.7 0.333\n",
      "MULT+1HL 9 14.8 53.4 0.273\n",
      "CAT+1HL 6 25.3 60.0 0.365\n",
      "query type which we refer to as visual relation prediction. We train the deep architectures\n",
      "using the training and validation triples and images, respectively. For each triple (h,r,t)\n",
      "in the training data set, we sample one training image uniformly at random for both the\n",
      "head and the tail entity. We use the architecture depicted in Figure 5a with the softmax\n",
      "activation and the categorical cross-entropy loss. For each test triple, we sample one image\n",
      "uniformly at random from the test images of the head and tail entity, respectively. We then\n",
      "use the pair of images to query the trained deep neural networks. To get a more robust\n",
      "statistical estimate of the evaluation measures, we repeat the above process three times per\n",
      "test triple. Again, none of the test triples and images are seen during training nor are any\n",
      "of the training images used during testing. Computing the answer to one query takes the\n",
      "model 20 ms.\n",
      "We compare the proposed architectures to two different baselines: one based on entity\n",
      "classificationfollowedbyaKBembeddingmethodforrelationprediction(VGG16+DistMult),\n",
      "and a probabilistic baseline (Prob. Baseline). The entity classification baseline consists of\n",
      "fine-tuning a pretrained VGG16 to classify images into the 14,870 entities of ImageGraph.\n",
      "To obtain the relation type ranking at test time, we predict the entities for the head and\n",
      "the tail using the VGG16 and then use the KB embedding method DistMult[Yang et al.,\n",
      "2014] to return a ranking of relation types for the given (head, tail) pair. DistMult is a KB\n",
      "embeddingmethodthatachievesstateoftheartresultsforKBcompletiononFB15k[Kadlec\n",
      "et al., 2017]. Therefore, for this experiment we just substitute the original output layer of\n",
      "the VGG16 pretrained on ImageNet with a new output layer suitable for our problem.\n",
      "To train, we join the train an validation splits, we set the learning rate to 10−5 for all the\n",
      "layers and we train following the same strategy that we use in all of our experiments. Once\n",
      "the system is trained, we test the model by classifying the entities of the images in the test\n",
      "set. To train DistMult, we sample 500 negatives triples for each positive triple and used an\n",
      "embedding size of 100. Figure 5b illustrates the VGG16+DistMult baseline and contrasts\n",
      "it with our proposed approach. The second baseline (probabilistic baseline) computes the\n",
      "probability of each relation type using the set of training and validation triples. The baseline\n",
      "ranks relation types based on these prior probabilities.\n",
      "Table 2 lists the results for the two baselines and the different proposed architectures.\n",
      "The probabilistic baseline outperforms the VGG16+DistMult baseline in 3 of the metrics.\n",
      "This is due to the highly skewed distribution of relation types in the training, validation, and\n",
      "11\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "...\n",
      "wonAward 3 genreOf 159407\n",
      "...\n",
      "directedBy 2 succeededBy 106817\n",
      "Figure 6: Example queries and qualitative results for the multi-relational image retrieval\n",
      "problem.\n",
      "Table 3: Results for multi-relational image retrieval.\n",
      "Median Hits@100 MRR\n",
      "Model Head Tail Head Tail Head Tail\n",
      "Baseline 6504 2789 11.9 18.4 0.065 0.115\n",
      "DIFF 1301 877 19.6 26.3 0.051 0.094\n",
      "MULT 1676 1136 16.8 22.9 0.040 0.080\n",
      "CAT 1022 727 21.4 27.5 0.050 0.087\n",
      "DIFF+1HL 1644 1141 15.9 21.9 0.045 0.085\n",
      "MULT+1HL 2004 1397 14.6 20.5 0.034 0.069\n",
      "CAT+1HL 1323 919 17.8 23.6 0.042 0.080\n",
      "CAT-SIG 814 540 23.2 30.1 0.049 0.082\n",
      "test triples. A small number of relation types makes up a large fraction of triples. Figure 3\n",
      "(left) and 3 (right) depicts the plots of the counts of relation types and entities. Moreover,\n",
      "despite DistMult achieving a hits@1 value of 0.46 for the relation prediction problem\n",
      "between entity pairs the baseline VGG16+DistMult performs poorly. This is due to the poor\n",
      "entity classification performance of the VGG (accurracy: 0.082, F1: 0.068). In the remainder\n",
      "of the experiments, therefore, we only compare to the probabilistic baseline. In the lower\n",
      "part of Table 2, we lists the results of the experiments. DIFF, MULT, and CAT stand\n",
      "for the different possible composition operations. We omitted the composition operation\n",
      "circular correlation since we were not able to make the corresponding model converge,\n",
      "despite trying several different optimizers and hyperparameter settings. The post-fix 1HL\n",
      "stands for architectures where we added an additional hidden layer with ReLu activation\n",
      "before the softmax. The concatenation operation clearly outperforms the multiplication and\n",
      "difference operations. This is contrary to findings in the KG completion literature where\n",
      "MULTandDIFFoutperformedtheconcatenationoperation. Themodelswiththeadditional\n",
      "hidden layer did not perform better than their shallower counterparts with the exception\n",
      "of the DIFF model. We hypothesize that this is due to difference being the only linear\n",
      "composition operation, benefiting from an additional non-linearity. Each of the proposed\n",
      "models outperforms the baselines.\n",
      "12\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "Median Hits@1 Hits@10 MRR\n",
      "hasCrewJob\n",
      "H T H T H T H T hasGenre\n",
      "Zero-ShotQuery(3) hasProfession\n",
      "Back to the Future Special EffectsSupervisor\n",
      "Base 34 31 1.9 2.3 18.2 28.7 0.074 0.089\n",
      "CAT 8 7 19.1 22.4 54.2 57.9 0.306 0.342 hasNutrient\n",
      "Zero-ShotQuery(4) filmHasLocation\n",
      "people.B.o.rnHere\n",
      "Base 9 5 13.0 22.6 52.3 64.8 0.251 0.359\n",
      "CAT 5 3 26.9 33.7 62.5 70.4 0.388 0.461 Library of Congress TaxonomyHasEntry Card Game\n",
      "Figure 7: (Left) Results for the zero-shot learning experiments. (Right) Example results for\n",
      "zero-shot learning. For each pair of images the top three relation types (as ranked\n",
      "by the CAT model) are listed. For the pair of images at the top, the first relation\n",
      "type is correct. For the pair of images at the bottom, the correct relation type\n",
      "TaxonomyHasEntry is not among the top three relation types.\n",
      "5.3 Multi-Relational Image Retrieval\n",
      "Given an unseen image, for which we do not know the underlying KG entity, and a relation\n",
      "type, we want to retrieve existing images that complete the query. If the image for the head\n",
      "entityisgiven,wereturnarankingofimagesforthetailentity; ifthetailentityimageisgiven\n",
      "we return a ranking of images for the head entity. This problem corresponds to query type\n",
      "(2) in Figure 1b. Note that this is equivalent to performing multi-relational metric learning\n",
      "which, to the best of our knowledge, has not been done before. We performed experiments\n",
      "with each of the three composition functions f and for two different activation/loss functions.\n",
      "First, we used the models trained with the softmax activation and the categorical cross-\n",
      "entropy loss to rank images. Second, we took the models trained with the softmax activation\n",
      "and substituted the softmax activation with a sigmoid activation and the corresponding\n",
      "binary cross-entropy loss. For each training triple (h,r,t) we then created two negative\n",
      "triples by sampling once the head and once the tail entity from the set of entities. The\n",
      "negative triples are then used in conjunction with the binary cross-entropy loss of equation 1\n",
      "to refine the pretrained weights. Directly training a model with the binary cross-entropy\n",
      "loss was not possible since the model did not converge properly. Pretraining with softmax\n",
      "and categorical cross-entropy loss was crucial to make the binary loss work.\n",
      "Duringtesting,weusedthetesttriplesandrankedtheimagesbasedontheprobabilitiesre-\n",
      "turnedbytherespectivemodels. Forinstance,giventhequery(img Senso-ji,locatedIn,img t?),\n",
      "we substituted img? with all training and validation images, one at a time, and ranked\n",
      "t\n",
      "the images according to the probabilities returned by the models. We use the rank of the\n",
      "highest ranked image belonging to the true entity (here: Japan) to compute the values for\n",
      "the evaluation measures. We repeat the same experiment three times (each time randomly\n",
      "sampling the images) and report average values. Again, we compare the results for the\n",
      "different architectures with a probabilistic baseline. For the baseline, however, we compute\n",
      "a distribution of head and tail entities for each of the relation types. For example, for the\n",
      "relation type locatedIn we compute two distributions, one for head and one for tail entities.\n",
      "We used the same measures as in the previous experiment to evaluate the returned image\n",
      "rankings.\n",
      "13\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "Table3liststheresultsoftheexperiments. Asforrelationprediction,thebestperforming\n",
      "modelsarebasedontheconcatenationoperation,followedbythedifferenceandmultiplication\n",
      "operations. Thearchitectureswithanadditionalhiddenlayerdonotimprovetheperformance.\n",
      "We also provide the results for the concatenation-based model with softmax activation where\n",
      "we refined the weights using a sigmoid activation and negative sampling as described before.\n",
      "This model is the best performing model. All neural network models are significantly better\n",
      "than the baseline with respect to the median and hits@100. However, the baseline has\n",
      "slightly superior results for the MRR. This is due to the skewed distribution of entities and\n",
      "relations in the KG (see Figure 3 (right) and Figure 3 (left)). This shows once more that\n",
      "the baseline is highly competitive for the given KG. Figure 6 visualizes the answers the\n",
      "CAT-SIG model provided for a set of four example queries. For the two queries on the left,\n",
      "the model performed well and ranked the correct entity in the top 3 (green frame). The\n",
      "examples on the right illustrate queries for which the model returned an inaccurate ranking.\n",
      "To perform query answering in a highly efficient manner, we precomputed and stored all\n",
      "image embeddings once, and only compute the scoring function (involving the composition\n",
      "operation and a dot product with φ ) at query time. Answering one multi-relational image\n",
      "r\n",
      "retrieval query (which would otherwise require 613,138 individual queries, one per possible\n",
      "image) took only 90 ms.\n",
      "5.4 Zero-Shot Visual Relation Prediction\n",
      "The last set of experiments addresses the problem of zero-shot learning. For both query\n",
      "types, we are given an new image of an entirely new entity that is not part of the KG.\n",
      "The first query type asks for relations between the given image and an unseen image for\n",
      "which we do not know the underlying KG entity. The second query type asks for the\n",
      "relations between the given image and an existing KG entity. We believe that creating\n",
      "multi-relational links to existing KG entities is a reasonable approach to zero-shot learning\n",
      "since the relations to existing visual concepts and their attributes provide a characterization\n",
      "of the new entity/category.\n",
      "For the zero-shot experiments, we generated a new set of training, validation, and test\n",
      "triples. We randomly sampled 500 entities that occur as head (tail) in the set of test triples.\n",
      "We then removed all training and validation triples whose head or tail is one of these 1000\n",
      "entities. Finally, we only kept those test triples with one of the 1000 entities either as head\n",
      "or tail but not both. For query type (4) where we know the target entity, we sample 10\n",
      "of its images and use the models 10 times to compute a probability. We use the average\n",
      "probabilities to rank the relations. For query type (3) we only use one image sampled\n",
      "randomly. As with previous experiments, we repeated procedure three times and averaged\n",
      "the results. For the baseline, we compute the probabilities of relation in the training and\n",
      "validation set (for query type (3)) and the probabilities of relations conditioned on the target\n",
      "entity (for query type (4)). Again, these are very competitive baselines due to the skewed\n",
      "distribution of relations and entities. Table 7 (left) lists the results of the experiments. The\n",
      "model based on the concatenation operation (CAT) outperforms the baseline and performs\n",
      "surprisingly well. The deep models are able to generalize to unseen images since their\n",
      "performance is comparable to the performance in the relation prediction task (query type\n",
      "(1)) where the entity was part of the KG during training (see Table 2). Figure 7 (right)\n",
      "14\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "RReellaattiioonn RRaannkk RReellaattiioonn RRaannkk\n",
      "00..221133 •••IIInnnssstttrrruuummmeeennnttt ppplllaaayyyeeeddd 00..440000 •••SSStttaaarrrrrriiinnnggg aaaccctttooorrr\n",
      "00..119911 •••TTTaaaccckkk cccooonnntttrrriiibbbuuutttiiiooonnn 00..112255 •••AAAwwwaaarrrddd nnnooommmiiinnnaaattteeeddd wwwooorrrkkk\n",
      "00..118855 •••MMMuuusssiiicccaaalll gggrrrooouuuppp mmmeeemmmbbbeeerrrsss •••CCChhhaaarrraaacccttteeerrr\n",
      "Michael\n",
      "Synthesizer Inception Imperioli\n",
      "(Known) (Unseen) (Known)\n",
      "e? e\n",
      "RReellaattiioonn RRaannkk\n",
      "00..669955 •••FFFiiilllmmm rrreeellleeeaaassseee rrreeegggiiiooonnn\n",
      "BAFTA\n",
      "•••FFFeeeaaatttuuurrreeeddd fffiiilllmmm lllooocccaaatttiiiooonnnsss Award e\n",
      "•••PPPlllaaaccceee ooofff bbbiiirrrttthhh\n",
      "(Known)\n",
      "Tokio\n",
      "e\n",
      "(Known) RReellaattiioonn RRaannkk\n",
      "00..559933 •••AAAwwwaaarrrddd nnnooommmiiinnnaaattteeeddd wwwooorrrkkk\n",
      "00..117722 •••AAAwwwaaarrrddd wwwooonnn\n",
      "•••AAAwwwaaarrrddd nnnooommmiiinnneeeeee\n",
      "Figure 8: Qualitative example of the zero-shot learning problem. The plot shows the most\n",
      "probable relations that link a sample from an unknown entity (green) with samples\n",
      "of known entities (blue) of the KG.\n",
      "depicts example queries for the zero-shot query type (3). For the first query example, the\n",
      "CAT model ranked the correct relation type first (indicated by the green bounding box).\n",
      "The second example is more challenging and the correct relation type was not part of the top\n",
      "10 ranked relation types. Figure 5.4 shows one concrete example of the zero-shot learning\n",
      "problem. In green, visual data from an unknown entity is linked with visual data from\n",
      "KG entities (blue) by ranking the most probable relation types. This problem cannot be\n",
      "addressed with standard relation prediction methods since entities need to be part of the\n",
      "KG during training for these models to work.\n",
      "6. Conclusion\n",
      "KGs are at the core of numerous AI applications. Research has focused either on link\n",
      "prediction working only on the relational structure or on scene understanding in a single\n",
      "image. We present a novel visual-relational KG where the entities are enriched with visual\n",
      "data. We proposed several novel query types and introduce neural architectures suitable for\n",
      "probabilistic query answering. We propose a novel approach to zero-shot learning as the\n",
      "problem of visually mapping an image of an entirely new entity to a KG.\n",
      "References\n",
      "Sungjin Ahn, Heeyoul Choi, Tanel Parnamaa, and Yoshua Bengio. A neural knowledge\n",
      "language model. arXiv preprint arXiv:1608.00318, 2016.\n",
      "15\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "Mohammad Al Hasan and Mohammed J Zaki. A survey of link prediction in social networks.\n",
      "In Social network data analytics, pages 243–275. Springer, 2011.\n",
      "Michael Ashburner, Catherine A. Ball, Judith A. Blake, David Botstein, Heather Butler,\n",
      "J. Michael Cherry, Allan P. Davis, Kara Dolinski, Selina S. Dwight, Janan T. Eppig,\n",
      "Midori A. Harris, David P. Hill, Laurie Issel-Tarver, Andrew Kasarskis, Suzanna Lewis,\n",
      "John C. Matese, Joel E. Richardson, Martin Ringwald, Gerald M. Rubin, and Gavin\n",
      "Sherlock. Gene Ontology: tool for the unification of biology. Nat Genet, 25(1):25–29,\n",
      "2000.\n",
      "J. Ba, K. Swersky, S. Fidler, and R. Salakhutdinov. Predicting deep zero-shot convolutional\n",
      "neural networks using textual descriptions. In CVPR, 2015.\n",
      "Sean Bell and Kavita Bala. Learning visual similarity for product design with convolutional\n",
      "neural networks. ACM Transactions on Graphics (TOG), 34(4):98, 2015.\n",
      "Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: A\n",
      "collaboratively created graph database for structuring human knowledge. In SIGMOD,\n",
      "pages 1247–1250, 2008.\n",
      "Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana\n",
      "Yakhnenko. Translating embeddings for modeling multi-relational data. In Advances in\n",
      "Neural Information Processing Systems, pages 2787–2795, 2013.\n",
      "Li C., Lai Y., Goldwasser D., and Neville J. Joint embedding models for textual and social\n",
      "analysis. In ICML Workshop, 2017.\n",
      "Xinlei Chen, Abhinav Shrivastava, and Abhinav Gupta. Neil: Extracting visual knowledge\n",
      "from web data. In Proceedings of the IEEE International Conference on Computer Vision,\n",
      "pages 1409–1416, 2013.\n",
      "Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose M. F. Moura,\n",
      "Devi Parikh, and Dhruv Batra. Visual dialog. In CVPR, July 2017.\n",
      "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale\n",
      "Hierarchical Image Database. In CVPR, 2009.\n",
      "Carl Doersch, Abhinav Gupta, and Alexei A Efros. Mid-level visual element discovery as\n",
      "discriminative mode seeking. In Advances in Neural Information Processing Systems 26,\n",
      "pages 494–502. 2013.\n",
      "Ali Farhadi, Ian Endres, Derek Hoiem, and David A. Forsyth. Describing objects by their\n",
      "attributes. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern\n",
      "Recognition, pages 1778–1785, 2009.\n",
      "Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object\n",
      "detection with discriminatively trained part-based models. IEEE transactions on pattern\n",
      "analysis and machine intelligence, 32(9):1627–1645, 2010.\n",
      "16\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "Matt Gardner and Tom M Mitchell. Efficient and expressive knowledge base completion\n",
      "using subgraph feature extraction. In EMNLP, pages 1488–1498, 2015.\n",
      "Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies\n",
      "for accurate object detection and semantic segmentation. In Proceedings of CVPR, pages\n",
      "580–587, 2014.\n",
      "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward\n",
      "neural networks. In AISTATS, 2010.\n",
      "Abhinav Gupta, Aniruddha Kembhavi, and Larry S. Davis. Observing human-object\n",
      "interactions: Usingspatialandfunctionalcompatibilityforrecognition. IEEE Transactions\n",
      "on Pattern Analysis and Machine Intelligence, 31:1775–1789, 2009.\n",
      "Kelvin Guu, John Miller, and Percy Liang. Traversing knowledge graphs in vector space.\n",
      "arXiv preprint arXiv:1506.01094, 2015.\n",
      "Ido Guy, Alexander Nus, Dan Pelleg, and Idan Szpektor. Care to share?: Learning to rank\n",
      "personal photos for public sharing. In WSDM, pages 207–215. ACM, 2018.\n",
      "Hamid Izadinia, Fereshteh Sadeghi, and Ali Farhadi. Incorporating scene context and object\n",
      "layout into appearance modeling. In Proceedings of the IEEE Conference on Computer\n",
      "Vision and Pattern Recognition, pages 232–239, 2014.\n",
      "YangqingJia,EvanShelhamer,JeffDonahue,SergeyKarayev,JonathanLong,RossGirshick,\n",
      "Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature\n",
      "embedding. arXiv preprint arXiv:1408.5093, 2014.\n",
      "Lu Jiang, Yannis Kalantidis, Liangliang Cao, Sachin Farfade, Jiliang Tang, and Alexander G.\n",
      "Hauptmann. Delving deep into personal photo and video search. In Proceedings of the\n",
      "Tenth ACM International Conference on Web Search and Data Mining, WSDM, 2017.\n",
      "J. Johnson, R. Krishna, M. Stark, L.-J. Li, D. A. Shamma, M. Bernstein, and L. Fei-Fei.\n",
      "Image retrieval using scene graphs. In CVPR, 2015.\n",
      "Rudolf Kadlec, Ondrej Bajgar, and Jan Kleindienst. Knowledge base completion: Baselines\n",
      "strike back. arXiv preprint arXiv:1705.10744, 2017.\n",
      "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz,\n",
      "Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, and\n",
      "Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense\n",
      "image annotations. In arXiv preprint arXiv:1602.07332, 2016.\n",
      "Ni Lao, Tom Mitchell, and William W Cohen. Random walk inference and learning in a\n",
      "large scale knowledge base. In Proceedings of the Conference on Empirical Methods in\n",
      "Natural Language Processing, pages 529–539. Association for Computational Linguistics,\n",
      "2011.\n",
      "17\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N.\n",
      "Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick van Kleef, Sören Auer, and\n",
      "Christian Bizer. DBpedia - a large-scale, multilingual knowledge base extracted from\n",
      "wikipedia. Semantic Web Journal, (2):167–195, 2015.\n",
      "Yining Li, Chen Huang, Xiaoou Tang, and Chen Change Loy. Learning to disambiguate by\n",
      "asking discriminative questions. In ICCV, 2017.\n",
      "Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering\n",
      "robust clothes recognition and retrieval with rich annotations. In CVPR, June 2016.\n",
      "C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei. Visual relationship detection with language\n",
      "priors. In ECCV, 2016.\n",
      "Tomasz Malisiewicz and Alexei A. Efros. Beyond categories: The visual memex model\n",
      "for reasoning about object relationships. In Advances in Neural Information Processing\n",
      "Systems, 2009.\n",
      "Kenneth Marino, Ruslan Salakhutdinov, and Abhinav Gupta. The more you know: Using\n",
      "knowledge graphs for image classification. In CVPR, 2017.\n",
      "Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective\n",
      "learning on multi-relational data. In Proceedings of the 28th international conference on\n",
      "machine learning (ICML-11), pages 809–816, 2011.\n",
      "Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of\n",
      "relational machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):11–33,\n",
      "2016a.\n",
      "Maximilian Nickel, Lorenzo Rosasco, and Tomaso A. Poggio. Holographic embeddings of\n",
      "knowledge graphs. In Proceedings of the Thirtieth Conference on Artificial Intelligence,\n",
      "pages 1955–1961, 2016b.\n",
      "Wei Niu, James Caverlee, and Haokai Lu. Neural personalized ranking for image recommen-\n",
      "dation. In Proceedings of the Eleventh ACM International Conference on Web Search and\n",
      "Data Mining, WSDM, 2018.\n",
      "Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via\n",
      "lifted structured feature embedding. In Proceedings of the IEEE Conference on Computer\n",
      "Vision and Pattern Recognition, pages 4004–4012, 2016.\n",
      "Megha Pandey and Svetlana Lazebnik. Scene recognition and weakly supervised object\n",
      "localization with deformable part-based models. In Computer Vision (ICCV), 2011 IEEE\n",
      "International Conference on, pages 1307–1314, 2011.\n",
      "Pouya Pezeshkpour, Liyan Chen, and Sameer Singh. Embedding multimodal relational data\n",
      "for knowledge base completion. In EMNLP, 2018.\n",
      "B. Romera-Paredes and P. Torr. An embarrassingly simple approach to zero-shot learning.\n",
      "In ICML, 2015.\n",
      "18\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "Olga Russakovsky, Jia Deng, Zhiheng Huang, Alexander C. Berg, and Li Fei-Fei. Detecting\n",
      "avocados to zucchinis: what have we done, and where are we going? In International\n",
      "Conference on Computer Vision (ICCV), 2013.\n",
      "Fereshteh Sadeghi and Marshall F. Tappen. Latent pyramidal regions for recognizing scenes.\n",
      "In Proceedings of the 12th European Conference on Computer Vision - Volume Part V,\n",
      "pages 228–241, 2012.\n",
      "F.Schroff,D.Kalenichenko,andJ.Philbin. Facenet: Aunifiedembeddingforfacerecognition\n",
      "and clustering. In 2015 IEEE Conference on Computer Vision and Pattern Recognition\n",
      "(CVPR), pages 815–823, 2015.\n",
      "Iulian Vlad Serban, Alberto García-Durán, Caglar Gulcehre, Sungjin Ahn, Sarath Chandar,\n",
      "Aaron Courville, and Yoshua Bengio. Generating factoid questions with recurrent neural\n",
      "networks: The 30m factoid question-answer corpus. arXiv preprint arXiv:1603.06807,\n",
      "2016.\n",
      "K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image\n",
      "recognition. CoRR, 2014.\n",
      "Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In\n",
      "Advances in Neural Information Processing Systems, pages 1857–1865, 2016.\n",
      "Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, and Kevin Murphy. Deep metric learning\n",
      "via facility location. In Conference on Computer Vision and Pattern Recognition (CVPR),\n",
      "2017.\n",
      "Damien Teney, Lingqiao Liu, and Anton van den Hengel. Graph-structured representations\n",
      "for visual question answering. In CVPR, July 2017.\n",
      "Steffen Thoma, Achim Rettinger, and Fabian Both. Towards holistic concept representations:\n",
      "Embedding relational knowledge, visual attributes, and distributional word semantics. In\n",
      "International Semantic Web Conference (1), volume 10587 of Lecture Notes in Computer\n",
      "Science, pages 694–710. Springer, 2017.\n",
      "Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and\n",
      "text inference. In Proceedings of the 3rd Workshop on Continuous Vector Space Models\n",
      "and their Compositionality, pages 57–66, 2015.\n",
      "Rakshit Trivedi, Hanjun Dai, Yichen Wang, and Le Song. Know-evolve: Deep temporal\n",
      "reasoning for dynamic knowledge graphs. In ICML, 2017.\n",
      "Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard.\n",
      "Complex embeddings for simple link prediction. arXiv preprint arXiv:1606.06357, 2016.\n",
      "Andreas Veit, Balazs Kovacs, Sean Bell, Julian McAuley, Kavita Bala, and Serge Belongie.\n",
      "Learning visual clothing style with heterogeneous dyadic co-occurrences. In ICCV, 2015.\n",
      "Lonij Vincent, Rawat Ambrish, and Nicolae Maria-Irina. Extending knowledge bases using\n",
      "images. In AKBC, 2017.\n",
      "19<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  11461,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['ImageNet', 'Freebase', 'Visual Genome', 'DBpedia', 'FB15k', 'ImageGraph']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  on Continuous Vector Space Models\n",
      "and their Compositionality, pages 57–66, 2015.\n",
      "Rakshit Trivedi, Hanjun Dai, Yichen Wang, and Le Song. Know-evolve: Deep temporal\n",
      "reasoning for dynamic knowledge graphs. In ICML, 2017.\n",
      "Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard.\n",
      "Complex embeddings for simple link prediction. arXiv preprint arXiv:1606.06357, 2016.\n",
      "Andreas Veit, Balazs Kovacs, Sean Bell, Julian McAuley, Kavita Bala, and Serge Belongie.\n",
      "Learning visual clothing style with heterogeneous dyadic co-occurrences. In ICCV, 2015.\n",
      "Lonij Vincent, Rawat Ambrish, and Nicolae Maria-Irina. Extending knowledge bases using\n",
      "images. In AKBC, 2017.\n",
      "19\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "Jian Wang, Feng Zhou, Shilei Wen, Xiao Liu, and Yuanqing Lin. Deep metric learning with\n",
      "angular loss. In International Conference on Computer Vision (ICCV), 2017.\n",
      "X. Wang, S. Qiu, K. Liu, and X. Tang. Web image re-ranking usingquery-specific semantic\n",
      "signatures. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(4):\n",
      "810–823, April 2014.\n",
      "David S. Wishart, Craig Knox, Anchi Guo, Dean Cheng, Savita Shrivastava, Dan Tzur,\n",
      "Bijaya Gautam, and Murtaza Hassanali. Drugbank: a knowledgebase for drugs, drug\n",
      "actions and drug targets. Nucleic Acids Research, 36:901–906, 2008.\n",
      "Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. SUN\n",
      "database: Large-scale scene recognition from abbey to zoo. In The Twenty-Third IEEE\n",
      "Conference on Computer Vision and Pattern Recognition, pages 3485–3492, 2010.\n",
      "Mohamed Yahya, Denilson Barbosa, Klaus Berberich, Qiuyue Wang, and Gerhard Weikum.\n",
      "Relationship queries on extended knowledge graphs. In WSDM, 2016.\n",
      "Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Learning multi-\n",
      "relational semantics using neural-embedding models. arXiv preprint arXiv:1411.4072,\n",
      "2014.\n",
      "X. Yang, T. Mei, Y. Zhang, J. Liu, and S. Satoh. Web image search re-ranking with\n",
      "click-based similarity and typicality. IEEE Transactions on Image Processing, 25(10):\n",
      "4617–4630, 2016.\n",
      "Bangpeng Yao and Li Fei-Fei. Modeling mutual context of object and human pose in human-\n",
      "object interaction activities. In Computer Vision and Pattern Recognition (CVPR), 2010\n",
      "IEEE Conference on, pages 17–24, 2010.\n",
      "Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J. Smola, and Le Song. Variational\n",
      "reasoning for question answering with knowledge graph. In AAAI, 2018.\n",
      "Z. Zhang and V. Saligrama. Zero-shot learning via semantic similarity embedding. In ICCV,\n",
      "2015.\n",
      "Feng Zhou and Yuanqing Lin. Fine-grained image classification by exploring bipartite-graph\n",
      "labels. In CVPR, June 2016.\n",
      "Yuke Zhu, Alireza Fathi, and Li Fei-Fei. Reasoning about object affordances in a knowledge\n",
      "base representation. In European conference on computer vision, pages 408–424, 2014.\n",
      "20<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "            220,    389,  70067,   4290,  11746,  27972,    198,    438,    872,\n",
      "          68206,   2786,     11,   6959,    220,   3226,   4235,   2287,     11,\n",
      "            220,    679,     20,    627,     49,    587,  41153,   1183,   2270,\n",
      "             72,     11,  21296,  58781,  80223,     11,    816,  29424,  29346,\n",
      "             11,    323,   2009,  19508,     13,  14521,  91345,   4035,     25,\n",
      "          18682,  37015,    198,  20489,    287,    369,   8915,   6677,  40099,\n",
      "             13,    763,  19845,   2735,     11,    220,    679,     22,    627,\n",
      "           1016,  89577,  42782,  43588,     11,  55205,  26056,   2067,     11,\n",
      "          44609,    432,   1142,    301,     11,  29124,   2265,  94316,   1291,\n",
      "             11,    323,   4673,  99112,    426,   3102,    569,    627,  32237,\n",
      "          71647,    369,   4382,   2723,  20212,     13,    802,     55,    344,\n",
      "            864,   1374,    802,     55,    344,     25,   6330,     21,     13,\n",
      "          24254,   3226,     11,    220,    679,     21,    627,   3112,  51518,\n",
      "          23404,    275,     11,  19984,   1394,     82,  93981,  19807,     11,\n",
      "          26044,  18366,     11,  38897,  65813,   1130,     88,     11,    735,\n",
      "            402,   6388,    426,   6181,     11,    323,  33022,   7984,    647,\n",
      "            648,    627,  48567,   9302,  17895,   1742,    449,  98882,  14282,\n",
      "          37314,   1080,     12,  14310,  34346,     13,    763,  59332,     53,\n",
      "             11,    220,    679,     20,    627,  54324,   3251,  35407,     11,\n",
      "          23886,    266,   3383,   1347,    819,     11,    323,  83440,   6043,\n",
      "          23880,  22197,  41071,     13,   9634,   2518,   6677,  23963,   1701,\n",
      "            198,   3726,     13,    763,  31672,   5002,     11,    220,    679,\n",
      "             22,    627,    777,    198,     46,   5771,  18812,  11151,    392,\n",
      "            822,     11,  53383,  77468,     11,  85341,   9607,    324,  11644,\n",
      "             11,  33555,  97465,   6354,  99634,     11,    612,  92075,   6354,\n",
      "            561,    265,    198,     41,   1122,  29346,     11,  43758,  67927,\n",
      "             11,   1443,    458,     72,  62323,     11,  66690,  38805,     11,\n",
      "            323,  69984,  90684,   8732,     13,  18682,  18767,   6975,    449,\n",
      "            198,   4328,   4814,     13,    763,   7327,  15217,    389,  17863,\n",
      "          31541,    320,   1341,  20161,    705,    220,    679,     22,    627,\n",
      "             55,     13,  29346,     11,    328,     13,   1229,  19260,     11,\n",
      "            735,     13,  38805,     11,    323,   1630,     13,  41462,     13,\n",
      "           5000,   2217,    312,  72539,   1701,   1663,  19440,  42833,    198,\n",
      "           7908,   2859,     13,  40135,  56385,    389,  19365,  18825,    323,\n",
      "          13257,  22107,     11,    220,   1927,      7,     19,    997,  19232,\n",
      "           4235,  23848,     11,   5936,    220,    679,     19,    627,  23083,\n",
      "            328,     13,  38747,    472,     11,  29517,  54450,     11,   1556,\n",
      "          14946,   4673,     78,     11,  25028,  57807,     11,  20680,   6388,\n",
      "           1443,  77467,    561,   2979,     11,  11824,    350,     89,    324,\n",
      "            345,     33,   3251,  12874,  96154,    309,     11,    323,    386,\n",
      "           5757,  12997,  60777,   8115,     13,  26166,  17469,     25,    264,\n",
      "           6677,   3231,    369,  11217,     11,   5623,    198,   4109,    323,\n",
      "           5623,  11811,     13,    452,  22935,    292,   6515,   3447,   8483,\n",
      "             11,    220,   1927,     25,  19319,   4235,  22224,     11,    220,\n",
      "           1049,     23,    627,     41,   1122,     87,    290,     70,  66690,\n",
      "             11,   7957,    473,    954,     11,  27973,     64,    362,     13,\n",
      "          61651,   5248,     11,    362,    799,  12225,  10126,     11,    323,\n",
      "          23245,   8611,   3545,   4749,     13,  57328,    198,  12494,     25,\n",
      "          20902,  13230,   6237,  18324,    505,  30195,   1216,    311,  42014,\n",
      "             13,    763,    578,  44956,     12,  38075,  40135,    198,  92348,\n",
      "            389,  17863,  31541,    323,  19365,  48698,     11,   6959,    220,\n",
      "          19746,     20,   4235,  18634,     17,     11,    220,    679,     15,\n",
      "            627,  83705,   3690,  83562,   7911,     11,   9973,    321,    942,\n",
      "          47142,  12252,     11,  82197,   9084,    655,    718,     11,  58094,\n",
      "           4168,    361,  29346,     11,    323,  20524,  19221,   1226,   1609,\n",
      "            372,    627,  51922,  20126,    389,  11838,   6677,  40099,     13,\n",
      "            763,    468,   5608,     44,     11,    220,    679,     21,    627,\n",
      "             33,    819,    276,  25482,     11,  62323,   2442,   2933,    816,\n",
      "           7141,     11,  41235,    347,    647,   1283,     11,  88404,     69,\n",
      "            833,    480,   3524,     11,    323,  14851,  92829,     13,  21579,\n",
      "           7447,   7058,   3833,   1697,  53794,   1701,  30828,     12,  95711,\n",
      "           4211,     13,    802,     55,    344,    864,   1374,    802,     55,\n",
      "            344,     25,   9335,     16,     13,  18501,     17,    345,    679,\n",
      "             19,    627,     55,     13,  25482,     11,    350,     13,  92033,\n",
      "             11,    816,     13,  37120,     11,    622,     13,  38805,     11,\n",
      "            323,    328,     13,  13479,   2319,     13,   5000,   2217,   2778,\n",
      "            312,  72539,    449,    198,   3763,   6108,  38723,    323,  14595,\n",
      "            488,     13,  40135,  56385,    389,   4758,  29225,     11,    220,\n",
      "            914,      7,    605,    997,  19608,     22,   4235,  21290,     15,\n",
      "             11,    220,    679,     21,    627,  63919,  64183,  91030,    323,\n",
      "          14851,   3926,     72,     12,   6251,     72,     13,  77349,  27848,\n",
      "           2317,    315,   1665,    323,   3823,  17477,    304,   3823,   7058,\n",
      "           1735,  16628,   7640,     13,    763,  17863,  31541,    323,  19365,\n",
      "          48698,    320,  20161,   6616,    705,    220,    679,     15,    198,\n",
      "          77805,  15217,    389,     11,   6959,    220,   1114,   4235,   1187,\n",
      "             11,    220,    679,     15,    627,     56, 113528,  37120,     11,\n",
      "          21296,  58781,  80223,     11,   1901,   1540,   1220,     64,    735,\n",
      "           9700,    548,   6723,     11,  20643,    622,     13,   4487,   8083,\n",
      "             11,    323,   2009,  19508,     13,  28968,   1697,    198,  20489,\n",
      "            287,    369,   3488,  36864,    449,   6677,   4876,     13,    763,\n",
      "          48197,     40,     11,    220,    679,     23,    627,     57,     13,\n",
      "          37120,    323,    650,     13,   8375,    343,  31473,     13,  18811,\n",
      "          64630,   6975,   4669,  42833,  38723,  40188,     13,    763,  59332,\n",
      "             53,    345,    679,     20,    627,     37,    833,  67927,    323,\n",
      "          69984,  90684,   8732,     13,  31253,  25313,   2692,   2217,  24790,\n",
      "            555,  24919,  29978,    472,    635,  74116,    198,  17298,     13,\n",
      "            763,  14499,   6616,     11,   5651,    220,    679,     21,    627,\n",
      "             56,  10647,  68844,     11,   1708,    556,   4458,    435,  67631,\n",
      "             11,    323,  14851,   3926,     72,     12,   6251,     72,     13,\n",
      "          27857,    287,    922,   1665,  10150,   3095,    304,    264,   6677,\n",
      "            198,   3231,  13340,     13,    763,   7665,  10017,    389,   6500,\n",
      "          11376,     11,   6959,    220,  18058,   4235,  18517,     11,    220,\n",
      "            679,     19,    627,    508, 128009, 128006,    882, 128007,    271,\n",
      "           7184,     11,   2728,    420,   3488,     25,   3639,    527,    279,\n",
      "            836,    315,  30525,   1511,    304,    279,   5684,   4710,    220,\n",
      "          21335,   1203,    279,   4320,   1193,    304,    264,  13325,   1160,\n",
      "           3645,     11,    369,   3187,     25,   2570,     32,   1882,     33,\n",
      "           7352,   1442,    499,   1541,    956,   1440,    279,   4320,     11,\n",
      "           1120,    471,    459,   4384,   1160,     13, 128009, 128006,  78191,\n",
      "         128007,    271]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "            220,    389,  70067,   4290,  11746,  27972,    198,    438,    872,\n",
      "          68206,   2786,     11,   6959,    220,   3226,   4235,   2287,     11,\n",
      "            220,    679,     20,    627,     49,    587,  41153,   1183,   2270,\n",
      "             72,     11,  21296,  58781,  80223,     11,    816,  29424,  29346,\n",
      "             11,    323,   2009,  19508,     13,  14521,  91345,   4035,     25,\n",
      "          18682,  37015,    198,  20489,    287,    369,   8915,   6677,  40099,\n",
      "             13,    763,  19845,   2735,     11,    220,    679,     22,    627,\n",
      "           1016,  89577,  42782,  43588,     11,  55205,  26056,   2067,     11,\n",
      "          44609,    432,   1142,    301,     11,  29124,   2265,  94316,   1291,\n",
      "             11,    323,   4673,  99112,    426,   3102,    569,    627,  32237,\n",
      "          71647,    369,   4382,   2723,  20212,     13,    802,     55,    344,\n",
      "            864,   1374,    802,     55,    344,     25,   6330,     21,     13,\n",
      "          24254,   3226,     11,    220,    679,     21,    627,   3112,  51518,\n",
      "          23404,    275,     11,  19984,   1394,     82,  93981,  19807,     11,\n",
      "          26044,  18366,     11,  38897,  65813,   1130,     88,     11,    735,\n",
      "            402,   6388,    426,   6181,     11,    323,  33022,   7984,    647,\n",
      "            648,    627,  48567,   9302,  17895,   1742,    449,  98882,  14282,\n",
      "          37314,   1080,     12,  14310,  34346,     13,    763,  59332,     53,\n",
      "             11,    220,    679,     20,    627,  54324,   3251,  35407,     11,\n",
      "          23886,    266,   3383,   1347,    819,     11,    323,  83440,   6043,\n",
      "          23880,  22197,  41071,     13,   9634,   2518,   6677,  23963,   1701,\n",
      "            198,   3726,     13,    763,  31672,   5002,     11,    220,    679,\n",
      "             22,    627,    777,    198,     46,   5771,  18812,  11151,    392,\n",
      "            822,     11,  53383,  77468,     11,  85341,   9607,    324,  11644,\n",
      "             11,  33555,  97465,   6354,  99634,     11,    612,  92075,   6354,\n",
      "            561,    265,    198,     41,   1122,  29346,     11,  43758,  67927,\n",
      "             11,   1443,    458,     72,  62323,     11,  66690,  38805,     11,\n",
      "            323,  69984,  90684,   8732,     13,  18682,  18767,   6975,    449,\n",
      "            198,   4328,   4814,     13,    763,   7327,  15217,    389,  17863,\n",
      "          31541,    320,   1341,  20161,    705,    220,    679,     22,    627,\n",
      "             55,     13,  29346,     11,    328,     13,   1229,  19260,     11,\n",
      "            735,     13,  38805,     11,    323,   1630,     13,  41462,     13,\n",
      "           5000,   2217,    312,  72539,   1701,   1663,  19440,  42833,    198,\n",
      "           7908,   2859,     13,  40135,  56385,    389,  19365,  18825,    323,\n",
      "          13257,  22107,     11,    220,   1927,      7,     19,    997,  19232,\n",
      "           4235,  23848,     11,   5936,    220,    679,     19,    627,  23083,\n",
      "            328,     13,  38747,    472,     11,  29517,  54450,     11,   1556,\n",
      "          14946,   4673,     78,     11,  25028,  57807,     11,  20680,   6388,\n",
      "           1443,  77467,    561,   2979,     11,  11824,    350,     89,    324,\n",
      "            345,     33,   3251,  12874,  96154,    309,     11,    323,    386,\n",
      "           5757,  12997,  60777,   8115,     13,  26166,  17469,     25,    264,\n",
      "           6677,   3231,    369,  11217,     11,   5623,    198,   4109,    323,\n",
      "           5623,  11811,     13,    452,  22935,    292,   6515,   3447,   8483,\n",
      "             11,    220,   1927,     25,  19319,   4235,  22224,     11,    220,\n",
      "           1049,     23,    627,     41,   1122,     87,    290,     70,  66690,\n",
      "             11,   7957,    473,    954,     11,  27973,     64,    362,     13,\n",
      "          61651,   5248,     11,    362,    799,  12225,  10126,     11,    323,\n",
      "          23245,   8611,   3545,   4749,     13,  57328,    198,  12494,     25,\n",
      "          20902,  13230,   6237,  18324,    505,  30195,   1216,    311,  42014,\n",
      "             13,    763,    578,  44956,     12,  38075,  40135,    198,  92348,\n",
      "            389,  17863,  31541,    323,  19365,  48698,     11,   6959,    220,\n",
      "          19746,     20,   4235,  18634,     17,     11,    220,    679,     15,\n",
      "            627,  83705,   3690,  83562,   7911,     11,   9973,    321,    942,\n",
      "          47142,  12252,     11,  82197,   9084,    655,    718,     11,  58094,\n",
      "           4168,    361,  29346,     11,    323,  20524,  19221,   1226,   1609,\n",
      "            372,    627,  51922,  20126,    389,  11838,   6677,  40099,     13,\n",
      "            763,    468,   5608,     44,     11,    220,    679,     21,    627,\n",
      "             33,    819,    276,  25482,     11,  62323,   2442,   2933,    816,\n",
      "           7141,     11,  41235,    347,    647,   1283,     11,  88404,     69,\n",
      "            833,    480,   3524,     11,    323,  14851,  92829,     13,  21579,\n",
      "           7447,   7058,   3833,   1697,  53794,   1701,  30828,     12,  95711,\n",
      "           4211,     13,    802,     55,    344,    864,   1374,    802,     55,\n",
      "            344,     25,   9335,     16,     13,  18501,     17,    345,    679,\n",
      "             19,    627,     55,     13,  25482,     11,    350,     13,  92033,\n",
      "             11,    816,     13,  37120,     11,    622,     13,  38805,     11,\n",
      "            323,    328,     13,  13479,   2319,     13,   5000,   2217,   2778,\n",
      "            312,  72539,    449,    198,   3763,   6108,  38723,    323,  14595,\n",
      "            488,     13,  40135,  56385,    389,   4758,  29225,     11,    220,\n",
      "            914,      7,    605,    997,  19608,     22,   4235,  21290,     15,\n",
      "             11,    220,    679,     21,    627,  63919,  64183,  91030,    323,\n",
      "          14851,   3926,     72,     12,   6251,     72,     13,  77349,  27848,\n",
      "           2317,    315,   1665,    323,   3823,  17477,    304,   3823,   7058,\n",
      "           1735,  16628,   7640,     13,    763,  17863,  31541,    323,  19365,\n",
      "          48698,    320,  20161,   6616,    705,    220,    679,     15,    198,\n",
      "          77805,  15217,    389,     11,   6959,    220,   1114,   4235,   1187,\n",
      "             11,    220,    679,     15,    627,     56, 113528,  37120,     11,\n",
      "          21296,  58781,  80223,     11,   1901,   1540,   1220,     64,    735,\n",
      "           9700,    548,   6723,     11,  20643,    622,     13,   4487,   8083,\n",
      "             11,    323,   2009,  19508,     13,  28968,   1697,    198,  20489,\n",
      "            287,    369,   3488,  36864,    449,   6677,   4876,     13,    763,\n",
      "          48197,     40,     11,    220,    679,     23,    627,     57,     13,\n",
      "          37120,    323,    650,     13,   8375,    343,  31473,     13,  18811,\n",
      "          64630,   6975,   4669,  42833,  38723,  40188,     13,    763,  59332,\n",
      "             53,    345,    679,     20,    627,     37,    833,  67927,    323,\n",
      "          69984,  90684,   8732,     13,  31253,  25313,   2692,   2217,  24790,\n",
      "            555,  24919,  29978,    472,    635,  74116,    198,  17298,     13,\n",
      "            763,  14499,   6616,     11,   5651,    220,    679,     21,    627,\n",
      "             56,  10647,  68844,     11,   1708,    556,   4458,    435,  67631,\n",
      "             11,    323,  14851,   3926,     72,     12,   6251,     72,     13,\n",
      "          27857,    287,    922,   1665,  10150,   3095,    304,    264,   6677,\n",
      "            198,   3231,  13340,     13,    763,   7665,  10017,    389,   6500,\n",
      "          11376,     11,   6959,    220,  18058,   4235,  18517,     11,    220,\n",
      "            679,     19,    627,    508, 128009, 128006,    882, 128007,    271,\n",
      "           7184,     11,   2728,    420,   3488,     25,   3639,    527,    279,\n",
      "            836,    315,  30525,   1511,    304,    279,   5684,   4710,    220,\n",
      "          21335,   1203,    279,   4320,   1193,    304,    264,  13325,   1160,\n",
      "           3645,     11,    369,   3187,     25,   2570,     32,   1882,     33,\n",
      "           7352,   1442,    499,   1541,    956,   1440,    279,   4320,     11,\n",
      "           1120,    471,    459,   4384,   1160,     13, 128009, 128006,  78191,\n",
      "         128007,    271,    681,     50,   1899,   4729,    518,    364,  78893,\n",
      "          17469,    663, 128009]], device='cuda:0')\n",
      "Decoded output:\n",
      " ['SUN database', 'Drugbank']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: AutomatedKnowledgeBaseConstruction(2019) Conferencepaper\n",
      "Answering Visual-Relational Queries in\n",
      "Web-Extracted Knowledge Graphs\n",
      "Daniel Oñoro-Rubio daniel.onoro@neclab.eu\n",
      "Mathias Niepert mathias.niepert@neclab.eu\n",
      "Alberto García-Durán alberto.duran@neclab.eu\n",
      "Roberto González-Sánchez roberto.gonzalez@neclab.eu\n",
      "NEC Labs Europe\n",
      "Roberto J. López-Sastre robertoj.lopez@uah.es\n",
      "University of Alcalá\n",
      "Abstract\n",
      "A visual-relational knowledge graph (KG) is a multi-relational graph whose entities\n",
      "are associated with images. We explore novel machine learning approaches for answering\n",
      "visual-relational queries in web-extracted knowledge graphs. To this end, we have created\n",
      "ImageGraph, a KG with 1,330 relation types, 14,870 entities, and 829,931 images crawled\n",
      "from the web. With visual-relational KGs such as ImageGraph one can introduce novel\n",
      "probabilistic query types in which images are treated as first-class citizens. Both the\n",
      "prediction of relations between unseen images as well as multi-relational image retrieval\n",
      "can be expressed with specific families of visual-relational queries. We introduce novel\n",
      "combinationsofconvolutionalnetworksandknowledgegraphembeddingmethodstoanswer\n",
      "such queries. We also explore a zero-shot learning scenario where an image of an entirely\n",
      "new entity is linked with multiple relations to entities of an existing KG. The resulting\n",
      "multi-relational grounding of unseen entity images into a knowledge graph serves as a\n",
      "semantic entity representation. We conduct experiments to demonstrate that the proposed\n",
      "methods can answer these visual-relational queries efficiently and accurately.\n",
      "1. Introduction\n",
      "Numerous applications can be modeled with a knowledge graph representing entities with\n",
      "nodes, object attributes with node attributes, and relationships between entities by directed\n",
      "typed edges. For instance, a product recommendation system can be represented as a\n",
      "knowledge graph where nodes represent customers and products and where typed edges\n",
      "represent customer reviews and purchasing events. In the medical domain, there are several\n",
      "knowledge graphs that model diseases, symptoms, drugs, genes, and their interactions (cf.\n",
      "[Ashburner et al., 2000, Wishart et al., 2008]). Increasingly, entities in these knowledge\n",
      "graphs are associated with visual data. For instance, in the online retail domain, there are\n",
      "product and advertising images and in the medical domain, there are patient-associated\n",
      "imaging data sets (MRIs, CTs, and so on). In addition, visual data is a large part of social\n",
      "networks and, in general, the world wide web.\n",
      "Knowledge graphs facilitate the integration, organization, and retrieval of structured\n",
      "data and support various forms of search applications. In recent years KGs have been\n",
      "1Project URL: https://github.com/nle-ml/mmkb.git.\n",
      "2This paper is part of the proceedings of AKBC 2019.\n",
      "1\n",
      "9102\n",
      "yaM\n",
      "3\n",
      "]GL.sc[\n",
      "6v41320.9071:viXra\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "?\n",
      "(1)\n",
      "(2) locatedIn?\n",
      "Gotoh Museum Murasaki Shikibu\n",
      "n\n",
      "hasArtAbout\n",
      "catedI b\n",
      "o rn\n",
      "In\n",
      "(3)\n",
      "?\n",
      "lo captialOf\n",
      "Tokyo Japan New entity\n",
      "locatedIn\n",
      "?\n",
      "(4)\n",
      "Sensō-ji\n",
      "Japan\n",
      "New entity\n",
      "(a)\n",
      "(b)\n",
      "Figure 1: (a) a small part of a visual-relational knowledge graph and a set of query types;\n",
      "and (b) some visual-relational query types;\n",
      "playing an increasingly crucial role in fields such as question answering [Das et al., 2017],\n",
      "language modeling [Ahn et al., 2016], and text generation [Serban et al., 2016]. Even\n",
      "though there is a large body of work on constructing and maintaining KGs, the setting of\n",
      "visual-relational KGs, where entities are associated with visual data, has not received much\n",
      "attention. A visual-relational KG represents entities, relations between these entities, and a\n",
      "large number of images associated with the entities (see Figure 1a for an example). While\n",
      "ImageNet [Deng et al., 2009] and the VisualGenome [Krishna et al., 2016] datasets are\n",
      "based on KGs such as WordNet they are predominantly used as either an object classification\n",
      "data set as in the case of ImageNet or to facilitate scene understanding in a single image.\n",
      "With this work, we address the problem of reasoning about visual concepts across a large set\n",
      "of images organized in a knowledge graph. We want to explore to what extent web-extracted\n",
      "visual data can be used to enrich existing KGs so as to facilitate complex visual search\n",
      "applications going beyond basic image retrieval.\n",
      "The core idea of our work is to treat images as first-class citizens both in KGs and\n",
      "visual-relational queries. The main objective of our work is to understand to what extent\n",
      "visual data associated with entities of a KG can be used in conjunction with deep learning\n",
      "methods to answer these visual-relational queries. Allowing images to be arguments of\n",
      "queries facilitates numerous novel query types. In Figure 1b we list some of the query types\n",
      "we address in this paper. In order to answer these queries, we built on KG embedding\n",
      "methods as well as deep representation learning approaches for visual data. This allows us\n",
      "to answer these visual queries both accurately and efficiently.\n",
      "Therearenumerousapplicationdomainsthatcouldbenefitfromqueryansweringinvisual\n",
      "KGs. Forinstance,inonlineretail,visualrepresentationsofnovelproductscouldbeleveraged\n",
      "for zero-shot product recommendations. Crucially, instead of only being able to retrieve\n",
      "similar products, a visual-relational KG would support the prediction of product attributes\n",
      "and more specifically what attributes customers might be interested in. For instance, in\n",
      "2\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "Table 1: Statistics of the knowledge graphs used in this paper.\n",
      "Entities Relations Triples Images\n",
      "|E| |R| Train Valid Test Train Valid Test\n",
      "ImageNet[Dengetal.,2009] 21,841 18 - 14,197,122\n",
      "VisualGenome[Krishnaetal.,2016] 75,729 40,480 1,531,448 108,077\n",
      "FB15k[Bordesetal.,2013] 14,951 1,345 483,142 50,000 59,071 0 0 0\n",
      "ImageGraph 14,870 1,330 460,406 47,533 56,071 411,306 201,832 216,793\n",
      "Japan Football Michael Jackson Madrid The Simpsons Drummer\n",
      "Figure 2: Image samples for some entities of ImageGraph.\n",
      "the fashion industry visual attributes are crucial for product recommendations [Liu et al.,\n",
      "2016, Veit et al., 2015]. Being able to ground novel visual concepts into an existing KG with\n",
      "attributes and various relation types is a reasonable approach to zero-shot learning.\n",
      "We make the following contributions. First, we introduce ImageGraph, a visual-\n",
      "relational web-extracted KG with 1,330 relations where 829,931 images are associated with\n",
      "14,870 different entities. Second, we introduce a new set of visual-relational query types.\n",
      "Third, we propose a novel set of neural architectures and objectives that we use for answering\n",
      "these novel query types. These query types generalize image retrieval and link prediction\n",
      "queries. This is the first time that deep CNNs and KG embedding learning objectives\n",
      "are combined into a joint model. Fourth, we show that the proposed class of deep neural\n",
      "networks are also successful for zero-shot learning, that is, creating relations between entirely\n",
      "unseen entities and the KG using only visual data at query time.\n",
      "2. Related Work\n",
      "We discuss the relation of our contributions to previous work with an emphasis on relational\n",
      "learning, image retrieval, object detection, scene understanding, existing data sets, and\n",
      "zero-shot learning.\n",
      "Relational Learning\n",
      "There has been a flurry of approaches tailored to specific problems such as link prediction\n",
      "in multi-relational graphs. Examples are knowledge base factorization and embedding\n",
      "approaches [Bordes et al., 2013, Nickel et al., 2011, Guu et al., 2015] and random-walk based\n",
      "ML models [Lao et al., 2011, Gardner and Mitchell, 2015]. More recently, the focus has been\n",
      "on integrating additional attribute types such as text [Yahya et al., 2016, C. et al., 2017],\n",
      "temporal graph dynamics [Trivedi et al., 2017], and multiple modalities [Pezeshkpour et al.,\n",
      "2018]. Another line of research is concerned with extensions of the link prediction problem to\n",
      "multi-hopreasoning[Zhangetal.,2018]. Wecannotlistallpriorlinkpredictionmethodshere\n",
      "3\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "103\n",
      "101\n",
      "100 101 102 103\n",
      "Relation types\n",
      "tnuoc\n",
      "elpirT\n",
      "1200\n",
      "104\n",
      "award_nomineperofession\n",
      "disea ps re oduction_company\n",
      "tnuoC\n",
      "ytitnE\n",
      "1 24680 00000 00000\n",
      "11 00 23\n",
      "tv_actor 0\n",
      "ingredient 101\n",
      "100\n",
      "101 103\n",
      "Entities\n",
      "tnuoc\n",
      "elpirT\n",
      "United States of America\n",
      "English Language\n",
      "Executive Producer\n",
      "University of Oxford\n",
      "Parlophone\n",
      "Vigor Shipyards\n",
      "Figure 3: (Left) The distribution of relation types; (center) the 10 most frequent entity\n",
      "types; and (right) the distribution of entities in ImageGraph.\n",
      "and instead refer the reader to two survey papers [Nickel et al., 2016a, Al Hasan and Zaki,\n",
      "2011]. Contrarytoexistingapproaches, weaddresstheproblemofansweringvisual-relational\n",
      "queries in knowledge graphs where the entities are associated with web-extracted images.\n",
      "We also address the zero-shot learning scenario, a problem that has not been addressed in\n",
      "the context of link prediction in multi-relational graphs.\n",
      "Image ranking\n",
      "Image retrieval is a popular problem and has been addressed by several authors [Wang et al.,\n",
      "2014, Yang et al., 2016, Jiang et al., 2017, Niu et al., 2018, Guy et al., 2018]. In [Yang\n",
      "et al., 2016] a re-ranking of the output of a given search engine by learning a click-based\n",
      "multi-feature similarity is proposed. The authors performed spectral clustering and obtained\n",
      "the final ranked results by computing click-based clusters. In [Guy et al., 2018] the authors\n",
      "fine-tune a DNN to rank photos a user might like to share in social media as well as a\n",
      "mechanism to detect duplicates. In [Niu et al., 2018] a joint user-image embedding is learned\n",
      "to generate a ranking based on user preferences. Contrary to these previous approaches we\n",
      "introduce a set of novel visual query types in a web-extracted KG with images and provide\n",
      "methods to answer these queries efficiently.\n",
      "Relational and Visual Data\n",
      "Previous work on combining relational and visual data has focused on object detection\n",
      "[Felzenszwalb et al., 2010, Girshick et al., 2014, Russakovsky et al., 2013, Marino et al.,\n",
      "2017, Li et al., 2017] and scene recognition [Doersch et al., 2013, Pandey and Lazebnik, 2011,\n",
      "Sadeghi and Tappen, 2012, Xiao et al., 2010, Teney et al., 2017] which are required for more\n",
      "complex visual-relational reasoning. Recent years have witnessed a surge in reasoning about\n",
      "human-object, object-object, and object-attribute relationships [Gupta et al., 2009, Farhadi\n",
      "et al., 2009, Malisiewicz and Efros, 2009, Yao and Fei-Fei, 2010, Felzenszwalb et al., 2010,\n",
      "Chen et al., 2013, Izadinia et al., 2014, Zhu et al., 2014]. The VisualGenome project [Krishna\n",
      "et al., 2016] is a knowledge base that integrates language and vision modalities. The project\n",
      "provides a knowledge graph, based on WordNet, which provides annotations of categories,\n",
      "attributes, and relation types for each image. Recent work has used the dataset to focus on\n",
      "scene understanding in single images. For instance, Lu et al. [Lu et al., 2016] proposed a\n",
      "model to detect relation types between objects depicted in an image by inferring sentences\n",
      "4\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "such as “man riding bicycle.\" Veit et al. [Veit et al., 2015] propose a siamese CNN to learn a\n",
      "metric representation on pairs of textile products so as to learn which products have similar\n",
      "styles. There is a large body of work on metric learning where the objective is to generate\n",
      "image embeddings such that a pairwise distance-based loss is minimized [Schroff et al., 2015,\n",
      "Bell and Bala, 2015, Oh Song et al., 2016, Sohn, 2016, Wang et al., 2017]. Recent work has\n",
      "extended this idea to directly optimize a clustering quality metric [Song et al., 2017]. In\n",
      "Vincent et al. [Vincent et al., 2017] they proposed a mutual embedding space for images and\n",
      "knowledge graphs so the relationships between an image and known entities in a knowledge\n",
      "graph are jointly encoded. Zhou et al. [Zhou and Lin, 2016] propose a method based on a\n",
      "bipartite graph that links depictions of meals to its ingredients. Johnson et al. [Johnson\n",
      "et al., 2015] propose to use the VisualGenome data to recover images from text queries. In\n",
      "the work of Thoma et al. [Thoma et al., 2017], they merge in a joint representation the\n",
      "embeddings from images, text, and KG and use the representation to perform link prediction\n",
      "on DBpedia [Lehmann et al., 2015]. ImageGraph is different from these data sets in that\n",
      "the relation types hold between different images and image annotated entities. This defines\n",
      "a novel class of problems where one seeks to answer queries such as “How are these two\n",
      "images related?\" With this work, we address problems ranging from predicting the relation\n",
      "types for image pairs to multi-relational image retrieval.\n",
      "Zero-shot Learning\n",
      "We focus on exploring ways in which KGs can be used to find relationships between visual\n",
      "data of unseen entities, that is, entities not part of the KG during training, and visual data\n",
      "of known KG entities. This is a form of zero-shot learning (ZSL) where the objective is to\n",
      "generalize to novel visual concepts. Generally, ZSL methods (e.g. [Romera-Paredes and\n",
      "Torr, 2015, Zhang and Saligrama, 2015]) rely on an underlying embedding space, such as\n",
      "one based on visual attributes, to recognize unseen categories. With this paper, we do not\n",
      "assume the availability of such a common embedding space but we assume the existence of\n",
      "an external visual-relational KG. Similar to our approach, when this explicit knowledge is\n",
      "not encoded in the underlying embedding space, other works rely on finding the similarities\n",
      "through linguistic patterns (e.g. [Ba et al., 2015, Lu et al., 2016]), leveraging distributional\n",
      "word representations so as to capture a notion of similarity. These approaches, however,\n",
      "address scene understanding in a single image, i.e. these models are able to detect the\n",
      "visual relationships in one given image. Our approach, on the other hand, finds relationships\n",
      "between different images and entities.\n",
      "3. ImageGraph: A Web-Extracted Visual Knowledge Graph\n",
      "ImageGraphisavisual-relationalKGwhoserelationalstructureisbasedonFreebase[Bol-\n",
      "lacker et al., 2008] and, more specifically, on FB15k, a subset of FreeBase and a popular\n",
      "benchmark data set [Nickel et al., 2016a]. Since FB15k does not include visual data, we\n",
      "perform the following steps to enrich the KG entities with image data. We implemented a\n",
      "web crawler that is able to parse query results for the image search engines Google Images,\n",
      "Bing Images, and Yahoo Image Search. To minimize the amount of noise due to polysemous\n",
      "entity labels (for example, there are more than 100 Freebase entities with the text label\n",
      "“Springfield\") we extracted, for each entity in FB15k, all Wikipedia URIs from the 1.9 billion\n",
      "5\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "Relationtype Example(h,r,t) Symmetric Others\n",
      "Symmetric (EmmaThompson,sibling,SophieThompson) 4%8%\n",
      "(SophieThompson,sibling,EmmaThompson)\n",
      "(Non-profitorganization,company_type,ApacheSoftwareFoundation)\n",
      "Asymmetric\n",
      "(Statistics,students_majoring,PhD)\n",
      "(StarWars,film_series,StarWars)\n",
      "88%\n",
      "Others (StarWarsEpisodeI:ThePhantomMenace,film_series,StarWars)\n",
      "(StarWarsEpisodeII:AttackoftheClones,film_series,StarWars) Asymmetric\n",
      "Figure 4: (Left) Example triples for symmetric, asymmetric and others relation types.\n",
      "(Right) Fraction of symmetric, asymmetric, and other relation types among all\n",
      "relation types in ImageGraph.\n",
      "triple Freebase RDF dump. For instance, for Springfield, Massachusetts, we obtained such\n",
      "URIs as Springfield_(Massachusetts,United_States) and Springfield_(MA). These\n",
      "URIs were processed and used as search queries for disambiguation purposes. We used\n",
      "the crawler to download more than 2.4M images (more than 462Gb of data). We removed\n",
      "corrupted, low quality, and duplicate images and we used the 25 top images returned by\n",
      "each of the image search engines whenever there were more than 25 results. The images\n",
      "were scaled to have a maximum height or width of 500 pixels while maintaining their\n",
      "aspect ratio. This resulted in 829,931 images associated with 14,870 different entities (55.8\n",
      "images per entity). After filtering out triples where either the head or tail entity could\n",
      "not be associated with an image, the visual KG consists of 564,010 triples expressing 1,330\n",
      "different relation types between 14,870 entities. We provide three sets of triples for training,\n",
      "validation, and testing plus three more image splits also for training, validation and test.\n",
      "Table 1 lists the statistics of the resulting visual KG. Any KG derived from FB15k such\n",
      "as FB15k-237[Toutanova and Chen, 2015] can also be associated with the crawled images.\n",
      "Since providing the images themselves would violate copyright law, we provide the code\n",
      "for the distributed crawler and the list of image URLs crawled for the experiments in this\n",
      "paper2.\n",
      "The distribution of relation types is depicted in Figure 3 (left). It plots for each relation\n",
      "type the number of triples it occurs in. Some relation types such as award_nominee or\n",
      "profession occur quite frequently while others such as ingredient have only few instances.\n",
      "4% of the relation types are symmetric, 88% are asymmetric, and 8% are others (see\n",
      "Table 4 (left)). Table 4 (right) lists specific instances of some relation types. There are 585\n",
      "distinct entity types such as Person,Athlete, and City. Figure 3 (center) shows the most\n",
      "frequent entity types. Figure 3 (right) visualizes the distribution of entities in the triples of\n",
      "ImageGraph and some example entities.\n",
      "Table 1 lists some statistics of the ImageGraph KG and other KGs from related work.\n",
      "First, we would like to emphasize the differences between ImageGraph and the Visual\n",
      "Genome project (VG) [Krishna et al., 2016]. With ImageGraph we address the problem\n",
      "of learning a representation for a KG with canonical relation types and not for relation\n",
      "types expressed through text. On a high level, we focus on answering visual-relational\n",
      "queries in a web-extracted KG. This is related to information retrieval except that in our\n",
      "proposed work, images are first-class citizens and we introduce novel and more complex\n",
      "2ImageGraph crawler and URLs: https://github.com/robegs/imageDownloader.\n",
      "6\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "query types. In contrast, VGD is focused on modeling relations between objects in images\n",
      "and the relation types are expressed in natural language. Additional differences between\n",
      "ImageGraph and ImageNet are the following. ImageNet is based on WordNet a lexical\n",
      "database where synonymous words from the same lexical category are grouped into synsets.\n",
      "There are 18 relations expressing connections between synsets. In Freebase, on the other\n",
      "hand, there are two orders of magnitudes more relations. In FB15k, the subset we focus on,\n",
      "there are 1,345 relations expressing location of places, positions of basketball players, and\n",
      "gender of entities. Moreover, entities in ImageNet exclusively represent entity types such\n",
      "as Cats and Cars whereas entities in FB15k are either entity types or instances of entity\n",
      "types such as Albert Einstein and Paris. This renders the computer vision problems\n",
      "associated with ImageGraph more challenging than those for existing datasets. Moreover,\n",
      "with ImageGraph the focus is on learning relational ML models that incorporate visual\n",
      "data both during learning and at query time.\n",
      "4. Representation Learning for Visual-Relational Graphs\n",
      "A knowledge graph (KG) K is given by a set of triples T, that is, statements of the form\n",
      "(h,r,t), where h,t ∈ E are the head and tail entities, respectively, and r ∈ R is a relation\n",
      "type. Figure 1a depicts a small fragment of a KG with relations between entities and images\n",
      "associated with the entities. Prior work has not included image data and has, therefore,\n",
      "focused on the following two types of queries. First, the query type (h,r?,t) asks for the\n",
      "relations between a given pair of head and tail entities. Second, the query types (h,r,t?)\n",
      "and (h?,r,t), asks for entities correctly completing the triple. The latter query type is often\n",
      "referred to as knowledge base completion. Here, we focus on queries that involve visual data\n",
      "as query objects, that is, objects that are either contained in the queries, the answers to the\n",
      "queries, or both.\n",
      "4.1 Visual-Relational Query Answering\n",
      "When entities are associated with image data, several completely novel query types are\n",
      "possible. Figure 1b lists the query types we focus on in this paper. We refer to images used\n",
      "during training as seen and all other images as unseen.\n",
      "(1) Given a pair of unseen images for which we do not know their KG entities, determine\n",
      "the unknown relations between the underlying entities.\n",
      "(2) Given an unseen image, for which we do not know the underlying KG entity, and a\n",
      "relation type, determine the seen images that complete the query.\n",
      "(3) Given an unseen image of an entirely new entity that is not part of the KG, and an\n",
      "unseen image for which we do not know the underlying KG entity, determine the\n",
      "unknown relations between the two underlying entities.\n",
      "(4) Given an unseen image of an entirely new entity that is not part of the KG, and a\n",
      "known KG entity, determine the unknown relations between the two entities.\n",
      "For each of these query types, the sought-after relations between the underlying entities\n",
      "have never been observed during training. Query types (3) and (4) are a form of zero-shot\n",
      "7\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "Sensō-ji Japan\n",
      "DistMult\n",
      "2\n",
      "VGG16 5\n",
      "6 r?\n",
      "g\n",
      "op VGG16 VGG16\n",
      "r?\n",
      "2\n",
      "VGG16 5 r\n",
      "6\n",
      "g\n",
      "f\n",
      "(a) (b)\n",
      "Figure 5: (a) the proposed architecture for query answering; and (b) an illustration of two\n",
      "possible approaches to visual-relational query answering. One can predict relation\n",
      "types between two images directly (green arrow; our approach) or combine an\n",
      "entity classifier with a KB embedding model for relation prediction (red arrows;\n",
      "baseline VGG16+DistMult).\n",
      "learning since neither the new entity’s relationships with other entities nor its images have\n",
      "been observed during training. These considerations illustrate the novel nature of the visual\n",
      "query types. The machine learning models have to be able to learn the relational semantics\n",
      "of the KG and not simply a classifier that assigns images to entities. These query types are\n",
      "also motivated by the fact that for typical KGs the number of entities is orders of magnitude\n",
      "greater than the number of relations.\n",
      "4.2 Deep Representation Learning for Visual-Relational Query Answering\n",
      "We first discuss KG completion methods and translate the concepts to query answering in\n",
      "visual-relational KGs. Let raw be the raw feature representation for entity i ∈ E and let\n",
      "i\n",
      "f and g be differentiable functions. Most KG completion methods learn an embedding of\n",
      "the entities in a vector space via some scoring function that is trained to assign high scores\n",
      "to correct triples and low scores to incorrect triples. Scoring functions have often the form\n",
      "f (e,e ) where r is a relation type, e and e are d-dimensional vectors (the embeddings of\n",
      "r h t h t\n",
      "the head and tail entities, respectively), and where e = g(raw ) is an embedding function\n",
      "i i\n",
      "that maps the raw input representation of entities to the embedding space. In the case of\n",
      "KGs without visual data, the raw representation of an entity is simply its one-hot encoding.\n",
      "(cid:124)\n",
      "Existing KG completion methods use the embedding function g(raw ) = raw W where\n",
      "i i\n",
      "W is a |E|×d matrix, and differ only in their scoring function, that is, in the way the\n",
      "embeddings of the head and tail entities are combined with the parameter vector φ :\n",
      "r\n",
      "• Difference (TransE[Bordes et al., 2013]): f (e,e ) = −||e +φ −e || where φ is\n",
      "r h t h r t 2 r\n",
      "a d-dimensional vector;\n",
      "• Multiplication (DistMult[Yang et al., 2014]): f (e,e ) = (e ∗e )·φ where ∗ is\n",
      "r h t h t r\n",
      "the element-wise product and φ a d-dimensional vector;\n",
      "r\n",
      "• Circular correlation (HolE[Nickel et al., 2016b]): f (e,e ) = (e?e )·φ where\n",
      "r h t h t r\n",
      "[a?b] = Pd−1a b and φ a d-dimensional vector; and\n",
      "k i=0 i (i+k) mod d r\n",
      "8\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "• Concatenation: f (e,e ) = (e (cid:12)e )·φ where (cid:12) is the concatenation operator and\n",
      "r h t h t r\n",
      "φ a 2d-dimensional vector.\n",
      "r\n",
      "For each of these instances, the matrix W (storing the entity embeddings) and the\n",
      "vectors φ are learned during training. In general, the parameters are trained such that\n",
      "r\n",
      "f (e,e ) is high for true triples and low for triples assumed not to hold in the KG. The\n",
      "r h t\n",
      "training objective is often based on the logistic loss, which has been shown to be superior\n",
      "for most of the composition functions [Trouillon et al., 2016],\n",
      "min X log(1+exp(−f (e,e ))+ X log(1+exp(f (e,e )))+λ||Θ||2, (1)\n",
      "r h t r h t 2\n",
      "Θ\n",
      "(h,r,t)∈Tpos (h,r,t)∈Tneg\n",
      "where T and T are the set of positive and negative training triples, respectively, Θ are\n",
      "pos neg\n",
      "the parameters trained during learning and λ is a regularization hyperparameter. For the\n",
      "above objective, a process for creating corrupted triples T is required. This often involves\n",
      "neg\n",
      "sampling a random entity for either the head or tail entity. To answer queries of the types\n",
      "(h,r,t?) and (h?,r,t) after training, we form all possible completions of the queries and\n",
      "compute a ranking based on the scores assigned by the trained model to these completions.\n",
      "For the queries of type (h,r?,t) one typically uses the softmax activation in conjunction\n",
      "with the categorical cross-entropy loss, which does not require negative triples\n",
      "!\n",
      "min X −log exp(f r(e h,e t)) +λ||Θ||2, (2)\n",
      "Θ\n",
      "(h,r,t)∈Tpos\n",
      "P r∈Rexp(f r(e h,e t)) 2\n",
      "where Θ are the parameters trained during learning.\n",
      "For visual-relational KGs, the input consists of raw image data instead of the one-hot\n",
      "encodings of entities. The approach we propose builds on the ideas and methods developed\n",
      "for KG completion. Instead of having a simple embedding function g that multiplies the\n",
      "input with a weight matrix, however, we use deep convolutional neural networks to extract\n",
      "meaningfulvisualfeaturesfromtheinputimages. Forthecompositionfunctionfweevaluate\n",
      "thefouroperationsthatwereusedintheKGcompletionliterature: difference, multiplication,\n",
      "concatenation, and circular correlation. Figure 5a depicts the basic architecture we trained\n",
      "forqueryanswering. Theweightsofthepartsoftheneuralnetworkresponsibleforembedding\n",
      "the raw image input, denoted by g, are tied. We also experimented with additional hidden\n",
      "layers indicated by the dashed dense layer. The composition operation op is either difference,\n",
      "multiplication, concatenation, or circular correlation. To the best of our knowledge, this\n",
      "is the first time that KG embedding learning and deep CNNs have been combined for\n",
      "visual-relationsl query answering.\n",
      "5. Experiments\n",
      "We conduct a series of experiments to evaluate the proposed approach. First, we describe\n",
      "the experimental set-up that applies to all experiments. Second, we report and interpret\n",
      "results for the different types of visual-relational queries.\n",
      "9\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "5.1 General Set-up\n",
      "We used Caffe, a deep learning framework [Jia et al., 2014] for designing, training, and\n",
      "evaluating the proposed models. The embedding function g is based on the VGG16 model in-\n",
      "troducedin[SimonyanandZisserman,2014]. Wepre-trainedtheVGG16ontheILSVRC2012\n",
      "data set derived from ImageNet [Deng et al., 2009] and removed the softmax layer of\n",
      "the original VGG16. We added a 256-dimensional layer after the last dense layer of the\n",
      "VGG16. The output of this layer serves as the embedding of the input images. The reason\n",
      "for reducing the embedding dimensionality from 4096 to 256 is motivated by the objective\n",
      "to obtain an efficient and compact latent representation that is feasible for KGs with billion\n",
      "of entities. For the composition function f, we performed either of the four operations\n",
      "difference, multiplication, concatenation, and circular correlation. We also experimented\n",
      "with an additional hidden layer with ReLu activation. Figure 5a depicts the generic network\n",
      "architecture. The output layer of the architecture has a softmax or sigmoid activation with\n",
      "cross-entropy loss. We initialized the weights of the newly added layers with the Xavier\n",
      "method [Glorot and Bengio, 2010].\n",
      "We used a batch size of 45 which was the maximal possible fitting into GPU memory.\n",
      "To create the training batches, we sample a random triple uniformly at random from the\n",
      "training triples. For the given triple, we randomly sample one image for the head and one\n",
      "for the tail from the set of training images. We applied SGD with a learning rate of 10−5\n",
      "for the parameters of the VGG16 and a learning rate of 10−3 for the remaining parameters.\n",
      "It is crucial to use two different learning rates since the large gradients in the newly added\n",
      "layers would lead to unreasonable changes in the pretrained part of the network. We set\n",
      "the weight decay to 5×10−4. We reduced the learning rate by a factor of 0.1 every 40,000\n",
      "iterations. Each of the models was trained for 100,000 iterations.\n",
      "Since the answers to all query types are either rankings of images or rankings of relations,\n",
      "we utilize metrics measuring the quality of rankings. In particular, we report results for\n",
      "hits@1 (hits@10, hits@100) measuring the percentage of times the correct relation was\n",
      "ranked highest (ranked in the top 10, top 100). We also compute the median of the ranks\n",
      "of the correct entities or relations and the Mean Reciprocal Rank (MRR) for entity and\n",
      "relation rankings, respectively, defined as follows:\n",
      "!\n",
      "1 X 1 1\n",
      "MRR = + (3)\n",
      "2|T| rank rank<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   6975,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Query answering', 'KG completion', 'Zero-shot learning']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  gradients in the newly added\n",
      "layers would lead to unreasonable changes in the pretrained part of the network. We set\n",
      "the weight decay to 5×10−4. We reduced the learning rate by a factor of 0.1 every 40,000\n",
      "iterations. Each of the models was trained for 100,000 iterations.\n",
      "Since the answers to all query types are either rankings of images or rankings of relations,\n",
      "we utilize metrics measuring the quality of rankings. In particular, we report results for\n",
      "hits@1 (hits@10, hits@100) measuring the percentage of times the correct relation was\n",
      "ranked highest (ranked in the top 10, top 100). We also compute the median of the ranks\n",
      "of the correct entities or relations and the Mean Reciprocal Rank (MRR) for entity and\n",
      "relation rankings, respectively, defined as follows:\n",
      "!\n",
      "1 X 1 1\n",
      "MRR = + (3)\n",
      "2|T| rank rank\n",
      "(h,r,t)∈T\n",
      "img(h) img(t)\n",
      "1 X 1\n",
      "MRR =, (4)\n",
      "|T| rank\n",
      "r\n",
      "(h,r,t)∈T\n",
      "where T is the set of all test triples, rank is the rank of the correct relation, and rank\n",
      "r img(h)\n",
      "is the rank of the highest ranked image of entity h. For each query, we remove all triples\n",
      "that are also correct answers to the query from the ranking. All experiments were run on\n",
      "commodity hardware with 128GB RAM, a single 2.8 GHz CPU, and a NVIDIA 1080 Ti.\n",
      "5.2 Visual Relation Prediction\n",
      "Given a pair of unseen images we want to determine the relations between their underlying\n",
      "unknown entities. This can be expressed with (img,r?,img ). Figure 1b illustrates this\n",
      "h t\n",
      "10\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "Table 2: Results for the relation prediction problem.\n",
      "Model Median Hits@1 Hits@10 MRR\n",
      "VGG16+DistMult 94 6.0 11.4 0.087\n",
      "Prob. Baseline 35 3.7 26.5 0.104\n",
      "DIFF 11 21.1 50.0 0.307\n",
      "MULT 8 15.5 54.3 0.282\n",
      "CAT 6 26.7 61.0 0.378\n",
      "DIFF+1HL 8 22.6 55.7 0.333\n",
      "MULT+1HL 9 14.8 53.4 0.273\n",
      "CAT+1HL 6 25.3 60.0 0.365\n",
      "query type which we refer to as visual relation prediction. We train the deep architectures\n",
      "using the training and validation triples and images, respectively. For each triple (h,r,t)\n",
      "in the training data set, we sample one training image uniformly at random for both the\n",
      "head and the tail entity. We use the architecture depicted in Figure 5a with the softmax\n",
      "activation and the categorical cross-entropy loss. For each test triple, we sample one image\n",
      "uniformly at random from the test images of the head and tail entity, respectively. We then\n",
      "use the pair of images to query the trained deep neural networks. To get a more robust\n",
      "statistical estimate of the evaluation measures, we repeat the above process three times per\n",
      "test triple. Again, none of the test triples and images are seen during training nor are any\n",
      "of the training images used during testing. Computing the answer to one query takes the\n",
      "model 20 ms.\n",
      "We compare the proposed architectures to two different baselines: one based on entity\n",
      "classificationfollowedbyaKBembeddingmethodforrelationprediction(VGG16+DistMult),\n",
      "and a probabilistic baseline (Prob. Baseline). The entity classification baseline consists of\n",
      "fine-tuning a pretrained VGG16 to classify images into the 14,870 entities of ImageGraph.\n",
      "To obtain the relation type ranking at test time, we predict the entities for the head and\n",
      "the tail using the VGG16 and then use the KB embedding method DistMult[Yang et al.,\n",
      "2014] to return a ranking of relation types for the given (head, tail) pair. DistMult is a KB\n",
      "embeddingmethodthatachievesstateoftheartresultsforKBcompletiononFB15k[Kadlec\n",
      "et al., 2017]. Therefore, for this experiment we just substitute the original output layer of\n",
      "the VGG16 pretrained on ImageNet with a new output layer suitable for our problem.\n",
      "To train, we join the train an validation splits, we set the learning rate to 10−5 for all the\n",
      "layers and we train following the same strategy that we use in all of our experiments. Once\n",
      "the system is trained, we test the model by classifying the entities of the images in the test\n",
      "set. To train DistMult, we sample 500 negatives triples for each positive triple and used an\n",
      "embedding size of 100. Figure 5b illustrates the VGG16+DistMult baseline and contrasts\n",
      "it with our proposed approach. The second baseline (probabilistic baseline) computes the\n",
      "probability of each relation type using the set of training and validation triples. The baseline\n",
      "ranks relation types based on these prior probabilities.\n",
      "Table 2 lists the results for the two baselines and the different proposed architectures.\n",
      "The probabilistic baseline outperforms the VGG16+DistMult baseline in 3 of the metrics.\n",
      "This is due to the highly skewed distribution of relation types in the training, validation, and\n",
      "11\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "...\n",
      "wonAward 3 genreOf 159407\n",
      "...\n",
      "directedBy 2 succeededBy 106817\n",
      "Figure 6: Example queries and qualitative results for the multi-relational image retrieval\n",
      "problem.\n",
      "Table 3: Results for multi-relational image retrieval.\n",
      "Median Hits@100 MRR\n",
      "Model Head Tail Head Tail Head Tail\n",
      "Baseline 6504 2789 11.9 18.4 0.065 0.115\n",
      "DIFF 1301 877 19.6 26.3 0.051 0.094\n",
      "MULT 1676 1136 16.8 22.9 0.040 0.080\n",
      "CAT 1022 727 21.4 27.5 0.050 0.087\n",
      "DIFF+1HL 1644 1141 15.9 21.9 0.045 0.085\n",
      "MULT+1HL 2004 1397 14.6 20.5 0.034 0.069\n",
      "CAT+1HL 1323 919 17.8 23.6 0.042 0.080\n",
      "CAT-SIG 814 540 23.2 30.1 0.049 0.082\n",
      "test triples. A small number of relation types makes up a large fraction of triples. Figure 3\n",
      "(left) and 3 (right) depicts the plots of the counts of relation types and entities. Moreover,\n",
      "despite DistMult achieving a hits@1 value of 0.46 for the relation prediction problem\n",
      "between entity pairs the baseline VGG16+DistMult performs poorly. This is due to the poor\n",
      "entity classification performance of the VGG (accurracy: 0.082, F1: 0.068). In the remainder\n",
      "of the experiments, therefore, we only compare to the probabilistic baseline. In the lower\n",
      "part of Table 2, we lists the results of the experiments. DIFF, MULT, and CAT stand\n",
      "for the different possible composition operations. We omitted the composition operation\n",
      "circular correlation since we were not able to make the corresponding model converge,\n",
      "despite trying several different optimizers and hyperparameter settings. The post-fix 1HL\n",
      "stands for architectures where we added an additional hidden layer with ReLu activation\n",
      "before the softmax. The concatenation operation clearly outperforms the multiplication and\n",
      "difference operations. This is contrary to findings in the KG completion literature where\n",
      "MULTandDIFFoutperformedtheconcatenationoperation. Themodelswiththeadditional\n",
      "hidden layer did not perform better than their shallower counterparts with the exception\n",
      "of the DIFF model. We hypothesize that this is due to difference being the only linear\n",
      "composition operation, benefiting from an additional non-linearity. Each of the proposed\n",
      "models outperforms the baselines.\n",
      "12\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "Median Hits@1 Hits@10 MRR\n",
      "hasCrewJob\n",
      "H T H T H T H T hasGenre\n",
      "Zero-ShotQuery(3) hasProfession\n",
      "Back to the Future Special EffectsSupervisor\n",
      "Base 34 31 1.9 2.3 18.2 28.7 0.074 0.089\n",
      "CAT 8 7 19.1 22.4 54.2 57.9 0.306 0.342 hasNutrient\n",
      "Zero-ShotQuery(4) filmHasLocation\n",
      "people.B.o.rnHere\n",
      "Base 9 5 13.0 22.6 52.3 64.8 0.251 0.359\n",
      "CAT 5 3 26.9 33.7 62.5 70.4 0.388 0.461 Library of Congress TaxonomyHasEntry Card Game\n",
      "Figure 7: (Left) Results for the zero-shot learning experiments. (Right) Example results for\n",
      "zero-shot learning. For each pair of images the top three relation types (as ranked\n",
      "by the CAT model) are listed. For the pair of images at the top, the first relation\n",
      "type is correct. For the pair of images at the bottom, the correct relation type\n",
      "TaxonomyHasEntry is not among the top three relation types.\n",
      "5.3 Multi-Relational Image Retrieval\n",
      "Given an unseen image, for which we do not know the underlying KG entity, and a relation\n",
      "type, we want to retrieve existing images that complete the query. If the image for the head\n",
      "entityisgiven,wereturnarankingofimagesforthetailentity; ifthetailentityimageisgiven\n",
      "we return a ranking of images for the head entity. This problem corresponds to query type\n",
      "(2) in Figure 1b. Note that this is equivalent to performing multi-relational metric learning\n",
      "which, to the best of our knowledge, has not been done before. We performed experiments\n",
      "with each of the three composition functions f and for two different activation/loss functions.\n",
      "First, we used the models trained with the softmax activation and the categorical cross-\n",
      "entropy loss to rank images. Second, we took the models trained with the softmax activation\n",
      "and substituted the softmax activation with a sigmoid activation and the corresponding\n",
      "binary cross-entropy loss. For each training triple (h,r,t) we then created two negative\n",
      "triples by sampling once the head and once the tail entity from the set of entities. The\n",
      "negative triples are then used in conjunction with the binary cross-entropy loss of equation 1\n",
      "to refine the pretrained weights. Directly training a model with the binary cross-entropy\n",
      "loss was not possible since the model did not converge properly. Pretraining with softmax\n",
      "and categorical cross-entropy loss was crucial to make the binary loss work.\n",
      "Duringtesting,weusedthetesttriplesandrankedtheimagesbasedontheprobabilitiesre-\n",
      "turnedbytherespectivemodels. Forinstance,giventhequery(img Senso-ji,locatedIn,img t?),\n",
      "we substituted img? with all training and validation images, one at a time, and ranked\n",
      "t\n",
      "the images according to the probabilities returned by the models. We use the rank of the\n",
      "highest ranked image belonging to the true entity (here: Japan) to compute the values for\n",
      "the evaluation measures. We repeat the same experiment three times (each time randomly\n",
      "sampling the images) and report average values. Again, we compare the results for the\n",
      "different architectures with a probabilistic baseline. For the baseline, however, we compute\n",
      "a distribution of head and tail entities for each of the relation types. For example, for the\n",
      "relation type locatedIn we compute two distributions, one for head and one for tail entities.\n",
      "We used the same measures as in the previous experiment to evaluate the returned image\n",
      "rankings.\n",
      "13\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "Table3liststheresultsoftheexperiments. Asforrelationprediction,thebestperforming\n",
      "modelsarebasedontheconcatenationoperation,followedbythedifferenceandmultiplication\n",
      "operations. Thearchitectureswithanadditionalhiddenlayerdonotimprovetheperformance.\n",
      "We also provide the results for the concatenation-based model with softmax activation where\n",
      "we refined the weights using a sigmoid activation and negative sampling as described before.\n",
      "This model is the best performing model. All neural network models are significantly better\n",
      "than the baseline with respect to the median and hits@100. However, the baseline has\n",
      "slightly superior results for the MRR. This is due to the skewed distribution of entities and\n",
      "relations in the KG (see Figure 3 (right) and Figure 3 (left)). This shows once more that\n",
      "the baseline is highly competitive for the given KG. Figure 6 visualizes the answers the\n",
      "CAT-SIG model provided for a set of four example queries. For the two queries on the left,\n",
      "the model performed well and ranked the correct entity in the top 3 (green frame). The\n",
      "examples on the right illustrate queries for which the model returned an inaccurate ranking.\n",
      "To perform query answering in a highly efficient manner, we precomputed and stored all\n",
      "image embeddings once, and only compute the scoring function (involving the composition\n",
      "operation and a dot product with φ ) at query time. Answering one multi-relational image\n",
      "r\n",
      "retrieval query (which would otherwise require 613,138 individual queries, one per possible\n",
      "image) took only 90 ms.\n",
      "5.4 Zero-Shot Visual Relation Prediction\n",
      "The last set of experiments addresses the problem of zero-shot learning. For both query\n",
      "types, we are given an new image of an entirely new entity that is not part of the KG.\n",
      "The first query type asks for relations between the given image and an unseen image for\n",
      "which we do not know the underlying KG entity. The second query type asks for the\n",
      "relations between the given image and an existing KG entity. We believe that creating\n",
      "multi-relational links to existing KG entities is a reasonable approach to zero-shot learning\n",
      "since the relations to existing visual concepts and their attributes provide a characterization\n",
      "of the new entity/category.\n",
      "For the zero-shot experiments, we generated a new set of training, validation, and test\n",
      "triples. We randomly sampled 500 entities that occur as head (tail) in the set of test triples.\n",
      "We then removed all training and validation triples whose head or tail is one of these 1000\n",
      "entities. Finally, we only kept those test triples with one of the 1000 entities either as head\n",
      "or tail but not both. For query type (4) where we know the target entity, we sample 10\n",
      "of its images and use the models 10 times to compute a probability. We use the average\n",
      "probabilities to rank the relations. For query type (3) we only use one image sampled\n",
      "randomly. As with previous experiments, we repeated procedure three times and averaged\n",
      "the results. For the baseline, we compute the probabilities of relation in the training and\n",
      "validation set (for query type (3)) and the probabilities of relations conditioned on the target\n",
      "entity (for query type (4)). Again, these are very competitive baselines due to the skewed\n",
      "distribution of relations and entities. Table 7 (left) lists the results of the experiments. The\n",
      "model based on the concatenation operation (CAT) outperforms the baseline and performs\n",
      "surprisingly well. The deep models are able to generalize to unseen images since their\n",
      "performance is comparable to the performance in the relation prediction task (query type\n",
      "(1)) where the entity was part of the KG during training (see Table 2). Figure 7 (right)\n",
      "14\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "RReellaattiioonn RRaannkk RReellaattiioonn RRaannkk\n",
      "00..221133 •••IIInnnssstttrrruuummmeeennnttt ppplllaaayyyeeeddd 00..440000 •••SSStttaaarrrrrriiinnnggg aaaccctttooorrr\n",
      "00..119911 •••TTTaaaccckkk cccooonnntttrrriiibbbuuutttiiiooonnn 00..112255 •••AAAwwwaaarrrddd nnnooommmiiinnnaaattteeeddd wwwooorrrkkk\n",
      "00..118855 •••MMMuuusssiiicccaaalll gggrrrooouuuppp mmmeeemmmbbbeeerrrsss •••CCChhhaaarrraaacccttteeerrr\n",
      "Michael\n",
      "Synthesizer Inception Imperioli\n",
      "(Known) (Unseen) (Known)\n",
      "e? e\n",
      "RReellaattiioonn RRaannkk\n",
      "00..669955 •••FFFiiilllmmm rrreeellleeeaaassseee rrreeegggiiiooonnn\n",
      "BAFTA\n",
      "•••FFFeeeaaatttuuurrreeeddd fffiiilllmmm lllooocccaaatttiiiooonnnsss Award e\n",
      "•••PPPlllaaaccceee ooofff bbbiiirrrttthhh\n",
      "(Known)\n",
      "Tokio\n",
      "e\n",
      "(Known) RReellaattiioonn RRaannkk\n",
      "00..559933 •••AAAwwwaaarrrddd nnnooommmiiinnnaaattteeeddd wwwooorrrkkk\n",
      "00..117722 •••AAAwwwaaarrrddd wwwooonnn\n",
      "•••AAAwwwaaarrrddd nnnooommmiiinnneeeeee\n",
      "Figure 8: Qualitative example of the zero-shot learning problem. The plot shows the most\n",
      "probable relations that link a sample from an unknown entity (green) with samples\n",
      "of known entities (blue) of the KG.\n",
      "depicts example queries for the zero-shot query type (3). For the first query example, the\n",
      "CAT model ranked the correct relation type first (indicated by the green bounding box).\n",
      "The second example is more challenging and the correct relation type was not part of the top\n",
      "10 ranked relation types. Figure 5.4 shows one concrete example of the zero-shot learning\n",
      "problem. In green, visual data from an unknown entity is linked with visual data from\n",
      "KG entities (blue) by ranking the most probable relation types. This problem cannot be\n",
      "addressed with standard relation prediction methods since entities need to be part of the\n",
      "KG during training for these models to work.\n",
      "6. Conclusion\n",
      "KGs are at the core of numerous AI applications. Research has focused either on link\n",
      "prediction working only on the relational structure or on scene understanding in a single\n",
      "image. We present a novel visual-relational KG where the entities are enriched with visual\n",
      "data. We proposed several novel query types and introduce neural architectures suitable for\n",
      "probabilistic query answering. We propose a novel approach to zero-shot learning as the\n",
      "problem of visually mapping an image of an entirely new entity to a KG.\n",
      "References\n",
      "Sungjin Ahn, Heeyoul Choi, Tanel Parnamaa, and Yoshua Bengio. A neural knowledge\n",
      "language model. arXiv preprint arXiv:1608.00318, 2016.\n",
      "15\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "Mohammad Al Hasan and Mohammed J Zaki. A survey of link prediction in social networks.\n",
      "In Social network data analytics, pages 243–275. Springer, 2011.\n",
      "Michael Ashburner, Catherine A. Ball, Judith A. Blake, David Botstein, Heather Butler,\n",
      "J. Michael Cherry, Allan P. Davis, Kara Dolinski, Selina S. Dwight, Janan T. Eppig,\n",
      "Midori A. Harris, David P. Hill, Laurie Issel-Tarver, Andrew Kasarskis, Suzanna Lewis,\n",
      "John C. Matese, Joel E. Richardson, Martin Ringwald, Gerald M. Rubin, and Gavin\n",
      "Sherlock. Gene Ontology: tool for the unification of biology. Nat Genet, 25(1):25–29,\n",
      "2000.\n",
      "J. Ba, K. Swersky, S. Fidler, and R. Salakhutdinov. Predicting deep zero-shot convolutional\n",
      "neural networks using textual descriptions. In CVPR, 2015.\n",
      "Sean Bell and Kavita Bala. Learning visual similarity for product design with convolutional\n",
      "neural networks. ACM Transactions on Graphics (TOG), 34(4):98, 2015.\n",
      "Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: A\n",
      "collaboratively created graph database for structuring human knowledge. In SIGMOD,\n",
      "pages 1247–1250, 2008.\n",
      "Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana\n",
      "Yakhnenko. Translating embeddings for modeling multi-relational data. In Advances in\n",
      "Neural Information Processing Systems, pages 2787–2795, 2013.\n",
      "Li C., Lai Y., Goldwasser D., and Neville J. Joint embedding models for textual and social\n",
      "analysis. In ICML Workshop, 2017.\n",
      "Xinlei Chen, Abhinav Shrivastava, and Abhinav Gupta. Neil: Extracting visual knowledge\n",
      "from web data. In Proceedings of the IEEE International Conference on Computer Vision,\n",
      "pages 1409–1416, 2013.\n",
      "Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose M. F. Moura,\n",
      "Devi Parikh, and Dhruv Batra. Visual dialog. In CVPR, July 2017.\n",
      "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale\n",
      "Hierarchical Image Database. In CVPR, 2009.\n",
      "Carl Doersch, Abhinav Gupta, and Alexei A Efros. Mid-level visual element discovery as\n",
      "discriminative mode seeking. In Advances in Neural Information Processing Systems 26,\n",
      "pages 494–502. 2013.\n",
      "Ali Farhadi, Ian Endres, Derek Hoiem, and David A. Forsyth. Describing objects by their\n",
      "attributes. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern\n",
      "Recognition, pages 1778–1785, 2009.\n",
      "Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object\n",
      "detection with discriminatively trained part-based models. IEEE transactions on pattern\n",
      "analysis and machine intelligence, 32(9):1627–1645, 2010.\n",
      "16\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "Matt Gardner and Tom M Mitchell. Efficient and expressive knowledge base completion\n",
      "using subgraph feature extraction. In EMNLP, pages 1488–1498, 2015.\n",
      "Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies\n",
      "for accurate object detection and semantic segmentation. In Proceedings of CVPR, pages\n",
      "580–587, 2014.\n",
      "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward\n",
      "neural networks. In AISTATS, 2010.\n",
      "Abhinav Gupta, Aniruddha Kembhavi, and Larry S. Davis. Observing human-object\n",
      "interactions: Usingspatialandfunctionalcompatibilityforrecognition. IEEE Transactions\n",
      "on Pattern Analysis and Machine Intelligence, 31:1775–1789, 2009.\n",
      "Kelvin Guu, John Miller, and Percy Liang. Traversing knowledge graphs in vector space.\n",
      "arXiv preprint arXiv:1506.01094, 2015.\n",
      "Ido Guy, Alexander Nus, Dan Pelleg, and Idan Szpektor. Care to share?: Learning to rank\n",
      "personal photos for public sharing. In WSDM, pages 207–215. ACM, 2018.\n",
      "Hamid Izadinia, Fereshteh Sadeghi, and Ali Farhadi. Incorporating scene context and object\n",
      "layout into appearance modeling. In Proceedings of the IEEE Conference on Computer\n",
      "Vision and Pattern Recognition, pages 232–239, 2014.\n",
      "YangqingJia,EvanShelhamer,JeffDonahue,SergeyKarayev,JonathanLong,RossGirshick,\n",
      "Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature\n",
      "embedding. arXiv preprint arXiv:1408.5093, 2014.\n",
      "Lu Jiang, Yannis Kalantidis, Liangliang Cao, Sachin Farfade, Jiliang Tang, and Alexander G.\n",
      "Hauptmann. Delving deep into personal photo and video search. In Proceedings of the\n",
      "Tenth ACM International Conference on Web Search and Data Mining, WSDM, 2017.\n",
      "J. Johnson, R. Krishna, M. Stark, L.-J. Li, D. A. Shamma, M. Bernstein, and L. Fei-Fei.\n",
      "Image retrieval using scene graphs. In CVPR, 2015.\n",
      "Rudolf Kadlec, Ondrej Bajgar, and Jan Kleindienst. Knowledge base completion: Baselines\n",
      "strike back. arXiv preprint arXiv:1705.10744, 2017.\n",
      "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz,\n",
      "Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, and\n",
      "Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense\n",
      "image annotations. In arXiv preprint arXiv:1602.07332, 2016.\n",
      "Ni Lao, Tom Mitchell, and William W Cohen. Random walk inference and learning in a\n",
      "large scale knowledge base. In Proceedings of the Conference on Empirical Methods in\n",
      "Natural Language Processing, pages 529–539. Association for Computational Linguistics,\n",
      "2011.\n",
      "17\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N.\n",
      "Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick van Kleef, Sören Auer, and\n",
      "Christian Bizer. DBpedia - a large-scale, multilingual knowledge base extracted from\n",
      "wikipedia. Semantic Web Journal, (2):167–195, 2015.\n",
      "Yining Li, Chen Huang, Xiaoou Tang, and Chen Change Loy. Learning to disambiguate by\n",
      "asking discriminative questions. In ICCV, 2017.\n",
      "Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering\n",
      "robust clothes recognition and retrieval with rich annotations. In CVPR, June 2016.\n",
      "C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei. Visual relationship detection with language\n",
      "priors. In ECCV, 2016.\n",
      "Tomasz Malisiewicz and Alexei A. Efros. Beyond categories: The visual memex model\n",
      "for reasoning about object relationships. In Advances in Neural Information Processing\n",
      "Systems, 2009.\n",
      "Kenneth Marino, Ruslan Salakhutdinov, and Abhinav Gupta. The more you know: Using\n",
      "knowledge graphs for image classification. In CVPR, 2017.\n",
      "Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective\n",
      "learning on multi-relational data. In Proceedings of the 28th international conference on\n",
      "machine learning (ICML-11), pages 809–816, 2011.\n",
      "Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of\n",
      "relational machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):11–33,\n",
      "2016a.\n",
      "Maximilian Nickel, Lorenzo Rosasco, and Tomaso A. Poggio. Holographic embeddings of\n",
      "knowledge graphs. In Proceedings of the Thirtieth Conference on Artificial Intelligence,\n",
      "pages 1955–1961, 2016b.\n",
      "Wei Niu, James Caverlee, and Haokai Lu. Neural personalized ranking for image recommen-\n",
      "dation. In Proceedings of the Eleventh ACM International Conference on Web Search and\n",
      "Data Mining, WSDM, 2018.\n",
      "Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via\n",
      "lifted structured feature embedding. In Proceedings of the IEEE Conference on Computer\n",
      "Vision and Pattern Recognition, pages 4004–4012, 2016.\n",
      "Megha Pandey and Svetlana Lazebnik. Scene recognition and weakly supervised object\n",
      "localization with deformable part-based models. In Computer Vision (ICCV), 2011 IEEE\n",
      "International Conference on, pages 1307–1314, 2011.\n",
      "Pouya Pezeshkpour, Liyan Chen, and Sameer Singh. Embedding multimodal relational data\n",
      "for knowledge base completion. In EMNLP, 2018.\n",
      "B. Romera-Paredes and P. Torr. An embarrassingly simple approach to zero-shot learning.\n",
      "In ICML, 2015.\n",
      "18\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "Olga Russakovsky, Jia Deng, Zhiheng Huang, Alexander C. Berg, and Li Fei-Fei. Detecting\n",
      "avocados to zucchinis: what have we done, and where are we going? In International\n",
      "Conference on Computer Vision (ICCV), 2013.\n",
      "Fereshteh Sadeghi and Marshall F. Tappen. Latent pyramidal regions for recognizing scenes.\n",
      "In Proceedings of the 12th European Conference on Computer Vision - Volume Part V,\n",
      "pages 228–241, 2012.\n",
      "F.Schroff,D.Kalenichenko,andJ.Philbin. Facenet: Aunifiedembeddingforfacerecognition\n",
      "and clustering. In 2015 IEEE Conference on Computer Vision and Pattern Recognition\n",
      "(CVPR), pages 815–823, 2015.\n",
      "Iulian Vlad Serban, Alberto García-Durán, Caglar Gulcehre, Sungjin Ahn, Sarath Chandar,\n",
      "Aaron Courville, and Yoshua Bengio. Generating factoid questions with recurrent neural\n",
      "networks: The 30m factoid question-answer corpus. arXiv preprint arXiv:1603.06807,\n",
      "2016.\n",
      "K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image\n",
      "recognition. CoRR, 2014.\n",
      "Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In\n",
      "Advances in Neural Information Processing Systems, pages 1857–1865, 2016.\n",
      "Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, and Kevin Murphy. Deep metric learning\n",
      "via facility location. In Conference on Computer Vision and Pattern Recognition (CVPR),\n",
      "2017.\n",
      "Damien Teney, Lingqiao Liu, and Anton van den Hengel. Graph-structured representations\n",
      "for visual question answering. In CVPR, July 2017.\n",
      "Steffen Thoma, Achim Rettinger, and Fabian Both. Towards holistic concept representations:\n",
      "Embedding relational knowledge, visual attributes, and distributional word semantics. In\n",
      "International Semantic Web Conference (1), volume 10587 of Lecture Notes in Computer\n",
      "Science, pages 694–710. Springer, 2017.\n",
      "Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and\n",
      "text inference. In Proceedings of the 3rd Workshop on Continuous Vector Space Models\n",
      "and their Compositionality, pages 57–66, 2015.\n",
      "Rakshit Trivedi, Hanjun Dai, Yichen Wang, and Le Song. Know-evolve: Deep temporal\n",
      "reasoning for dynamic knowledge graphs. In ICML, 2017.\n",
      "Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard.\n",
      "Complex embeddings for simple link prediction. arXiv preprint arXiv:1606.06357, 2016.\n",
      "Andreas Veit, Balazs Kovacs, Sean Bell, Julian McAuley, Kavita Bala, and Serge Belongie.\n",
      "Learning visual clothing style with heterogeneous dyadic co-occurrences. In ICCV, 2015.\n",
      "Lonij Vincent, Rawat Ambrish, and Nicolae Maria-Irina. Extending knowledge bases using\n",
      "images. In AKBC, 2017.\n",
      "19<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  62965,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Visual Relation Prediction', 'Multi-Relational Image Retrieval', 'Zero-Shot Visual Relation Prediction']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  on Continuous Vector Space Models\n",
      "and their Compositionality, pages 57–66, 2015.\n",
      "Rakshit Trivedi, Hanjun Dai, Yichen Wang, and Le Song. Know-evolve: Deep temporal\n",
      "reasoning for dynamic knowledge graphs. In ICML, 2017.\n",
      "Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard.\n",
      "Complex embeddings for simple link prediction. arXiv preprint arXiv:1606.06357, 2016.\n",
      "Andreas Veit, Balazs Kovacs, Sean Bell, Julian McAuley, Kavita Bala, and Serge Belongie.\n",
      "Learning visual clothing style with heterogeneous dyadic co-occurrences. In ICCV, 2015.\n",
      "Lonij Vincent, Rawat Ambrish, and Nicolae Maria-Irina. Extending knowledge bases using\n",
      "images. In AKBC, 2017.\n",
      "19\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "Jian Wang, Feng Zhou, Shilei Wen, Xiao Liu, and Yuanqing Lin. Deep metric learning with\n",
      "angular loss. In International Conference on Computer Vision (ICCV), 2017.\n",
      "X. Wang, S. Qiu, K. Liu, and X. Tang. Web image re-ranking usingquery-specific semantic\n",
      "signatures. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(4):\n",
      "810–823, April 2014.\n",
      "David S. Wishart, Craig Knox, Anchi Guo, Dean Cheng, Savita Shrivastava, Dan Tzur,\n",
      "Bijaya Gautam, and Murtaza Hassanali. Drugbank: a knowledgebase for drugs, drug\n",
      "actions and drug targets. Nucleic Acids Research, 36:901–906, 2008.\n",
      "Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. SUN\n",
      "database: Large-scale scene recognition from abbey to zoo. In The Twenty-Third IEEE\n",
      "Conference on Computer Vision and Pattern Recognition, pages 3485–3492, 2010.\n",
      "Mohamed Yahya, Denilson Barbosa, Klaus Berberich, Qiuyue Wang, and Gerhard Weikum.\n",
      "Relationship queries on extended knowledge graphs. In WSDM, 2016.\n",
      "Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Learning multi-\n",
      "relational semantics using neural-embedding models. arXiv preprint arXiv:1411.4072,\n",
      "2014.\n",
      "X. Yang, T. Mei, Y. Zhang, J. Liu, and S. Satoh. Web image search re-ranking with\n",
      "click-based similarity and typicality. IEEE Transactions on Image Processing, 25(10):\n",
      "4617–4630, 2016.\n",
      "Bangpeng Yao and Li Fei-Fei. Modeling mutual context of object and human pose in human-\n",
      "object interaction activities. In Computer Vision and Pattern Recognition (CVPR), 2010\n",
      "IEEE Conference on, pages 17–24, 2010.\n",
      "Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J. Smola, and Le Song. Variational\n",
      "reasoning for question answering with knowledge graph. In AAAI, 2018.\n",
      "Z. Zhang and V. Saligrama. Zero-shot learning via semantic similarity embedding. In ICCV,\n",
      "2015.\n",
      "Feng Zhou and Yuanqing Lin. Fine-grained image classification by exploring bipartite-graph\n",
      "labels. In CVPR, June 2016.\n",
      "Yuke Zhu, Alireza Fathi, and Li Fei-Fei. Reasoning about object affordances in a knowledge\n",
      "base representation. In European conference on computer vision, pages 408–424, 2014.\n",
      "20<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "            220,    389,  70067,   4290,  11746,  27972,    198,    438,    872,\n",
      "          68206,   2786,     11,   6959,    220,   3226,   4235,   2287,     11,\n",
      "            220,    679,     20,    627,     49,    587,  41153,   1183,   2270,\n",
      "             72,     11,  21296,  58781,  80223,     11,    816,  29424,  29346,\n",
      "             11,    323,   2009,  19508,     13,  14521,  91345,   4035,     25,\n",
      "          18682,  37015,    198,  20489,    287,    369,   8915,   6677,  40099,\n",
      "             13,    763,  19845,   2735,     11,    220,    679,     22,    627,\n",
      "           1016,  89577,  42782,  43588,     11,  55205,  26056,   2067,     11,\n",
      "          44609,    432,   1142,    301,     11,  29124,   2265,  94316,   1291,\n",
      "             11,    323,   4673,  99112,    426,   3102,    569,    627,  32237,\n",
      "          71647,    369,   4382,   2723,  20212,     13,    802,     55,    344,\n",
      "            864,   1374,    802,     55,    344,     25,   6330,     21,     13,\n",
      "          24254,   3226,     11,    220,    679,     21,    627,   3112,  51518,\n",
      "          23404,    275,     11,  19984,   1394,     82,  93981,  19807,     11,\n",
      "          26044,  18366,     11,  38897,  65813,   1130,     88,     11,    735,\n",
      "            402,   6388,    426,   6181,     11,    323,  33022,   7984,    647,\n",
      "            648,    627,  48567,   9302,  17895,   1742,    449,  98882,  14282,\n",
      "          37314,   1080,     12,  14310,  34346,     13,    763,  59332,     53,\n",
      "             11,    220,    679,     20,    627,  54324,   3251,  35407,     11,\n",
      "          23886,    266,   3383,   1347,    819,     11,    323,  83440,   6043,\n",
      "          23880,  22197,  41071,     13,   9634,   2518,   6677,  23963,   1701,\n",
      "            198,   3726,     13,    763,  31672,   5002,     11,    220,    679,\n",
      "             22,    627,    777,    198,     46,   5771,  18812,  11151,    392,\n",
      "            822,     11,  53383,  77468,     11,  85341,   9607,    324,  11644,\n",
      "             11,  33555,  97465,   6354,  99634,     11,    612,  92075,   6354,\n",
      "            561,    265,    198,     41,   1122,  29346,     11,  43758,  67927,\n",
      "             11,   1443,    458,     72,  62323,     11,  66690,  38805,     11,\n",
      "            323,  69984,  90684,   8732,     13,  18682,  18767,   6975,    449,\n",
      "            198,   4328,   4814,     13,    763,   7327,  15217,    389,  17863,\n",
      "          31541,    320,   1341,  20161,    705,    220,    679,     22,    627,\n",
      "             55,     13,  29346,     11,    328,     13,   1229,  19260,     11,\n",
      "            735,     13,  38805,     11,    323,   1630,     13,  41462,     13,\n",
      "           5000,   2217,    312,  72539,   1701,   1663,  19440,  42833,    198,\n",
      "           7908,   2859,     13,  40135,  56385,    389,  19365,  18825,    323,\n",
      "          13257,  22107,     11,    220,   1927,      7,     19,    997,  19232,\n",
      "           4235,  23848,     11,   5936,    220,    679,     19,    627,  23083,\n",
      "            328,     13,  38747,    472,     11,  29517,  54450,     11,   1556,\n",
      "          14946,   4673,     78,     11,  25028,  57807,     11,  20680,   6388,\n",
      "           1443,  77467,    561,   2979,     11,  11824,    350,     89,    324,\n",
      "            345,     33,   3251,  12874,  96154,    309,     11,    323,    386,\n",
      "           5757,  12997,  60777,   8115,     13,  26166,  17469,     25,    264,\n",
      "           6677,   3231,    369,  11217,     11,   5623,    198,   4109,    323,\n",
      "           5623,  11811,     13,    452,  22935,    292,   6515,   3447,   8483,\n",
      "             11,    220,   1927,     25,  19319,   4235,  22224,     11,    220,\n",
      "           1049,     23,    627,     41,   1122,     87,    290,     70,  66690,\n",
      "             11,   7957,    473,    954,     11,  27973,     64,    362,     13,\n",
      "          61651,   5248,     11,    362,    799,  12225,  10126,     11,    323,\n",
      "          23245,   8611,   3545,   4749,     13,  57328,    198,  12494,     25,\n",
      "          20902,  13230,   6237,  18324,    505,  30195,   1216,    311,  42014,\n",
      "             13,    763,    578,  44956,     12,  38075,  40135,    198,  92348,\n",
      "            389,  17863,  31541,    323,  19365,  48698,     11,   6959,    220,\n",
      "          19746,     20,   4235,  18634,     17,     11,    220,    679,     15,\n",
      "            627,  83705,   3690,  83562,   7911,     11,   9973,    321,    942,\n",
      "          47142,  12252,     11,  82197,   9084,    655,    718,     11,  58094,\n",
      "           4168,    361,  29346,     11,    323,  20524,  19221,   1226,   1609,\n",
      "            372,    627,  51922,  20126,    389,  11838,   6677,  40099,     13,\n",
      "            763,    468,   5608,     44,     11,    220,    679,     21,    627,\n",
      "             33,    819,    276,  25482,     11,  62323,   2442,   2933,    816,\n",
      "           7141,     11,  41235,    347,    647,   1283,     11,  88404,     69,\n",
      "            833,    480,   3524,     11,    323,  14851,  92829,     13,  21579,\n",
      "           7447,   7058,   3833,   1697,  53794,   1701,  30828,     12,  95711,\n",
      "           4211,     13,    802,     55,    344,    864,   1374,    802,     55,\n",
      "            344,     25,   9335,     16,     13,  18501,     17,    345,    679,\n",
      "             19,    627,     55,     13,  25482,     11,    350,     13,  92033,\n",
      "             11,    816,     13,  37120,     11,    622,     13,  38805,     11,\n",
      "            323,    328,     13,  13479,   2319,     13,   5000,   2217,   2778,\n",
      "            312,  72539,    449,    198,   3763,   6108,  38723,    323,  14595,\n",
      "            488,     13,  40135,  56385,    389,   4758,  29225,     11,    220,\n",
      "            914,      7,    605,    997,  19608,     22,   4235,  21290,     15,\n",
      "             11,    220,    679,     21,    627,  63919,  64183,  91030,    323,\n",
      "          14851,   3926,     72,     12,   6251,     72,     13,  77349,  27848,\n",
      "           2317,    315,   1665,    323,   3823,  17477,    304,   3823,   7058,\n",
      "           1735,  16628,   7640,     13,    763,  17863,  31541,    323,  19365,\n",
      "          48698,    320,  20161,   6616,    705,    220,    679,     15,    198,\n",
      "          77805,  15217,    389,     11,   6959,    220,   1114,   4235,   1187,\n",
      "             11,    220,    679,     15,    627,     56, 113528,  37120,     11,\n",
      "          21296,  58781,  80223,     11,   1901,   1540,   1220,     64,    735,\n",
      "           9700,    548,   6723,     11,  20643,    622,     13,   4487,   8083,\n",
      "             11,    323,   2009,  19508,     13,  28968,   1697,    198,  20489,\n",
      "            287,    369,   3488,  36864,    449,   6677,   4876,     13,    763,\n",
      "          48197,     40,     11,    220,    679,     23,    627,     57,     13,\n",
      "          37120,    323,    650,     13,   8375,    343,  31473,     13,  18811,\n",
      "          64630,   6975,   4669,  42833,  38723,  40188,     13,    763,  59332,\n",
      "             53,    345,    679,     20,    627,     37,    833,  67927,    323,\n",
      "          69984,  90684,   8732,     13,  31253,  25313,   2692,   2217,  24790,\n",
      "            555,  24919,  29978,    472,    635,  74116,    198,  17298,     13,\n",
      "            763,  14499,   6616,     11,   5651,    220,    679,     21,    627,\n",
      "             56,  10647,  68844,     11,   1708,    556,   4458,    435,  67631,\n",
      "             11,    323,  14851,   3926,     72,     12,   6251,     72,     13,\n",
      "          27857,    287,    922,   1665,  10150,   3095,    304,    264,   6677,\n",
      "            198,   3231,  13340,     13,    763,   7665,  10017,    389,   6500,\n",
      "          11376,     11,   6959,    220,  18058,   4235,  18517,     11,    220,\n",
      "            679,     19,    627,    508, 128009, 128006,    882, 128007,    271,\n",
      "           7184,     11,   2728,    420,   3488,     25,   3639,    527,    279,\n",
      "           9256,    430,    279,   1646,    374,  16572,    369,   4710,    220,\n",
      "          21335,   1203,    279,   4320,   1193,    304,    264,  13325,   1160,\n",
      "           3645,     11,    369,   3187,     25,   2570,     32,   1882,     33,\n",
      "           7352,   1442,    499,   1541,    956,   1440,    279,   4320,     11,\n",
      "           1120,    471,    459,   4384,   1160,     13, 128009, 128006,  78191,\n",
      "         128007,    271]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "            220,    389,  70067,   4290,  11746,  27972,    198,    438,    872,\n",
      "          68206,   2786,     11,   6959,    220,   3226,   4235,   2287,     11,\n",
      "            220,    679,     20,    627,     49,    587,  41153,   1183,   2270,\n",
      "             72,     11,  21296,  58781,  80223,     11,    816,  29424,  29346,\n",
      "             11,    323,   2009,  19508,     13,  14521,  91345,   4035,     25,\n",
      "          18682,  37015,    198,  20489,    287,    369,   8915,   6677,  40099,\n",
      "             13,    763,  19845,   2735,     11,    220,    679,     22,    627,\n",
      "           1016,  89577,  42782,  43588,     11,  55205,  26056,   2067,     11,\n",
      "          44609,    432,   1142,    301,     11,  29124,   2265,  94316,   1291,\n",
      "             11,    323,   4673,  99112,    426,   3102,    569,    627,  32237,\n",
      "          71647,    369,   4382,   2723,  20212,     13,    802,     55,    344,\n",
      "            864,   1374,    802,     55,    344,     25,   6330,     21,     13,\n",
      "          24254,   3226,     11,    220,    679,     21,    627,   3112,  51518,\n",
      "          23404,    275,     11,  19984,   1394,     82,  93981,  19807,     11,\n",
      "          26044,  18366,     11,  38897,  65813,   1130,     88,     11,    735,\n",
      "            402,   6388,    426,   6181,     11,    323,  33022,   7984,    647,\n",
      "            648,    627,  48567,   9302,  17895,   1742,    449,  98882,  14282,\n",
      "          37314,   1080,     12,  14310,  34346,     13,    763,  59332,     53,\n",
      "             11,    220,    679,     20,    627,  54324,   3251,  35407,     11,\n",
      "          23886,    266,   3383,   1347,    819,     11,    323,  83440,   6043,\n",
      "          23880,  22197,  41071,     13,   9634,   2518,   6677,  23963,   1701,\n",
      "            198,   3726,     13,    763,  31672,   5002,     11,    220,    679,\n",
      "             22,    627,    777,    198,     46,   5771,  18812,  11151,    392,\n",
      "            822,     11,  53383,  77468,     11,  85341,   9607,    324,  11644,\n",
      "             11,  33555,  97465,   6354,  99634,     11,    612,  92075,   6354,\n",
      "            561,    265,    198,     41,   1122,  29346,     11,  43758,  67927,\n",
      "             11,   1443,    458,     72,  62323,     11,  66690,  38805,     11,\n",
      "            323,  69984,  90684,   8732,     13,  18682,  18767,   6975,    449,\n",
      "            198,   4328,   4814,     13,    763,   7327,  15217,    389,  17863,\n",
      "          31541,    320,   1341,  20161,    705,    220,    679,     22,    627,\n",
      "             55,     13,  29346,     11,    328,     13,   1229,  19260,     11,\n",
      "            735,     13,  38805,     11,    323,   1630,     13,  41462,     13,\n",
      "           5000,   2217,    312,  72539,   1701,   1663,  19440,  42833,    198,\n",
      "           7908,   2859,     13,  40135,  56385,    389,  19365,  18825,    323,\n",
      "          13257,  22107,     11,    220,   1927,      7,     19,    997,  19232,\n",
      "           4235,  23848,     11,   5936,    220,    679,     19,    627,  23083,\n",
      "            328,     13,  38747,    472,     11,  29517,  54450,     11,   1556,\n",
      "          14946,   4673,     78,     11,  25028,  57807,     11,  20680,   6388,\n",
      "           1443,  77467,    561,   2979,     11,  11824,    350,     89,    324,\n",
      "            345,     33,   3251,  12874,  96154,    309,     11,    323,    386,\n",
      "           5757,  12997,  60777,   8115,     13,  26166,  17469,     25,    264,\n",
      "           6677,   3231,    369,  11217,     11,   5623,    198,   4109,    323,\n",
      "           5623,  11811,     13,    452,  22935,    292,   6515,   3447,   8483,\n",
      "             11,    220,   1927,     25,  19319,   4235,  22224,     11,    220,\n",
      "           1049,     23,    627,     41,   1122,     87,    290,     70,  66690,\n",
      "             11,   7957,    473,    954,     11,  27973,     64,    362,     13,\n",
      "          61651,   5248,     11,    362,    799,  12225,  10126,     11,    323,\n",
      "          23245,   8611,   3545,   4749,     13,  57328,    198,  12494,     25,\n",
      "          20902,  13230,   6237,  18324,    505,  30195,   1216,    311,  42014,\n",
      "             13,    763,    578,  44956,     12,  38075,  40135,    198,  92348,\n",
      "            389,  17863,  31541,    323,  19365,  48698,     11,   6959,    220,\n",
      "          19746,     20,   4235,  18634,     17,     11,    220,    679,     15,\n",
      "            627,  83705,   3690,  83562,   7911,     11,   9973,    321,    942,\n",
      "          47142,  12252,     11,  82197,   9084,    655,    718,     11,  58094,\n",
      "           4168,    361,  29346,     11,    323,  20524,  19221,   1226,   1609,\n",
      "            372,    627,  51922,  20126,    389,  11838,   6677,  40099,     13,\n",
      "            763,    468,   5608,     44,     11,    220,    679,     21,    627,\n",
      "             33,    819,    276,  25482,     11,  62323,   2442,   2933,    816,\n",
      "           7141,     11,  41235,    347,    647,   1283,     11,  88404,     69,\n",
      "            833,    480,   3524,     11,    323,  14851,  92829,     13,  21579,\n",
      "           7447,   7058,   3833,   1697,  53794,   1701,  30828,     12,  95711,\n",
      "           4211,     13,    802,     55,    344,    864,   1374,    802,     55,\n",
      "            344,     25,   9335,     16,     13,  18501,     17,    345,    679,\n",
      "             19,    627,     55,     13,  25482,     11,    350,     13,  92033,\n",
      "             11,    816,     13,  37120,     11,    622,     13,  38805,     11,\n",
      "            323,    328,     13,  13479,   2319,     13,   5000,   2217,   2778,\n",
      "            312,  72539,    449,    198,   3763,   6108,  38723,    323,  14595,\n",
      "            488,     13,  40135,  56385,    389,   4758,  29225,     11,    220,\n",
      "            914,      7,    605,    997,  19608,     22,   4235,  21290,     15,\n",
      "             11,    220,    679,     21,    627,  63919,  64183,  91030,    323,\n",
      "          14851,   3926,     72,     12,   6251,     72,     13,  77349,  27848,\n",
      "           2317,    315,   1665,    323,   3823,  17477,    304,   3823,   7058,\n",
      "           1735,  16628,   7640,     13,    763,  17863,  31541,    323,  19365,\n",
      "          48698,    320,  20161,   6616,    705,    220,    679,     15,    198,\n",
      "          77805,  15217,    389,     11,   6959,    220,   1114,   4235,   1187,\n",
      "             11,    220,    679,     15,    627,     56, 113528,  37120,     11,\n",
      "          21296,  58781,  80223,     11,   1901,   1540,   1220,     64,    735,\n",
      "           9700,    548,   6723,     11,  20643,    622,     13,   4487,   8083,\n",
      "             11,    323,   2009,  19508,     13,  28968,   1697,    198,  20489,\n",
      "            287,    369,   3488,  36864,    449,   6677,   4876,     13,    763,\n",
      "          48197,     40,     11,    220,    679,     23,    627,     57,     13,\n",
      "          37120,    323,    650,     13,   8375,    343,  31473,     13,  18811,\n",
      "          64630,   6975,   4669,  42833,  38723,  40188,     13,    763,  59332,\n",
      "             53,    345,    679,     20,    627,     37,    833,  67927,    323,\n",
      "          69984,  90684,   8732,     13,  31253,  25313,   2692,   2217,  24790,\n",
      "            555,  24919,  29978,    472,    635,  74116,    198,  17298,     13,\n",
      "            763,  14499,   6616,     11,   5651,    220,    679,     21,    627,\n",
      "             56,  10647,  68844,     11,   1708,    556,   4458,    435,  67631,\n",
      "             11,    323,  14851,   3926,     72,     12,   6251,     72,     13,\n",
      "          27857,    287,    922,   1665,  10150,   3095,    304,    264,   6677,\n",
      "            198,   3231,  13340,     13,    763,   7665,  10017,    389,   6500,\n",
      "          11376,     11,   6959,    220,  18058,   4235,  18517,     11,    220,\n",
      "            679,     19,    627,    508, 128009, 128006,    882, 128007,    271,\n",
      "           7184,     11,   2728,    420,   3488,     25,   3639,    527,    279,\n",
      "           9256,    430,    279,   1646,    374,  16572,    369,   4710,    220,\n",
      "          21335,   1203,    279,   4320,   1193,    304,    264,  13325,   1160,\n",
      "           3645,     11,    369,   3187,     25,   2570,     32,   1882,     33,\n",
      "           7352,   1442,    499,   1541,    956,   1440,    279,   4320,     11,\n",
      "           1120,    471,    459,   4384,   1160,     13, 128009, 128006,  78191,\n",
      "         128007,    271,    681,   4026,  20212,    518,    364,  14924,  36864,\n",
      "            518,    364,   1945,  24790,    518,    364,  64816,  25313,   2692,\n",
      "           2217,  24790,    518,    364,   6109,   2217,    312,  72539,    518,\n",
      "            364,  10224,  18324,    518,    364,   1211,  10150,   3095,    518,\n",
      "            364,  18483,  64630,   6975,    518,    364,  34564,  18767,   6975,\n",
      "            518,    364,  93950,  33811,    518,    364,   6899,   2518,   6677,\n",
      "          23963,   1701,   5448,    518,    364,  48567,   9302,  17895,   1742,\n",
      "            518,    364,  51922,  20126,    389,  11838,   6677,  40099,    663,\n",
      "         128009]], device='cuda:0')\n",
      "Decoded output:\n",
      " ['Link prediction', 'Question answering', 'Image classification', 'Fine-grained image classification', 'Web image re-ranking', 'Scene recognition', 'Object affordances', 'Zero-shot learning', 'Deep metric learning', 'Temporal reasoning', 'Extending knowledge bases using images', 'Learning visual clothing style', 'Relationship queries on extended knowledge graphs']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: AutomatedKnowledgeBaseConstruction(2019) Conferencepaper\n",
      "Answering Visual-Relational Queries in\n",
      "Web-Extracted Knowledge Graphs\n",
      "Daniel Oñoro-Rubio daniel.onoro@neclab.eu\n",
      "Mathias Niepert mathias.niepert@neclab.eu\n",
      "Alberto García-Durán alberto.duran@neclab.eu\n",
      "Roberto González-Sánchez roberto.gonzalez@neclab.eu\n",
      "NEC Labs Europe\n",
      "Roberto J. López-Sastre robertoj.lopez@uah.es\n",
      "University of Alcalá\n",
      "Abstract\n",
      "A visual-relational knowledge graph (KG) is a multi-relational graph whose entities\n",
      "are associated with images. We explore novel machine learning approaches for answering\n",
      "visual-relational queries in web-extracted knowledge graphs. To this end, we have created\n",
      "ImageGraph, a KG with 1,330 relation types, 14,870 entities, and 829,931 images crawled\n",
      "from the web. With visual-relational KGs such as ImageGraph one can introduce novel\n",
      "probabilistic query types in which images are treated as first-class citizens. Both the\n",
      "prediction of relations between unseen images as well as multi-relational image retrieval\n",
      "can be expressed with specific families of visual-relational queries. We introduce novel\n",
      "combinationsofconvolutionalnetworksandknowledgegraphembeddingmethodstoanswer\n",
      "such queries. We also explore a zero-shot learning scenario where an image of an entirely\n",
      "new entity is linked with multiple relations to entities of an existing KG. The resulting\n",
      "multi-relational grounding of unseen entity images into a knowledge graph serves as a\n",
      "semantic entity representation. We conduct experiments to demonstrate that the proposed\n",
      "methods can answer these visual-relational queries efficiently and accurately.\n",
      "1. Introduction\n",
      "Numerous applications can be modeled with a knowledge graph representing entities with\n",
      "nodes, object attributes with node attributes, and relationships between entities by directed\n",
      "typed edges. For instance, a product recommendation system can be represented as a\n",
      "knowledge graph where nodes represent customers and products and where typed edges\n",
      "represent customer reviews and purchasing events. In the medical domain, there are several\n",
      "knowledge graphs that model diseases, symptoms, drugs, genes, and their interactions (cf.\n",
      "[Ashburner et al., 2000, Wishart et al., 2008]). Increasingly, entities in these knowledge\n",
      "graphs are associated with visual data. For instance, in the online retail domain, there are\n",
      "product and advertising images and in the medical domain, there are patient-associated\n",
      "imaging data sets (MRIs, CTs, and so on). In addition, visual data is a large part of social\n",
      "networks and, in general, the world wide web.\n",
      "Knowledge graphs facilitate the integration, organization, and retrieval of structured\n",
      "data and support various forms of search applications. In recent years KGs have been\n",
      "1Project URL: https://github.com/nle-ml/mmkb.git.\n",
      "2This paper is part of the proceedings of AKBC 2019.\n",
      "1\n",
      "9102\n",
      "yaM\n",
      "3\n",
      "]GL.sc[\n",
      "6v41320.9071:viXra\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "?\n",
      "(1)\n",
      "(2) locatedIn?\n",
      "Gotoh Museum Murasaki Shikibu\n",
      "n\n",
      "hasArtAbout\n",
      "catedI b\n",
      "o rn\n",
      "In\n",
      "(3)\n",
      "?\n",
      "lo captialOf\n",
      "Tokyo Japan New entity\n",
      "locatedIn\n",
      "?\n",
      "(4)\n",
      "Sensō-ji\n",
      "Japan\n",
      "New entity\n",
      "(a)\n",
      "(b)\n",
      "Figure 1: (a) a small part of a visual-relational knowledge graph and a set of query types;\n",
      "and (b) some visual-relational query types;\n",
      "playing an increasingly crucial role in fields such as question answering [Das et al., 2017],\n",
      "language modeling [Ahn et al., 2016], and text generation [Serban et al., 2016]. Even\n",
      "though there is a large body of work on constructing and maintaining KGs, the setting of\n",
      "visual-relational KGs, where entities are associated with visual data, has not received much\n",
      "attention. A visual-relational KG represents entities, relations between these entities, and a\n",
      "large number of images associated with the entities (see Figure 1a for an example). While\n",
      "ImageNet [Deng et al., 2009] and the VisualGenome [Krishna et al., 2016] datasets are\n",
      "based on KGs such as WordNet they are predominantly used as either an object classification\n",
      "data set as in the case of ImageNet or to facilitate scene understanding in a single image.\n",
      "With this work, we address the problem of reasoning about visual concepts across a large set\n",
      "of images organized in a knowledge graph. We want to explore to what extent web-extracted\n",
      "visual data can be used to enrich existing KGs so as to facilitate complex visual search\n",
      "applications going beyond basic image retrieval.\n",
      "The core idea of our work is to treat images as first-class citizens both in KGs and\n",
      "visual-relational queries. The main objective of our work is to understand to what extent\n",
      "visual data associated with entities of a KG can be used in conjunction with deep learning\n",
      "methods to answer these visual-relational queries. Allowing images to be arguments of\n",
      "queries facilitates numerous novel query types. In Figure 1b we list some of the query types\n",
      "we address in this paper. In order to answer these queries, we built on KG embedding\n",
      "methods as well as deep representation learning approaches for visual data. This allows us\n",
      "to answer these visual queries both accurately and efficiently.\n",
      "Therearenumerousapplicationdomainsthatcouldbenefitfromqueryansweringinvisual\n",
      "KGs. Forinstance,inonlineretail,visualrepresentationsofnovelproductscouldbeleveraged\n",
      "for zero-shot product recommendations. Crucially, instead of only being able to retrieve\n",
      "similar products, a visual-relational KG would support the prediction of product attributes\n",
      "and more specifically what attributes customers might be interested in. For instance, in\n",
      "2\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "Table 1: Statistics of the knowledge graphs used in this paper.\n",
      "Entities Relations Triples Images\n",
      "|E| |R| Train Valid Test Train Valid Test\n",
      "ImageNet[Dengetal.,2009] 21,841 18 - 14,197,122\n",
      "VisualGenome[Krishnaetal.,2016] 75,729 40,480 1,531,448 108,077\n",
      "FB15k[Bordesetal.,2013] 14,951 1,345 483,142 50,000 59,071 0 0 0\n",
      "ImageGraph 14,870 1,330 460,406 47,533 56,071 411,306 201,832 216,793\n",
      "Japan Football Michael Jackson Madrid The Simpsons Drummer\n",
      "Figure 2: Image samples for some entities of ImageGraph.\n",
      "the fashion industry visual attributes are crucial for product recommendations [Liu et al.,\n",
      "2016, Veit et al., 2015]. Being able to ground novel visual concepts into an existing KG with\n",
      "attributes and various relation types is a reasonable approach to zero-shot learning.\n",
      "We make the following contributions. First, we introduce ImageGraph, a visual-\n",
      "relational web-extracted KG with 1,330 relations where 829,931 images are associated with\n",
      "14,870 different entities. Second, we introduce a new set of visual-relational query types.\n",
      "Third, we propose a novel set of neural architectures and objectives that we use for answering\n",
      "these novel query types. These query types generalize image retrieval and link prediction\n",
      "queries. This is the first time that deep CNNs and KG embedding learning objectives\n",
      "are combined into a joint model. Fourth, we show that the proposed class of deep neural\n",
      "networks are also successful for zero-shot learning, that is, creating relations between entirely\n",
      "unseen entities and the KG using only visual data at query time.\n",
      "2. Related Work\n",
      "We discuss the relation of our contributions to previous work with an emphasis on relational\n",
      "learning, image retrieval, object detection, scene understanding, existing data sets, and\n",
      "zero-shot learning.\n",
      "Relational Learning\n",
      "There has been a flurry of approaches tailored to specific problems such as link prediction\n",
      "in multi-relational graphs. Examples are knowledge base factorization and embedding\n",
      "approaches [Bordes et al., 2013, Nickel et al., 2011, Guu et al., 2015] and random-walk based\n",
      "ML models [Lao et al., 2011, Gardner and Mitchell, 2015]. More recently, the focus has been\n",
      "on integrating additional attribute types such as text [Yahya et al., 2016, C. et al., 2017],\n",
      "temporal graph dynamics [Trivedi et al., 2017], and multiple modalities [Pezeshkpour et al.,\n",
      "2018]. Another line of research is concerned with extensions of the link prediction problem to\n",
      "multi-hopreasoning[Zhangetal.,2018]. Wecannotlistallpriorlinkpredictionmethodshere\n",
      "3\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "103\n",
      "101\n",
      "100 101 102 103\n",
      "Relation types\n",
      "tnuoc\n",
      "elpirT\n",
      "1200\n",
      "104\n",
      "award_nomineperofession\n",
      "disea ps re oduction_company\n",
      "tnuoC\n",
      "ytitnE\n",
      "1 24680 00000 00000\n",
      "11 00 23\n",
      "tv_actor 0\n",
      "ingredient 101\n",
      "100\n",
      "101 103\n",
      "Entities\n",
      "tnuoc\n",
      "elpirT\n",
      "United States of America\n",
      "English Language\n",
      "Executive Producer\n",
      "University of Oxford\n",
      "Parlophone\n",
      "Vigor Shipyards\n",
      "Figure 3: (Left) The distribution of relation types; (center) the 10 most frequent entity\n",
      "types; and (right) the distribution of entities in ImageGraph.\n",
      "and instead refer the reader to two survey papers [Nickel et al., 2016a, Al Hasan and Zaki,\n",
      "2011]. Contrarytoexistingapproaches, weaddresstheproblemofansweringvisual-relational\n",
      "queries in knowledge graphs where the entities are associated with web-extracted images.\n",
      "We also address the zero-shot learning scenario, a problem that has not been addressed in\n",
      "the context of link prediction in multi-relational graphs.\n",
      "Image ranking\n",
      "Image retrieval is a popular problem and has been addressed by several authors [Wang et al.,\n",
      "2014, Yang et al., 2016, Jiang et al., 2017, Niu et al., 2018, Guy et al., 2018]. In [Yang\n",
      "et al., 2016] a re-ranking of the output of a given search engine by learning a click-based\n",
      "multi-feature similarity is proposed. The authors performed spectral clustering and obtained\n",
      "the final ranked results by computing click-based clusters. In [Guy et al., 2018] the authors\n",
      "fine-tune a DNN to rank photos a user might like to share in social media as well as a\n",
      "mechanism to detect duplicates. In [Niu et al., 2018] a joint user-image embedding is learned\n",
      "to generate a ranking based on user preferences. Contrary to these previous approaches we\n",
      "introduce a set of novel visual query types in a web-extracted KG with images and provide\n",
      "methods to answer these queries efficiently.\n",
      "Relational and Visual Data\n",
      "Previous work on combining relational and visual data has focused on object detection\n",
      "[Felzenszwalb et al., 2010, Girshick et al., 2014, Russakovsky et al., 2013, Marino et al.,\n",
      "2017, Li et al., 2017] and scene recognition [Doersch et al., 2013, Pandey and Lazebnik, 2011,\n",
      "Sadeghi and Tappen, 2012, Xiao et al., 2010, Teney et al., 2017] which are required for more\n",
      "complex visual-relational reasoning. Recent years have witnessed a surge in reasoning about\n",
      "human-object, object-object, and object-attribute relationships [Gupta et al., 2009, Farhadi\n",
      "et al., 2009, Malisiewicz and Efros, 2009, Yao and Fei-Fei, 2010, Felzenszwalb et al., 2010,\n",
      "Chen et al., 2013, Izadinia et al., 2014, Zhu et al., 2014]. The VisualGenome project [Krishna\n",
      "et al., 2016] is a knowledge base that integrates language and vision modalities. The project\n",
      "provides a knowledge graph, based on WordNet, which provides annotations of categories,\n",
      "attributes, and relation types for each image. Recent work has used the dataset to focus on\n",
      "scene understanding in single images. For instance, Lu et al. [Lu et al., 2016] proposed a\n",
      "model to detect relation types between objects depicted in an image by inferring sentences\n",
      "4\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "such as “man riding bicycle.\" Veit et al. [Veit et al., 2015] propose a siamese CNN to learn a\n",
      "metric representation on pairs of textile products so as to learn which products have similar\n",
      "styles. There is a large body of work on metric learning where the objective is to generate\n",
      "image embeddings such that a pairwise distance-based loss is minimized [Schroff et al., 2015,\n",
      "Bell and Bala, 2015, Oh Song et al., 2016, Sohn, 2016, Wang et al., 2017]. Recent work has\n",
      "extended this idea to directly optimize a clustering quality metric [Song et al., 2017]. In\n",
      "Vincent et al. [Vincent et al., 2017] they proposed a mutual embedding space for images and\n",
      "knowledge graphs so the relationships between an image and known entities in a knowledge\n",
      "graph are jointly encoded. Zhou et al. [Zhou and Lin, 2016] propose a method based on a\n",
      "bipartite graph that links depictions of meals to its ingredients. Johnson et al. [Johnson\n",
      "et al., 2015] propose to use the VisualGenome data to recover images from text queries. In\n",
      "the work of Thoma et al. [Thoma et al., 2017], they merge in a joint representation the\n",
      "embeddings from images, text, and KG and use the representation to perform link prediction\n",
      "on DBpedia [Lehmann et al., 2015]. ImageGraph is different from these data sets in that\n",
      "the relation types hold between different images and image annotated entities. This defines\n",
      "a novel class of problems where one seeks to answer queries such as “How are these two\n",
      "images related?\" With this work, we address problems ranging from predicting the relation\n",
      "types for image pairs to multi-relational image retrieval.\n",
      "Zero-shot Learning\n",
      "We focus on exploring ways in which KGs can be used to find relationships between visual\n",
      "data of unseen entities, that is, entities not part of the KG during training, and visual data\n",
      "of known KG entities. This is a form of zero-shot learning (ZSL) where the objective is to\n",
      "generalize to novel visual concepts. Generally, ZSL methods (e.g. [Romera-Paredes and\n",
      "Torr, 2015, Zhang and Saligrama, 2015]) rely on an underlying embedding space, such as\n",
      "one based on visual attributes, to recognize unseen categories. With this paper, we do not\n",
      "assume the availability of such a common embedding space but we assume the existence of\n",
      "an external visual-relational KG. Similar to our approach, when this explicit knowledge is\n",
      "not encoded in the underlying embedding space, other works rely on finding the similarities\n",
      "through linguistic patterns (e.g. [Ba et al., 2015, Lu et al., 2016]), leveraging distributional\n",
      "word representations so as to capture a notion of similarity. These approaches, however,\n",
      "address scene understanding in a single image, i.e. these models are able to detect the\n",
      "visual relationships in one given image. Our approach, on the other hand, finds relationships\n",
      "between different images and entities.\n",
      "3. ImageGraph: A Web-Extracted Visual Knowledge Graph\n",
      "ImageGraphisavisual-relationalKGwhoserelationalstructureisbasedonFreebase[Bol-\n",
      "lacker et al., 2008] and, more specifically, on FB15k, a subset of FreeBase and a popular\n",
      "benchmark data set [Nickel et al., 2016a]. Since FB15k does not include visual data, we\n",
      "perform the following steps to enrich the KG entities with image data. We implemented a\n",
      "web crawler that is able to parse query results for the image search engines Google Images,\n",
      "Bing Images, and Yahoo Image Search. To minimize the amount of noise due to polysemous\n",
      "entity labels (for example, there are more than 100 Freebase entities with the text label\n",
      "“Springfield\") we extracted, for each entity in FB15k, all Wikipedia URIs from the 1.9 billion\n",
      "5\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "Relationtype Example(h,r,t) Symmetric Others\n",
      "Symmetric (EmmaThompson,sibling,SophieThompson) 4%8%\n",
      "(SophieThompson,sibling,EmmaThompson)\n",
      "(Non-profitorganization,company_type,ApacheSoftwareFoundation)\n",
      "Asymmetric\n",
      "(Statistics,students_majoring,PhD)\n",
      "(StarWars,film_series,StarWars)\n",
      "88%\n",
      "Others (StarWarsEpisodeI:ThePhantomMenace,film_series,StarWars)\n",
      "(StarWarsEpisodeII:AttackoftheClones,film_series,StarWars) Asymmetric\n",
      "Figure 4: (Left) Example triples for symmetric, asymmetric and others relation types.\n",
      "(Right) Fraction of symmetric, asymmetric, and other relation types among all\n",
      "relation types in ImageGraph.\n",
      "triple Freebase RDF dump. For instance, for Springfield, Massachusetts, we obtained such\n",
      "URIs as Springfield_(Massachusetts,United_States) and Springfield_(MA). These\n",
      "URIs were processed and used as search queries for disambiguation purposes. We used\n",
      "the crawler to download more than 2.4M images (more than 462Gb of data). We removed\n",
      "corrupted, low quality, and duplicate images and we used the 25 top images returned by\n",
      "each of the image search engines whenever there were more than 25 results. The images\n",
      "were scaled to have a maximum height or width of 500 pixels while maintaining their\n",
      "aspect ratio. This resulted in 829,931 images associated with 14,870 different entities (55.8\n",
      "images per entity). After filtering out triples where either the head or tail entity could\n",
      "not be associated with an image, the visual KG consists of 564,010 triples expressing 1,330\n",
      "different relation types between 14,870 entities. We provide three sets of triples for training,\n",
      "validation, and testing plus three more image splits also for training, validation and test.\n",
      "Table 1 lists the statistics of the resulting visual KG. Any KG derived from FB15k such\n",
      "as FB15k-237[Toutanova and Chen, 2015] can also be associated with the crawled images.\n",
      "Since providing the images themselves would violate copyright law, we provide the code\n",
      "for the distributed crawler and the list of image URLs crawled for the experiments in this\n",
      "paper2.\n",
      "The distribution of relation types is depicted in Figure 3 (left). It plots for each relation\n",
      "type the number of triples it occurs in. Some relation types such as award_nominee or\n",
      "profession occur quite frequently while others such as ingredient have only few instances.\n",
      "4% of the relation types are symmetric, 88% are asymmetric, and 8% are others (see\n",
      "Table 4 (left)). Table 4 (right) lists specific instances of some relation types. There are 585\n",
      "distinct entity types such as Person,Athlete, and City. Figure 3 (center) shows the most\n",
      "frequent entity types. Figure 3 (right) visualizes the distribution of entities in the triples of\n",
      "ImageGraph and some example entities.\n",
      "Table 1 lists some statistics of the ImageGraph KG and other KGs from related work.\n",
      "First, we would like to emphasize the differences between ImageGraph and the Visual\n",
      "Genome project (VG) [Krishna et al., 2016]. With ImageGraph we address the problem\n",
      "of learning a representation for a KG with canonical relation types and not for relation\n",
      "types expressed through text. On a high level, we focus on answering visual-relational\n",
      "queries in a web-extracted KG. This is related to information retrieval except that in our\n",
      "proposed work, images are first-class citizens and we introduce novel and more complex\n",
      "2ImageGraph crawler and URLs: https://github.com/robegs/imageDownloader.\n",
      "6\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "query types. In contrast, VGD is focused on modeling relations between objects in images\n",
      "and the relation types are expressed in natural language. Additional differences between\n",
      "ImageGraph and ImageNet are the following. ImageNet is based on WordNet a lexical\n",
      "database where synonymous words from the same lexical category are grouped into synsets.\n",
      "There are 18 relations expressing connections between synsets. In Freebase, on the other\n",
      "hand, there are two orders of magnitudes more relations. In FB15k, the subset we focus on,\n",
      "there are 1,345 relations expressing location of places, positions of basketball players, and\n",
      "gender of entities. Moreover, entities in ImageNet exclusively represent entity types such\n",
      "as Cats and Cars whereas entities in FB15k are either entity types or instances of entity\n",
      "types such as Albert Einstein and Paris. This renders the computer vision problems\n",
      "associated with ImageGraph more challenging than those for existing datasets. Moreover,\n",
      "with ImageGraph the focus is on learning relational ML models that incorporate visual\n",
      "data both during learning and at query time.\n",
      "4. Representation Learning for Visual-Relational Graphs\n",
      "A knowledge graph (KG) K is given by a set of triples T, that is, statements of the form\n",
      "(h,r,t), where h,t ∈ E are the head and tail entities, respectively, and r ∈ R is a relation\n",
      "type. Figure 1a depicts a small fragment of a KG with relations between entities and images\n",
      "associated with the entities. Prior work has not included image data and has, therefore,\n",
      "focused on the following two types of queries. First, the query type (h,r?,t) asks for the\n",
      "relations between a given pair of head and tail entities. Second, the query types (h,r,t?)\n",
      "and (h?,r,t), asks for entities correctly completing the triple. The latter query type is often\n",
      "referred to as knowledge base completion. Here, we focus on queries that involve visual data\n",
      "as query objects, that is, objects that are either contained in the queries, the answers to the\n",
      "queries, or both.\n",
      "4.1 Visual-Relational Query Answering\n",
      "When entities are associated with image data, several completely novel query types are\n",
      "possible. Figure 1b lists the query types we focus on in this paper. We refer to images used\n",
      "during training as seen and all other images as unseen.\n",
      "(1) Given a pair of unseen images for which we do not know their KG entities, determine\n",
      "the unknown relations between the underlying entities.\n",
      "(2) Given an unseen image, for which we do not know the underlying KG entity, and a\n",
      "relation type, determine the seen images that complete the query.\n",
      "(3) Given an unseen image of an entirely new entity that is not part of the KG, and an\n",
      "unseen image for which we do not know the underlying KG entity, determine the\n",
      "unknown relations between the two underlying entities.\n",
      "(4) Given an unseen image of an entirely new entity that is not part of the KG, and a\n",
      "known KG entity, determine the unknown relations between the two entities.\n",
      "For each of these query types, the sought-after relations between the underlying entities\n",
      "have never been observed during training. Query types (3) and (4) are a form of zero-shot\n",
      "7\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "Sensō-ji Japan\n",
      "DistMult\n",
      "2\n",
      "VGG16 5\n",
      "6 r?\n",
      "g\n",
      "op VGG16 VGG16\n",
      "r?\n",
      "2\n",
      "VGG16 5 r\n",
      "6\n",
      "g\n",
      "f\n",
      "(a) (b)\n",
      "Figure 5: (a) the proposed architecture for query answering; and (b) an illustration of two\n",
      "possible approaches to visual-relational query answering. One can predict relation\n",
      "types between two images directly (green arrow; our approach) or combine an\n",
      "entity classifier with a KB embedding model for relation prediction (red arrows;\n",
      "baseline VGG16+DistMult).\n",
      "learning since neither the new entity’s relationships with other entities nor its images have\n",
      "been observed during training. These considerations illustrate the novel nature of the visual\n",
      "query types. The machine learning models have to be able to learn the relational semantics\n",
      "of the KG and not simply a classifier that assigns images to entities. These query types are\n",
      "also motivated by the fact that for typical KGs the number of entities is orders of magnitude\n",
      "greater than the number of relations.\n",
      "4.2 Deep Representation Learning for Visual-Relational Query Answering\n",
      "We first discuss KG completion methods and translate the concepts to query answering in\n",
      "visual-relational KGs. Let raw be the raw feature representation for entity i ∈ E and let\n",
      "i\n",
      "f and g be differentiable functions. Most KG completion methods learn an embedding of\n",
      "the entities in a vector space via some scoring function that is trained to assign high scores\n",
      "to correct triples and low scores to incorrect triples. Scoring functions have often the form\n",
      "f (e,e ) where r is a relation type, e and e are d-dimensional vectors (the embeddings of\n",
      "r h t h t\n",
      "the head and tail entities, respectively), and where e = g(raw ) is an embedding function\n",
      "i i\n",
      "that maps the raw input representation of entities to the embedding space. In the case of\n",
      "KGs without visual data, the raw representation of an entity is simply its one-hot encoding.\n",
      "(cid:124)\n",
      "Existing KG completion methods use the embedding function g(raw ) = raw W where\n",
      "i i\n",
      "W is a |E|×d matrix, and differ only in their scoring function, that is, in the way the\n",
      "embeddings of the head and tail entities are combined with the parameter vector φ :\n",
      "r\n",
      "• Difference (TransE[Bordes et al., 2013]): f (e,e ) = −||e +φ −e || where φ is\n",
      "r h t h r t 2 r\n",
      "a d-dimensional vector;\n",
      "• Multiplication (DistMult[Yang et al., 2014]): f (e,e ) = (e ∗e )·φ where ∗ is\n",
      "r h t h t r\n",
      "the element-wise product and φ a d-dimensional vector;\n",
      "r\n",
      "• Circular correlation (HolE[Nickel et al., 2016b]): f (e,e ) = (e?e )·φ where\n",
      "r h t h t r\n",
      "[a?b] = Pd−1a b and φ a d-dimensional vector; and\n",
      "k i=0 i (i+k) mod d r\n",
      "8\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "• Concatenation: f (e,e ) = (e (cid:12)e )·φ where (cid:12) is the concatenation operator and\n",
      "r h t h t r\n",
      "φ a 2d-dimensional vector.\n",
      "r\n",
      "For each of these instances, the matrix W (storing the entity embeddings) and the\n",
      "vectors φ are learned during training. In general, the parameters are trained such that\n",
      "r\n",
      "f (e,e ) is high for true triples and low for triples assumed not to hold in the KG. The\n",
      "r h t\n",
      "training objective is often based on the logistic loss, which has been shown to be superior\n",
      "for most of the composition functions [Trouillon et al., 2016],\n",
      "min X log(1+exp(−f (e,e ))+ X log(1+exp(f (e,e )))+λ||Θ||2, (1)\n",
      "r h t r h t 2\n",
      "Θ\n",
      "(h,r,t)∈Tpos (h,r,t)∈Tneg\n",
      "where T and T are the set of positive and negative training triples, respectively, Θ are\n",
      "pos neg\n",
      "the parameters trained during learning and λ is a regularization hyperparameter. For the\n",
      "above objective, a process for creating corrupted triples T is required. This often involves\n",
      "neg\n",
      "sampling a random entity for either the head or tail entity. To answer queries of the types\n",
      "(h,r,t?) and (h?,r,t) after training, we form all possible completions of the queries and\n",
      "compute a ranking based on the scores assigned by the trained model to these completions.\n",
      "For the queries of type (h,r?,t) one typically uses the softmax activation in conjunction\n",
      "with the categorical cross-entropy loss, which does not require negative triples\n",
      "!\n",
      "min X −log exp(f r(e h,e t)) +λ||Θ||2, (2)\n",
      "Θ\n",
      "(h,r,t)∈Tpos\n",
      "P r∈Rexp(f r(e h,e t)) 2\n",
      "where Θ are the parameters trained during learning.\n",
      "For visual-relational KGs, the input consists of raw image data instead of the one-hot\n",
      "encodings of entities. The approach we propose builds on the ideas and methods developed\n",
      "for KG completion. Instead of having a simple embedding function g that multiplies the\n",
      "input with a weight matrix, however, we use deep convolutional neural networks to extract\n",
      "meaningfulvisualfeaturesfromtheinputimages. Forthecompositionfunctionfweevaluate\n",
      "thefouroperationsthatwereusedintheKGcompletionliterature: difference, multiplication,\n",
      "concatenation, and circular correlation. Figure 5a depicts the basic architecture we trained\n",
      "forqueryanswering. Theweightsofthepartsoftheneuralnetworkresponsibleforembedding\n",
      "the raw image input, denoted by g, are tied. We also experimented with additional hidden\n",
      "layers indicated by the dashed dense layer. The composition operation op is either difference,\n",
      "multiplication, concatenation, or circular correlation. To the best of our knowledge, this\n",
      "is the first time that KG embedding learning and deep CNNs have been combined for\n",
      "visual-relationsl query answering.\n",
      "5. Experiments\n",
      "We conduct a series of experiments to evaluate the proposed approach. First, we describe\n",
      "the experimental set-up that applies to all experiments. Second, we report and interpret\n",
      "results for the different types of visual-relational queries.\n",
      "9\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "5.1 General Set-up\n",
      "We used Caffe, a deep learning framework [Jia et al., 2014] for designing, training, and\n",
      "evaluating the proposed models. The embedding function g is based on the VGG16 model in-\n",
      "troducedin[SimonyanandZisserman,2014]. Wepre-trainedtheVGG16ontheILSVRC2012\n",
      "data set derived from ImageNet [Deng et al., 2009] and removed the softmax layer of\n",
      "the original VGG16. We added a 256-dimensional layer after the last dense layer of the\n",
      "VGG16. The output of this layer serves as the embedding of the input images. The reason\n",
      "for reducing the embedding dimensionality from 4096 to 256 is motivated by the objective\n",
      "to obtain an efficient and compact latent representation that is feasible for KGs with billion\n",
      "of entities. For the composition function f, we performed either of the four operations\n",
      "difference, multiplication, concatenation, and circular correlation. We also experimented\n",
      "with an additional hidden layer with ReLu activation. Figure 5a depicts the generic network\n",
      "architecture. The output layer of the architecture has a softmax or sigmoid activation with\n",
      "cross-entropy loss. We initialized the weights of the newly added layers with the Xavier\n",
      "method [Glorot and Bengio, 2010].\n",
      "We used a batch size of 45 which was the maximal possible fitting into GPU memory.\n",
      "To create the training batches, we sample a random triple uniformly at random from the\n",
      "training triples. For the given triple, we randomly sample one image for the head and one\n",
      "for the tail from the set of training images. We applied SGD with a learning rate of 10−5\n",
      "for the parameters of the VGG16 and a learning rate of 10−3 for the remaining parameters.\n",
      "It is crucial to use two different learning rates since the large gradients in the newly added\n",
      "layers would lead to unreasonable changes in the pretrained part of the network. We set\n",
      "the weight decay to 5×10−4. We reduced the learning rate by a factor of 0.1 every 40,000\n",
      "iterations. Each of the models was trained for 100,000 iterations.\n",
      "Since the answers to all query types are either rankings of images or rankings of relations,\n",
      "we utilize metrics measuring the quality of rankings. In particular, we report results for\n",
      "hits@1 (hits@10, hits@100) measuring the percentage of times the correct relation was\n",
      "ranked highest (ranked in the top 10, top 100). We also compute the median of the ranks\n",
      "of the correct entities or relations and the Mean Reciprocal Rank (MRR) for entity and\n",
      "relation rankings, respectively, defined as follows:\n",
      "!\n",
      "1 X 1 1\n",
      "MRR = + (3)\n",
      "2|T| rank rank<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,    265,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Daniel Oñoro-Rubio', 'Mathias Niepert', 'Alberto García-Durán', 'Roberto González-Sánchez', 'Roberto J. López-Sastre']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  gradients in the newly added\n",
      "layers would lead to unreasonable changes in the pretrained part of the network. We set\n",
      "the weight decay to 5×10−4. We reduced the learning rate by a factor of 0.1 every 40,000\n",
      "iterations. Each of the models was trained for 100,000 iterations.\n",
      "Since the answers to all query types are either rankings of images or rankings of relations,\n",
      "we utilize metrics measuring the quality of rankings. In particular, we report results for\n",
      "hits@1 (hits@10, hits@100) measuring the percentage of times the correct relation was\n",
      "ranked highest (ranked in the top 10, top 100). We also compute the median of the ranks\n",
      "of the correct entities or relations and the Mean Reciprocal Rank (MRR) for entity and\n",
      "relation rankings, respectively, defined as follows:\n",
      "!\n",
      "1 X 1 1\n",
      "MRR = + (3)\n",
      "2|T| rank rank\n",
      "(h,r,t)∈T\n",
      "img(h) img(t)\n",
      "1 X 1\n",
      "MRR =, (4)\n",
      "|T| rank\n",
      "r\n",
      "(h,r,t)∈T\n",
      "where T is the set of all test triples, rank is the rank of the correct relation, and rank\n",
      "r img(h)\n",
      "is the rank of the highest ranked image of entity h. For each query, we remove all triples\n",
      "that are also correct answers to the query from the ranking. All experiments were run on\n",
      "commodity hardware with 128GB RAM, a single 2.8 GHz CPU, and a NVIDIA 1080 Ti.\n",
      "5.2 Visual Relation Prediction\n",
      "Given a pair of unseen images we want to determine the relations between their underlying\n",
      "unknown entities. This can be expressed with (img,r?,img ). Figure 1b illustrates this\n",
      "h t\n",
      "10\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "Table 2: Results for the relation prediction problem.\n",
      "Model Median Hits@1 Hits@10 MRR\n",
      "VGG16+DistMult 94 6.0 11.4 0.087\n",
      "Prob. Baseline 35 3.7 26.5 0.104\n",
      "DIFF 11 21.1 50.0 0.307\n",
      "MULT 8 15.5 54.3 0.282\n",
      "CAT 6 26.7 61.0 0.378\n",
      "DIFF+1HL 8 22.6 55.7 0.333\n",
      "MULT+1HL 9 14.8 53.4 0.273\n",
      "CAT+1HL 6 25.3 60.0 0.365\n",
      "query type which we refer to as visual relation prediction. We train the deep architectures\n",
      "using the training and validation triples and images, respectively. For each triple (h,r,t)\n",
      "in the training data set, we sample one training image uniformly at random for both the\n",
      "head and the tail entity. We use the architecture depicted in Figure 5a with the softmax\n",
      "activation and the categorical cross-entropy loss. For each test triple, we sample one image\n",
      "uniformly at random from the test images of the head and tail entity, respectively. We then\n",
      "use the pair of images to query the trained deep neural networks. To get a more robust\n",
      "statistical estimate of the evaluation measures, we repeat the above process three times per\n",
      "test triple. Again, none of the test triples and images are seen during training nor are any\n",
      "of the training images used during testing. Computing the answer to one query takes the\n",
      "model 20 ms.\n",
      "We compare the proposed architectures to two different baselines: one based on entity\n",
      "classificationfollowedbyaKBembeddingmethodforrelationprediction(VGG16+DistMult),\n",
      "and a probabilistic baseline (Prob. Baseline). The entity classification baseline consists of\n",
      "fine-tuning a pretrained VGG16 to classify images into the 14,870 entities of ImageGraph.\n",
      "To obtain the relation type ranking at test time, we predict the entities for the head and\n",
      "the tail using the VGG16 and then use the KB embedding method DistMult[Yang et al.,\n",
      "2014] to return a ranking of relation types for the given (head, tail) pair. DistMult is a KB\n",
      "embeddingmethodthatachievesstateoftheartresultsforKBcompletiononFB15k[Kadlec\n",
      "et al., 2017]. Therefore, for this experiment we just substitute the original output layer of\n",
      "the VGG16 pretrained on ImageNet with a new output layer suitable for our problem.\n",
      "To train, we join the train an validation splits, we set the learning rate to 10−5 for all the\n",
      "layers and we train following the same strategy that we use in all of our experiments. Once\n",
      "the system is trained, we test the model by classifying the entities of the images in the test\n",
      "set. To train DistMult, we sample 500 negatives triples for each positive triple and used an\n",
      "embedding size of 100. Figure 5b illustrates the VGG16+DistMult baseline and contrasts\n",
      "it with our proposed approach. The second baseline (probabilistic baseline) computes the\n",
      "probability of each relation type using the set of training and validation triples. The baseline\n",
      "ranks relation types based on these prior probabilities.\n",
      "Table 2 lists the results for the two baselines and the different proposed architectures.\n",
      "The probabilistic baseline outperforms the VGG16+DistMult baseline in 3 of the metrics.\n",
      "This is due to the highly skewed distribution of relation types in the training, validation, and\n",
      "11\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "...\n",
      "wonAward 3 genreOf 159407\n",
      "...\n",
      "directedBy 2 succeededBy 106817\n",
      "Figure 6: Example queries and qualitative results for the multi-relational image retrieval\n",
      "problem.\n",
      "Table 3: Results for multi-relational image retrieval.\n",
      "Median Hits@100 MRR\n",
      "Model Head Tail Head Tail Head Tail\n",
      "Baseline 6504 2789 11.9 18.4 0.065 0.115\n",
      "DIFF 1301 877 19.6 26.3 0.051 0.094\n",
      "MULT 1676 1136 16.8 22.9 0.040 0.080\n",
      "CAT 1022 727 21.4 27.5 0.050 0.087\n",
      "DIFF+1HL 1644 1141 15.9 21.9 0.045 0.085\n",
      "MULT+1HL 2004 1397 14.6 20.5 0.034 0.069\n",
      "CAT+1HL 1323 919 17.8 23.6 0.042 0.080\n",
      "CAT-SIG 814 540 23.2 30.1 0.049 0.082\n",
      "test triples. A small number of relation types makes up a large fraction of triples. Figure 3\n",
      "(left) and 3 (right) depicts the plots of the counts of relation types and entities. Moreover,\n",
      "despite DistMult achieving a hits@1 value of 0.46 for the relation prediction problem\n",
      "between entity pairs the baseline VGG16+DistMult performs poorly. This is due to the poor\n",
      "entity classification performance of the VGG (accurracy: 0.082, F1: 0.068). In the remainder\n",
      "of the experiments, therefore, we only compare to the probabilistic baseline. In the lower\n",
      "part of Table 2, we lists the results of the experiments. DIFF, MULT, and CAT stand\n",
      "for the different possible composition operations. We omitted the composition operation\n",
      "circular correlation since we were not able to make the corresponding model converge,\n",
      "despite trying several different optimizers and hyperparameter settings. The post-fix 1HL\n",
      "stands for architectures where we added an additional hidden layer with ReLu activation\n",
      "before the softmax. The concatenation operation clearly outperforms the multiplication and\n",
      "difference operations. This is contrary to findings in the KG completion literature where\n",
      "MULTandDIFFoutperformedtheconcatenationoperation. Themodelswiththeadditional\n",
      "hidden layer did not perform better than their shallower counterparts with the exception\n",
      "of the DIFF model. We hypothesize that this is due to difference being the only linear\n",
      "composition operation, benefiting from an additional non-linearity. Each of the proposed\n",
      "models outperforms the baselines.\n",
      "12\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "Median Hits@1 Hits@10 MRR\n",
      "hasCrewJob\n",
      "H T H T H T H T hasGenre\n",
      "Zero-ShotQuery(3) hasProfession\n",
      "Back to the Future Special EffectsSupervisor\n",
      "Base 34 31 1.9 2.3 18.2 28.7 0.074 0.089\n",
      "CAT 8 7 19.1 22.4 54.2 57.9 0.306 0.342 hasNutrient\n",
      "Zero-ShotQuery(4) filmHasLocation\n",
      "people.B.o.rnHere\n",
      "Base 9 5 13.0 22.6 52.3 64.8 0.251 0.359\n",
      "CAT 5 3 26.9 33.7 62.5 70.4 0.388 0.461 Library of Congress TaxonomyHasEntry Card Game\n",
      "Figure 7: (Left) Results for the zero-shot learning experiments. (Right) Example results for\n",
      "zero-shot learning. For each pair of images the top three relation types (as ranked\n",
      "by the CAT model) are listed. For the pair of images at the top, the first relation\n",
      "type is correct. For the pair of images at the bottom, the correct relation type\n",
      "TaxonomyHasEntry is not among the top three relation types.\n",
      "5.3 Multi-Relational Image Retrieval\n",
      "Given an unseen image, for which we do not know the underlying KG entity, and a relation\n",
      "type, we want to retrieve existing images that complete the query. If the image for the head\n",
      "entityisgiven,wereturnarankingofimagesforthetailentity; ifthetailentityimageisgiven\n",
      "we return a ranking of images for the head entity. This problem corresponds to query type\n",
      "(2) in Figure 1b. Note that this is equivalent to performing multi-relational metric learning\n",
      "which, to the best of our knowledge, has not been done before. We performed experiments\n",
      "with each of the three composition functions f and for two different activation/loss functions.\n",
      "First, we used the models trained with the softmax activation and the categorical cross-\n",
      "entropy loss to rank images. Second, we took the models trained with the softmax activation\n",
      "and substituted the softmax activation with a sigmoid activation and the corresponding\n",
      "binary cross-entropy loss. For each training triple (h,r,t) we then created two negative\n",
      "triples by sampling once the head and once the tail entity from the set of entities. The\n",
      "negative triples are then used in conjunction with the binary cross-entropy loss of equation 1\n",
      "to refine the pretrained weights. Directly training a model with the binary cross-entropy\n",
      "loss was not possible since the model did not converge properly. Pretraining with softmax\n",
      "and categorical cross-entropy loss was crucial to make the binary loss work.\n",
      "Duringtesting,weusedthetesttriplesandrankedtheimagesbasedontheprobabilitiesre-\n",
      "turnedbytherespectivemodels. Forinstance,giventhequery(img Senso-ji,locatedIn,img t?),\n",
      "we substituted img? with all training and validation images, one at a time, and ranked\n",
      "t\n",
      "the images according to the probabilities returned by the models. We use the rank of the\n",
      "highest ranked image belonging to the true entity (here: Japan) to compute the values for\n",
      "the evaluation measures. We repeat the same experiment three times (each time randomly\n",
      "sampling the images) and report average values. Again, we compare the results for the\n",
      "different architectures with a probabilistic baseline. For the baseline, however, we compute\n",
      "a distribution of head and tail entities for each of the relation types. For example, for the\n",
      "relation type locatedIn we compute two distributions, one for head and one for tail entities.\n",
      "We used the same measures as in the previous experiment to evaluate the returned image\n",
      "rankings.\n",
      "13\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "Table3liststheresultsoftheexperiments. Asforrelationprediction,thebestperforming\n",
      "modelsarebasedontheconcatenationoperation,followedbythedifferenceandmultiplication\n",
      "operations. Thearchitectureswithanadditionalhiddenlayerdonotimprovetheperformance.\n",
      "We also provide the results for the concatenation-based model with softmax activation where\n",
      "we refined the weights using a sigmoid activation and negative sampling as described before.\n",
      "This model is the best performing model. All neural network models are significantly better\n",
      "than the baseline with respect to the median and hits@100. However, the baseline has\n",
      "slightly superior results for the MRR. This is due to the skewed distribution of entities and\n",
      "relations in the KG (see Figure 3 (right) and Figure 3 (left)). This shows once more that\n",
      "the baseline is highly competitive for the given KG. Figure 6 visualizes the answers the\n",
      "CAT-SIG model provided for a set of four example queries. For the two queries on the left,\n",
      "the model performed well and ranked the correct entity in the top 3 (green frame). The\n",
      "examples on the right illustrate queries for which the model returned an inaccurate ranking.\n",
      "To perform query answering in a highly efficient manner, we precomputed and stored all\n",
      "image embeddings once, and only compute the scoring function (involving the composition\n",
      "operation and a dot product with φ ) at query time. Answering one multi-relational image\n",
      "r\n",
      "retrieval query (which would otherwise require 613,138 individual queries, one per possible\n",
      "image) took only 90 ms.\n",
      "5.4 Zero-Shot Visual Relation Prediction\n",
      "The last set of experiments addresses the problem of zero-shot learning. For both query\n",
      "types, we are given an new image of an entirely new entity that is not part of the KG.\n",
      "The first query type asks for relations between the given image and an unseen image for\n",
      "which we do not know the underlying KG entity. The second query type asks for the\n",
      "relations between the given image and an existing KG entity. We believe that creating\n",
      "multi-relational links to existing KG entities is a reasonable approach to zero-shot learning\n",
      "since the relations to existing visual concepts and their attributes provide a characterization\n",
      "of the new entity/category.\n",
      "For the zero-shot experiments, we generated a new set of training, validation, and test\n",
      "triples. We randomly sampled 500 entities that occur as head (tail) in the set of test triples.\n",
      "We then removed all training and validation triples whose head or tail is one of these 1000\n",
      "entities. Finally, we only kept those test triples with one of the 1000 entities either as head\n",
      "or tail but not both. For query type (4) where we know the target entity, we sample 10\n",
      "of its images and use the models 10 times to compute a probability. We use the average\n",
      "probabilities to rank the relations. For query type (3) we only use one image sampled\n",
      "randomly. As with previous experiments, we repeated procedure three times and averaged\n",
      "the results. For the baseline, we compute the probabilities of relation in the training and\n",
      "validation set (for query type (3)) and the probabilities of relations conditioned on the target\n",
      "entity (for query type (4)). Again, these are very competitive baselines due to the skewed\n",
      "distribution of relations and entities. Table 7 (left) lists the results of the experiments. The\n",
      "model based on the concatenation operation (CAT) outperforms the baseline and performs\n",
      "surprisingly well. The deep models are able to generalize to unseen images since their\n",
      "performance is comparable to the performance in the relation prediction task (query type\n",
      "(1)) where the entity was part of the KG during training (see Table 2). Figure 7 (right)\n",
      "14\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "RReellaattiioonn RRaannkk RReellaattiioonn RRaannkk\n",
      "00..221133 •••IIInnnssstttrrruuummmeeennnttt ppplllaaayyyeeeddd 00..440000 •••SSStttaaarrrrrriiinnnggg aaaccctttooorrr\n",
      "00..119911 •••TTTaaaccckkk cccooonnntttrrriiibbbuuutttiiiooonnn 00..112255 •••AAAwwwaaarrrddd nnnooommmiiinnnaaattteeeddd wwwooorrrkkk\n",
      "00..118855 •••MMMuuusssiiicccaaalll gggrrrooouuuppp mmmeeemmmbbbeeerrrsss •••CCChhhaaarrraaacccttteeerrr\n",
      "Michael\n",
      "Synthesizer Inception Imperioli\n",
      "(Known) (Unseen) (Known)\n",
      "e? e\n",
      "RReellaattiioonn RRaannkk\n",
      "00..669955 •••FFFiiilllmmm rrreeellleeeaaassseee rrreeegggiiiooonnn\n",
      "BAFTA\n",
      "•••FFFeeeaaatttuuurrreeeddd fffiiilllmmm lllooocccaaatttiiiooonnnsss Award e\n",
      "•••PPPlllaaaccceee ooofff bbbiiirrrttthhh\n",
      "(Known)\n",
      "Tokio\n",
      "e\n",
      "(Known) RReellaattiioonn RRaannkk\n",
      "00..559933 •••AAAwwwaaarrrddd nnnooommmiiinnnaaattteeeddd wwwooorrrkkk\n",
      "00..117722 •••AAAwwwaaarrrddd wwwooonnn\n",
      "•••AAAwwwaaarrrddd nnnooommmiiinnneeeeee\n",
      "Figure 8: Qualitative example of the zero-shot learning problem. The plot shows the most\n",
      "probable relations that link a sample from an unknown entity (green) with samples\n",
      "of known entities (blue) of the KG.\n",
      "depicts example queries for the zero-shot query type (3). For the first query example, the\n",
      "CAT model ranked the correct relation type first (indicated by the green bounding box).\n",
      "The second example is more challenging and the correct relation type was not part of the top\n",
      "10 ranked relation types. Figure 5.4 shows one concrete example of the zero-shot learning\n",
      "problem. In green, visual data from an unknown entity is linked with visual data from\n",
      "KG entities (blue) by ranking the most probable relation types. This problem cannot be\n",
      "addressed with standard relation prediction methods since entities need to be part of the\n",
      "KG during training for these models to work.\n",
      "6. Conclusion\n",
      "KGs are at the core of numerous AI applications. Research has focused either on link\n",
      "prediction working only on the relational structure or on scene understanding in a single\n",
      "image. We present a novel visual-relational KG where the entities are enriched with visual\n",
      "data. We proposed several novel query types and introduce neural architectures suitable for\n",
      "probabilistic query answering. We propose a novel approach to zero-shot learning as the\n",
      "problem of visually mapping an image of an entirely new entity to a KG.\n",
      "References\n",
      "Sungjin Ahn, Heeyoul Choi, Tanel Parnamaa, and Yoshua Bengio. A neural knowledge\n",
      "language model. arXiv preprint arXiv:1608.00318, 2016.\n",
      "15\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "Mohammad Al Hasan and Mohammed J Zaki. A survey of link prediction in social networks.\n",
      "In Social network data analytics, pages 243–275. Springer, 2011.\n",
      "Michael Ashburner, Catherine A. Ball, Judith A. Blake, David Botstein, Heather Butler,\n",
      "J. Michael Cherry, Allan P. Davis, Kara Dolinski, Selina S. Dwight, Janan T. Eppig,\n",
      "Midori A. Harris, David P. Hill, Laurie Issel-Tarver, Andrew Kasarskis, Suzanna Lewis,\n",
      "John C. Matese, Joel E. Richardson, Martin Ringwald, Gerald M. Rubin, and Gavin\n",
      "Sherlock. Gene Ontology: tool for the unification of biology. Nat Genet, 25(1):25–29,\n",
      "2000.\n",
      "J. Ba, K. Swersky, S. Fidler, and R. Salakhutdinov. Predicting deep zero-shot convolutional\n",
      "neural networks using textual descriptions. In CVPR, 2015.\n",
      "Sean Bell and Kavita Bala. Learning visual similarity for product design with convolutional\n",
      "neural networks. ACM Transactions on Graphics (TOG), 34(4):98, 2015.\n",
      "Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: A\n",
      "collaboratively created graph database for structuring human knowledge. In SIGMOD,\n",
      "pages 1247–1250, 2008.\n",
      "Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana\n",
      "Yakhnenko. Translating embeddings for modeling multi-relational data. In Advances in\n",
      "Neural Information Processing Systems, pages 2787–2795, 2013.\n",
      "Li C., Lai Y., Goldwasser D., and Neville J. Joint embedding models for textual and social\n",
      "analysis. In ICML Workshop, 2017.\n",
      "Xinlei Chen, Abhinav Shrivastava, and Abhinav Gupta. Neil: Extracting visual knowledge\n",
      "from web data. In Proceedings of the IEEE International Conference on Computer Vision,\n",
      "pages 1409–1416, 2013.\n",
      "Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose M. F. Moura,\n",
      "Devi Parikh, and Dhruv Batra. Visual dialog. In CVPR, July 2017.\n",
      "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale\n",
      "Hierarchical Image Database. In CVPR, 2009.\n",
      "Carl Doersch, Abhinav Gupta, and Alexei A Efros. Mid-level visual element discovery as\n",
      "discriminative mode seeking. In Advances in Neural Information Processing Systems 26,\n",
      "pages 494–502. 2013.\n",
      "Ali Farhadi, Ian Endres, Derek Hoiem, and David A. Forsyth. Describing objects by their\n",
      "attributes. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern\n",
      "Recognition, pages 1778–1785, 2009.\n",
      "Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object\n",
      "detection with discriminatively trained part-based models. IEEE transactions on pattern\n",
      "analysis and machine intelligence, 32(9):1627–1645, 2010.\n",
      "16\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "Matt Gardner and Tom M Mitchell. Efficient and expressive knowledge base completion\n",
      "using subgraph feature extraction. In EMNLP, pages 1488–1498, 2015.\n",
      "Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies\n",
      "for accurate object detection and semantic segmentation. In Proceedings of CVPR, pages\n",
      "580–587, 2014.\n",
      "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward\n",
      "neural networks. In AISTATS, 2010.\n",
      "Abhinav Gupta, Aniruddha Kembhavi, and Larry S. Davis. Observing human-object\n",
      "interactions: Usingspatialandfunctionalcompatibilityforrecognition. IEEE Transactions\n",
      "on Pattern Analysis and Machine Intelligence, 31:1775–1789, 2009.\n",
      "Kelvin Guu, John Miller, and Percy Liang. Traversing knowledge graphs in vector space.\n",
      "arXiv preprint arXiv:1506.01094, 2015.\n",
      "Ido Guy, Alexander Nus, Dan Pelleg, and Idan Szpektor. Care to share?: Learning to rank\n",
      "personal photos for public sharing. In WSDM, pages 207–215. ACM, 2018.\n",
      "Hamid Izadinia, Fereshteh Sadeghi, and Ali Farhadi. Incorporating scene context and object\n",
      "layout into appearance modeling. In Proceedings of the IEEE Conference on Computer\n",
      "Vision and Pattern Recognition, pages 232–239, 2014.\n",
      "YangqingJia,EvanShelhamer,JeffDonahue,SergeyKarayev,JonathanLong,RossGirshick,\n",
      "Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature\n",
      "embedding. arXiv preprint arXiv:1408.5093, 2014.\n",
      "Lu Jiang, Yannis Kalantidis, Liangliang Cao, Sachin Farfade, Jiliang Tang, and Alexander G.\n",
      "Hauptmann. Delving deep into personal photo and video search. In Proceedings of the\n",
      "Tenth ACM International Conference on Web Search and Data Mining, WSDM, 2017.\n",
      "J. Johnson, R. Krishna, M. Stark, L.-J. Li, D. A. Shamma, M. Bernstein, and L. Fei-Fei.\n",
      "Image retrieval using scene graphs. In CVPR, 2015.\n",
      "Rudolf Kadlec, Ondrej Bajgar, and Jan Kleindienst. Knowledge base completion: Baselines\n",
      "strike back. arXiv preprint arXiv:1705.10744, 2017.\n",
      "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz,\n",
      "Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, and\n",
      "Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense\n",
      "image annotations. In arXiv preprint arXiv:1602.07332, 2016.\n",
      "Ni Lao, Tom Mitchell, and William W Cohen. Random walk inference and learning in a\n",
      "large scale knowledge base. In Proceedings of the Conference on Empirical Methods in\n",
      "Natural Language Processing, pages 529–539. Association for Computational Linguistics,\n",
      "2011.\n",
      "17\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N.\n",
      "Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick van Kleef, Sören Auer, and\n",
      "Christian Bizer. DBpedia - a large-scale, multilingual knowledge base extracted from\n",
      "wikipedia. Semantic Web Journal, (2):167–195, 2015.\n",
      "Yining Li, Chen Huang, Xiaoou Tang, and Chen Change Loy. Learning to disambiguate by\n",
      "asking discriminative questions. In ICCV, 2017.\n",
      "Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering\n",
      "robust clothes recognition and retrieval with rich annotations. In CVPR, June 2016.\n",
      "C. Lu, R. Krishna, M. Bernstein, and L. Fei-Fei. Visual relationship detection with language\n",
      "priors. In ECCV, 2016.\n",
      "Tomasz Malisiewicz and Alexei A. Efros. Beyond categories: The visual memex model\n",
      "for reasoning about object relationships. In Advances in Neural Information Processing\n",
      "Systems, 2009.\n",
      "Kenneth Marino, Ruslan Salakhutdinov, and Abhinav Gupta. The more you know: Using\n",
      "knowledge graphs for image classification. In CVPR, 2017.\n",
      "Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective\n",
      "learning on multi-relational data. In Proceedings of the 28th international conference on\n",
      "machine learning (ICML-11), pages 809–816, 2011.\n",
      "Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of\n",
      "relational machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):11–33,\n",
      "2016a.\n",
      "Maximilian Nickel, Lorenzo Rosasco, and Tomaso A. Poggio. Holographic embeddings of\n",
      "knowledge graphs. In Proceedings of the Thirtieth Conference on Artificial Intelligence,\n",
      "pages 1955–1961, 2016b.\n",
      "Wei Niu, James Caverlee, and Haokai Lu. Neural personalized ranking for image recommen-\n",
      "dation. In Proceedings of the Eleventh ACM International Conference on Web Search and\n",
      "Data Mining, WSDM, 2018.\n",
      "Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via\n",
      "lifted structured feature embedding. In Proceedings of the IEEE Conference on Computer\n",
      "Vision and Pattern Recognition, pages 4004–4012, 2016.\n",
      "Megha Pandey and Svetlana Lazebnik. Scene recognition and weakly supervised object\n",
      "localization with deformable part-based models. In Computer Vision (ICCV), 2011 IEEE\n",
      "International Conference on, pages 1307–1314, 2011.\n",
      "Pouya Pezeshkpour, Liyan Chen, and Sameer Singh. Embedding multimodal relational data\n",
      "for knowledge base completion. In EMNLP, 2018.\n",
      "B. Romera-Paredes and P. Torr. An embarrassingly simple approach to zero-shot learning.\n",
      "In ICML, 2015.\n",
      "18\n",
      "Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs\n",
      "Olga Russakovsky, Jia Deng, Zhiheng Huang, Alexander C. Berg, and Li Fei-Fei. Detecting\n",
      "avocados to zucchinis: what have we done, and where are we going? In International\n",
      "Conference on Computer Vision (ICCV), 2013.\n",
      "Fereshteh Sadeghi and Marshall F. Tappen. Latent pyramidal regions for recognizing scenes.\n",
      "In Proceedings of the 12th European Conference on Computer Vision - Volume Part V,\n",
      "pages 228–241, 2012.\n",
      "F.Schroff,D.Kalenichenko,andJ.Philbin. Facenet: Aunifiedembeddingforfacerecognition\n",
      "and clustering. In 2015 IEEE Conference on Computer Vision and Pattern Recognition\n",
      "(CVPR), pages 815–823, 2015.\n",
      "Iulian Vlad Serban, Alberto García-Durán, Caglar Gulcehre, Sungjin Ahn, Sarath Chandar,\n",
      "Aaron Courville, and Yoshua Bengio. Generating factoid questions with recurrent neural\n",
      "networks: The 30m factoid question-answer corpus. arXiv preprint arXiv:1603.06807,\n",
      "2016.\n",
      "K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image\n",
      "recognition. CoRR, 2014.\n",
      "Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In\n",
      "Advances in Neural Information Processing Systems, pages 1857–1865, 2016.\n",
      "Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, and Kevin Murphy. Deep metric learning\n",
      "via facility location. In Conference on Computer Vision and Pattern Recognition (CVPR),\n",
      "2017.\n",
      "Damien Teney, Lingqiao Liu, and Anton van den Hengel. Graph-structured representations\n",
      "for visual question answering. In CVPR, July 2017.\n",
      "Steffen Thoma, Achim Rettinger, and Fabian Both. Towards holistic concept representations:\n",
      "Embedding relational knowledge, visual attributes, and distributional word semantics. In\n",
      "International Semantic Web Conference (1), volume 10587 of Lecture Notes in Computer\n",
      "Science, pages 694–710. Springer, 2017.\n",
      "Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and\n",
      "text inference. In Proceedings of the 3rd Workshop on Continuous Vector Space Models\n",
      "and their Compositionality, pages 57–66, 2015.\n",
      "Rakshit Trivedi, Hanjun Dai, Yichen Wang, and Le Song. Know-evolve: Deep temporal\n",
      "reasoning for dynamic knowledge graphs. In ICML, 2017.\n",
      "Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard.\n",
      "Complex embeddings for simple link prediction. arXiv preprint arXiv:1606.06357, 2016.\n",
      "Andreas Veit, Balazs Kovacs, Sean Bell, Julian McAuley, Kavita Bala, and Serge Belongie.\n",
      "Learning visual clothing style with heterogeneous dyadic co-occurrences. In ICCV, 2015.\n",
      "Lonij Vincent, Rawat Ambrish, and Nicolae Maria-Irina. Extending knowledge bases using\n",
      "images. In AKBC, 2017.\n",
      "19<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,    265,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Oñoro-Rubio', 'Niepert', 'García-Durán', 'González-Sánchez', 'López-Sastre']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  on Continuous Vector Space Models\n",
      "and their Compositionality, pages 57–66, 2015.\n",
      "Rakshit Trivedi, Hanjun Dai, Yichen Wang, and Le Song. Know-evolve: Deep temporal\n",
      "reasoning for dynamic knowledge graphs. In ICML, 2017.\n",
      "Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard.\n",
      "Complex embeddings for simple link prediction. arXiv preprint arXiv:1606.06357, 2016.\n",
      "Andreas Veit, Balazs Kovacs, Sean Bell, Julian McAuley, Kavita Bala, and Serge Belongie.\n",
      "Learning visual clothing style with heterogeneous dyadic co-occurrences. In ICCV, 2015.\n",
      "Lonij Vincent, Rawat Ambrish, and Nicolae Maria-Irina. Extending knowledge bases using\n",
      "images. In AKBC, 2017.\n",
      "19\n",
      "Oñoro-Rubio, Niepert, García-Durán, González-Sánchez, & López-Sastre\n",
      "Jian Wang, Feng Zhou, Shilei Wen, Xiao Liu, and Yuanqing Lin. Deep metric learning with\n",
      "angular loss. In International Conference on Computer Vision (ICCV), 2017.\n",
      "X. Wang, S. Qiu, K. Liu, and X. Tang. Web image re-ranking usingquery-specific semantic\n",
      "signatures. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(4):\n",
      "810–823, April 2014.\n",
      "David S. Wishart, Craig Knox, Anchi Guo, Dean Cheng, Savita Shrivastava, Dan Tzur,\n",
      "Bijaya Gautam, and Murtaza Hassanali. Drugbank: a knowledgebase for drugs, drug\n",
      "actions and drug targets. Nucleic Acids Research, 36:901–906, 2008.\n",
      "Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. SUN\n",
      "database: Large-scale scene recognition from abbey to zoo. In The Twenty-Third IEEE\n",
      "Conference on Computer Vision and Pattern Recognition, pages 3485–3492, 2010.\n",
      "Mohamed Yahya, Denilson Barbosa, Klaus Berberich, Qiuyue Wang, and Gerhard Weikum.\n",
      "Relationship queries on extended knowledge graphs. In WSDM, 2016.\n",
      "Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Learning multi-\n",
      "relational semantics using neural-embedding models. arXiv preprint arXiv:1411.4072,\n",
      "2014.\n",
      "X. Yang, T. Mei, Y. Zhang, J. Liu, and S. Satoh. Web image search re-ranking with\n",
      "click-based similarity and typicality. IEEE Transactions on Image Processing, 25(10):\n",
      "4617–4630, 2016.\n",
      "Bangpeng Yao and Li Fei-Fei. Modeling mutual context of object and human pose in human-\n",
      "object interaction activities. In Computer Vision and Pattern Recognition (CVPR), 2010\n",
      "IEEE Conference on, pages 17–24, 2010.\n",
      "Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J. Smola, and Le Song. Variational\n",
      "reasoning for question answering with knowledge graph. In AAAI, 2018.\n",
      "Z. Zhang and V. Saligrama. Zero-shot learning via semantic similarity embedding. In ICCV,\n",
      "2015.\n",
      "Feng Zhou and Yuanqing Lin. Fine-grained image classification by exploring bipartite-graph\n",
      "labels. In CVPR, June 2016.\n",
      "Yuke Zhu, Alireza Fathi, and Li Fei-Fei. Reasoning about object affordances in a knowledge\n",
      "base representation. In European conference on computer vision, pages 408–424, 2014.\n",
      "20<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "            220,    389,  70067,   4290,  11746,  27972,    198,    438,    872,\n",
      "          68206,   2786,     11,   6959,    220,   3226,   4235,   2287,     11,\n",
      "            220,    679,     20,    627,     49,    587,  41153,   1183,   2270,\n",
      "             72,     11,  21296,  58781,  80223,     11,    816,  29424,  29346,\n",
      "             11,    323,   2009,  19508,     13,  14521,  91345,   4035,     25,\n",
      "          18682,  37015,    198,  20489,    287,    369,   8915,   6677,  40099,\n",
      "             13,    763,  19845,   2735,     11,    220,    679,     22,    627,\n",
      "           1016,  89577,  42782,  43588,     11,  55205,  26056,   2067,     11,\n",
      "          44609,    432,   1142,    301,     11,  29124,   2265,  94316,   1291,\n",
      "             11,    323,   4673,  99112,    426,   3102,    569,    627,  32237,\n",
      "          71647,    369,   4382,   2723,  20212,     13,    802,     55,    344,\n",
      "            864,   1374,    802,     55,    344,     25,   6330,     21,     13,\n",
      "          24254,   3226,     11,    220,    679,     21,    627,   3112,  51518,\n",
      "          23404,    275,     11,  19984,   1394,     82,  93981,  19807,     11,\n",
      "          26044,  18366,     11,  38897,  65813,   1130,     88,     11,    735,\n",
      "            402,   6388,    426,   6181,     11,    323,  33022,   7984,    647,\n",
      "            648,    627,  48567,   9302,  17895,   1742,    449,  98882,  14282,\n",
      "          37314,   1080,     12,  14310,  34346,     13,    763,  59332,     53,\n",
      "             11,    220,    679,     20,    627,  54324,   3251,  35407,     11,\n",
      "          23886,    266,   3383,   1347,    819,     11,    323,  83440,   6043,\n",
      "          23880,  22197,  41071,     13,   9634,   2518,   6677,  23963,   1701,\n",
      "            198,   3726,     13,    763,  31672,   5002,     11,    220,    679,\n",
      "             22,    627,    777,    198,     46,   5771,  18812,  11151,    392,\n",
      "            822,     11,  53383,  77468,     11,  85341,   9607,    324,  11644,\n",
      "             11,  33555,  97465,   6354,  99634,     11,    612,  92075,   6354,\n",
      "            561,    265,    198,     41,   1122,  29346,     11,  43758,  67927,\n",
      "             11,   1443,    458,     72,  62323,     11,  66690,  38805,     11,\n",
      "            323,  69984,  90684,   8732,     13,  18682,  18767,   6975,    449,\n",
      "            198,   4328,   4814,     13,    763,   7327,  15217,    389,  17863,\n",
      "          31541,    320,   1341,  20161,    705,    220,    679,     22,    627,\n",
      "             55,     13,  29346,     11,    328,     13,   1229,  19260,     11,\n",
      "            735,     13,  38805,     11,    323,   1630,     13,  41462,     13,\n",
      "           5000,   2217,    312,  72539,   1701,   1663,  19440,  42833,    198,\n",
      "           7908,   2859,     13,  40135,  56385,    389,  19365,  18825,    323,\n",
      "          13257,  22107,     11,    220,   1927,      7,     19,    997,  19232,\n",
      "           4235,  23848,     11,   5936,    220,    679,     19,    627,  23083,\n",
      "            328,     13,  38747,    472,     11,  29517,  54450,     11,   1556,\n",
      "          14946,   4673,     78,     11,  25028,  57807,     11,  20680,   6388,\n",
      "           1443,  77467,    561,   2979,     11,  11824,    350,     89,    324,\n",
      "            345,     33,   3251,  12874,  96154,    309,     11,    323,    386,\n",
      "           5757,  12997,  60777,   8115,     13,  26166,  17469,     25,    264,\n",
      "           6677,   3231,    369,  11217,     11,   5623,    198,   4109,    323,\n",
      "           5623,  11811,     13,    452,  22935,    292,   6515,   3447,   8483,\n",
      "             11,    220,   1927,     25,  19319,   4235,  22224,     11,    220,\n",
      "           1049,     23,    627,     41,   1122,     87,    290,     70,  66690,\n",
      "             11,   7957,    473,    954,     11,  27973,     64,    362,     13,\n",
      "          61651,   5248,     11,    362,    799,  12225,  10126,     11,    323,\n",
      "          23245,   8611,   3545,   4749,     13,  57328,    198,  12494,     25,\n",
      "          20902,  13230,   6237,  18324,    505,  30195,   1216,    311,  42014,\n",
      "             13,    763,    578,  44956,     12,  38075,  40135,    198,  92348,\n",
      "            389,  17863,  31541,    323,  19365,  48698,     11,   6959,    220,\n",
      "          19746,     20,   4235,  18634,     17,     11,    220,    679,     15,\n",
      "            627,  83705,   3690,  83562,   7911,     11,   9973,    321,    942,\n",
      "          47142,  12252,     11,  82197,   9084,    655,    718,     11,  58094,\n",
      "           4168,    361,  29346,     11,    323,  20524,  19221,   1226,   1609,\n",
      "            372,    627,  51922,  20126,    389,  11838,   6677,  40099,     13,\n",
      "            763,    468,   5608,     44,     11,    220,    679,     21,    627,\n",
      "             33,    819,    276,  25482,     11,  62323,   2442,   2933,    816,\n",
      "           7141,     11,  41235,    347,    647,   1283,     11,  88404,     69,\n",
      "            833,    480,   3524,     11,    323,  14851,  92829,     13,  21579,\n",
      "           7447,   7058,   3833,   1697,  53794,   1701,  30828,     12,  95711,\n",
      "           4211,     13,    802,     55,    344,    864,   1374,    802,     55,\n",
      "            344,     25,   9335,     16,     13,  18501,     17,    345,    679,\n",
      "             19,    627,     55,     13,  25482,     11,    350,     13,  92033,\n",
      "             11,    816,     13,  37120,     11,    622,     13,  38805,     11,\n",
      "            323,    328,     13,  13479,   2319,     13,   5000,   2217,   2778,\n",
      "            312,  72539,    449,    198,   3763,   6108,  38723,    323,  14595,\n",
      "            488,     13,  40135,  56385,    389,   4758,  29225,     11,    220,\n",
      "            914,      7,    605,    997,  19608,     22,   4235,  21290,     15,\n",
      "             11,    220,    679,     21,    627,  63919,  64183,  91030,    323,\n",
      "          14851,   3926,     72,     12,   6251,     72,     13,  77349,  27848,\n",
      "           2317,    315,   1665,    323,   3823,  17477,    304,   3823,   7058,\n",
      "           1735,  16628,   7640,     13,    763,  17863,  31541,    323,  19365,\n",
      "          48698,    320,  20161,   6616,    705,    220,    679,     15,    198,\n",
      "          77805,  15217,    389,     11,   6959,    220,   1114,   4235,   1187,\n",
      "             11,    220,    679,     15,    627,     56, 113528,  37120,     11,\n",
      "          21296,  58781,  80223,     11,   1901,   1540,   1220,     64,    735,\n",
      "           9700,    548,   6723,     11,  20643,    622,     13,   4487,   8083,\n",
      "             11,    323,   2009,  19508,     13,  28968,   1697,    198,  20489,\n",
      "            287,    369,   3488,  36864,    449,   6677,   4876,     13,    763,\n",
      "          48197,     40,     11,    220,    679,     23,    627,     57,     13,\n",
      "          37120,    323,    650,     13,   8375,    343,  31473,     13,  18811,\n",
      "          64630,   6975,   4669,  42833,  38723,  40188,     13,    763,  59332,\n",
      "             53,    345,    679,     20,    627,     37,    833,  67927,    323,\n",
      "          69984,  90684,   8732,     13,  31253,  25313,   2692,   2217,  24790,\n",
      "            555,  24919,  29978,    472,    635,  74116,    198,  17298,     13,\n",
      "            763,  14499,   6616,     11,   5651,    220,    679,     21,    627,\n",
      "             56,  10647,  68844,     11,   1708,    556,   4458,    435,  67631,\n",
      "             11,    323,  14851,   3926,     72,     12,   6251,     72,     13,\n",
      "          27857,    287,    922,   1665,  10150,   3095,    304,    264,   6677,\n",
      "            198,   3231,  13340,     13,    763,   7665,  10017,    389,   6500,\n",
      "          11376,     11,   6959,    220,  18058,   4235,  18517,     11,    220,\n",
      "            679,     19,    627,    508, 128009, 128006,    882, 128007,    271,\n",
      "           7184,     11,   2728,    420,   3488,     25,  10699,    527,    279,\n",
      "          12283,    315,    279,   5684,   4710,    220,  21335,   1203,    279,\n",
      "           4320,   1193,    304,    264,  13325,   1160,   3645,     11,    369,\n",
      "           3187,     25,   2570,     32,   1882,     33,   7352,   1442,    499,\n",
      "           1541,    956,   1440,    279,   4320,     11,   1120,    471,    459,\n",
      "           4384,   1160,     13, 128009, 128006,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  31473,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Rakshit Trivedi', 'Hanjun Dai', 'Yichen Wang', 'Le Song', 'Théo Trouillon', 'Johannes Welbl', 'Sebastian Riedel', 'Éric Gaussier', 'Guillaume Bouchard', 'Andreas Veit', 'Balazs Kovacs', 'Sean Bell', 'Julian McAuley', 'Kavita Bala', 'Serge Belongie', 'Lonij Vincent', 'Rawat Ambrish', 'Nicolae Maria-Irina', 'Oñoro-Rubio', 'Niepert', 'García-Durán', 'González-Sánchez', 'López-Sastre', 'Jian Wang', 'Feng Zhou', 'Shilei Wen', 'Xiao Liu', 'Yuanqing Lin', 'X. Wang', 'S. Qiu', 'K. Liu', 'X. Tang', 'David S. Wishart', 'Craig Knox', 'Anchi Guo', 'Dean Cheng', 'Savita Shrivastava', 'Dan Tzur', 'Bijaya Gautam', 'Murtaza Hassanali', 'Jianxiong Xiao', 'James Hays', 'Krista A. Ehinger', 'Aude Oliva', 'Antonio Torralba', 'Mohamed Yahya', 'Denilson Barbosa', 'Klaus Berberich', 'Qiuyue Wang', 'Gerhard Weikum', 'Bishan Yang', 'Wen-tau Yih', 'Xiaodong He', 'Jianfeng Gao', 'Li Deng', 'X. Yang', 'T. Mei', 'Y. Zhang', 'J. Liu', 'S. Satoh', 'Bangpeng Yao', 'Li Fei-Fei', 'Yuyu Zhang', 'Zornitsa Kozareva', 'Alexander J. Smola', 'Feng Zhou', 'Yuanqing Lin', 'Yuke Zhu', 'Alireza Fathi', 'Z. Zhang', 'V. Saligrama']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Expeditious Generation of Knowledge Graph\n",
      "Embeddings\n",
      "TommasoSoru,StefanoRuberto,DiegoMoussallem,AndréValdestilhas,\n",
      "AlexanderBigerl,EdgardMarxandDiegoEsteves\n",
      "Thisworkwasacceptedforpresentationatthe5thEuropeanConference\n",
      "on Data Analysis (ECDA 2018) under the title “A Simple and Fast Ap-\n",
      "proachtoKnowledgeGraphEmbedding”.\n",
      "Abstract Knowledge Graph Embedding methods aim at representing enti-\n",
      "ties and relations in a knowledge base as points or vectors in a continuous\n",
      "vector space. Several approaches using embeddings have shown promising\n",
      "results on tasks such as link prediction, entity recommendation, question an-\n",
      "swering,andtripletclassification.However,onlyafewmethodscancompute\n",
      "low-dimensionalembeddingsofverylargeknowledgebaseswithoutneeding\n",
      "TommasoSoru,DiegoMoussallem,AndréValdestilhas,EdgardMarx\n",
      "AKSW,UniversityofLeipzig,Germany\n",
      "(cid:0)tsoru,moussallem,valdestilhas,marx@informatik.uni-leipzig.de\n",
      "StefanoRuberto\n",
      "UniversityofPennsylvania,UnitedStates\n",
      "(cid:0)stefano.ruberto@pennmedicine.upenn.edu\n",
      "AlexanderBigerl\n",
      "DICE,PaderbornUniversity,Germany\n",
      "(cid:0)alexander.bigerl@uni-paderborn.de\n",
      "DiegoEsteves\n",
      "SDA,UniversityofBonn,Germany\n",
      "(cid:0)esteves@cs.uni-bonn.de\n",
      "ARCHIVES OF DATA SCIENCE, SERIES A\n",
      "(ONLINE FIRST) DOI10.5445/KSP/XXXXXXXX/XX\n",
      "KIT SCIENTIFIC PUBLISHING ISSN2363-9881\n",
      "Vol.-,No.-,-\n",
      "8102\n",
      "voN\n",
      "9\n",
      "]LC.sc[\n",
      "2v82870.3081:viXra\n",
      "2 TommasoSoruetal.\n",
      "state-of-the-artcomputationalresources.Inthispaper,weproposeKG2Vec,a\n",
      "simpleandfastapproachtoKnowledgeGraphEmbeddingbasedontheskip-\n",
      "grammodel.Insteadofusingapredefinedscoringfunction,welearnitrelying\n",
      "onLongShort-TermMemories.Weshowthatourembeddingsachieveresults\n",
      "comparablewiththemostscalableapproachesonknowledgegraphcompletion\n",
      "aswellasonanewmetric.Yet,KG2Veccanembedlargegraphsinlessertime\n",
      "by processing more than 250 million triples in less than 7 hours on common\n",
      "hardware.\n",
      "1 Introduction\n",
      "Recently,the number of public datasets in the Linked Data cloud has signifi-\n",
      "cantly grown to almost 10 thousands. At the time of writing, at least four of\n",
      "thesedatasetscontainmorethanonebilliontripleseach.1 Thishugeamountof\n",
      "availabledatahasbecomeafertilegroundforMachineLearningandDataMin-\n",
      "ingalgorithms.Today,applicationsofmachine-learningtechniquescomprisea\n",
      "broadvarietyofresearchareasrelatedtoLinkedData,suchasLinkDiscovery,\n",
      "NamedEntityRecognition,andStructuredQuestionAnswering.Thefieldof\n",
      "Knowledge Graph Embedding (KGE) has emerged in the Machine Learning\n",
      "communityduringthelastfiveyears. TheunderlyingconceptofKGEisthat\n",
      "in a knowledge base, each entity and relation can be regarded as a vector in\n",
      "a continuous space. The generated vector representations can be used by al-\n",
      "gorithms employing machine learning, deep learning, or statistical relational\n",
      "learning to accomplish a given task. Several KGE approaches have already\n",
      "shownpromisingresultsontaskssuchaslinkprediction,entityrecommenda-\n",
      "tion,questionanswering,andtripletclassification(Xiaoetal,2015;Linetal,\n",
      "2015a,b; Nickel et al, 2016). Moreover, Distributional Semantics techniques\n",
      "(e.g.,Word2VecorDoc2Vec)arerelativelynewintheSemanticWebcommunity.\n",
      "TheRDF2Vecapproaches(RistoskiandPaulheim,2016;Cochezetal,2017a)\n",
      "are examples of pioneering research and to date, they represent the only op-\n",
      "tionforlearningembeddingsonalargeknowledgegraphwithouttheneedfor\n",
      "state-of-the-arthardware.Tothisend,wedevisetheKG2Vecapproach,which\n",
      "comprisesskip-gramtechniquesforcreatingembeddingsonlargeknowledge\n",
      "graphs in a feasible time but still maintaining the quality of state-of-the-art\n",
      "embeddings.OurevaluationshowsthatKG2Vecachievesavectorqualitycom-\n",
      "1http://lodstats.aksw.org\n",
      "ExpeditiousGenerationofKnowledgeGraphEmbeddings 3\n",
      "parabletothemostscalableapproachesandcanprocessmorethan250million\n",
      "triplesinlessthan7hoursonamachinewithsuboptimalperformances.\n",
      "2 RelatedWork\n",
      "Anearlyefforttoautomaticallygeneratefeaturesfromstructuredknowledge\n",
      "was proposed in (Cheng et al, 2011). RESCAL (Nickel et al, 2011) is a\n",
      "relational-learning algorithm based on Tensor Factorization using Alternat-\n",
      "ing Least-Squares which has showed to scale to large RDF datasets such as\n",
      "YAGO(Nickel et al, 2012) and reach good results in the tasks of link predic-\n",
      "tion,entityresolution,orcollectiveclassification(Nickeletal,2014).Manifold\n",
      "approacheswhichrelyontranslationshavebeenimplementedsofar(Bordes\n",
      "et al, 2013; Wang et al, 2014b; Jia et al, 2015; Lin et al, 2015b; Wang et al,\n",
      "2015;Xiaoetal,2015).TransEisthefirstmethodwhererelationshipsarein-\n",
      "terpretedastranslationsoperatingonthelow-dimensionalembeddingsofthe\n",
      "entities (Bordes et al,2013). On the other hand,TransH models a relation as\n",
      "a hyperplane together with a translation operation on it (Wang et al, 2014b).\n",
      "TransAexploresembeddingmethodsforentitiesandrelationsbelongingtotwo\n",
      "differentknowledgegraphsfindingtheoptimallossfunction(Jiaetal,2015),\n",
      "whilstPTransEreliesonpathstobuildthefinalvectors(Linetal,2015a).The\n",
      "algorithmsTransRandCTransRproposedinLinetal(2015b)aimatbuilding\n",
      "entityandrelationembeddingsinseparateentityspaceandrelationspaces,so\n",
      "astolearnembeddingsthroughprojectedtranslationsintherelationspace;an\n",
      "extensionofthisalgorithmmakesuseofrulestolearnembeddings(Wangetal,\n",
      "2015).Anefforttojointlyembedstructuredandunstructureddata(suchastext)\n",
      "wasproposedinWangetal(2014a).TheideabehindtheDistMultapproachis\n",
      "toconsiderentitiesaslow-dimensionalvectorslearnedfromaneuralnetwork\n",
      "and relations as bilinear and/or linear mapping functions Yang et al (2014).\n",
      "TransG,agenerativemodeladdresstheissueofmultiplerelationsemanticsof\n",
      "arelation,hasshowedtogobeyondstate-of-the-artresults(Xiaoetal,2015).\n",
      "ComplExisbasedonlatentfactorizationand,withtheuseofcomplex-valued\n",
      "embeddings, it facilitates composition and handles a large variety of binary\n",
      "relations Trouillon et al (2016). The fastText algorithm was meant for word\n",
      "embeddings,however Joulin et al (2017) showed that a simple bag-of-words\n",
      "cangeneratesurprisinglygoodKGEs.\n",
      "4 TommasoSoruetal.\n",
      "The field of KGE has considerably grown during the last two years, earn-\n",
      "ing a spotalso in the SemanticWebcommunity. In 2016,Nickelet al (2016)\n",
      "proposedHolE,whichreliesonholographicmodelsofassociativememoryby\n",
      "employing circularcorrelation to create compositional representations. HolE\n",
      "cancapturerichinteractionsbyusingcorrelationasthecompositionaloperator\n",
      "butitsimultaneouslyremainsefficienttocompute,easytotrain,andscalable\n",
      "to large datasets. In the same year, Ristoski and Paulheim (2016) presented\n",
      "RDF2Vecwhichuseslanguagemodelingapproachesforunsupervisedfeature\n",
      "extraction from sequences of words and adapts them to RDF graphs. After\n",
      "generatingsequencesbyleveraginglocalinformationfromgraphsubstructures\n",
      "byrandomwalks,RDF2Veclearnslatentnumericalrepresentationsofentities\n",
      "inRDFgraphs.Thealgorithmhasbeenextendedinordertoreducethecompu-\n",
      "tationaltimeandthebiasedregardedtherandomwalking(Cochezetal,2017a).\n",
      "Morerecently,Cochezetal(2017b)exploitedtheGlobalVectorsalgorithmto\n",
      "compute embeddings from the co-occurrence matrix of entities and relations\n",
      "withoutgeneratingtherandomwalks.Infollowingresearch,theauthorsrefer\n",
      "totheiralgorithmasKGloVe.2\n",
      "3 KG2Vec\n",
      "Thisstudyaddressesthefollowingresearchquestions:\n",
      "1. Canwegenerateembeddingsatahighratewhilepreservingaccuracy?\n",
      "2. HowcanwetestthedistributionalhypothesisofKGEs?\n",
      "3. Canwelearnascoringfunctionforknowledgebasecompletionwhichper-\n",
      "formsbetterthanthestandardone?\n",
      "Formally,lett =(s,p,o)beatriplecontainingasubject,apredicate,andan\n",
      "objectinaknowledgebaseK.Foranytriple,(s,p,o)⊆E×R×(E∩L),where\n",
      "E is the set of all entities, R is the set of all relations, and L is the set of all\n",
      "literals(i.e.,stringornumericalvalues).ArepresentationfunctionF definedas\n",
      "F :(E∩R∩L)→Rd (1)\n",
      "assigns a vectorof dimensionality d to an entity,a relation,ora literal. How-\n",
      "ever, some approaches consider only the vector representations of entities or\n",
      "2https://datalab.rwth-aachen.de/embedding/KGloVe/\n",
      "ExpeditiousGenerationofKnowledgeGraphEmbeddings 5\n",
      "subjects (i.e, {s∈E :∃(s,p,o)∈K}). For instance, in approaches based on\n",
      "Tensor Factorization, given a relation, its subjects and objects are processed\n",
      "andtransformedintosparsematrices;allthematricesarethencombinedintoa\n",
      "tensorwhosedepthisthenumberofrelations.Forthefinalembedding,current\n",
      "approachesrelyondimensionalityreductiontodecreasetheoverallcomplex-\n",
      "ity(Nickeletal,2014;Jiaetal,2015;Linetal,2015b).Thereductionisper-\n",
      "formedthroughanembeddingmapΦ :Rd →Rk,whichisahomomorphism\n",
      "that maps the initial vector space into a smaller, reduced space. The positive\n",
      "valuek<d iscalledtherank oftheembedding.Notethateachdimensionof\n",
      "the reduced common space does not necessarily have an explicit connection\n",
      "withaparticularrelation.DimensionalityreductionmethodsincludePrincipal\n",
      "Component Analysis techniques (Nickel et al, 2014) and generative statisti-\n",
      "cal models such as Latent Dirichlet Allocation (Jurgens and Stevens, 2010;\n",
      "Rˇehu˚ˇrekandSojka,2010).\n",
      "ExistingKGEapproachesbasedontheskip-grammodelsuchasRDF2Vec(Ris-\n",
      "toskiandPaulheim,2016)submitpathsbuiltusingrandomwalkstoaWord2Vec\n",
      "algorithm.Instead,wepreprocesstheinputknowledgebasebyconvertingeach\n",
      "tripleintoasmallsentenceofthreewords.Ourmethodisfasterasitallowsus\n",
      "toavoidthepathgenerationstep.Thegeneratedtextcorpusisthusprocessed\n",
      "bytheskip-grammodelasfollows.\n",
      "3.1 Adaptingtheskip-grammodel\n",
      "We adapt the skip-gram model (Mikolov et al,2013a) to deal with our small\n",
      "sequences of length three. In this work, we only consider URIs and discard\n",
      "literals,thereforewecomputeavectorforeachelementu∈E∩R.Considering\n",
      "a triple as a sequence of three URIs T ={u,u,u },the aim is to maximize\n",
      "s p o\n",
      "theaveragelogprobability\n",
      "1\n",
      "∑ ∑ logp(u|u(cid:48)) (2)\n",
      "3\n",
      "u∈Tu(cid:48)∈T\\u\n",
      "whichmeans,inotherwords,toadoptacontextwindowof2,sincethesequence\n",
      "sizeisalways|T|=3.Theprobabilityaboveistheoreticallydefinedas:\n",
      "exp(vO(cid:62) vI )\n",
      "p(u|u(cid:48))= u u(cid:48) (3)\n",
      "∑ exp(vO(cid:62) vI )\n",
      "x∈E∩R x u(cid:48)\n",
      "6 TommasoSoruetal.\n",
      "wherevI andvO arerespectivelytheinputandoutputvectorrepresentationsof\n",
      "x x\n",
      "aURIx.Weimplyanegativesamplingof5,i.e.5wordsarerandomlyselected\n",
      "tohaveanoutputof0andconsequentlyupdatetheweights.\n",
      "3.2 Scoringfunctions\n",
      "3.2.1 Scoringbyanalogy\n",
      "Severalmethodshavebeenproposedtoevaluatewordembeddings.Themost\n",
      "commononesarebasedonanalogies(Mikolovetal,2013b;LevyandGoldberg,\n",
      "2014),wherewordvectorsaresummeduptogether,e.g.:\n",
      "v[”queen”]≈v[”king”]+v[”woman”]−v[”man”] (4)\n",
      "Ananalogywheretheapproximationaboveissatisfiedwithinacertainthresh-\n",
      "oldcanthuspredicthiddenrelationshipsamongwords,whichinourenviron-\n",
      "mentmeanstopredictnewlinksamongentitiesRistoskiandPaulheim(2016).\n",
      "The analogy-based score function for a given triple (s¯,p¯,o¯) is defined as fol-\n",
      "lows.\n",
      "(cid:40)\n",
      "1 1 if (cid:107)v +v −v −v (cid:107)≤ε\n",
      "score(s¯,p¯,o¯)= ∑ s¯ o s o¯ (5)\n",
      "|{(s,p¯,o)∈K}| 0 otherwise\n",
      "(s,p¯,o)∈K\n",
      "whereε isanarbitrarilysmallpositivevalue.Inwords,givenapredicate p¯,we\n",
      "selectalltripleswhereitoccurs.Foreachtriple,wecomputetherelationvector\n",
      "asthedifferencebetweentheobjectandthesubjectvectors. Wethencounta\n",
      "match whenever the vector sum of subject s¯and relation is close to object o¯\n",
      "withinaradiusε.Thescoreisequaltotherateofmatchesoverthenumberof\n",
      "selectedtriples.\n",
      "3.2.2 Scoringbyneuralnetworks\n",
      "We evaluate the scoring function above against a neural network based on\n",
      "LongShort-TermMemories(LSTM).Theneuralnetworktakesasequenceof\n",
      "embeddingsasinput,namelyv,v,v foratriple(s,p,o)∈K.Adensehidden\n",
      "s p o\n",
      "layerofthesamesizeoftheembeddingsisconnectedtoasingleoutputneuron\n",
      "withsigmoidactivation,whichreturnsavaluebetween0and1.Thenegative\n",
      "ExpeditiousGenerationofKnowledgeGraphEmbeddings 7\n",
      "Fig.1 AselectionofDB-\n",
      "pediaresourcesalongwith\n",
      "theirvectors in 3 dimen-\n",
      "sionsobtainedusingPrincipal\n",
      "ComponentAnalysis.Blue\n",
      "pointsareresources,whilst\n",
      "redpointsareclasses.Ascan\n",
      "beseen,resourcesfollowthe\n",
      "distributionalhypothesis.\n",
      "triplesaregeneratedusingtwostrategies,i.e.foreachtripleinthetrainingset\n",
      "(1)randomlyextractarelationanditstwonodesor(2)corruptthesubjector\n",
      "theobject.WeusetheAdamoptimizerand100epochsoftraining.\n",
      "3.3 Metrics\n",
      "AsrecentlyhighlightedbyseveralmembersoftheMLandNLPcommunities,\n",
      "KGEsarerarelyevaluatedondownstreamtasksdifferentfromlinkprediction\n",
      "(alsoknownasknowledgebasecompletion).Achievinghighperformanceson\n",
      "link prediction does not necessarily mean that the generated embeddings are\n",
      "good,sincetheinferencetaskisoftencarriedoutincombinationwithanexter-\n",
      "nalalgorithmsuchasaneuralnetworkorascoringfunction.Thecomplexityis\n",
      "thusapproach-dependentanddistributedbetweenthelatentstructureinthevec-\n",
      "tormodelandtheparameters(ifany)oftheinferencealgorithm.Forinstance,\n",
      "atranslationalmodelsuchasTransEBordesetal(2013)wouldlikelyfeature\n",
      "verycomplexembeddings,sinceinmostapproachestheinferencefunctionis\n",
      "a simple addition. On the other hand, we may find less structure in a tensor\n",
      "factorization model such as RESCAL Nickel et al (2011),as the inference is\n",
      "performed by a feed-forward neural network which extrapolates the hidden\n",
      "semanticslayerbylayer.\n",
      "8 TommasoSoruetal.\n",
      "3.3.1 NeighbourSimilarityTest\n",
      "In this paper, we introduce two metrics inspired by The Identity of Indis-\n",
      "cernibles (Black, 1952) to gain insights over the distributional quality of the\n",
      "learnedembeddings.\n",
      "Themorecharacteristicstwoentitiesshare,themoresimilartheyareand\n",
      "soshouldbetheirvectorrepresentations.\n",
      "Considering the setofcharacteristicsC (s)={(p,o ),...,(p,o )} ofa\n",
      "K 1 1 m m\n",
      "subjectsinatriple,wecandefineametricthatexpressesthesimilarityamong\n",
      "two entities e,e as the Jaccard index between their sets of characteristics\n",
      "1 2\n",
      "C (e )andC (e ).GivenasetofentitiesE˜ andtheirN nearestneighboursin\n",
      "K 1 K 2\n",
      "thevectorspace,theoverallNeighbourSimilarityTest(NST)metricisdefined\n",
      "as:\n",
      "(e)\n",
      "NST(E˜,N,K)=\n",
      "1\n",
      "∑\n",
      "∑N |C K(e)∩C K(n\n",
      "j\n",
      ")|\n",
      "(6)\n",
      "N|E˜|\n",
      "e∈E˜ j=1|C K(e)∪C\n",
      "K(n( je)\n",
      ")|\n",
      "(e)\n",
      "wheren isthe jthnearestneighbourofeinthevectorspace.\n",
      "j\n",
      "3.3.2 TypeandCategoryTest\n",
      "ThesecondmetricistheTypeandCategoryTest(TCT),basedontheassump-\n",
      "tion that two entities which share types and categories should be close in\n",
      "the vector space. This assumption is suggested by the human bias for which\n",
      "rdf:type and dct:subject would be predicates with a higher weight\n",
      "thantheothers.Althoughthisdoesnothappen,wecomputeitforameresake\n",
      "ofcomparisonwiththeNSTmetric.TheTCTformulaisequaltoEquation6ex-\n",
      "ceptforsetsC (e),whicharereplacedbysetsoftypesandcategoriesTC (e).\n",
      "K K\n",
      "4 Evaluation\n",
      "WeimplementedKG2VecinPython2.7usingtheGensimandKeraslibraries\n",
      "withTheanoenvironment.Sourcecode,datasets,andvectorsobtainedareavail-\n",
      "ExpeditiousGenerationofKnowledgeGraphEmbeddings 9\n",
      "ableonline.3 AllexperimentswerecarriedoutonanUbuntu16.04serverwith\n",
      "128GBRAMand40CPUs.\n",
      "Table1 DetailsandruntimesforthegenerationofKG2Vecembeddingsontwodatasets.\n",
      "DBpedia\n",
      "Dataset AKSW-bib DBpedia2016-04\n",
      "2015-10\n",
      "Numberoftriples 3922 164,369,887 276,316,003 276,316,003\n",
      "Numberofvectors 954 14,921,691 23,816,469 36,596,967\n",
      "Dimensionality 10 300 200 200\n",
      "Runtime(s) 2.2 18,332 25,380 46,099\n",
      "Rate(triples/s) 1,604 8,966 10,887 5,994\n",
      "ThedatasetusedintheexperimentsaredescribedinTable1.TheAKSW-bib\n",
      "dataset–employedforthelinkpredictionevaluation–wascreatedusinginfor-\n",
      "mationfrompeopleandprojectsontheAKSW.orgwebsiteandbibliographical\n",
      "datafromBibsonomy.WebuiltamodelontopoftheEnglish2015-10version\n",
      "oftheDBpediaknowledgegraph(Lehmann etal,2009); Figure1showsa3-\n",
      "dimensionalplotofselectedentities.FortheEnglishDBpedia2016-04dataset,\n",
      "webuilttwomodels.Inthefirst,wesetathresholdtoembedonlytheentities\n",
      "occurring at least 5 times in the dataset; we chose this setting to be aligned\n",
      "to the related works’ models. In the second model, all 36 million entities in\n",
      "DBpedia are associated a vector. More insights about the first model can be\n",
      "foundinthenexttwosubsections,whiletheresourceconsumptionforcreating\n",
      "thesecondmodelcanbeseeninFigure5.\n",
      "4.1 Runtime\n",
      "Inthisstudy,weaimatgeneratingembeddingsatahighratewhilepreserving\n",
      "accuracy.InTable1,wealreadyshowedthatoursimplepipelinecanachieve\n",
      "arateofalmost11,000triplespersecondonalargedatasetsuchasDBpedia\n",
      "2016-04.InTable2,wecompareKG2Vecwiththreeotherscalableapproaches\n",
      "for embedding knowledge bases. We selected the best settings of RDF2Vec\n",
      "and KGloVe according to their respective articles, since both algorithms had\n",
      "alreadybeensuccessfullyevaluatedonDBpedia(RistoskiandPaulheim,2016;\n",
      "3http://github.com/AKSW/KG2Vec\n",
      "10 TommasoSoruetal.\n",
      "Cochez et al, 2017b). We also tried to compute fastText embeddings on our\n",
      "machine, however we had to halt the process after three days. As the goal\n",
      "of our investigation is efficiency,we discarded any other KGE approach that\n",
      "would have needed more than three days of computation to deliver the final\n",
      "model(Cochezetal,2017b).\n",
      "RDF2Vec has shown to be the most expensive in terms of disk space con-\n",
      "sumed,asthecreatedrandomwalksamountedto∼300GBoftext.Moreover,\n",
      "we could not measure the runtime for the first phase of KGloVe, i.e. the cal-\n",
      "culationofthePersonalizedPageRankvaluesofDBpediaentities.Infact,the\n",
      "authorsusedpre-computedentityranksfromThalhammerandRettinger(2016)\n",
      "andtheKGloVesourcecodedoesnotfeatureaPageRankalgorithm. Weesti-\n",
      "mated the runtime comparing their hardware specs with ours. Despite being\n",
      "unabletoreproduceanyexperimentsfromtheotherthreeapproaches,weman-\n",
      "agedtoevaluatetheirembeddingsbydownloadingthepretrainedmodels4 and\n",
      "creating a KG2Vec embedding model of the same DBpedia dataset there em-\n",
      "ployed.\n",
      "Table2 Runtimecomparisonofthesinglephases.Thosewith(*)areestimatedruntimes.\n",
      "ApproachSteps Time\n",
      "Randomwalksgeneration 123minutes\n",
      "RDF2Vec\n",
      "Word2Vectraining >96hours(*)\n",
      "PersonalizedPageRank N/A\n",
      "KGloVe Co-occurrencecountmatrix\n",
      "12hours(*)\n",
      "GloVetraining\n",
      "Conversiontotext 5minutes\n",
      "KG2Vec\n",
      "Word2Vectraining 6hours58minutes\n",
      "Conversiontotext 5minutes\n",
      "fastText\n",
      "fastTexttraining >72hours(*)\n",
      "4.2 Preliminaryresultsonlinkprediction\n",
      "For the link prediction task,we partition the dataset into training and test set\n",
      "witharatioof9:1.InTable3,weshowpreliminaryresultsbetweenthedifferent\n",
      "strategies on the AKSW-bib dataset using KG2Vec embeddings. As can be\n",
      "seen,ourLSTM-basedscoringfunctionsignificantlyoutperformstheanalogy-\n",
      "based one in both settings. According to the Hits@10 accuracy we obtained,\n",
      "4http://data.dws.informatik.uni-mannheim.de/rdf2vec/\n",
      "ExpeditiousGenerationofKnowledgeGraphEmbeddings 11\n",
      "corrupting triples to generate negative examples is the better strategy. This\n",
      "firstinsightcanfosternewresearchonoptimizingascoringfunctionforKGE\n",
      "approachesbasedondistributionalsemantics.\n",
      "Table3 FilteredHits@10valuesonlinkpredictiononAKSW-bibusingdifferentstrategies.\n",
      "Hits@1 Hits@3 Hits@10\n",
      "LSTM+corrupted 3.84% 9.79% 19.23%\n",
      "LSTM+random 1.39% 4.89% 10.49%\n",
      "Analogy 0.00% 0.51% 3.82%\n",
      "4.3 Distributionalquality\n",
      "ComputingtheNSTandTCTdistributionalqualitymetricsontheentireDB-\n",
      "pediadatasetistime-demanding,sinceforeachentity,themodelandthegraph\n",
      "needtobequeriedfortheN nearestneighboursandtheirrespectivesets.How-\n",
      "ever,weapproximatethefinalvaluebytracingthepartialvaluesofNSTand\n",
      "TCTovertime.Inotherwords,ateachiterationi,wecomputethemetricsover\n",
      "E˜ ={e,...,e}.Figure2showsthepartialTCTvalueonthemostimportant\n",
      "i 1 i\n",
      "10,000entitiesforN ={1,10}accordingtotherankscomputedbyThalham-\n",
      "merandRettinger(2016).Here,KG2Vecmaintainsahigherindexthantheother\n",
      "twoapproaches,despitethesearesteadilyincreasingafterthe∼2,000thentity.\n",
      "WeinterpretthelowerTCTforthetop2,000entitiesasnoiseproducedbythe\n",
      "factthatthesenodesarehyperconnectedtotherestofthegraph,thereforeitis\n",
      "hardforthemtoremainclosetotheirtypepeers.InFigures3and4,theTCT\n",
      "andNSTmetricsrespectivelyarecomputedon10,000randomentities.Inboth\n",
      "cases, the values for the two settings of all approaches stabilize after around\n",
      "1,000entities,howeverweclearlyseethatRDF2Vecembeddingsachievethe\n",
      "highestdistributionalquality by type andcategory. The highernumberofoc-\n",
      "currencesperentityinthehugecorpusofrandomwalksinRDF2Vecmightbe\n",
      "thereasonofthisresultforrarerentities.\n",
      "InFigure5,weshowtheCPU,Memory,anddiskconsumptionforKG2Vec\n",
      "onthelargermodelofDBpedia2016-04.Allthreesubphasesofthealgorithm\n",
      "are visible in the plot. For 2.7 hours, tokens are counted; then, the learning\n",
      "proceedsfor7.7hours;finallyinthelast2.3hours,themodelissaved.\n",
      "12 TommasoSoruetal.\n",
      "0.3\n",
      "0.25 0.2\n",
      "0.15\n",
      "0.1\n",
      "0.05\n",
      "0\n",
      "0 2000 4000 6000 8000 10000\n",
      "Iteration\n",
      "xednI\n",
      "laitraP\n",
      "1\n",
      "0.8\n",
      "KGloVe, N=10 0.6\n",
      "RDF2Vec, N=10\n",
      "KG2Vec, N=10 0.4\n",
      "KGloVe, N=1\n",
      "RDF2Vec, N=1\n",
      "KG2Vec, N=1 0.2\n",
      "0\n",
      "0 2000 4000 6000 8000 10000\n",
      "Iteration\n",
      "Fig. 2 PartialTCT value on DBpedia 2016-04\n",
      "forthetop10,000entities.\n",
      "xednI\n",
      "laitraP\n",
      "KGlove,n=10\n",
      "RDF2Vec,n=10\n",
      "KG2Vec,n=10 KGlove,n=1 RDF2Vec,n=1\n",
      "KG2Vec,n=1\n",
      "Fig. 3 PartialTCT value on DBpedia 2016-04\n",
      "for10,000randomentities.\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "0\n",
      "0 2000 4000 6000 8000 10000\n",
      "Iteration\n",
      "xednI\n",
      "laitraP\n",
      "KGlove, N=10 120000000 RAM (GB) 200 GB\n",
      "RDF2Vec, N=10 Disk (GB)\n",
      "KG2Vec, N=10 100000000 CPU (%)\n",
      "KGlove, N=1 150 GB\n",
      "RDF2Vec, N=1 80000000\n",
      "KG2Vec, N=1\n",
      "60000000 100 GB\n",
      "40000000\n",
      "50 GB\n",
      "20000000\n",
      "0 0 GB\n",
      "0 2 4 6 8 10 12 14\n",
      "Fig.5 CPU,Memory,anddiskconsumptionfor\n",
      "Fig. 4 PartialNST value on DBpedia 2016-04\n",
      "KG2VeconthelargermodelofDBpedia2016-04.\n",
      "for10,000randomentities.\n",
      "100\n",
      "Fig.6 Weshowthecompar-\n",
      "80\n",
      "isonoftherun-timesforall\n",
      "fourapproaches.Notethat 60\n",
      "sincewedonotknowhow\n",
      "40\n",
      "longthePageRankcompu-\n",
      "tationtakes,wereportedthe 20\n",
      "estimatedruntimefortheplain\n",
      "0\n",
      "versionofKGloVe. RDF2Vec fastText KGloVe KG2Vec\n",
      ")\n",
      "sruoh\n",
      "(\n",
      "emiT\n",
      "5 ConclusionandFutureWork\n",
      "We presenteda fastapproachforgenerating KGEs dubbedKG2Vec. We con-\n",
      "cludethattheskip-grammodel,iftraineddirectlyontriplesassmallsentences\n",
      "oflengththree,significantlygainsinruntimewhilepreservingadecentvector\n",
      "quality. Moreover,theKG2Vecembeddingshaveshownhigherdistributional\n",
      "quality for the most important entities in the graph according to PageRank.\n",
      "As a future work, we plan to extend the link prediction evaluation to other\n",
      "benchmarksbyusinganalogiesandourLSTM-basedscoringfunctionoverthe\n",
      "embeddingmodelsoftheapproachesherecompared.\n",
      "ExpeditiousGenerationofKnowledgeGraphEmbeddings 13\n",
      "References\n",
      "BlackM(1952)Theidentityofindiscernibles.Mind61(242):153–164\n",
      "BordesA,UsunierN,Garcia-DuranA,WestonJ,YakhnenkoO(2013)Translat-\n",
      "ingembeddingsformodelingmulti-relationaldata.In:AdvancesinNeural\n",
      "InformationProcessingSystems,pp2787–2795\n",
      "ChengW,KasneciG,GraepelT,SternD,HerbrichR(2011)Automatedfeature\n",
      "generationfromstructuredknowledge.In:CIKM,ACM,pp1395–1404\n",
      "CochezM,RistoskiP,PonzettoSP,PaulheimH(2017a)Biasedgraphwalksfor\n",
      "rdfgraphembeddings.In:Proceedingsofthe7thInternationalConference\n",
      "onWebIntelligence,MiningandSemantics,ACM,p21\n",
      "Cochez M, Ristoski P, Ponzetto SP, Paulheim H (2017b) Global rdf vector\n",
      "spaceembeddings.In:InternationalSemanticWebConference,Springer,pp\n",
      "190–207\n",
      "JiaY,WangY,LinH,JinX,ChengX(2015)Locallyadaptivetranslationfor\n",
      "knowledgegraphembedding.arXivpreprintarXiv:151201370\n",
      "JoulinA,GraveE,BojanowskiP,NickelM,MikolovT(2017)Fastlinearmodel\n",
      "forknowledgegraphembeddings.arXivpreprintarXiv:171010881\n",
      "JurgensD,StevensK(2010)Thes-spacepackage:anopensourcepackagefor\n",
      "word space models. In: Proceedings of the ACL 2010 System Demonstra-\n",
      "tions,AssociationforComputationalLinguistics,pp30–35\n",
      "LehmannJ,BizerC,KobilarovG,AuerS,BeckerC,CyganiakR,HellmannS\n",
      "(2009)DBpedia-acrystallizationpointforthewebofdata.JournalofWeb\n",
      "Semantics7(3):154–165\n",
      "LevyO,GoldbergY(2014)Linguisticregularitiesinsparseandexplicitword\n",
      "representations. In: Proceedings of the eighteenth conference on computa-\n",
      "tionalnaturallanguagelearning,pp171–180\n",
      "LinY,LiuZ,SunM(2015a)Modelingrelationpathsforrepresentationlearning\n",
      "ofknowledgebases.CoRRabs/1506.00379,URLhttp://arxiv.org/\n",
      "abs/1506.00379\n",
      "Lin Y,Liu Z,Sun M,Liu Y,Zhu X (2015b) Learning entity and relation em-\n",
      "beddingsforknowledgegraphcompletion.In:AAAI,pp2181–2187\n",
      "MikolovT,SutskeverI,ChenK,CorradoGS,DeanJ(2013a)Distributedrep-\n",
      "resentationsofwordsandphrasesandtheircompositionality.In:BurgesC,\n",
      "BottouL,WellingM,GhahramaniZ,WeinbergerK(eds)AdvancesinNeural\n",
      "InformationProcessingSystems26,CurranAssociates,Inc.,pp3111–3119\n",
      "14 TommasoS<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,    605,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['AKSW-bib', 'DBpedia2016-04', 'DBpedia2015-10']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: naturallanguagelearning,pp171–180\n",
      "LinY,LiuZ,SunM(2015a)Modelingrelationpathsforrepresentationlearning\n",
      "ofknowledgebases.CoRRabs/1506.00379,URLhttp://arxiv.org/\n",
      "abs/1506.00379\n",
      "Lin Y,Liu Z,Sun M,Liu Y,Zhu X (2015b) Learning entity and relation em-\n",
      "beddingsforknowledgegraphcompletion.In:AAAI,pp2181–2187\n",
      "MikolovT,SutskeverI,ChenK,CorradoGS,DeanJ(2013a)Distributedrep-\n",
      "resentationsofwordsandphrasesandtheircompositionality.In:BurgesC,\n",
      "BottouL,WellingM,GhahramaniZ,WeinbergerK(eds)AdvancesinNeural\n",
      "InformationProcessingSystems26,CurranAssociates,Inc.,pp3111–3119\n",
      "14 TommasoSoruetal.\n",
      "Mikolov T, Yih Wt, Zweig G (2013b) Linguistic regularities in continuous\n",
      "spacewordrepresentations.In:HLT-NAACL,pp746–751\n",
      "NickelM,TrespV,KriegelHP(2011)Athree-waymodelforcollectivelearning\n",
      "onmulti-relationaldata.In:Proceedingsofthe28thinternationalconference\n",
      "onmachinelearning(ICML-11),pp809–816\n",
      "Nickel M, Tresp V, Kriegel HP (2012) Factorizing yago: scalable machine\n",
      "learningforlinkeddata.In:WWW,ACM,pp271–280\n",
      "NickelM,JiangX,TrespV(2014)Reducingtherankinrelationalfactorization\n",
      "modelsbyincludingobservablepatterns.In:AdvancesinNeuralInformation\n",
      "ProcessingSystems,pp1179–1187\n",
      "Nickel M, Rosasco L, Poggio TA, et al (2016) Holographic embeddings of\n",
      "knowledgegraphs.In:AAAI,pp1955–1961\n",
      "Rˇehu˚ˇrek R, Sojka P (2010) Software Framework for Topic Modelling with\n",
      "LargeCorpora.In:ProceedingsoftheLREC2010WorkshoponNewChal-\n",
      "lengesforNLPFrameworks,ELRA,Valletta,Malta,pp45–50\n",
      "RistoskiP,PaulheimH(2016)Rdf2vec:Rdfgraphembeddingsfordatamining.\n",
      "In:InternationalSemanticWebConference,Springer,pp498–514\n",
      "ThalhammerA,RettingerA(2016)PageRankonWikipedia:TowardsGeneral\n",
      "ImportanceScoresforEntities.In:TheSemanticWeb:ESWC2016Satellite\n",
      "Events,Cham,pp227–240\n",
      "TrouillonT,WelblJ,RiedelS,GaussierÉ,BouchardG(2016)Complexembed-\n",
      "dings for simple link prediction. In: International Conference on Machine\n",
      "Learning,pp2071–2080\n",
      "WangQ,WangB,GuoL(2015)Knowledgebasecompletionusingembeddings\n",
      "andrules.In:IJCAI,pp1859–1865\n",
      "Wang Z,Zhang J,Feng J,Chen Z (2014a) Knowledge graph and text jointly\n",
      "embedding.In:EMNLP,Citeseer,pp1591–1601\n",
      "Wang Z, Zhang J, Feng J, Chen Z (2014b) Knowledge graph embedding by\n",
      "translatingonhyperplanes.In:AAAI,Citeseer,pp1112–1119\n",
      "XiaoH,HuangM,HaoY,ZhuX(2015)Transg:Agenerativemixturemodel\n",
      "forknowledgegraphembedding.arXivpreprintarXiv:150905488\n",
      "YangMC,DuanN,ZhouM,RimHC(2014)Jointrelationalembeddingsfor\n",
      "knowledge-based question answering. In: Proceedings of the 2014 confer-\n",
      "ence on empirical methods in natural language processing (EMNLP), pp\n",
      "645–650<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "          14818,    543,   2681,  21656,     11,    604,  11123,   4235,   5245,\n",
      "            198,  51697,     56,  31214,  19260,     57,  21861,    359,     44,\n",
      "              7,    679,     20,     64,      8,   1747,    287,  23013,  22354,\n",
      "           2000,  84216,  21656,    198,   1073,  90153,  79367,  53214,   8268,\n",
      "           3518,     14,   3965,     21,     13,   6268,   4643,     11,   3222,\n",
      "           1277,   1129,    277,  89833,   2726,   6018,   3518,     14,   3965,\n",
      "             21,     13,   6268,   4643,    198,  51697,    816,  31214,  19260,\n",
      "           1901,  21861,    359,    386,  31214,  19260,    816,  51932,  17156,\n",
      "           1630,    320,    679,     20,     65,      8,  21579,   5502,    323,\n",
      "          12976,    991,   7058,   2788,  25624,  45838,  52286,   4539,  44412,\n",
      "           5450,     25,   6157,  15836,     11,    604,  13302,     16,   4235,\n",
      "          13302,     22,    198,     44,   1609,    337,    869,     51,  21861,\n",
      "           6256,    441,    424,     40,     11,   1163,    268,     42,     11,\n",
      "          10803,  77927,  16929,     11,  80966,     41,      7,    679,     18,\n",
      "             64,      8,     35,  26204,  10200,   7058,  13898,    708,     69,\n",
      "           5880,    438,    764,  27663,    438,  50828,  77907,   2786,   5450,\n",
      "             25,  62339,   4282,     34,    345,     33,   1751,    283,     43,\n",
      "          50640,   6427,     44,  38406,     71,   1494,   2453,   5676,     57,\n",
      "             11,   1687,    258,  49120,     42,      7,   6910,      8,  24350,\n",
      "           3095,    258,   8989,   4269,    198,  15218,  29992,  49165,   1627,\n",
      "          11541,    324,   6713,  30915,    988,     11,  40345,   2637,    604,\n",
      "          15134,     16,   4235,  15134,     24,    198,    975,   8529,   7044,\n",
      "          73843,    269,  14127,    278,    627,     44,   1609,    337,    869,\n",
      "            350,     11,    816,   7141,    468,     83,     11,  84226,    343,\n",
      "            480,    320,    679,     18,     65,      8,  89333,   4633,   5912,\n",
      "           1385,    304,  19815,    198,   8920,   1178,  36369,    811,   5450,\n",
      "             25,  81158,     12,   7476,  56493,     11,    604,  25594,   4235,\n",
      "          23986,    198,  31456,    301,     44,  20594,  18744,     53,  44754,\n",
      "          83591,    301,   6748,      7,    679,     16,      8,     32,  28956,\n",
      "          27896,   2658,    491,    269,  17840,    535,  21656,    198,    263,\n",
      "          27364,  48712,   1697,    695,   5450,     25,  85438,    287,    708,\n",
      "             69,   1820,   1591,    339,  98697,  79590,    198,    263,  33156,\n",
      "          21656,  90104,   2735,     12,    806,    705,    604,  21474,   4235,\n",
      "          23713,    198,  31456,    301,    386,     11,    350,  18744,    650,\n",
      "             11,    735,  83591,    301,  12478,    320,    679,     17,      8,\n",
      "          38829,   4954,    379,   6438,     25,  69311,   5780,    198,  21656,\n",
      "           2000,  44233,    695,   5450,     25,  46608,     11,   1741,     44,\n",
      "             11,    604,  15828,   4235,  11209,    198,  31456,    301,     44,\n",
      "          59962,  28323,     55,  20594,  18744,     53,      7,    679,     19,\n",
      "              8,  17020,    287,    700,   1201,    258,   3833,   1697,  38691,\n",
      "           2065,    198,   6644,   1729,  16564,  34595,  27061,   5450,     25,\n",
      "          24350,   3095,    258,   8989,   4269,  15218,    198,  29992,  49165,\n",
      "             11,    604,   8546,     24,   4235,   8899,     22,    198,  31456,\n",
      "            301,    386,     11,  16870,  80457,    445,     11,    393,  16499,\n",
      "            822,  39991,     11,   1880,    453,    320,    679,     21,      8,\n",
      "            473,   1640,  79173,  71647,    315,    198,  90153,  87286,   5450,\n",
      "             25,   6157,  15836,     11,    604,   6280,     20,   4235,   5162,\n",
      "             16,    198,     49,    135,    229,   2701,     84,    135,    248,\n",
      "            135,    229,  42961,    432,     11,   2100,     73,   4657,    393,\n",
      "            320,    679,     15,      8,   4476,  24686,    369,  34011,   5768,\n",
      "           6427,    449,    198,  35353,  10803,  71764,   5450,     25,  85438,\n",
      "            287,    708,     69,   1820,     43,  67713,    679,     15,   6919,\n",
      "           8845,    263,   3648,   1163,    278,   7058,     75,    833,    288,\n",
      "           2000,     45,  12852,  90715,     11,   2818,   5726,     11,   2257,\n",
      "           1169,   2629,  28112,  69151,     11,    604,   1774,   4235,   1135,\n",
      "            198,     49,    380,    437,   6780,     47,     11,  26368,  21215,\n",
      "             39,      7,    679,     21,      8,     49,   3013,     17,   4175,\n",
      "             25,     49,   3013,   4539,  12529,  25624,   8350,    266,    309,\n",
      "           5859,    627,    644,     25,  34746,  99031,   6109,  92348,  21861,\n",
      "           2702,    261,     11,    604,  21962,   4235,  20998,    198,   1016,\n",
      "            278,  46434,     32,     11,  12289,   1303,    261,     32,      7,\n",
      "            679,     21,      8,   2732,  23366,    263,     54,  15288,  69761,\n",
      "          71839,  15777,    198,  11772,    685,  57036,   2000,  16206,   5450,\n",
      "          75145,  99031,   6109,     25,   1600,  26538,    679,     21,  35982,\n",
      "          18652,    198,   8059,     11,   1163,    309,     11,    604,  14206,\n",
      "           4235,   8273,    198,  91635,  43588,     51,  50640,    301,   2067,\n",
      "             41,  24412,   1142,    301,     50,  38406,  64151,   1291,  27887,\n",
      "           8324,   3102,    569,     38,      7,    679,     21,      8,  32237,\n",
      "          12529,   7058,  25624,    369,   4382,   2723,  20212,     13,    763,\n",
      "             25,   7327,  15217,    389,  13257,    198,  48567,     11,    604,\n",
      "          12060,     16,   4235,  12171,     15,    198,     54,    526,     48,\n",
      "          50640,    526,     33,     11,  17198,     78,     43,      7,    679,\n",
      "             20,      8,  81434,   3231,  44412,    985,  12529,  25624,    198,\n",
      "            438,  22746,   5450,  58255,     41,   5158,     40,     11,    604,\n",
      "           9741,     24,   4235,   9714,     20,    198,     54,    526,   1901,\n",
      "          51932,  21313,    622,  28328,    833,    622,     11,   1163,    268,\n",
      "           1901,    320,    679,     19,     64,      8,  33025,   4876,    323,\n",
      "           1495,  53258,    198,  95711,   5450,     25,   2783,     45,  12852,\n",
      "          11541,    275,   2423,    261,     11,    604,  11068,     16,   4235,\n",
      "           6330,     16,    198,     54,    526,   1901,     11,  37120,    622,\n",
      "             11,  43758,    622,     11,  25507,   1901,    320,    679,     19,\n",
      "             65,      8,  33025,   4876,  40188,    555,    198,   1485,     75,\n",
      "           1113,    263,  69292,  39157,   5450,     25,   6157,  15836,  11541,\n",
      "            275,   2423,    261,     11,    604,   5037,     17,   4235,   5037,\n",
      "             24,    198,     55,  23332,     39,  44639,  69710,     44,  44639,\n",
      "           3524,     56,  51932,  17156,     55,      7,    679,     20,      8,\n",
      "           3246,     70,  56748,   7642,  20053,    336,  13025,   2590,    198,\n",
      "          45838,  52286,   4539,  95711,  17126,     55,    344,   1762,   1374,\n",
      "            277,     55,    344,     25,   3965,  22393,  21310,    198,  76065,\n",
      "          11865,  28365,  10602,     45,  51932,  18664,     44,  24412,    318,\n",
      "          23263,      7,    679,     19,      8,  42097,  23013,  22317,   2788,\n",
      "          25624,   2000,    198,  90153,   6108,   3488,  36864,     13,    763,\n",
      "             25,  55227,    315,    279,    220,    679,     19,  49843,   7058,\n",
      "            768,    389,  46763,   5528,    304,   5933,   4221,   8863,    320,\n",
      "           2783,     45,  12852,    705,  12086,    198,  22926,   4235,  13655,\n",
      "         128009, 128006,    882, 128007,    271,   7184,     11,   2728,    420,\n",
      "           3488,     25,   3639,    527,    279,    836,    315,  30525,   1511,\n",
      "            304,    279,   5684,   4710,    220,  21335,   1203,    279,   4320,\n",
      "           1193,    304,    264,  13325,   1160,   3645,     11,    369,   3187,\n",
      "             25,   2570,     32,   1882,     33,   7352,   1442,    499,   1541,\n",
      "            956,   1440,    279,   4320,     11,   1120,    471,    459,   4384,\n",
      "           1160,     13, 128009, 128006,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "          14818,    543,   2681,  21656,     11,    604,  11123,   4235,   5245,\n",
      "            198,  51697,     56,  31214,  19260,     57,  21861,    359,     44,\n",
      "              7,    679,     20,     64,      8,   1747,    287,  23013,  22354,\n",
      "           2000,  84216,  21656,    198,   1073,  90153,  79367,  53214,   8268,\n",
      "           3518,     14,   3965,     21,     13,   6268,   4643,     11,   3222,\n",
      "           1277,   1129,    277,  89833,   2726,   6018,   3518,     14,   3965,\n",
      "             21,     13,   6268,   4643,    198,  51697,    816,  31214,  19260,\n",
      "           1901,  21861,    359,    386,  31214,  19260,    816,  51932,  17156,\n",
      "           1630,    320,    679,     20,     65,      8,  21579,   5502,    323,\n",
      "          12976,    991,   7058,   2788,  25624,  45838,  52286,   4539,  44412,\n",
      "           5450,     25,   6157,  15836,     11,    604,  13302,     16,   4235,\n",
      "          13302,     22,    198,     44,   1609,    337,    869,     51,  21861,\n",
      "           6256,    441,    424,     40,     11,   1163,    268,     42,     11,\n",
      "          10803,  77927,  16929,     11,  80966,     41,      7,    679,     18,\n",
      "             64,      8,     35,  26204,  10200,   7058,  13898,    708,     69,\n",
      "           5880,    438,    764,  27663,    438,  50828,  77907,   2786,   5450,\n",
      "             25,  62339,   4282,     34,    345,     33,   1751,    283,     43,\n",
      "          50640,   6427,     44,  38406,     71,   1494,   2453,   5676,     57,\n",
      "             11,   1687,    258,  49120,     42,      7,   6910,      8,  24350,\n",
      "           3095,    258,   8989,   4269,    198,  15218,  29992,  49165,   1627,\n",
      "          11541,    324,   6713,  30915,    988,     11,  40345,   2637,    604,\n",
      "          15134,     16,   4235,  15134,     24,    198,    975,   8529,   7044,\n",
      "          73843,    269,  14127,    278,    627,     44,   1609,    337,    869,\n",
      "            350,     11,    816,   7141,    468,     83,     11,  84226,    343,\n",
      "            480,    320,    679,     18,     65,      8,  89333,   4633,   5912,\n",
      "           1385,    304,  19815,    198,   8920,   1178,  36369,    811,   5450,\n",
      "             25,  81158,     12,   7476,  56493,     11,    604,  25594,   4235,\n",
      "          23986,    198,  31456,    301,     44,  20594,  18744,     53,  44754,\n",
      "          83591,    301,   6748,      7,    679,     16,      8,     32,  28956,\n",
      "          27896,   2658,    491,    269,  17840,    535,  21656,    198,    263,\n",
      "          27364,  48712,   1697,    695,   5450,     25,  85438,    287,    708,\n",
      "             69,   1820,   1591,    339,  98697,  79590,    198,    263,  33156,\n",
      "          21656,  90104,   2735,     12,    806,    705,    604,  21474,   4235,\n",
      "          23713,    198,  31456,    301,    386,     11,    350,  18744,    650,\n",
      "             11,    735,  83591,    301,  12478,    320,    679,     17,      8,\n",
      "          38829,   4954,    379,   6438,     25,  69311,   5780,    198,  21656,\n",
      "           2000,  44233,    695,   5450,     25,  46608,     11,   1741,     44,\n",
      "             11,    604,  15828,   4235,  11209,    198,  31456,    301,     44,\n",
      "          59962,  28323,     55,  20594,  18744,     53,      7,    679,     19,\n",
      "              8,  17020,    287,    700,   1201,    258,   3833,   1697,  38691,\n",
      "           2065,    198,   6644,   1729,  16564,  34595,  27061,   5450,     25,\n",
      "          24350,   3095,    258,   8989,   4269,  15218,    198,  29992,  49165,\n",
      "             11,    604,   8546,     24,   4235,   8899,     22,    198,  31456,\n",
      "            301,    386,     11,  16870,  80457,    445,     11,    393,  16499,\n",
      "            822,  39991,     11,   1880,    453,    320,    679,     21,      8,\n",
      "            473,   1640,  79173,  71647,    315,    198,  90153,  87286,   5450,\n",
      "             25,   6157,  15836,     11,    604,   6280,     20,   4235,   5162,\n",
      "             16,    198,     49,    135,    229,   2701,     84,    135,    248,\n",
      "            135,    229,  42961,    432,     11,   2100,     73,   4657,    393,\n",
      "            320,    679,     15,      8,   4476,  24686,    369,  34011,   5768,\n",
      "           6427,    449,    198,  35353,  10803,  71764,   5450,     25,  85438,\n",
      "            287,    708,     69,   1820,     43,  67713,    679,     15,   6919,\n",
      "           8845,    263,   3648,   1163,    278,   7058,     75,    833,    288,\n",
      "           2000,     45,  12852,  90715,     11,   2818,   5726,     11,   2257,\n",
      "           1169,   2629,  28112,  69151,     11,    604,   1774,   4235,   1135,\n",
      "            198,     49,    380,    437,   6780,     47,     11,  26368,  21215,\n",
      "             39,      7,    679,     21,      8,     49,   3013,     17,   4175,\n",
      "             25,     49,   3013,   4539,  12529,  25624,   8350,    266,    309,\n",
      "           5859,    627,    644,     25,  34746,  99031,   6109,  92348,  21861,\n",
      "           2702,    261,     11,    604,  21962,   4235,  20998,    198,   1016,\n",
      "            278,  46434,     32,     11,  12289,   1303,    261,     32,      7,\n",
      "            679,     21,      8,   2732,  23366,    263,     54,  15288,  69761,\n",
      "          71839,  15777,    198,  11772,    685,  57036,   2000,  16206,   5450,\n",
      "          75145,  99031,   6109,     25,   1600,  26538,    679,     21,  35982,\n",
      "          18652,    198,   8059,     11,   1163,    309,     11,    604,  14206,\n",
      "           4235,   8273,    198,  91635,  43588,     51,  50640,    301,   2067,\n",
      "             41,  24412,   1142,    301,     50,  38406,  64151,   1291,  27887,\n",
      "           8324,   3102,    569,     38,      7,    679,     21,      8,  32237,\n",
      "          12529,   7058,  25624,    369,   4382,   2723,  20212,     13,    763,\n",
      "             25,   7327,  15217,    389,  13257,    198,  48567,     11,    604,\n",
      "          12060,     16,   4235,  12171,     15,    198,     54,    526,     48,\n",
      "          50640,    526,     33,     11,  17198,     78,     43,      7,    679,\n",
      "             20,      8,  81434,   3231,  44412,    985,  12529,  25624,    198,\n",
      "            438,  22746,   5450,  58255,     41,   5158,     40,     11,    604,\n",
      "           9741,     24,   4235,   9714,     20,    198,     54,    526,   1901,\n",
      "          51932,  21313,    622,  28328,    833,    622,     11,   1163,    268,\n",
      "           1901,    320,    679,     19,     64,      8,  33025,   4876,    323,\n",
      "           1495,  53258,    198,  95711,   5450,     25,   2783,     45,  12852,\n",
      "          11541,    275,   2423,    261,     11,    604,  11068,     16,   4235,\n",
      "           6330,     16,    198,     54,    526,   1901,     11,  37120,    622,\n",
      "             11,  43758,    622,     11,  25507,   1901,    320,    679,     19,\n",
      "             65,      8,  33025,   4876,  40188,    555,    198,   1485,     75,\n",
      "           1113,    263,  69292,  39157,   5450,     25,   6157,  15836,  11541,\n",
      "            275,   2423,    261,     11,    604,   5037,     17,   4235,   5037,\n",
      "             24,    198,     55,  23332,     39,  44639,  69710,     44,  44639,\n",
      "           3524,     56,  51932,  17156,     55,      7,    679,     20,      8,\n",
      "           3246,     70,  56748,   7642,  20053,    336,  13025,   2590,    198,\n",
      "          45838,  52286,   4539,  95711,  17126,     55,    344,   1762,   1374,\n",
      "            277,     55,    344,     25,   3965,  22393,  21310,    198,  76065,\n",
      "          11865,  28365,  10602,     45,  51932,  18664,     44,  24412,    318,\n",
      "          23263,      7,    679,     19,      8,  42097,  23013,  22317,   2788,\n",
      "          25624,   2000,    198,  90153,   6108,   3488,  36864,     13,    763,\n",
      "             25,  55227,    315,    279,    220,    679,     19,  49843,   7058,\n",
      "            768,    389,  46763,   5528,    304,   5933,   4221,   8863,    320,\n",
      "           2783,     45,  12852,    705,  12086,    198,  22926,   4235,  13655,\n",
      "         128009, 128006,    882, 128007,    271,   7184,     11,   2728,    420,\n",
      "           3488,     25,   3639,    527,    279,    836,    315,  30525,   1511,\n",
      "            304,    279,   5684,   4710,    220,  21335,   1203,    279,   4320,\n",
      "           1193,    304,    264,  13325,   1160,   3645,     11,    369,   3187,\n",
      "             25,   2570,     32,   1882,     33,   7352,   1442,    499,   1541,\n",
      "            956,   1440,    279,   4320,     11,   1120,    471,    459,   4384,\n",
      "           1160,     13, 128009, 128006,  78191, 128007,    271,   1318, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " []\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Expeditious Generation of Knowledge Graph\n",
      "Embeddings\n",
      "TommasoSoru,StefanoRuberto,DiegoMoussallem,AndréValdestilhas,\n",
      "AlexanderBigerl,EdgardMarxandDiegoEsteves\n",
      "Thisworkwasacceptedforpresentationatthe5thEuropeanConference\n",
      "on Data Analysis (ECDA 2018) under the title “A Simple and Fast Ap-\n",
      "proachtoKnowledgeGraphEmbedding”.\n",
      "Abstract Knowledge Graph Embedding methods aim at representing enti-\n",
      "ties and relations in a knowledge base as points or vectors in a continuous\n",
      "vector space. Several approaches using embeddings have shown promising\n",
      "results on tasks such as link prediction, entity recommendation, question an-\n",
      "swering,andtripletclassification.However,onlyafewmethodscancompute\n",
      "low-dimensionalembeddingsofverylargeknowledgebaseswithoutneeding\n",
      "TommasoSoru,DiegoMoussallem,AndréValdestilhas,EdgardMarx\n",
      "AKSW,UniversityofLeipzig,Germany\n",
      "(cid:0)tsoru,moussallem,valdestilhas,marx@informatik.uni-leipzig.de\n",
      "StefanoRuberto\n",
      "UniversityofPennsylvania,UnitedStates\n",
      "(cid:0)stefano.ruberto@pennmedicine.upenn.edu\n",
      "AlexanderBigerl\n",
      "DICE,PaderbornUniversity,Germany\n",
      "(cid:0)alexander.bigerl@uni-paderborn.de\n",
      "DiegoEsteves\n",
      "SDA,UniversityofBonn,Germany\n",
      "(cid:0)esteves@cs.uni-bonn.de\n",
      "ARCHIVES OF DATA SCIENCE, SERIES A\n",
      "(ONLINE FIRST) DOI10.5445/KSP/XXXXXXXX/XX\n",
      "KIT SCIENTIFIC PUBLISHING ISSN2363-9881\n",
      "Vol.-,No.-,-\n",
      "8102\n",
      "voN\n",
      "9\n",
      "]LC.sc[\n",
      "2v82870.3081:viXra\n",
      "2 TommasoSoruetal.\n",
      "state-of-the-artcomputationalresources.Inthispaper,weproposeKG2Vec,a\n",
      "simpleandfastapproachtoKnowledgeGraphEmbeddingbasedontheskip-\n",
      "grammodel.Insteadofusingapredefinedscoringfunction,welearnitrelying\n",
      "onLongShort-TermMemories.Weshowthatourembeddingsachieveresults\n",
      "comparablewiththemostscalableapproachesonknowledgegraphcompletion\n",
      "aswellasonanewmetric.Yet,KG2Veccanembedlargegraphsinlessertime\n",
      "by processing more than 250 million triples in less than 7 hours on common\n",
      "hardware.\n",
      "1 Introduction\n",
      "Recently,the number of public datasets in the Linked Data cloud has signifi-\n",
      "cantly grown to almost 10 thousands. At the time of writing, at least four of\n",
      "thesedatasetscontainmorethanonebilliontripleseach.1 Thishugeamountof\n",
      "availabledatahasbecomeafertilegroundforMachineLearningandDataMin-\n",
      "ingalgorithms.Today,applicationsofmachine-learningtechniquescomprisea\n",
      "broadvarietyofresearchareasrelatedtoLinkedData,suchasLinkDiscovery,\n",
      "NamedEntityRecognition,andStructuredQuestionAnswering.Thefieldof\n",
      "Knowledge Graph Embedding (KGE) has emerged in the Machine Learning\n",
      "communityduringthelastfiveyears. TheunderlyingconceptofKGEisthat\n",
      "in a knowledge base, each entity and relation can be regarded as a vector in\n",
      "a continuous space. The generated vector representations can be used by al-\n",
      "gorithms employing machine learning, deep learning, or statistical relational\n",
      "learning to accomplish a given task. Several KGE approaches have already\n",
      "shownpromisingresultsontaskssuchaslinkprediction,entityrecommenda-\n",
      "tion,questionanswering,andtripletclassification(Xiaoetal,2015;Linetal,\n",
      "2015a,b; Nickel et al, 2016). Moreover, Distributional Semantics techniques\n",
      "(e.g.,Word2VecorDoc2Vec)arerelativelynewintheSemanticWebcommunity.\n",
      "TheRDF2Vecapproaches(RistoskiandPaulheim,2016;Cochezetal,2017a)\n",
      "are examples of pioneering research and to date, they represent the only op-\n",
      "tionforlearningembeddingsonalargeknowledgegraphwithouttheneedfor\n",
      "state-of-the-arthardware.Tothisend,wedevisetheKG2Vecapproach,which\n",
      "comprisesskip-gramtechniquesforcreatingembeddingsonlargeknowledge\n",
      "graphs in a feasible time but still maintaining the quality of state-of-the-art\n",
      "embeddings.OurevaluationshowsthatKG2Vecachievesavectorqualitycom-\n",
      "1http://lodstats.aksw.org\n",
      "ExpeditiousGenerationofKnowledgeGraphEmbeddings 3\n",
      "parabletothemostscalableapproachesandcanprocessmorethan250million\n",
      "triplesinlessthan7hoursonamachinewithsuboptimalperformances.\n",
      "2 RelatedWork\n",
      "Anearlyefforttoautomaticallygeneratefeaturesfromstructuredknowledge\n",
      "was proposed in (Cheng et al, 2011). RESCAL (Nickel et al, 2011) is a\n",
      "relational-learning algorithm based on Tensor Factorization using Alternat-\n",
      "ing Least-Squares which has showed to scale to large RDF datasets such as\n",
      "YAGO(Nickel et al, 2012) and reach good results in the tasks of link predic-\n",
      "tion,entityresolution,orcollectiveclassification(Nickeletal,2014).Manifold\n",
      "approacheswhichrelyontranslationshavebeenimplementedsofar(Bordes\n",
      "et al, 2013; Wang et al, 2014b; Jia et al, 2015; Lin et al, 2015b; Wang et al,\n",
      "2015;Xiaoetal,2015).TransEisthefirstmethodwhererelationshipsarein-\n",
      "terpretedastranslationsoperatingonthelow-dimensionalembeddingsofthe\n",
      "entities (Bordes et al,2013). On the other hand,TransH models a relation as\n",
      "a hyperplane together with a translation operation on it (Wang et al, 2014b).\n",
      "TransAexploresembeddingmethodsforentitiesandrelationsbelongingtotwo\n",
      "differentknowledgegraphsfindingtheoptimallossfunction(Jiaetal,2015),\n",
      "whilstPTransEreliesonpathstobuildthefinalvectors(Linetal,2015a).The\n",
      "algorithmsTransRandCTransRproposedinLinetal(2015b)aimatbuilding\n",
      "entityandrelationembeddingsinseparateentityspaceandrelationspaces,so\n",
      "astolearnembeddingsthroughprojectedtranslationsintherelationspace;an\n",
      "extensionofthisalgorithmmakesuseofrulestolearnembeddings(Wangetal,\n",
      "2015).Anefforttojointlyembedstructuredandunstructureddata(suchastext)\n",
      "wasproposedinWangetal(2014a).TheideabehindtheDistMultapproachis\n",
      "toconsiderentitiesaslow-dimensionalvectorslearnedfromaneuralnetwork\n",
      "and relations as bilinear and/or linear mapping functions Yang et al (2014).\n",
      "TransG,agenerativemodeladdresstheissueofmultiplerelationsemanticsof\n",
      "arelation,hasshowedtogobeyondstate-of-the-artresults(Xiaoetal,2015).\n",
      "ComplExisbasedonlatentfactorizationand,withtheuseofcomplex-valued\n",
      "embeddings, it facilitates composition and handles a large variety of binary\n",
      "relations Trouillon et al (2016). The fastText algorithm was meant for word\n",
      "embeddings,however Joulin et al (2017) showed that a simple bag-of-words\n",
      "cangeneratesurprisinglygoodKGEs.\n",
      "4 TommasoSoruetal.\n",
      "The field of KGE has considerably grown during the last two years, earn-\n",
      "ing a spotalso in the SemanticWebcommunity. In 2016,Nickelet al (2016)\n",
      "proposedHolE,whichreliesonholographicmodelsofassociativememoryby\n",
      "employing circularcorrelation to create compositional representations. HolE\n",
      "cancapturerichinteractionsbyusingcorrelationasthecompositionaloperator\n",
      "butitsimultaneouslyremainsefficienttocompute,easytotrain,andscalable\n",
      "to large datasets. In the same year, Ristoski and Paulheim (2016) presented\n",
      "RDF2Vecwhichuseslanguagemodelingapproachesforunsupervisedfeature\n",
      "extraction from sequences of words and adapts them to RDF graphs. After\n",
      "generatingsequencesbyleveraginglocalinformationfromgraphsubstructures\n",
      "byrandomwalks,RDF2Veclearnslatentnumericalrepresentationsofentities\n",
      "inRDFgraphs.Thealgorithmhasbeenextendedinordertoreducethecompu-\n",
      "tationaltimeandthebiasedregardedtherandomwalking(Cochezetal,2017a).\n",
      "Morerecently,Cochezetal(2017b)exploitedtheGlobalVectorsalgorithmto\n",
      "compute embeddings from the co-occurrence matrix of entities and relations\n",
      "withoutgeneratingtherandomwalks.Infollowingresearch,theauthorsrefer\n",
      "totheiralgorithmasKGloVe.2\n",
      "3 KG2Vec\n",
      "Thisstudyaddressesthefollowingresearchquestions:\n",
      "1. Canwegenerateembeddingsatahighratewhilepreservingaccuracy?\n",
      "2. HowcanwetestthedistributionalhypothesisofKGEs?\n",
      "3. Canwelearnascoringfunctionforknowledgebasecompletionwhichper-\n",
      "formsbetterthanthestandardone?\n",
      "Formally,lett =(s,p,o)beatriplecontainingasubject,apredicate,andan\n",
      "objectinaknowledgebaseK.Foranytriple,(s,p,o)⊆E×R×(E∩L),where\n",
      "E is the set of all entities, R is the set of all relations, and L is the set of all\n",
      "literals(i.e.,stringornumericalvalues).ArepresentationfunctionF definedas\n",
      "F :(E∩R∩L)→Rd (1)\n",
      "assigns a vectorof dimensionality d to an entity,a relation,ora literal. How-\n",
      "ever, some approaches consider only the vector representations of entities or\n",
      "2https://datalab.rwth-aachen.de/embedding/KGloVe/\n",
      "ExpeditiousGenerationofKnowledgeGraphEmbeddings 5\n",
      "subjects (i.e, {s∈E :∃(s,p,o)∈K}). For instance, in approaches based on\n",
      "Tensor Factorization, given a relation, its subjects and objects are processed\n",
      "andtransformedintosparsematrices;allthematricesarethencombinedintoa\n",
      "tensorwhosedepthisthenumberofrelations.Forthefinalembedding,current\n",
      "approachesrelyondimensionalityreductiontodecreasetheoverallcomplex-\n",
      "ity(Nickeletal,2014;Jiaetal,2015;Linetal,2015b).Thereductionisper-\n",
      "formedthroughanembeddingmapΦ :Rd →Rk,whichisahomomorphism\n",
      "that maps the initial vector space into a smaller, reduced space. The positive\n",
      "valuek<d iscalledtherank oftheembedding.Notethateachdimensionof\n",
      "the reduced common space does not necessarily have an explicit connection\n",
      "withaparticularrelation.DimensionalityreductionmethodsincludePrincipal\n",
      "Component Analysis techniques (Nickel et al, 2014) and generative statisti-\n",
      "cal models such as Latent Dirichlet Allocation (Jurgens and Stevens, 2010;\n",
      "Rˇehu˚ˇrekandSojka,2010).\n",
      "ExistingKGEapproachesbasedontheskip-grammodelsuchasRDF2Vec(Ris-\n",
      "toskiandPaulheim,2016)submitpathsbuiltusingrandomwalkstoaWord2Vec\n",
      "algorithm.Instead,wepreprocesstheinputknowledgebasebyconvertingeach\n",
      "tripleintoasmallsentenceofthreewords.Ourmethodisfasterasitallowsus\n",
      "toavoidthepathgenerationstep.Thegeneratedtextcorpusisthusprocessed\n",
      "bytheskip-grammodelasfollows.\n",
      "3.1 Adaptingtheskip-grammodel\n",
      "We adapt the skip-gram model (Mikolov et al,2013a) to deal with our small\n",
      "sequences of length three. In this work, we only consider URIs and discard\n",
      "literals,thereforewecomputeavectorforeachelementu∈E∩R.Considering\n",
      "a triple as a sequence of three URIs T ={u,u,u },the aim is to maximize\n",
      "s p o\n",
      "theaveragelogprobability\n",
      "1\n",
      "∑ ∑ logp(u|u(cid:48)) (2)\n",
      "3\n",
      "u∈Tu(cid:48)∈T\\u\n",
      "whichmeans,inotherwords,toadoptacontextwindowof2,sincethesequence\n",
      "sizeisalways|T|=3.Theprobabilityaboveistheoreticallydefinedas:\n",
      "exp(vO(cid:62) vI )\n",
      "p(u|u(cid:48))= u u(cid:48) (3)\n",
      "∑ exp(vO(cid:62) vI )\n",
      "x∈E∩R x u(cid:48)\n",
      "6 TommasoSoruetal.\n",
      "wherevI andvO arerespectivelytheinputandoutputvectorrepresentationsof\n",
      "x x\n",
      "aURIx.Weimplyanegativesamplingof5,i.e.5wordsarerandomlyselected\n",
      "tohaveanoutputof0andconsequentlyupdatetheweights.\n",
      "3.2 Scoringfunctions\n",
      "3.2.1 Scoringbyanalogy\n",
      "Severalmethodshavebeenproposedtoevaluatewordembeddings.Themost\n",
      "commononesarebasedonanalogies(Mikolovetal,2013b;LevyandGoldberg,\n",
      "2014),wherewordvectorsaresummeduptogether,e.g.:\n",
      "v[”queen”]≈v[”king”]+v[”woman”]−v[”man”] (4)\n",
      "Ananalogywheretheapproximationaboveissatisfiedwithinacertainthresh-\n",
      "oldcanthuspredicthiddenrelationshipsamongwords,whichinourenviron-\n",
      "mentmeanstopredictnewlinksamongentitiesRistoskiandPaulheim(2016).\n",
      "The analogy-based score function for a given triple (s¯,p¯,o¯) is defined as fol-\n",
      "lows.\n",
      "(cid:40)\n",
      "1 1 if (cid:107)v +v −v −v (cid:107)≤ε\n",
      "score(s¯,p¯,o¯)= ∑ s¯ o s o¯ (5)\n",
      "|{(s,p¯,o)∈K}| 0 otherwise\n",
      "(s,p¯,o)∈K\n",
      "whereε isanarbitrarilysmallpositivevalue.Inwords,givenapredicate p¯,we\n",
      "selectalltripleswhereitoccurs.Foreachtriple,wecomputetherelationvector\n",
      "asthedifferencebetweentheobjectandthesubjectvectors. Wethencounta\n",
      "match whenever the vector sum of subject s¯and relation is close to object o¯\n",
      "withinaradiusε.Thescoreisequaltotherateofmatchesoverthenumberof\n",
      "selectedtriples.\n",
      "3.2.2 Scoringbyneuralnetworks\n",
      "We evaluate the scoring function above against a neural network based on\n",
      "LongShort-TermMemories(LSTM).Theneuralnetworktakesasequenceof\n",
      "embeddingsasinput,namelyv,v,v foratriple(s,p,o)∈K.Adensehidden\n",
      "s p o\n",
      "layerofthesamesizeoftheembeddingsisconnectedtoasingleoutputneuron\n",
      "withsigmoidactivation,whichreturnsavaluebetween0and1.Thenegative\n",
      "ExpeditiousGenerationofKnowledgeGraphEmbeddings 7\n",
      "Fig.1 AselectionofDB-\n",
      "pediaresourcesalongwith\n",
      "theirvectors in 3 dimen-\n",
      "sionsobtainedusingPrincipal\n",
      "ComponentAnalysis.Blue\n",
      "pointsareresources,whilst\n",
      "redpointsareclasses.Ascan\n",
      "beseen,resourcesfollowthe\n",
      "distributionalhypothesis.\n",
      "triplesaregeneratedusingtwostrategies,i.e.foreachtripleinthetrainingset\n",
      "(1)randomlyextractarelationanditstwonodesor(2)corruptthesubjector\n",
      "theobject.WeusetheAdamoptimizerand100epochsoftraining.\n",
      "3.3 Metrics\n",
      "AsrecentlyhighlightedbyseveralmembersoftheMLandNLPcommunities,\n",
      "KGEsarerarelyevaluatedondownstreamtasksdifferentfromlinkprediction\n",
      "(alsoknownasknowledgebasecompletion).Achievinghighperformanceson\n",
      "link prediction does not necessarily mean that the generated embeddings are\n",
      "good,sincetheinferencetaskisoftencarriedoutincombinationwithanexter-\n",
      "nalalgorithmsuchasaneuralnetworkorascoringfunction.Thecomplexityis\n",
      "thusapproach-dependentanddistributedbetweenthelatentstructureinthevec-\n",
      "tormodelandtheparameters(ifany)oftheinferencealgorithm.Forinstance,\n",
      "atranslationalmodelsuchasTransEBordesetal(2013)wouldlikelyfeature\n",
      "verycomplexembeddings,sinceinmostapproachestheinferencefunctionis\n",
      "a simple addition. On the other hand, we may find less structure in a tensor\n",
      "factorization model such as RESCAL Nickel et al (2011),as the inference is\n",
      "performed by a feed-forward neural network which extrapolates the hidden\n",
      "semanticslayerbylayer.\n",
      "8 TommasoSoruetal.\n",
      "3.3.1 NeighbourSimilarityTest\n",
      "In this paper, we introduce two metrics inspired by The Identity of Indis-\n",
      "cernibles (Black, 1952) to gain insights over the distributional quality of the\n",
      "learnedembeddings.\n",
      "Themorecharacteristicstwoentitiesshare,themoresimilartheyareand\n",
      "soshouldbetheirvectorrepresentations.\n",
      "Considering the setofcharacteristicsC (s)={(p,o ),...,(p,o )} ofa\n",
      "K 1 1 m m\n",
      "subjectsinatriple,wecandefineametricthatexpressesthesimilarityamong\n",
      "two entities e,e as the Jaccard index between their sets of characteristics\n",
      "1 2\n",
      "C (e )andC (e ).GivenasetofentitiesE˜ andtheirN nearestneighboursin\n",
      "K 1 K 2\n",
      "thevectorspace,theoverallNeighbourSimilarityTest(NST)metricisdefined\n",
      "as:\n",
      "(e)\n",
      "NST(E˜,N,K)=\n",
      "1\n",
      "∑\n",
      "∑N |C K(e)∩C K(n\n",
      "j\n",
      ")|\n",
      "(6)\n",
      "N|E˜|\n",
      "e∈E˜ j=1|C K(e)∪C\n",
      "K(n( je)\n",
      ")|\n",
      "(e)\n",
      "wheren isthe jthnearestneighbourofeinthevectorspace.\n",
      "j\n",
      "3.3.2 TypeandCategoryTest\n",
      "ThesecondmetricistheTypeandCategoryTest(TCT),basedontheassump-\n",
      "tion that two entities which share types and categories should be close in\n",
      "the vector space. This assumption is suggested by the human bias for which\n",
      "rdf:type and dct:subject would be predicates with a higher weight\n",
      "thantheothers.Althoughthisdoesnothappen,wecomputeitforameresake\n",
      "ofcomparisonwiththeNSTmetric.TheTCTformulaisequaltoEquation6ex-\n",
      "ceptforsetsC (e),whicharereplacedbysetsoftypesandcategoriesTC (e).\n",
      "K K\n",
      "4 Evaluation\n",
      "WeimplementedKG2VecinPython2.7usingtheGensimandKeraslibraries\n",
      "withTheanoenvironment.Sourcecode,datasets,andvectorsobtainedareavail-\n",
      "ExpeditiousGenerationofKnowledgeGraphEmbeddings 9\n",
      "ableonline.3 AllexperimentswerecarriedoutonanUbuntu16.04serverwith\n",
      "128GBRAMand40CPUs.\n",
      "Table1 DetailsandruntimesforthegenerationofKG2Vecembeddingsontwodatasets.\n",
      "DBpedia\n",
      "Dataset AKSW-bib DBpedia2016-04\n",
      "2015-10\n",
      "Numberoftriples 3922 164,369,887 276,316,003 276,316,003\n",
      "Numberofvectors 954 14,921,691 23,816,469 36,596,967\n",
      "Dimensionality 10 300 200 200\n",
      "Runtime(s) 2.2 18,332 25,380 46,099\n",
      "Rate(triples/s) 1,604 8,966 10,887 5,994\n",
      "ThedatasetusedintheexperimentsaredescribedinTable1.TheAKSW-bib\n",
      "dataset–employedforthelinkpredictionevaluation–wascreatedusinginfor-\n",
      "mationfrompeopleandprojectsontheAKSW.orgwebsiteandbibliographical\n",
      "datafromBibsonomy.WebuiltamodelontopoftheEnglish2015-10version\n",
      "oftheDBpediaknowledgegraph(Lehmann etal,2009); Figure1showsa3-\n",
      "dimensionalplotofselectedentities.FortheEnglishDBpedia2016-04dataset,\n",
      "webuilttwomodels.Inthefirst,wesetathresholdtoembedonlytheentities\n",
      "occurring at least 5 times in the dataset; we chose this setting to be aligned\n",
      "to the related works’ models. In the second model, all 36 million entities in\n",
      "DBpedia are associated a vector. More insights about the first model can be\n",
      "foundinthenexttwosubsections,whiletheresourceconsumptionforcreating\n",
      "thesecondmodelcanbeseeninFigure5.\n",
      "4.1 Runtime\n",
      "Inthisstudy,weaimatgeneratingembeddingsatahighratewhilepreserving\n",
      "accuracy.InTable1,wealreadyshowedthatoursimplepipelinecanachieve\n",
      "arateofalmost11,000triplespersecondonalargedatasetsuchasDBpedia\n",
      "2016-04.InTable2,wecompareKG2Vecwiththreeotherscalableapproaches\n",
      "for embedding knowledge bases. We selected the best settings of RDF2Vec\n",
      "and KGloVe according to their respective articles, since both algorithms had\n",
      "alreadybeensuccessfullyevaluatedonDBpedia(RistoskiandPaulheim,2016;\n",
      "3http://github.com/AKSW/KG2Vec\n",
      "10 TommasoSoruetal.\n",
      "Cochez et al, 2017b). We also tried to compute fastText embeddings on our\n",
      "machine, however we had to halt the process after three days. As the goal\n",
      "of our investigation is efficiency,we discarded any other KGE approach that\n",
      "would have needed more than three days of computation to deliver the final\n",
      "model(Cochezetal,2017b).\n",
      "RDF2Vec has shown to be the most expensive in terms of disk space con-\n",
      "sumed,asthecreatedrandomwalksamountedto∼300GBoftext.Moreover,\n",
      "we could not measure the runtime for the first phase of KGloVe, i.e. the cal-\n",
      "culationofthePersonalizedPageRankvaluesofDBpediaentities.Infact,the\n",
      "authorsusedpre-computedentityranksfromThalhammerandRettinger(2016)\n",
      "andtheKGloVesourcecodedoesnotfeatureaPageRankalgorithm. Weesti-\n",
      "mated the runtime comparing their hardware specs with ours. Despite being\n",
      "unabletoreproduceanyexperimentsfromtheotherthreeapproaches,weman-\n",
      "agedtoevaluatetheirembeddingsbydownloadingthepretrainedmodels4 and\n",
      "creating a KG2Vec embedding model of the same DBpedia dataset there em-\n",
      "ployed.\n",
      "Table2 Runtimecomparisonofthesinglephases.Thosewith(*)areestimatedruntimes.\n",
      "ApproachSteps Time\n",
      "Randomwalksgeneration 123minutes\n",
      "RDF2Vec\n",
      "Word2Vectraining >96hours(*)\n",
      "PersonalizedPageRank N/A\n",
      "KGloVe Co-occurrencecountmatrix\n",
      "12hours(*)\n",
      "GloVetraining\n",
      "Conversiontotext 5minutes\n",
      "KG2Vec\n",
      "Word2Vectraining 6hours58minutes\n",
      "Conversiontotext 5minutes\n",
      "fastText\n",
      "fastTexttraining >72hours(*)\n",
      "4.2 Preliminaryresultsonlinkprediction\n",
      "For the link prediction task,we partition the dataset into training and test set\n",
      "witharatioof9:1.InTable3,weshowpreliminaryresultsbetweenthedifferent\n",
      "strategies on the AKSW-bib dataset using KG2Vec embeddings. As can be\n",
      "seen,ourLSTM-basedscoringfunctionsignificantlyoutperformstheanalogy-\n",
      "based one in both settings. According to the Hits@10 accuracy we obtained,\n",
      "4http://data.dws.informatik.uni-mannheim.de/rdf2vec/\n",
      "ExpeditiousGenerationofKnowledgeGraphEmbeddings 11\n",
      "corrupting triples to generate negative examples is the better strategy. This\n",
      "firstinsightcanfosternewresearchonoptimizingascoringfunctionforKGE\n",
      "approachesbasedondistributionalsemantics.\n",
      "Table3 FilteredHits@10valuesonlinkpredictiononAKSW-bibusingdifferentstrategies.\n",
      "Hits@1 Hits@3 Hits@10\n",
      "LSTM+corrupted 3.84% 9.79% 19.23%\n",
      "LSTM+random 1.39% 4.89% 10.49%\n",
      "Analogy 0.00% 0.51% 3.82%\n",
      "4.3 Distributionalquality\n",
      "ComputingtheNSTandTCTdistributionalqualitymetricsontheentireDB-\n",
      "pediadatasetistime-demanding,sinceforeachentity,themodelandthegraph\n",
      "needtobequeriedfortheN nearestneighboursandtheirrespectivesets.How-\n",
      "ever,weapproximatethefinalvaluebytracingthepartialvaluesofNSTand\n",
      "TCTovertime.Inotherwords,ateachiterationi,wecomputethemetricsover\n",
      "E˜ ={e,...,e}.Figure2showsthepartialTCTvalueonthemostimportant\n",
      "i 1 i\n",
      "10,000entitiesforN ={1,10}accordingtotherankscomputedbyThalham-\n",
      "merandRettinger(2016).Here,KG2Vecmaintainsahigherindexthantheother\n",
      "twoapproaches,despitethesearesteadilyincreasingafterthe∼2,000thentity.\n",
      "WeinterpretthelowerTCTforthetop2,000entitiesasnoiseproducedbythe\n",
      "factthatthesenodesarehyperconnectedtotherestofthegraph,thereforeitis\n",
      "hardforthemtoremainclosetotheirtypepeers.InFigures3and4,theTCT\n",
      "andNSTmetricsrespectivelyarecomputedon10,000randomentities.Inboth\n",
      "cases, the values for the two settings of all approaches stabilize after around\n",
      "1,000entities,howeverweclearlyseethatRDF2Vecembeddingsachievethe\n",
      "highestdistributionalquality by type andcategory. The highernumberofoc-\n",
      "currencesperentityinthehugecorpusofrandomwalksinRDF2Vecmightbe\n",
      "thereasonofthisresultforrarerentities.\n",
      "InFigure5,weshowtheCPU,Memory,anddiskconsumptionforKG2Vec\n",
      "onthelargermodelofDBpedia2016-04.Allthreesubphasesofthealgorithm\n",
      "are visible in the plot. For 2.7 hours, tokens are counted; then, the learning\n",
      "proceedsfor7.7hours;finallyinthelast2.3hours,themodelissaved.\n",
      "12 TommasoSoruetal.\n",
      "0.3\n",
      "0.25 0.2\n",
      "0.15\n",
      "0.1\n",
      "0.05\n",
      "0\n",
      "0 2000 4000 6000 8000 10000\n",
      "Iteration\n",
      "xednI\n",
      "laitraP\n",
      "1\n",
      "0.8\n",
      "KGloVe, N=10 0.6\n",
      "RDF2Vec, N=10\n",
      "KG2Vec, N=10 0.4\n",
      "KGloVe, N=1\n",
      "RDF2Vec, N=1\n",
      "KG2Vec, N=1 0.2\n",
      "0\n",
      "0 2000 4000 6000 8000 10000\n",
      "Iteration\n",
      "Fig. 2 PartialTCT value on DBpedia 2016-04\n",
      "forthetop10,000entities.\n",
      "xednI\n",
      "laitraP\n",
      "KGlove,n=10\n",
      "RDF2Vec,n=10\n",
      "KG2Vec,n=10 KGlove,n=1 RDF2Vec,n=1\n",
      "KG2Vec,n=1\n",
      "Fig. 3 PartialTCT value on DBpedia 2016-04\n",
      "for10,000randomentities.\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "0\n",
      "0 2000 4000 6000 8000 10000\n",
      "Iteration\n",
      "xednI\n",
      "laitraP\n",
      "KGlove, N=10 120000000 RAM (GB) 200 GB\n",
      "RDF2Vec, N=10 Disk (GB)\n",
      "KG2Vec, N=10 100000000 CPU (%)\n",
      "KGlove, N=1 150 GB\n",
      "RDF2Vec, N=1 80000000\n",
      "KG2Vec, N=1\n",
      "60000000 100 GB\n",
      "40000000\n",
      "50 GB\n",
      "20000000\n",
      "0 0 GB\n",
      "0 2 4 6 8 10 12 14\n",
      "Fig.5 CPU,Memory,anddiskconsumptionfor\n",
      "Fig. 4 PartialNST value on DBpedia 2016-04\n",
      "KG2VeconthelargermodelofDBpedia2016-04.\n",
      "for10,000randomentities.\n",
      "100\n",
      "Fig.6 Weshowthecompar-\n",
      "80\n",
      "isonoftherun-timesforall\n",
      "fourapproaches.Notethat 60\n",
      "sincewedonotknowhow\n",
      "40\n",
      "longthePageRankcompu-\n",
      "tationtakes,wereportedthe 20\n",
      "estimatedruntimefortheplain\n",
      "0\n",
      "versionofKGloVe. RDF2Vec fastText KGloVe KG2Vec\n",
      ")\n",
      "sruoh\n",
      "(\n",
      "emiT\n",
      "5 ConclusionandFutureWork\n",
      "We presenteda fastapproachforgenerating KGEs dubbedKG2Vec. We con-\n",
      "cludethattheskip-grammodel,iftraineddirectlyontriplesassmallsentences\n",
      "oflengththree,significantlygainsinruntimewhilepreservingadecentvector\n",
      "quality. Moreover,theKG2Vecembeddingshaveshownhigherdistributional\n",
      "quality for the most important entities in the graph according to PageRank.\n",
      "As a future work, we plan to extend the link prediction evaluation to other\n",
      "benchmarksbyusinganalogiesandourLSTM-basedscoringfunctionoverthe\n",
      "embeddingmodelsoftheapproachesherecompared.\n",
      "ExpeditiousGenerationofKnowledgeGraphEmbeddings 13\n",
      "References\n",
      "BlackM(1952)Theidentityofindiscernibles.Mind61(242):153–164\n",
      "BordesA,UsunierN,Garcia-DuranA,WestonJ,YakhnenkoO(2013)Translat-\n",
      "ingembeddingsformodelingmulti-relationaldata.In:AdvancesinNeural\n",
      "InformationProcessingSystems,pp2787–2795\n",
      "ChengW,KasneciG,GraepelT,SternD,HerbrichR(2011)Automatedfeature\n",
      "generationfromstructuredknowledge.In:CIKM,ACM,pp1395–1404\n",
      "CochezM,RistoskiP,PonzettoSP,PaulheimH(2017a)Biasedgraphwalksfor\n",
      "rdfgraphembeddings.In:Proceedingsofthe7thInternationalConference\n",
      "onWebIntelligence,MiningandSemantics,ACM,p21\n",
      "Cochez M, Ristoski P, Ponzetto SP, Paulheim H (2017b) Global rdf vector\n",
      "spaceembeddings.In:InternationalSemanticWebConference,Springer,pp\n",
      "190–207\n",
      "JiaY,WangY,LinH,JinX,ChengX(2015)Locallyadaptivetranslationfor\n",
      "knowledgegraphembedding.arXivpreprintarXiv:151201370\n",
      "JoulinA,GraveE,BojanowskiP,NickelM,MikolovT(2017)Fastlinearmodel\n",
      "forknowledgegraphembeddings.arXivpreprintarXiv:171010881\n",
      "JurgensD,StevensK(2010)Thes-spacepackage:anopensourcepackagefor\n",
      "word space models. In: Proceedings of the ACL 2010 System Demonstra-\n",
      "tions,AssociationforComputationalLinguistics,pp30–35\n",
      "LehmannJ,BizerC,KobilarovG,AuerS,BeckerC,CyganiakR,HellmannS\n",
      "(2009)DBpedia-acrystallizationpointforthewebofdata.JournalofWeb\n",
      "Semantics7(3):154–165\n",
      "LevyO,GoldbergY(2014)Linguisticregularitiesinsparseandexplicitword\n",
      "representations. In: Proceedings of the eighteenth conference on computa-\n",
      "tionalnaturallanguagelearning,pp171–180\n",
      "LinY,LiuZ,SunM(2015a)Modelingrelationpathsforrepresentationlearning\n",
      "ofknowledgebases.CoRRabs/1506.00379,URLhttp://arxiv.org/\n",
      "abs/1506.00379\n",
      "Lin Y,Liu Z,Sun M,Liu Y,Zhu X (2015b) Learning entity and relation em-\n",
      "beddingsforknowledgegraphcompletion.In:AAAI,pp2181–2187\n",
      "MikolovT,SutskeverI,ChenK,CorradoGS,DeanJ(2013a)Distributedrep-\n",
      "resentationsofwordsandphrasesandtheircompositionality.In:BurgesC,\n",
      "BottouL,WellingM,GhahramaniZ,WeinbergerK(eds)AdvancesinNeural\n",
      "InformationProcessingSystems26,CurranAssociates,Inc.,pp3111–3119\n",
      "14 TommasoS<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  55982,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Knowledge Graph Embedding', 'Link Prediction', 'Entity Recommendation', 'Question Answering', 'Triples Classification']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: naturallanguagelearning,pp171–180\n",
      "LinY,LiuZ,SunM(2015a)Modelingrelationpathsforrepresentationlearning\n",
      "ofknowledgebases.CoRRabs/1506.00379,URLhttp://arxiv.org/\n",
      "abs/1506.00379\n",
      "Lin Y,Liu Z,Sun M,Liu Y,Zhu X (2015b) Learning entity and relation em-\n",
      "beddingsforknowledgegraphcompletion.In:AAAI,pp2181–2187\n",
      "MikolovT,SutskeverI,ChenK,CorradoGS,DeanJ(2013a)Distributedrep-\n",
      "resentationsofwordsandphrasesandtheircompositionality.In:BurgesC,\n",
      "BottouL,WellingM,GhahramaniZ,WeinbergerK(eds)AdvancesinNeural\n",
      "InformationProcessingSystems26,CurranAssociates,Inc.,pp3111–3119\n",
      "14 TommasoSoruetal.\n",
      "Mikolov T, Yih Wt, Zweig G (2013b) Linguistic regularities in continuous\n",
      "spacewordrepresentations.In:HLT-NAACL,pp746–751\n",
      "NickelM,TrespV,KriegelHP(2011)Athree-waymodelforcollectivelearning\n",
      "onmulti-relationaldata.In:Proceedingsofthe28thinternationalconference\n",
      "onmachinelearning(ICML-11),pp809–816\n",
      "Nickel M, Tresp V, Kriegel HP (2012) Factorizing yago: scalable machine\n",
      "learningforlinkeddata.In:WWW,ACM,pp271–280\n",
      "NickelM,JiangX,TrespV(2014)Reducingtherankinrelationalfactorization\n",
      "modelsbyincludingobservablepatterns.In:AdvancesinNeuralInformation\n",
      "ProcessingSystems,pp1179–1187\n",
      "Nickel M, Rosasco L, Poggio TA, et al (2016) Holographic embeddings of\n",
      "knowledgegraphs.In:AAAI,pp1955–1961\n",
      "Rˇehu˚ˇrek R, Sojka P (2010) Software Framework for Topic Modelling with\n",
      "LargeCorpora.In:ProceedingsoftheLREC2010WorkshoponNewChal-\n",
      "lengesforNLPFrameworks,ELRA,Valletta,Malta,pp45–50\n",
      "RistoskiP,PaulheimH(2016)Rdf2vec:Rdfgraphembeddingsfordatamining.\n",
      "In:InternationalSemanticWebConference,Springer,pp498–514\n",
      "ThalhammerA,RettingerA(2016)PageRankonWikipedia:TowardsGeneral\n",
      "ImportanceScoresforEntities.In:TheSemanticWeb:ESWC2016Satellite\n",
      "Events,Cham,pp227–240\n",
      "TrouillonT,WelblJ,RiedelS,GaussierÉ,BouchardG(2016)Complexembed-\n",
      "dings for simple link prediction. In: International Conference on Machine\n",
      "Learning,pp2071–2080\n",
      "WangQ,WangB,GuoL(2015)Knowledgebasecompletionusingembeddings\n",
      "andrules.In:IJCAI,pp1859–1865\n",
      "Wang Z,Zhang J,Feng J,Chen Z (2014a) Knowledge graph and text jointly\n",
      "embedding.In:EMNLP,Citeseer,pp1591–1601\n",
      "Wang Z, Zhang J, Feng J, Chen Z (2014b) Knowledge graph embedding by\n",
      "translatingonhyperplanes.In:AAAI,Citeseer,pp1112–1119\n",
      "XiaoH,HuangM,HaoY,ZhuX(2015)Transg:Agenerativemixturemodel\n",
      "forknowledgegraphembedding.arXivpreprintarXiv:150905488\n",
      "YangMC,DuanN,ZhouM,RimHC(2014)Jointrelationalembeddingsfor\n",
      "knowledge-based question answering. In: Proceedings of the 2014 confer-\n",
      "ence on empirical methods in natural language processing (EMNLP), pp\n",
      "645–650<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "          14818,    543,   2681,  21656,     11,    604,  11123,   4235,   5245,\n",
      "            198,  51697,     56,  31214,  19260,     57,  21861,    359,     44,\n",
      "              7,    679,     20,     64,      8,   1747,    287,  23013,  22354,\n",
      "           2000,  84216,  21656,    198,   1073,  90153,  79367,  53214,   8268,\n",
      "           3518,     14,   3965,     21,     13,   6268,   4643,     11,   3222,\n",
      "           1277,   1129,    277,  89833,   2726,   6018,   3518,     14,   3965,\n",
      "             21,     13,   6268,   4643,    198,  51697,    816,  31214,  19260,\n",
      "           1901,  21861,    359,    386,  31214,  19260,    816,  51932,  17156,\n",
      "           1630,    320,    679,     20,     65,      8,  21579,   5502,    323,\n",
      "          12976,    991,   7058,   2788,  25624,  45838,  52286,   4539,  44412,\n",
      "           5450,     25,   6157,  15836,     11,    604,  13302,     16,   4235,\n",
      "          13302,     22,    198,     44,   1609,    337,    869,     51,  21861,\n",
      "           6256,    441,    424,     40,     11,   1163,    268,     42,     11,\n",
      "          10803,  77927,  16929,     11,  80966,     41,      7,    679,     18,\n",
      "             64,      8,     35,  26204,  10200,   7058,  13898,    708,     69,\n",
      "           5880,    438,    764,  27663,    438,  50828,  77907,   2786,   5450,\n",
      "             25,  62339,   4282,     34,    345,     33,   1751,    283,     43,\n",
      "          50640,   6427,     44,  38406,     71,   1494,   2453,   5676,     57,\n",
      "             11,   1687,    258,  49120,     42,      7,   6910,      8,  24350,\n",
      "           3095,    258,   8989,   4269,    198,  15218,  29992,  49165,   1627,\n",
      "          11541,    324,   6713,  30915,    988,     11,  40345,   2637,    604,\n",
      "          15134,     16,   4235,  15134,     24,    198,    975,   8529,   7044,\n",
      "          73843,    269,  14127,    278,    627,     44,   1609,    337,    869,\n",
      "            350,     11,    816,   7141,    468,     83,     11,  84226,    343,\n",
      "            480,    320,    679,     18,     65,      8,  89333,   4633,   5912,\n",
      "           1385,    304,  19815,    198,   8920,   1178,  36369,    811,   5450,\n",
      "             25,  81158,     12,   7476,  56493,     11,    604,  25594,   4235,\n",
      "          23986,    198,  31456,    301,     44,  20594,  18744,     53,  44754,\n",
      "          83591,    301,   6748,      7,    679,     16,      8,     32,  28956,\n",
      "          27896,   2658,    491,    269,  17840,    535,  21656,    198,    263,\n",
      "          27364,  48712,   1697,    695,   5450,     25,  85438,    287,    708,\n",
      "             69,   1820,   1591,    339,  98697,  79590,    198,    263,  33156,\n",
      "          21656,  90104,   2735,     12,    806,    705,    604,  21474,   4235,\n",
      "          23713,    198,  31456,    301,    386,     11,    350,  18744,    650,\n",
      "             11,    735,  83591,    301,  12478,    320,    679,     17,      8,\n",
      "          38829,   4954,    379,   6438,     25,  69311,   5780,    198,  21656,\n",
      "           2000,  44233,    695,   5450,     25,  46608,     11,   1741,     44,\n",
      "             11,    604,  15828,   4235,  11209,    198,  31456,    301,     44,\n",
      "          59962,  28323,     55,  20594,  18744,     53,      7,    679,     19,\n",
      "              8,  17020,    287,    700,   1201,    258,   3833,   1697,  38691,\n",
      "           2065,    198,   6644,   1729,  16564,  34595,  27061,   5450,     25,\n",
      "          24350,   3095,    258,   8989,   4269,  15218,    198,  29992,  49165,\n",
      "             11,    604,   8546,     24,   4235,   8899,     22,    198,  31456,\n",
      "            301,    386,     11,  16870,  80457,    445,     11,    393,  16499,\n",
      "            822,  39991,     11,   1880,    453,    320,    679,     21,      8,\n",
      "            473,   1640,  79173,  71647,    315,    198,  90153,  87286,   5450,\n",
      "             25,   6157,  15836,     11,    604,   6280,     20,   4235,   5162,\n",
      "             16,    198,     49,    135,    229,   2701,     84,    135,    248,\n",
      "            135,    229,  42961,    432,     11,   2100,     73,   4657,    393,\n",
      "            320,    679,     15,      8,   4476,  24686,    369,  34011,   5768,\n",
      "           6427,    449,    198,  35353,  10803,  71764,   5450,     25,  85438,\n",
      "            287,    708,     69,   1820,     43,  67713,    679,     15,   6919,\n",
      "           8845,    263,   3648,   1163,    278,   7058,     75,    833,    288,\n",
      "           2000,     45,  12852,  90715,     11,   2818,   5726,     11,   2257,\n",
      "           1169,   2629,  28112,  69151,     11,    604,   1774,   4235,   1135,\n",
      "            198,     49,    380,    437,   6780,     47,     11,  26368,  21215,\n",
      "             39,      7,    679,     21,      8,     49,   3013,     17,   4175,\n",
      "             25,     49,   3013,   4539,  12529,  25624,   8350,    266,    309,\n",
      "           5859,    627,    644,     25,  34746,  99031,   6109,  92348,  21861,\n",
      "           2702,    261,     11,    604,  21962,   4235,  20998,    198,   1016,\n",
      "            278,  46434,     32,     11,  12289,   1303,    261,     32,      7,\n",
      "            679,     21,      8,   2732,  23366,    263,     54,  15288,  69761,\n",
      "          71839,  15777,    198,  11772,    685,  57036,   2000,  16206,   5450,\n",
      "          75145,  99031,   6109,     25,   1600,  26538,    679,     21,  35982,\n",
      "          18652,    198,   8059,     11,   1163,    309,     11,    604,  14206,\n",
      "           4235,   8273,    198,  91635,  43588,     51,  50640,    301,   2067,\n",
      "             41,  24412,   1142,    301,     50,  38406,  64151,   1291,  27887,\n",
      "           8324,   3102,    569,     38,      7,    679,     21,      8,  32237,\n",
      "          12529,   7058,  25624,    369,   4382,   2723,  20212,     13,    763,\n",
      "             25,   7327,  15217,    389,  13257,    198,  48567,     11,    604,\n",
      "          12060,     16,   4235,  12171,     15,    198,     54,    526,     48,\n",
      "          50640,    526,     33,     11,  17198,     78,     43,      7,    679,\n",
      "             20,      8,  81434,   3231,  44412,    985,  12529,  25624,    198,\n",
      "            438,  22746,   5450,  58255,     41,   5158,     40,     11,    604,\n",
      "           9741,     24,   4235,   9714,     20,    198,     54,    526,   1901,\n",
      "          51932,  21313,    622,  28328,    833,    622,     11,   1163,    268,\n",
      "           1901,    320,    679,     19,     64,      8,  33025,   4876,    323,\n",
      "           1495,  53258,    198,  95711,   5450,     25,   2783,     45,  12852,\n",
      "          11541,    275,   2423,    261,     11,    604,  11068,     16,   4235,\n",
      "           6330,     16,    198,     54,    526,   1901,     11,  37120,    622,\n",
      "             11,  43758,    622,     11,  25507,   1901,    320,    679,     19,\n",
      "             65,      8,  33025,   4876,  40188,    555,    198,   1485,     75,\n",
      "           1113,    263,  69292,  39157,   5450,     25,   6157,  15836,  11541,\n",
      "            275,   2423,    261,     11,    604,   5037,     17,   4235,   5037,\n",
      "             24,    198,     55,  23332,     39,  44639,  69710,     44,  44639,\n",
      "           3524,     56,  51932,  17156,     55,      7,    679,     20,      8,\n",
      "           3246,     70,  56748,   7642,  20053,    336,  13025,   2590,    198,\n",
      "          45838,  52286,   4539,  95711,  17126,     55,    344,   1762,   1374,\n",
      "            277,     55,    344,     25,   3965,  22393,  21310,    198,  76065,\n",
      "          11865,  28365,  10602,     45,  51932,  18664,     44,  24412,    318,\n",
      "          23263,      7,    679,     19,      8,  42097,  23013,  22317,   2788,\n",
      "          25624,   2000,    198,  90153,   6108,   3488,  36864,     13,    763,\n",
      "             25,  55227,    315,    279,    220,    679,     19,  49843,   7058,\n",
      "            768,    389,  46763,   5528,    304,   5933,   4221,   8863,    320,\n",
      "           2783,     45,  12852,    705,  12086,    198,  22926,   4235,  13655,\n",
      "         128009, 128006,    882, 128007,    271,   7184,     11,   2728,    420,\n",
      "           3488,     25,   3639,    527,    279,   9256,    430,    279,   1646,\n",
      "            374,  16572,    369,   4710,    220,  21335,   1203,    279,   4320,\n",
      "           1193,    304,    264,  13325,   1160,   3645,     11,    369,   3187,\n",
      "             25,   2570,     32,   1882,     33,   7352,   1442,    499,   1541,\n",
      "            956,   1440,    279,   4320,     11,   1120,    471,    459,   4384,\n",
      "           1160,     13, 128009, 128006,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "          14818,    543,   2681,  21656,     11,    604,  11123,   4235,   5245,\n",
      "            198,  51697,     56,  31214,  19260,     57,  21861,    359,     44,\n",
      "              7,    679,     20,     64,      8,   1747,    287,  23013,  22354,\n",
      "           2000,  84216,  21656,    198,   1073,  90153,  79367,  53214,   8268,\n",
      "           3518,     14,   3965,     21,     13,   6268,   4643,     11,   3222,\n",
      "           1277,   1129,    277,  89833,   2726,   6018,   3518,     14,   3965,\n",
      "             21,     13,   6268,   4643,    198,  51697,    816,  31214,  19260,\n",
      "           1901,  21861,    359,    386,  31214,  19260,    816,  51932,  17156,\n",
      "           1630,    320,    679,     20,     65,      8,  21579,   5502,    323,\n",
      "          12976,    991,   7058,   2788,  25624,  45838,  52286,   4539,  44412,\n",
      "           5450,     25,   6157,  15836,     11,    604,  13302,     16,   4235,\n",
      "          13302,     22,    198,     44,   1609,    337,    869,     51,  21861,\n",
      "           6256,    441,    424,     40,     11,   1163,    268,     42,     11,\n",
      "          10803,  77927,  16929,     11,  80966,     41,      7,    679,     18,\n",
      "             64,      8,     35,  26204,  10200,   7058,  13898,    708,     69,\n",
      "           5880,    438,    764,  27663,    438,  50828,  77907,   2786,   5450,\n",
      "             25,  62339,   4282,     34,    345,     33,   1751,    283,     43,\n",
      "          50640,   6427,     44,  38406,     71,   1494,   2453,   5676,     57,\n",
      "             11,   1687,    258,  49120,     42,      7,   6910,      8,  24350,\n",
      "           3095,    258,   8989,   4269,    198,  15218,  29992,  49165,   1627,\n",
      "          11541,    324,   6713,  30915,    988,     11,  40345,   2637,    604,\n",
      "          15134,     16,   4235,  15134,     24,    198,    975,   8529,   7044,\n",
      "          73843,    269,  14127,    278,    627,     44,   1609,    337,    869,\n",
      "            350,     11,    816,   7141,    468,     83,     11,  84226,    343,\n",
      "            480,    320,    679,     18,     65,      8,  89333,   4633,   5912,\n",
      "           1385,    304,  19815,    198,   8920,   1178,  36369,    811,   5450,\n",
      "             25,  81158,     12,   7476,  56493,     11,    604,  25594,   4235,\n",
      "          23986,    198,  31456,    301,     44,  20594,  18744,     53,  44754,\n",
      "          83591,    301,   6748,      7,    679,     16,      8,     32,  28956,\n",
      "          27896,   2658,    491,    269,  17840,    535,  21656,    198,    263,\n",
      "          27364,  48712,   1697,    695,   5450,     25,  85438,    287,    708,\n",
      "             69,   1820,   1591,    339,  98697,  79590,    198,    263,  33156,\n",
      "          21656,  90104,   2735,     12,    806,    705,    604,  21474,   4235,\n",
      "          23713,    198,  31456,    301,    386,     11,    350,  18744,    650,\n",
      "             11,    735,  83591,    301,  12478,    320,    679,     17,      8,\n",
      "          38829,   4954,    379,   6438,     25,  69311,   5780,    198,  21656,\n",
      "           2000,  44233,    695,   5450,     25,  46608,     11,   1741,     44,\n",
      "             11,    604,  15828,   4235,  11209,    198,  31456,    301,     44,\n",
      "          59962,  28323,     55,  20594,  18744,     53,      7,    679,     19,\n",
      "              8,  17020,    287,    700,   1201,    258,   3833,   1697,  38691,\n",
      "           2065,    198,   6644,   1729,  16564,  34595,  27061,   5450,     25,\n",
      "          24350,   3095,    258,   8989,   4269,  15218,    198,  29992,  49165,\n",
      "             11,    604,   8546,     24,   4235,   8899,     22,    198,  31456,\n",
      "            301,    386,     11,  16870,  80457,    445,     11,    393,  16499,\n",
      "            822,  39991,     11,   1880,    453,    320,    679,     21,      8,\n",
      "            473,   1640,  79173,  71647,    315,    198,  90153,  87286,   5450,\n",
      "             25,   6157,  15836,     11,    604,   6280,     20,   4235,   5162,\n",
      "             16,    198,     49,    135,    229,   2701,     84,    135,    248,\n",
      "            135,    229,  42961,    432,     11,   2100,     73,   4657,    393,\n",
      "            320,    679,     15,      8,   4476,  24686,    369,  34011,   5768,\n",
      "           6427,    449,    198,  35353,  10803,  71764,   5450,     25,  85438,\n",
      "            287,    708,     69,   1820,     43,  67713,    679,     15,   6919,\n",
      "           8845,    263,   3648,   1163,    278,   7058,     75,    833,    288,\n",
      "           2000,     45,  12852,  90715,     11,   2818,   5726,     11,   2257,\n",
      "           1169,   2629,  28112,  69151,     11,    604,   1774,   4235,   1135,\n",
      "            198,     49,    380,    437,   6780,     47,     11,  26368,  21215,\n",
      "             39,      7,    679,     21,      8,     49,   3013,     17,   4175,\n",
      "             25,     49,   3013,   4539,  12529,  25624,   8350,    266,    309,\n",
      "           5859,    627,    644,     25,  34746,  99031,   6109,  92348,  21861,\n",
      "           2702,    261,     11,    604,  21962,   4235,  20998,    198,   1016,\n",
      "            278,  46434,     32,     11,  12289,   1303,    261,     32,      7,\n",
      "            679,     21,      8,   2732,  23366,    263,     54,  15288,  69761,\n",
      "          71839,  15777,    198,  11772,    685,  57036,   2000,  16206,   5450,\n",
      "          75145,  99031,   6109,     25,   1600,  26538,    679,     21,  35982,\n",
      "          18652,    198,   8059,     11,   1163,    309,     11,    604,  14206,\n",
      "           4235,   8273,    198,  91635,  43588,     51,  50640,    301,   2067,\n",
      "             41,  24412,   1142,    301,     50,  38406,  64151,   1291,  27887,\n",
      "           8324,   3102,    569,     38,      7,    679,     21,      8,  32237,\n",
      "          12529,   7058,  25624,    369,   4382,   2723,  20212,     13,    763,\n",
      "             25,   7327,  15217,    389,  13257,    198,  48567,     11,    604,\n",
      "          12060,     16,   4235,  12171,     15,    198,     54,    526,     48,\n",
      "          50640,    526,     33,     11,  17198,     78,     43,      7,    679,\n",
      "             20,      8,  81434,   3231,  44412,    985,  12529,  25624,    198,\n",
      "            438,  22746,   5450,  58255,     41,   5158,     40,     11,    604,\n",
      "           9741,     24,   4235,   9714,     20,    198,     54,    526,   1901,\n",
      "          51932,  21313,    622,  28328,    833,    622,     11,   1163,    268,\n",
      "           1901,    320,    679,     19,     64,      8,  33025,   4876,    323,\n",
      "           1495,  53258,    198,  95711,   5450,     25,   2783,     45,  12852,\n",
      "          11541,    275,   2423,    261,     11,    604,  11068,     16,   4235,\n",
      "           6330,     16,    198,     54,    526,   1901,     11,  37120,    622,\n",
      "             11,  43758,    622,     11,  25507,   1901,    320,    679,     19,\n",
      "             65,      8,  33025,   4876,  40188,    555,    198,   1485,     75,\n",
      "           1113,    263,  69292,  39157,   5450,     25,   6157,  15836,  11541,\n",
      "            275,   2423,    261,     11,    604,   5037,     17,   4235,   5037,\n",
      "             24,    198,     55,  23332,     39,  44639,  69710,     44,  44639,\n",
      "           3524,     56,  51932,  17156,     55,      7,    679,     20,      8,\n",
      "           3246,     70,  56748,   7642,  20053,    336,  13025,   2590,    198,\n",
      "          45838,  52286,   4539,  95711,  17126,     55,    344,   1762,   1374,\n",
      "            277,     55,    344,     25,   3965,  22393,  21310,    198,  76065,\n",
      "          11865,  28365,  10602,     45,  51932,  18664,     44,  24412,    318,\n",
      "          23263,      7,    679,     19,      8,  42097,  23013,  22317,   2788,\n",
      "          25624,   2000,    198,  90153,   6108,   3488,  36864,     13,    763,\n",
      "             25,  55227,    315,    279,    220,    679,     19,  49843,   7058,\n",
      "            768,    389,  46763,   5528,    304,   5933,   4221,   8863,    320,\n",
      "           2783,     45,  12852,    705,  12086,    198,  22926,   4235,  13655,\n",
      "         128009, 128006,    882, 128007,    271,   7184,     11,   2728,    420,\n",
      "           3488,     25,   3639,    527,    279,   9256,    430,    279,   1646,\n",
      "            374,  16572,    369,   4710,    220,  21335,   1203,    279,   4320,\n",
      "           1193,    304,    264,  13325,   1160,   3645,     11,    369,   3187,\n",
      "             25,   2570,     32,   1882,     33,   7352,   1442,    499,   1541,\n",
      "            956,   1440,    279,   4320,     11,   1120,    471,    459,   4384,\n",
      "           1160,     13, 128009, 128006,  78191, 128007,    271,    681,  90153,\n",
      "           4876,   9954,    518,    364,  90153,   6108,   3488,  36864,    518,\n",
      "            364,   2125,  20212,    518,    364,    695,  11935,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['knowledge graph completion', 'knowledge-based question answering', 'link prediction', 'data mining']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Expeditious Generation of Knowledge Graph\n",
      "Embeddings\n",
      "TommasoSoru,StefanoRuberto,DiegoMoussallem,AndréValdestilhas,\n",
      "AlexanderBigerl,EdgardMarxandDiegoEsteves\n",
      "Thisworkwasacceptedforpresentationatthe5thEuropeanConference\n",
      "on Data Analysis (ECDA 2018) under the title “A Simple and Fast Ap-\n",
      "proachtoKnowledgeGraphEmbedding”.\n",
      "Abstract Knowledge Graph Embedding methods aim at representing enti-\n",
      "ties and relations in a knowledge base as points or vectors in a continuous\n",
      "vector space. Several approaches using embeddings have shown promising\n",
      "results on tasks such as link prediction, entity recommendation, question an-\n",
      "swering,andtripletclassification.However,onlyafewmethodscancompute\n",
      "low-dimensionalembeddingsofverylargeknowledgebaseswithoutneeding\n",
      "TommasoSoru,DiegoMoussallem,AndréValdestilhas,EdgardMarx\n",
      "AKSW,UniversityofLeipzig,Germany\n",
      "(cid:0)tsoru,moussallem,valdestilhas,marx@informatik.uni-leipzig.de\n",
      "StefanoRuberto\n",
      "UniversityofPennsylvania,UnitedStates\n",
      "(cid:0)stefano.ruberto@pennmedicine.upenn.edu\n",
      "AlexanderBigerl\n",
      "DICE,PaderbornUniversity,Germany\n",
      "(cid:0)alexander.bigerl@uni-paderborn.de\n",
      "DiegoEsteves\n",
      "SDA,UniversityofBonn,Germany\n",
      "(cid:0)esteves@cs.uni-bonn.de\n",
      "ARCHIVES OF DATA SCIENCE, SERIES A\n",
      "(ONLINE FIRST) DOI10.5445/KSP/XXXXXXXX/XX\n",
      "KIT SCIENTIFIC PUBLISHING ISSN2363-9881\n",
      "Vol.-,No.-,-\n",
      "8102\n",
      "voN\n",
      "9\n",
      "]LC.sc[\n",
      "2v82870.3081:viXra\n",
      "2 TommasoSoruetal.\n",
      "state-of-the-artcomputationalresources.Inthispaper,weproposeKG2Vec,a\n",
      "simpleandfastapproachtoKnowledgeGraphEmbeddingbasedontheskip-\n",
      "grammodel.Insteadofusingapredefinedscoringfunction,welearnitrelying\n",
      "onLongShort-TermMemories.Weshowthatourembeddingsachieveresults\n",
      "comparablewiththemostscalableapproachesonknowledgegraphcompletion\n",
      "aswellasonanewmetric.Yet,KG2Veccanembedlargegraphsinlessertime\n",
      "by processing more than 250 million triples in less than 7 hours on common\n",
      "hardware.\n",
      "1 Introduction\n",
      "Recently,the number of public datasets in the Linked Data cloud has signifi-\n",
      "cantly grown to almost 10 thousands. At the time of writing, at least four of\n",
      "thesedatasetscontainmorethanonebilliontripleseach.1 Thishugeamountof\n",
      "availabledatahasbecomeafertilegroundforMachineLearningandDataMin-\n",
      "ingalgorithms.Today,applicationsofmachine-learningtechniquescomprisea\n",
      "broadvarietyofresearchareasrelatedtoLinkedData,suchasLinkDiscovery,\n",
      "NamedEntityRecognition,andStructuredQuestionAnswering.Thefieldof\n",
      "Knowledge Graph Embedding (KGE) has emerged in the Machine Learning\n",
      "communityduringthelastfiveyears. TheunderlyingconceptofKGEisthat\n",
      "in a knowledge base, each entity and relation can be regarded as a vector in\n",
      "a continuous space. The generated vector representations can be used by al-\n",
      "gorithms employing machine learning, deep learning, or statistical relational\n",
      "learning to accomplish a given task. Several KGE approaches have already\n",
      "shownpromisingresultsontaskssuchaslinkprediction,entityrecommenda-\n",
      "tion,questionanswering,andtripletclassification(Xiaoetal,2015;Linetal,\n",
      "2015a,b; Nickel et al, 2016). Moreover, Distributional Semantics techniques\n",
      "(e.g.,Word2VecorDoc2Vec)arerelativelynewintheSemanticWebcommunity.\n",
      "TheRDF2Vecapproaches(RistoskiandPaulheim,2016;Cochezetal,2017a)\n",
      "are examples of pioneering research and to date, they represent the only op-\n",
      "tionforlearningembeddingsonalargeknowledgegraphwithouttheneedfor\n",
      "state-of-the-arthardware.Tothisend,wedevisetheKG2Vecapproach,which\n",
      "comprisesskip-gramtechniquesforcreatingembeddingsonlargeknowledge\n",
      "graphs in a feasible time but still maintaining the quality of state-of-the-art\n",
      "embeddings.OurevaluationshowsthatKG2Vecachievesavectorqualitycom-\n",
      "1http://lodstats.aksw.org\n",
      "ExpeditiousGenerationofKnowledgeGraphEmbeddings 3\n",
      "parabletothemostscalableapproachesandcanprocessmorethan250million\n",
      "triplesinlessthan7hoursonamachinewithsuboptimalperformances.\n",
      "2 RelatedWork\n",
      "Anearlyefforttoautomaticallygeneratefeaturesfromstructuredknowledge\n",
      "was proposed in (Cheng et al, 2011). RESCAL (Nickel et al, 2011) is a\n",
      "relational-learning algorithm based on Tensor Factorization using Alternat-\n",
      "ing Least-Squares which has showed to scale to large RDF datasets such as\n",
      "YAGO(Nickel et al, 2012) and reach good results in the tasks of link predic-\n",
      "tion,entityresolution,orcollectiveclassification(Nickeletal,2014).Manifold\n",
      "approacheswhichrelyontranslationshavebeenimplementedsofar(Bordes\n",
      "et al, 2013; Wang et al, 2014b; Jia et al, 2015; Lin et al, 2015b; Wang et al,\n",
      "2015;Xiaoetal,2015).TransEisthefirstmethodwhererelationshipsarein-\n",
      "terpretedastranslationsoperatingonthelow-dimensionalembeddingsofthe\n",
      "entities (Bordes et al,2013). On the other hand,TransH models a relation as\n",
      "a hyperplane together with a translation operation on it (Wang et al, 2014b).\n",
      "TransAexploresembeddingmethodsforentitiesandrelationsbelongingtotwo\n",
      "differentknowledgegraphsfindingtheoptimallossfunction(Jiaetal,2015),\n",
      "whilstPTransEreliesonpathstobuildthefinalvectors(Linetal,2015a).The\n",
      "algorithmsTransRandCTransRproposedinLinetal(2015b)aimatbuilding\n",
      "entityandrelationembeddingsinseparateentityspaceandrelationspaces,so\n",
      "astolearnembeddingsthroughprojectedtranslationsintherelationspace;an\n",
      "extensionofthisalgorithmmakesuseofrulestolearnembeddings(Wangetal,\n",
      "2015).Anefforttojointlyembedstructuredandunstructureddata(suchastext)\n",
      "wasproposedinWangetal(2014a).TheideabehindtheDistMultapproachis\n",
      "toconsiderentitiesaslow-dimensionalvectorslearnedfromaneuralnetwork\n",
      "and relations as bilinear and/or linear mapping functions Yang et al (2014).\n",
      "TransG,agenerativemodeladdresstheissueofmultiplerelationsemanticsof\n",
      "arelation,hasshowedtogobeyondstate-of-the-artresults(Xiaoetal,2015).\n",
      "ComplExisbasedonlatentfactorizationand,withtheuseofcomplex-valued\n",
      "embeddings, it facilitates composition and handles a large variety of binary\n",
      "relations Trouillon et al (2016). The fastText algorithm was meant for word\n",
      "embeddings,however Joulin et al (2017) showed that a simple bag-of-words\n",
      "cangeneratesurprisinglygoodKGEs.\n",
      "4 TommasoSoruetal.\n",
      "The field of KGE has considerably grown during the last two years, earn-\n",
      "ing a spotalso in the SemanticWebcommunity. In 2016,Nickelet al (2016)\n",
      "proposedHolE,whichreliesonholographicmodelsofassociativememoryby\n",
      "employing circularcorrelation to create compositional representations. HolE\n",
      "cancapturerichinteractionsbyusingcorrelationasthecompositionaloperator\n",
      "butitsimultaneouslyremainsefficienttocompute,easytotrain,andscalable\n",
      "to large datasets. In the same year, Ristoski and Paulheim (2016) presented\n",
      "RDF2Vecwhichuseslanguagemodelingapproachesforunsupervisedfeature\n",
      "extraction from sequences of words and adapts them to RDF graphs. After\n",
      "generatingsequencesbyleveraginglocalinformationfromgraphsubstructures\n",
      "byrandomwalks,RDF2Veclearnslatentnumericalrepresentationsofentities\n",
      "inRDFgraphs.Thealgorithmhasbeenextendedinordertoreducethecompu-\n",
      "tationaltimeandthebiasedregardedtherandomwalking(Cochezetal,2017a).\n",
      "Morerecently,Cochezetal(2017b)exploitedtheGlobalVectorsalgorithmto\n",
      "compute embeddings from the co-occurrence matrix of entities and relations\n",
      "withoutgeneratingtherandomwalks.Infollowingresearch,theauthorsrefer\n",
      "totheiralgorithmasKGloVe.2\n",
      "3 KG2Vec\n",
      "Thisstudyaddressesthefollowingresearchquestions:\n",
      "1. Canwegenerateembeddingsatahighratewhilepreservingaccuracy?\n",
      "2. HowcanwetestthedistributionalhypothesisofKGEs?\n",
      "3. Canwelearnascoringfunctionforknowledgebasecompletionwhichper-\n",
      "formsbetterthanthestandardone?\n",
      "Formally,lett =(s,p,o)beatriplecontainingasubject,apredicate,andan\n",
      "objectinaknowledgebaseK.Foranytriple,(s,p,o)⊆E×R×(E∩L),where\n",
      "E is the set of all entities, R is the set of all relations, and L is the set of all\n",
      "literals(i.e.,stringornumericalvalues).ArepresentationfunctionF definedas\n",
      "F :(E∩R∩L)→Rd (1)\n",
      "assigns a vectorof dimensionality d to an entity,a relation,ora literal. How-\n",
      "ever, some approaches consider only the vector representations of entities or\n",
      "2https://datalab.rwth-aachen.de/embedding/KGloVe/\n",
      "ExpeditiousGenerationofKnowledgeGraphEmbeddings 5\n",
      "subjects (i.e, {s∈E :∃(s,p,o)∈K}). For instance, in approaches based on\n",
      "Tensor Factorization, given a relation, its subjects and objects are processed\n",
      "andtransformedintosparsematrices;allthematricesarethencombinedintoa\n",
      "tensorwhosedepthisthenumberofrelations.Forthefinalembedding,current\n",
      "approachesrelyondimensionalityreductiontodecreasetheoverallcomplex-\n",
      "ity(Nickeletal,2014;Jiaetal,2015;Linetal,2015b).Thereductionisper-\n",
      "formedthroughanembeddingmapΦ :Rd →Rk,whichisahomomorphism\n",
      "that maps the initial vector space into a smaller, reduced space. The positive\n",
      "valuek<d iscalledtherank oftheembedding.Notethateachdimensionof\n",
      "the reduced common space does not necessarily have an explicit connection\n",
      "withaparticularrelation.DimensionalityreductionmethodsincludePrincipal\n",
      "Component Analysis techniques (Nickel et al, 2014) and generative statisti-\n",
      "cal models such as Latent Dirichlet Allocation (Jurgens and Stevens, 2010;\n",
      "Rˇehu˚ˇrekandSojka,2010).\n",
      "ExistingKGEapproachesbasedontheskip-grammodelsuchasRDF2Vec(Ris-\n",
      "toskiandPaulheim,2016)submitpathsbuiltusingrandomwalkstoaWord2Vec\n",
      "algorithm.Instead,wepreprocesstheinputknowledgebasebyconvertingeach\n",
      "tripleintoasmallsentenceofthreewords.Ourmethodisfasterasitallowsus\n",
      "toavoidthepathgenerationstep.Thegeneratedtextcorpusisthusprocessed\n",
      "bytheskip-grammodelasfollows.\n",
      "3.1 Adaptingtheskip-grammodel\n",
      "We adapt the skip-gram model (Mikolov et al,2013a) to deal with our small\n",
      "sequences of length three. In this work, we only consider URIs and discard\n",
      "literals,thereforewecomputeavectorforeachelementu∈E∩R.Considering\n",
      "a triple as a sequence of three URIs T ={u,u,u },the aim is to maximize\n",
      "s p o\n",
      "theaveragelogprobability\n",
      "1\n",
      "∑ ∑ logp(u|u(cid:48)) (2)\n",
      "3\n",
      "u∈Tu(cid:48)∈T\\u\n",
      "whichmeans,inotherwords,toadoptacontextwindowof2,sincethesequence\n",
      "sizeisalways|T|=3.Theprobabilityaboveistheoreticallydefinedas:\n",
      "exp(vO(cid:62) vI )\n",
      "p(u|u(cid:48))= u u(cid:48) (3)\n",
      "∑ exp(vO(cid:62) vI )\n",
      "x∈E∩R x u(cid:48)\n",
      "6 TommasoSoruetal.\n",
      "wherevI andvO arerespectivelytheinputandoutputvectorrepresentationsof\n",
      "x x\n",
      "aURIx.Weimplyanegativesamplingof5,i.e.5wordsarerandomlyselected\n",
      "tohaveanoutputof0andconsequentlyupdatetheweights.\n",
      "3.2 Scoringfunctions\n",
      "3.2.1 Scoringbyanalogy\n",
      "Severalmethodshavebeenproposedtoevaluatewordembeddings.Themost\n",
      "commononesarebasedonanalogies(Mikolovetal,2013b;LevyandGoldberg,\n",
      "2014),wherewordvectorsaresummeduptogether,e.g.:\n",
      "v[”queen”]≈v[”king”]+v[”woman”]−v[”man”] (4)\n",
      "Ananalogywheretheapproximationaboveissatisfiedwithinacertainthresh-\n",
      "oldcanthuspredicthiddenrelationshipsamongwords,whichinourenviron-\n",
      "mentmeanstopredictnewlinksamongentitiesRistoskiandPaulheim(2016).\n",
      "The analogy-based score function for a given triple (s¯,p¯,o¯) is defined as fol-\n",
      "lows.\n",
      "(cid:40)\n",
      "1 1 if (cid:107)v +v −v −v (cid:107)≤ε\n",
      "score(s¯,p¯,o¯)= ∑ s¯ o s o¯ (5)\n",
      "|{(s,p¯,o)∈K}| 0 otherwise\n",
      "(s,p¯,o)∈K\n",
      "whereε isanarbitrarilysmallpositivevalue.Inwords,givenapredicate p¯,we\n",
      "selectalltripleswhereitoccurs.Foreachtriple,wecomputetherelationvector\n",
      "asthedifferencebetweentheobjectandthesubjectvectors. Wethencounta\n",
      "match whenever the vector sum of subject s¯and relation is close to object o¯\n",
      "withinaradiusε.Thescoreisequaltotherateofmatchesoverthenumberof\n",
      "selectedtriples.\n",
      "3.2.2 Scoringbyneuralnetworks\n",
      "We evaluate the scoring function above against a neural network based on\n",
      "LongShort-TermMemories(LSTM).Theneuralnetworktakesasequenceof\n",
      "embeddingsasinput,namelyv,v,v foratriple(s,p,o)∈K.Adensehidden\n",
      "s p o\n",
      "layerofthesamesizeoftheembeddingsisconnectedtoasingleoutputneuron\n",
      "withsigmoidactivation,whichreturnsavaluebetween0and1.Thenegative\n",
      "ExpeditiousGenerationofKnowledgeGraphEmbeddings 7\n",
      "Fig.1 AselectionofDB-\n",
      "pediaresourcesalongwith\n",
      "theirvectors in 3 dimen-\n",
      "sionsobtainedusingPrincipal\n",
      "ComponentAnalysis.Blue\n",
      "pointsareresources,whilst\n",
      "redpointsareclasses.Ascan\n",
      "beseen,resourcesfollowthe\n",
      "distributionalhypothesis.\n",
      "triplesaregeneratedusingtwostrategies,i.e.foreachtripleinthetrainingset\n",
      "(1)randomlyextractarelationanditstwonodesor(2)corruptthesubjector\n",
      "theobject.WeusetheAdamoptimizerand100epochsoftraining.\n",
      "3.3 Metrics\n",
      "AsrecentlyhighlightedbyseveralmembersoftheMLandNLPcommunities,\n",
      "KGEsarerarelyevaluatedondownstreamtasksdifferentfromlinkprediction\n",
      "(alsoknownasknowledgebasecompletion).Achievinghighperformanceson\n",
      "link prediction does not necessarily mean that the generated embeddings are\n",
      "good,sincetheinferencetaskisoftencarriedoutincombinationwithanexter-\n",
      "nalalgorithmsuchasaneuralnetworkorascoringfunction.Thecomplexityis\n",
      "thusapproach-dependentanddistributedbetweenthelatentstructureinthevec-\n",
      "tormodelandtheparameters(ifany)oftheinferencealgorithm.Forinstance,\n",
      "atranslationalmodelsuchasTransEBordesetal(2013)wouldlikelyfeature\n",
      "verycomplexembeddings,sinceinmostapproachestheinferencefunctionis\n",
      "a simple addition. On the other hand, we may find less structure in a tensor\n",
      "factorization model such as RESCAL Nickel et al (2011),as the inference is\n",
      "performed by a feed-forward neural network which extrapolates the hidden\n",
      "semanticslayerbylayer.\n",
      "8 TommasoSoruetal.\n",
      "3.3.1 NeighbourSimilarityTest\n",
      "In this paper, we introduce two metrics inspired by The Identity of Indis-\n",
      "cernibles (Black, 1952) to gain insights over the distributional quality of the\n",
      "learnedembeddings.\n",
      "Themorecharacteristicstwoentitiesshare,themoresimilartheyareand\n",
      "soshouldbetheirvectorrepresentations.\n",
      "Considering the setofcharacteristicsC (s)={(p,o ),...,(p,o )} ofa\n",
      "K 1 1 m m\n",
      "subjectsinatriple,wecandefineametricthatexpressesthesimilarityamong\n",
      "two entities e,e as the Jaccard index between their sets of characteristics\n",
      "1 2\n",
      "C (e )andC (e ).GivenasetofentitiesE˜ andtheirN nearestneighboursin\n",
      "K 1 K 2\n",
      "thevectorspace,theoverallNeighbourSimilarityTest(NST)metricisdefined\n",
      "as:\n",
      "(e)\n",
      "NST(E˜,N,K)=\n",
      "1\n",
      "∑\n",
      "∑N |C K(e)∩C K(n\n",
      "j\n",
      ")|\n",
      "(6)\n",
      "N|E˜|\n",
      "e∈E˜ j=1|C K(e)∪C\n",
      "K(n( je)\n",
      ")|\n",
      "(e)\n",
      "wheren isthe jthnearestneighbourofeinthevectorspace.\n",
      "j\n",
      "3.3.2 TypeandCategoryTest\n",
      "ThesecondmetricistheTypeandCategoryTest(TCT),basedontheassump-\n",
      "tion that two entities which share types and categories should be close in\n",
      "the vector space. This assumption is suggested by the human bias for which\n",
      "rdf:type and dct:subject would be predicates with a higher weight\n",
      "thantheothers.Althoughthisdoesnothappen,wecomputeitforameresake\n",
      "ofcomparisonwiththeNSTmetric.TheTCTformulaisequaltoEquation6ex-\n",
      "ceptforsetsC (e),whicharereplacedbysetsoftypesandcategoriesTC (e).\n",
      "K K\n",
      "4 Evaluation\n",
      "WeimplementedKG2VecinPython2.7usingtheGensimandKeraslibraries\n",
      "withTheanoenvironment.Sourcecode,datasets,andvectorsobtainedareavail-\n",
      "ExpeditiousGenerationofKnowledgeGraphEmbeddings 9\n",
      "ableonline.3 AllexperimentswerecarriedoutonanUbuntu16.04serverwith\n",
      "128GBRAMand40CPUs.\n",
      "Table1 DetailsandruntimesforthegenerationofKG2Vecembeddingsontwodatasets.\n",
      "DBpedia\n",
      "Dataset AKSW-bib DBpedia2016-04\n",
      "2015-10\n",
      "Numberoftriples 3922 164,369,887 276,316,003 276,316,003\n",
      "Numberofvectors 954 14,921,691 23,816,469 36,596,967\n",
      "Dimensionality 10 300 200 200\n",
      "Runtime(s) 2.2 18,332 25,380 46,099\n",
      "Rate(triples/s) 1,604 8,966 10,887 5,994\n",
      "ThedatasetusedintheexperimentsaredescribedinTable1.TheAKSW-bib\n",
      "dataset–employedforthelinkpredictionevaluation–wascreatedusinginfor-\n",
      "mationfrompeopleandprojectsontheAKSW.orgwebsiteandbibliographical\n",
      "datafromBibsonomy.WebuiltamodelontopoftheEnglish2015-10version\n",
      "oftheDBpediaknowledgegraph(Lehmann etal,2009); Figure1showsa3-\n",
      "dimensionalplotofselectedentities.FortheEnglishDBpedia2016-04dataset,\n",
      "webuilttwomodels.Inthefirst,wesetathresholdtoembedonlytheentities\n",
      "occurring at least 5 times in the dataset; we chose this setting to be aligned\n",
      "to the related works’ models. In the second model, all 36 million entities in\n",
      "DBpedia are associated a vector. More insights about the first model can be\n",
      "foundinthenexttwosubsections,whiletheresourceconsumptionforcreating\n",
      "thesecondmodelcanbeseeninFigure5.\n",
      "4.1 Runtime\n",
      "Inthisstudy,weaimatgeneratingembeddingsatahighratewhilepreserving\n",
      "accuracy.InTable1,wealreadyshowedthatoursimplepipelinecanachieve\n",
      "arateofalmost11,000triplespersecondonalargedatasetsuchasDBpedia\n",
      "2016-04.InTable2,wecompareKG2Vecwiththreeotherscalableapproaches\n",
      "for embedding knowledge bases. We selected the best settings of RDF2Vec\n",
      "and KGloVe according to their respective articles, since both algorithms had\n",
      "alreadybeensuccessfullyevaluatedonDBpedia(RistoskiandPaulheim,2016;\n",
      "3http://github.com/AKSW/KG2Vec\n",
      "10 TommasoSoruetal.\n",
      "Cochez et al, 2017b). We also tried to compute fastText embeddings on our\n",
      "machine, however we had to halt the process after three days. As the goal\n",
      "of our investigation is efficiency,we discarded any other KGE approach that\n",
      "would have needed more than three days of computation to deliver the final\n",
      "model(Cochezetal,2017b).\n",
      "RDF2Vec has shown to be the most expensive in terms of disk space con-\n",
      "sumed,asthecreatedrandomwalksamountedto∼300GBoftext.Moreover,\n",
      "we could not measure the runtime for the first phase of KGloVe, i.e. the cal-\n",
      "culationofthePersonalizedPageRankvaluesofDBpediaentities.Infact,the\n",
      "authorsusedpre-computedentityranksfromThalhammerandRettinger(2016)\n",
      "andtheKGloVesourcecodedoesnotfeatureaPageRankalgorithm. Weesti-\n",
      "mated the runtime comparing their hardware specs with ours. Despite being\n",
      "unabletoreproduceanyexperimentsfromtheotherthreeapproaches,weman-\n",
      "agedtoevaluatetheirembeddingsbydownloadingthepretrainedmodels4 and\n",
      "creating a KG2Vec embedding model of the same DBpedia dataset there em-\n",
      "ployed.\n",
      "Table2 Runtimecomparisonofthesinglephases.Thosewith(*)areestimatedruntimes.\n",
      "ApproachSteps Time\n",
      "Randomwalksgeneration 123minutes\n",
      "RDF2Vec\n",
      "Word2Vectraining >96hours(*)\n",
      "PersonalizedPageRank N/A\n",
      "KGloVe Co-occurrencecountmatrix\n",
      "12hours(*)\n",
      "GloVetraining\n",
      "Conversiontotext 5minutes\n",
      "KG2Vec\n",
      "Word2Vectraining 6hours58minutes\n",
      "Conversiontotext 5minutes\n",
      "fastText\n",
      "fastTexttraining >72hours(*)\n",
      "4.2 Preliminaryresultsonlinkprediction\n",
      "For the link prediction task,we partition the dataset into training and test set\n",
      "witharatioof9:1.InTable3,weshowpreliminaryresultsbetweenthedifferent\n",
      "strategies on the AKSW-bib dataset using KG2Vec embeddings. As can be\n",
      "seen,ourLSTM-basedscoringfunctionsignificantlyoutperformstheanalogy-\n",
      "based one in both settings. According to the Hits@10 accuracy we obtained,\n",
      "4http://data.dws.informatik.uni-mannheim.de/rdf2vec/\n",
      "ExpeditiousGenerationofKnowledgeGraphEmbeddings 11\n",
      "corrupting triples to generate negative examples is the better strategy. This\n",
      "firstinsightcanfosternewresearchonoptimizingascoringfunctionforKGE\n",
      "approachesbasedondistributionalsemantics.\n",
      "Table3 FilteredHits@10valuesonlinkpredictiononAKSW-bibusingdifferentstrategies.\n",
      "Hits@1 Hits@3 Hits@10\n",
      "LSTM+corrupted 3.84% 9.79% 19.23%\n",
      "LSTM+random 1.39% 4.89% 10.49%\n",
      "Analogy 0.00% 0.51% 3.82%\n",
      "4.3 Distributionalquality\n",
      "ComputingtheNSTandTCTdistributionalqualitymetricsontheentireDB-\n",
      "pediadatasetistime-demanding,sinceforeachentity,themodelandthegraph\n",
      "needtobequeriedfortheN nearestneighboursandtheirrespectivesets.How-\n",
      "ever,weapproximatethefinalvaluebytracingthepartialvaluesofNSTand\n",
      "TCTovertime.Inotherwords,ateachiterationi,wecomputethemetricsover\n",
      "E˜ ={e,...,e}.Figure2showsthepartialTCTvalueonthemostimportant\n",
      "i 1 i\n",
      "10,000entitiesforN ={1,10}accordingtotherankscomputedbyThalham-\n",
      "merandRettinger(2016).Here,KG2Vecmaintainsahigherindexthantheother\n",
      "twoapproaches,despitethesearesteadilyincreasingafterthe∼2,000thentity.\n",
      "WeinterpretthelowerTCTforthetop2,000entitiesasnoiseproducedbythe\n",
      "factthatthesenodesarehyperconnectedtotherestofthegraph,thereforeitis\n",
      "hardforthemtoremainclosetotheirtypepeers.InFigures3and4,theTCT\n",
      "andNSTmetricsrespectivelyarecomputedon10,000randomentities.Inboth\n",
      "cases, the values for the two settings of all approaches stabilize after around\n",
      "1,000entities,howeverweclearlyseethatRDF2Vecembeddingsachievethe\n",
      "highestdistributionalquality by type andcategory. The highernumberofoc-\n",
      "currencesperentityinthehugecorpusofrandomwalksinRDF2Vecmightbe\n",
      "thereasonofthisresultforrarerentities.\n",
      "InFigure5,weshowtheCPU,Memory,anddiskconsumptionforKG2Vec\n",
      "onthelargermodelofDBpedia2016-04.Allthreesubphasesofthealgorithm\n",
      "are visible in the plot. For 2.7 hours, tokens are counted; then, the learning\n",
      "proceedsfor7.7hours;finallyinthelast2.3hours,themodelissaved.\n",
      "12 TommasoSoruetal.\n",
      "0.3\n",
      "0.25 0.2\n",
      "0.15\n",
      "0.1\n",
      "0.05\n",
      "0\n",
      "0 2000 4000 6000 8000 10000\n",
      "Iteration\n",
      "xednI\n",
      "laitraP\n",
      "1\n",
      "0.8\n",
      "KGloVe, N=10 0.6\n",
      "RDF2Vec, N=10\n",
      "KG2Vec, N=10 0.4\n",
      "KGloVe, N=1\n",
      "RDF2Vec, N=1\n",
      "KG2Vec, N=1 0.2\n",
      "0\n",
      "0 2000 4000 6000 8000 10000\n",
      "Iteration\n",
      "Fig. 2 PartialTCT value on DBpedia 2016-04\n",
      "forthetop10,000entities.\n",
      "xednI\n",
      "laitraP\n",
      "KGlove,n=10\n",
      "RDF2Vec,n=10\n",
      "KG2Vec,n=10 KGlove,n=1 RDF2Vec,n=1\n",
      "KG2Vec,n=1\n",
      "Fig. 3 PartialTCT value on DBpedia 2016-04\n",
      "for10,000randomentities.\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "0\n",
      "0 2000 4000 6000 8000 10000\n",
      "Iteration\n",
      "xednI\n",
      "laitraP\n",
      "KGlove, N=10 120000000 RAM (GB) 200 GB\n",
      "RDF2Vec, N=10 Disk (GB)\n",
      "KG2Vec, N=10 100000000 CPU (%)\n",
      "KGlove, N=1 150 GB\n",
      "RDF2Vec, N=1 80000000\n",
      "KG2Vec, N=1\n",
      "60000000 100 GB\n",
      "40000000\n",
      "50 GB\n",
      "20000000\n",
      "0 0 GB\n",
      "0 2 4 6 8 10 12 14\n",
      "Fig.5 CPU,Memory,anddiskconsumptionfor\n",
      "Fig. 4 PartialNST value on DBpedia 2016-04\n",
      "KG2VeconthelargermodelofDBpedia2016-04.\n",
      "for10,000randomentities.\n",
      "100\n",
      "Fig.6 Weshowthecompar-\n",
      "80\n",
      "isonoftherun-timesforall\n",
      "fourapproaches.Notethat 60\n",
      "sincewedonotknowhow\n",
      "40\n",
      "longthePageRankcompu-\n",
      "tationtakes,wereportedthe 20\n",
      "estimatedruntimefortheplain\n",
      "0\n",
      "versionofKGloVe. RDF2Vec fastText KGloVe KG2Vec\n",
      ")\n",
      "sruoh\n",
      "(\n",
      "emiT\n",
      "5 ConclusionandFutureWork\n",
      "We presenteda fastapproachforgenerating KGEs dubbedKG2Vec. We con-\n",
      "cludethattheskip-grammodel,iftraineddirectlyontriplesassmallsentences\n",
      "oflengththree,significantlygainsinruntimewhilepreservingadecentvector\n",
      "quality. Moreover,theKG2Vecembeddingshaveshownhigherdistributional\n",
      "quality for the most important entities in the graph according to PageRank.\n",
      "As a future work, we plan to extend the link prediction evaluation to other\n",
      "benchmarksbyusinganalogiesandourLSTM-basedscoringfunctionoverthe\n",
      "embeddingmodelsoftheapproachesherecompared.\n",
      "ExpeditiousGenerationofKnowledgeGraphEmbeddings 13\n",
      "References\n",
      "BlackM(1952)Theidentityofindiscernibles.Mind61(242):153–164\n",
      "BordesA,UsunierN,Garcia-DuranA,WestonJ,YakhnenkoO(2013)Translat-\n",
      "ingembeddingsformodelingmulti-relationaldata.In:AdvancesinNeural\n",
      "InformationProcessingSystems,pp2787–2795\n",
      "ChengW,KasneciG,GraepelT,SternD,HerbrichR(2011)Automatedfeature\n",
      "generationfromstructuredknowledge.In:CIKM,ACM,pp1395–1404\n",
      "CochezM,RistoskiP,PonzettoSP,PaulheimH(2017a)Biasedgraphwalksfor\n",
      "rdfgraphembeddings.In:Proceedingsofthe7thInternationalConference\n",
      "onWebIntelligence,MiningandSemantics,ACM,p21\n",
      "Cochez M, Ristoski P, Ponzetto SP, Paulheim H (2017b) Global rdf vector\n",
      "spaceembeddings.In:InternationalSemanticWebConference,Springer,pp\n",
      "190–207\n",
      "JiaY,WangY,LinH,JinX,ChengX(2015)Locallyadaptivetranslationfor\n",
      "knowledgegraphembedding.arXivpreprintarXiv:151201370\n",
      "JoulinA,GraveE,BojanowskiP,NickelM,MikolovT(2017)Fastlinearmodel\n",
      "forknowledgegraphembeddings.arXivpreprintarXiv:171010881\n",
      "JurgensD,StevensK(2010)Thes-spacepackage:anopensourcepackagefor\n",
      "word space models. In: Proceedings of the ACL 2010 System Demonstra-\n",
      "tions,AssociationforComputationalLinguistics,pp30–35\n",
      "LehmannJ,BizerC,KobilarovG,AuerS,BeckerC,CyganiakR,HellmannS\n",
      "(2009)DBpedia-acrystallizationpointforthewebofdata.JournalofWeb\n",
      "Semantics7(3):154–165\n",
      "LevyO,GoldbergY(2014)Linguisticregularitiesinsparseandexplicitword\n",
      "representations. In: Proceedings of the eighteenth conference on computa-\n",
      "tionalnaturallanguagelearning,pp171–180\n",
      "LinY,LiuZ,SunM(2015a)Modelingrelationpathsforrepresentationlearning\n",
      "ofknowledgebases.CoRRabs/1506.00379,URLhttp://arxiv.org/\n",
      "abs/1506.00379\n",
      "Lin Y,Liu Z,Sun M,Liu Y,Zhu X (2015b) Learning entity and relation em-\n",
      "beddingsforknowledgegraphcompletion.In:AAAI,pp2181–2187\n",
      "MikolovT,SutskeverI,ChenK,CorradoGS,DeanJ(2013a)Distributedrep-\n",
      "resentationsofwordsandphrasesandtheircompositionality.In:BurgesC,\n",
      "BottouL,WellingM,GhahramaniZ,WeinbergerK(eds)AdvancesinNeural\n",
      "InformationProcessingSystems26,CurranAssociates,Inc.,pp3111–3119\n",
      "14 TommasoS<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   2396,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Tommaso Soru', 'Stefano Ruberto', 'Diego Moussallem', 'André Valdestilhas', 'Alexander Bigerl', 'Edgard Marx', 'Diego Esteves']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: naturallanguagelearning,pp171–180\n",
      "LinY,LiuZ,SunM(2015a)Modelingrelationpathsforrepresentationlearning\n",
      "ofknowledgebases.CoRRabs/1506.00379,URLhttp://arxiv.org/\n",
      "abs/1506.00379\n",
      "Lin Y,Liu Z,Sun M,Liu Y,Zhu X (2015b) Learning entity and relation em-\n",
      "beddingsforknowledgegraphcompletion.In:AAAI,pp2181–2187\n",
      "MikolovT,SutskeverI,ChenK,CorradoGS,DeanJ(2013a)Distributedrep-\n",
      "resentationsofwordsandphrasesandtheircompositionality.In:BurgesC,\n",
      "BottouL,WellingM,GhahramaniZ,WeinbergerK(eds)AdvancesinNeural\n",
      "InformationProcessingSystems26,CurranAssociates,Inc.,pp3111–3119\n",
      "14 TommasoSoruetal.\n",
      "Mikolov T, Yih Wt, Zweig G (2013b) Linguistic regularities in continuous\n",
      "spacewordrepresentations.In:HLT-NAACL,pp746–751\n",
      "NickelM,TrespV,KriegelHP(2011)Athree-waymodelforcollectivelearning\n",
      "onmulti-relationaldata.In:Proceedingsofthe28thinternationalconference\n",
      "onmachinelearning(ICML-11),pp809–816\n",
      "Nickel M, Tresp V, Kriegel HP (2012) Factorizing yago: scalable machine\n",
      "learningforlinkeddata.In:WWW,ACM,pp271–280\n",
      "NickelM,JiangX,TrespV(2014)Reducingtherankinrelationalfactorization\n",
      "modelsbyincludingobservablepatterns.In:AdvancesinNeuralInformation\n",
      "ProcessingSystems,pp1179–1187\n",
      "Nickel M, Rosasco L, Poggio TA, et al (2016) Holographic embeddings of\n",
      "knowledgegraphs.In:AAAI,pp1955–1961\n",
      "Rˇehu˚ˇrek R, Sojka P (2010) Software Framework for Topic Modelling with\n",
      "LargeCorpora.In:ProceedingsoftheLREC2010WorkshoponNewChal-\n",
      "lengesforNLPFrameworks,ELRA,Valletta,Malta,pp45–50\n",
      "RistoskiP,PaulheimH(2016)Rdf2vec:Rdfgraphembeddingsfordatamining.\n",
      "In:InternationalSemanticWebConference,Springer,pp498–514\n",
      "ThalhammerA,RettingerA(2016)PageRankonWikipedia:TowardsGeneral\n",
      "ImportanceScoresforEntities.In:TheSemanticWeb:ESWC2016Satellite\n",
      "Events,Cham,pp227–240\n",
      "TrouillonT,WelblJ,RiedelS,GaussierÉ,BouchardG(2016)Complexembed-\n",
      "dings for simple link prediction. In: International Conference on Machine\n",
      "Learning,pp2071–2080\n",
      "WangQ,WangB,GuoL(2015)Knowledgebasecompletionusingembeddings\n",
      "andrules.In:IJCAI,pp1859–1865\n",
      "Wang Z,Zhang J,Feng J,Chen Z (2014a) Knowledge graph and text jointly\n",
      "embedding.In:EMNLP,Citeseer,pp1591–1601\n",
      "Wang Z, Zhang J, Feng J, Chen Z (2014b) Knowledge graph embedding by\n",
      "translatingonhyperplanes.In:AAAI,Citeseer,pp1112–1119\n",
      "XiaoH,HuangM,HaoY,ZhuX(2015)Transg:Agenerativemixturemodel\n",
      "forknowledgegraphembedding.arXivpreprintarXiv:150905488\n",
      "YangMC,DuanN,ZhouM,RimHC(2014)Jointrelationalembeddingsfor\n",
      "knowledge-based question answering. In: Proceedings of the 2014 confer-\n",
      "ence on empirical methods in natural language processing (EMNLP), pp\n",
      "645–650<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "          14818,    543,   2681,  21656,     11,    604,  11123,   4235,   5245,\n",
      "            198,  51697,     56,  31214,  19260,     57,  21861,    359,     44,\n",
      "              7,    679,     20,     64,      8,   1747,    287,  23013,  22354,\n",
      "           2000,  84216,  21656,    198,   1073,  90153,  79367,  53214,   8268,\n",
      "           3518,     14,   3965,     21,     13,   6268,   4643,     11,   3222,\n",
      "           1277,   1129,    277,  89833,   2726,   6018,   3518,     14,   3965,\n",
      "             21,     13,   6268,   4643,    198,  51697,    816,  31214,  19260,\n",
      "           1901,  21861,    359,    386,  31214,  19260,    816,  51932,  17156,\n",
      "           1630,    320,    679,     20,     65,      8,  21579,   5502,    323,\n",
      "          12976,    991,   7058,   2788,  25624,  45838,  52286,   4539,  44412,\n",
      "           5450,     25,   6157,  15836,     11,    604,  13302,     16,   4235,\n",
      "          13302,     22,    198,     44,   1609,    337,    869,     51,  21861,\n",
      "           6256,    441,    424,     40,     11,   1163,    268,     42,     11,\n",
      "          10803,  77927,  16929,     11,  80966,     41,      7,    679,     18,\n",
      "             64,      8,     35,  26204,  10200,   7058,  13898,    708,     69,\n",
      "           5880,    438,    764,  27663,    438,  50828,  77907,   2786,   5450,\n",
      "             25,  62339,   4282,     34,    345,     33,   1751,    283,     43,\n",
      "          50640,   6427,     44,  38406,     71,   1494,   2453,   5676,     57,\n",
      "             11,   1687,    258,  49120,     42,      7,   6910,      8,  24350,\n",
      "           3095,    258,   8989,   4269,    198,  15218,  29992,  49165,   1627,\n",
      "          11541,    324,   6713,  30915,    988,     11,  40345,   2637,    604,\n",
      "          15134,     16,   4235,  15134,     24,    198,    975,   8529,   7044,\n",
      "          73843,    269,  14127,    278,    627,     44,   1609,    337,    869,\n",
      "            350,     11,    816,   7141,    468,     83,     11,  84226,    343,\n",
      "            480,    320,    679,     18,     65,      8,  89333,   4633,   5912,\n",
      "           1385,    304,  19815,    198,   8920,   1178,  36369,    811,   5450,\n",
      "             25,  81158,     12,   7476,  56493,     11,    604,  25594,   4235,\n",
      "          23986,    198,  31456,    301,     44,  20594,  18744,     53,  44754,\n",
      "          83591,    301,   6748,      7,    679,     16,      8,     32,  28956,\n",
      "          27896,   2658,    491,    269,  17840,    535,  21656,    198,    263,\n",
      "          27364,  48712,   1697,    695,   5450,     25,  85438,    287,    708,\n",
      "             69,   1820,   1591,    339,  98697,  79590,    198,    263,  33156,\n",
      "          21656,  90104,   2735,     12,    806,    705,    604,  21474,   4235,\n",
      "          23713,    198,  31456,    301,    386,     11,    350,  18744,    650,\n",
      "             11,    735,  83591,    301,  12478,    320,    679,     17,      8,\n",
      "          38829,   4954,    379,   6438,     25,  69311,   5780,    198,  21656,\n",
      "           2000,  44233,    695,   5450,     25,  46608,     11,   1741,     44,\n",
      "             11,    604,  15828,   4235,  11209,    198,  31456,    301,     44,\n",
      "          59962,  28323,     55,  20594,  18744,     53,      7,    679,     19,\n",
      "              8,  17020,    287,    700,   1201,    258,   3833,   1697,  38691,\n",
      "           2065,    198,   6644,   1729,  16564,  34595,  27061,   5450,     25,\n",
      "          24350,   3095,    258,   8989,   4269,  15218,    198,  29992,  49165,\n",
      "             11,    604,   8546,     24,   4235,   8899,     22,    198,  31456,\n",
      "            301,    386,     11,  16870,  80457,    445,     11,    393,  16499,\n",
      "            822,  39991,     11,   1880,    453,    320,    679,     21,      8,\n",
      "            473,   1640,  79173,  71647,    315,    198,  90153,  87286,   5450,\n",
      "             25,   6157,  15836,     11,    604,   6280,     20,   4235,   5162,\n",
      "             16,    198,     49,    135,    229,   2701,     84,    135,    248,\n",
      "            135,    229,  42961,    432,     11,   2100,     73,   4657,    393,\n",
      "            320,    679,     15,      8,   4476,  24686,    369,  34011,   5768,\n",
      "           6427,    449,    198,  35353,  10803,  71764,   5450,     25,  85438,\n",
      "            287,    708,     69,   1820,     43,  67713,    679,     15,   6919,\n",
      "           8845,    263,   3648,   1163,    278,   7058,     75,    833,    288,\n",
      "           2000,     45,  12852,  90715,     11,   2818,   5726,     11,   2257,\n",
      "           1169,   2629,  28112,  69151,     11,    604,   1774,   4235,   1135,\n",
      "            198,     49,    380,    437,   6780,     47,     11,  26368,  21215,\n",
      "             39,      7,    679,     21,      8,     49,   3013,     17,   4175,\n",
      "             25,     49,   3013,   4539,  12529,  25624,   8350,    266,    309,\n",
      "           5859,    627,    644,     25,  34746,  99031,   6109,  92348,  21861,\n",
      "           2702,    261,     11,    604,  21962,   4235,  20998,    198,   1016,\n",
      "            278,  46434,     32,     11,  12289,   1303,    261,     32,      7,\n",
      "            679,     21,      8,   2732,  23366,    263,     54,  15288,  69761,\n",
      "          71839,  15777,    198,  11772,    685,  57036,   2000,  16206,   5450,\n",
      "          75145,  99031,   6109,     25,   1600,  26538,    679,     21,  35982,\n",
      "          18652,    198,   8059,     11,   1163,    309,     11,    604,  14206,\n",
      "           4235,   8273,    198,  91635,  43588,     51,  50640,    301,   2067,\n",
      "             41,  24412,   1142,    301,     50,  38406,  64151,   1291,  27887,\n",
      "           8324,   3102,    569,     38,      7,    679,     21,      8,  32237,\n",
      "          12529,   7058,  25624,    369,   4382,   2723,  20212,     13,    763,\n",
      "             25,   7327,  15217,    389,  13257,    198,  48567,     11,    604,\n",
      "          12060,     16,   4235,  12171,     15,    198,     54,    526,     48,\n",
      "          50640,    526,     33,     11,  17198,     78,     43,      7,    679,\n",
      "             20,      8,  81434,   3231,  44412,    985,  12529,  25624,    198,\n",
      "            438,  22746,   5450,  58255,     41,   5158,     40,     11,    604,\n",
      "           9741,     24,   4235,   9714,     20,    198,     54,    526,   1901,\n",
      "          51932,  21313,    622,  28328,    833,    622,     11,   1163,    268,\n",
      "           1901,    320,    679,     19,     64,      8,  33025,   4876,    323,\n",
      "           1495,  53258,    198,  95711,   5450,     25,   2783,     45,  12852,\n",
      "          11541,    275,   2423,    261,     11,    604,  11068,     16,   4235,\n",
      "           6330,     16,    198,     54,    526,   1901,     11,  37120,    622,\n",
      "             11,  43758,    622,     11,  25507,   1901,    320,    679,     19,\n",
      "             65,      8,  33025,   4876,  40188,    555,    198,   1485,     75,\n",
      "           1113,    263,  69292,  39157,   5450,     25,   6157,  15836,  11541,\n",
      "            275,   2423,    261,     11,    604,   5037,     17,   4235,   5037,\n",
      "             24,    198,     55,  23332,     39,  44639,  69710,     44,  44639,\n",
      "           3524,     56,  51932,  17156,     55,      7,    679,     20,      8,\n",
      "           3246,     70,  56748,   7642,  20053,    336,  13025,   2590,    198,\n",
      "          45838,  52286,   4539,  95711,  17126,     55,    344,   1762,   1374,\n",
      "            277,     55,    344,     25,   3965,  22393,  21310,    198,  76065,\n",
      "          11865,  28365,  10602,     45,  51932,  18664,     44,  24412,    318,\n",
      "          23263,      7,    679,     19,      8,  42097,  23013,  22317,   2788,\n",
      "          25624,   2000,    198,  90153,   6108,   3488,  36864,     13,    763,\n",
      "             25,  55227,    315,    279,    220,    679,     19,  49843,   7058,\n",
      "            768,    389,  46763,   5528,    304,   5933,   4221,   8863,    320,\n",
      "           2783,     45,  12852,    705,  12086,    198,  22926,   4235,  13655,\n",
      "         128009, 128006,    882, 128007,    271,   7184,     11,   2728,    420,\n",
      "           3488,     25,  10699,    527,    279,  12283,    315,    279,   5684,\n",
      "           4710,    220,  21335,   1203,    279,   4320,   1193,    304,    264,\n",
      "          13325,   1160,   3645,     11,    369,   3187,     25,   2570,     32,\n",
      "           1882,     33,   7352,   1442,    499,   1541,    956,   1440,    279,\n",
      "           4320,     11,   1120,    471,    459,   4384,   1160,     13, 128009,\n",
      "         128006,  78191, 128007,    271]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "          14818,    543,   2681,  21656,     11,    604,  11123,   4235,   5245,\n",
      "            198,  51697,     56,  31214,  19260,     57,  21861,    359,     44,\n",
      "              7,    679,     20,     64,      8,   1747,    287,  23013,  22354,\n",
      "           2000,  84216,  21656,    198,   1073,  90153,  79367,  53214,   8268,\n",
      "           3518,     14,   3965,     21,     13,   6268,   4643,     11,   3222,\n",
      "           1277,   1129,    277,  89833,   2726,   6018,   3518,     14,   3965,\n",
      "             21,     13,   6268,   4643,    198,  51697,    816,  31214,  19260,\n",
      "           1901,  21861,    359,    386,  31214,  19260,    816,  51932,  17156,\n",
      "           1630,    320,    679,     20,     65,      8,  21579,   5502,    323,\n",
      "          12976,    991,   7058,   2788,  25624,  45838,  52286,   4539,  44412,\n",
      "           5450,     25,   6157,  15836,     11,    604,  13302,     16,   4235,\n",
      "          13302,     22,    198,     44,   1609,    337,    869,     51,  21861,\n",
      "           6256,    441,    424,     40,     11,   1163,    268,     42,     11,\n",
      "          10803,  77927,  16929,     11,  80966,     41,      7,    679,     18,\n",
      "             64,      8,     35,  26204,  10200,   7058,  13898,    708,     69,\n",
      "           5880,    438,    764,  27663,    438,  50828,  77907,   2786,   5450,\n",
      "             25,  62339,   4282,     34,    345,     33,   1751,    283,     43,\n",
      "          50640,   6427,     44,  38406,     71,   1494,   2453,   5676,     57,\n",
      "             11,   1687,    258,  49120,     42,      7,   6910,      8,  24350,\n",
      "           3095,    258,   8989,   4269,    198,  15218,  29992,  49165,   1627,\n",
      "          11541,    324,   6713,  30915,    988,     11,  40345,   2637,    604,\n",
      "          15134,     16,   4235,  15134,     24,    198,    975,   8529,   7044,\n",
      "          73843,    269,  14127,    278,    627,     44,   1609,    337,    869,\n",
      "            350,     11,    816,   7141,    468,     83,     11,  84226,    343,\n",
      "            480,    320,    679,     18,     65,      8,  89333,   4633,   5912,\n",
      "           1385,    304,  19815,    198,   8920,   1178,  36369,    811,   5450,\n",
      "             25,  81158,     12,   7476,  56493,     11,    604,  25594,   4235,\n",
      "          23986,    198,  31456,    301,     44,  20594,  18744,     53,  44754,\n",
      "          83591,    301,   6748,      7,    679,     16,      8,     32,  28956,\n",
      "          27896,   2658,    491,    269,  17840,    535,  21656,    198,    263,\n",
      "          27364,  48712,   1697,    695,   5450,     25,  85438,    287,    708,\n",
      "             69,   1820,   1591,    339,  98697,  79590,    198,    263,  33156,\n",
      "          21656,  90104,   2735,     12,    806,    705,    604,  21474,   4235,\n",
      "          23713,    198,  31456,    301,    386,     11,    350,  18744,    650,\n",
      "             11,    735,  83591,    301,  12478,    320,    679,     17,      8,\n",
      "          38829,   4954,    379,   6438,     25,  69311,   5780,    198,  21656,\n",
      "           2000,  44233,    695,   5450,     25,  46608,     11,   1741,     44,\n",
      "             11,    604,  15828,   4235,  11209,    198,  31456,    301,     44,\n",
      "          59962,  28323,     55,  20594,  18744,     53,      7,    679,     19,\n",
      "              8,  17020,    287,    700,   1201,    258,   3833,   1697,  38691,\n",
      "           2065,    198,   6644,   1729,  16564,  34595,  27061,   5450,     25,\n",
      "          24350,   3095,    258,   8989,   4269,  15218,    198,  29992,  49165,\n",
      "             11,    604,   8546,     24,   4235,   8899,     22,    198,  31456,\n",
      "            301,    386,     11,  16870,  80457,    445,     11,    393,  16499,\n",
      "            822,  39991,     11,   1880,    453,    320,    679,     21,      8,\n",
      "            473,   1640,  79173,  71647,    315,    198,  90153,  87286,   5450,\n",
      "             25,   6157,  15836,     11,    604,   6280,     20,   4235,   5162,\n",
      "             16,    198,     49,    135,    229,   2701,     84,    135,    248,\n",
      "            135,    229,  42961,    432,     11,   2100,     73,   4657,    393,\n",
      "            320,    679,     15,      8,   4476,  24686,    369,  34011,   5768,\n",
      "           6427,    449,    198,  35353,  10803,  71764,   5450,     25,  85438,\n",
      "            287,    708,     69,   1820,     43,  67713,    679,     15,   6919,\n",
      "           8845,    263,   3648,   1163,    278,   7058,     75,    833,    288,\n",
      "           2000,     45,  12852,  90715,     11,   2818,   5726,     11,   2257,\n",
      "           1169,   2629,  28112,  69151,     11,    604,   1774,   4235,   1135,\n",
      "            198,     49,    380,    437,   6780,     47,     11,  26368,  21215,\n",
      "             39,      7,    679,     21,      8,     49,   3013,     17,   4175,\n",
      "             25,     49,   3013,   4539,  12529,  25624,   8350,    266,    309,\n",
      "           5859,    627,    644,     25,  34746,  99031,   6109,  92348,  21861,\n",
      "           2702,    261,     11,    604,  21962,   4235,  20998,    198,   1016,\n",
      "            278,  46434,     32,     11,  12289,   1303,    261,     32,      7,\n",
      "            679,     21,      8,   2732,  23366,    263,     54,  15288,  69761,\n",
      "          71839,  15777,    198,  11772,    685,  57036,   2000,  16206,   5450,\n",
      "          75145,  99031,   6109,     25,   1600,  26538,    679,     21,  35982,\n",
      "          18652,    198,   8059,     11,   1163,    309,     11,    604,  14206,\n",
      "           4235,   8273,    198,  91635,  43588,     51,  50640,    301,   2067,\n",
      "             41,  24412,   1142,    301,     50,  38406,  64151,   1291,  27887,\n",
      "           8324,   3102,    569,     38,      7,    679,     21,      8,  32237,\n",
      "          12529,   7058,  25624,    369,   4382,   2723,  20212,     13,    763,\n",
      "             25,   7327,  15217,    389,  13257,    198,  48567,     11,    604,\n",
      "          12060,     16,   4235,  12171,     15,    198,     54,    526,     48,\n",
      "          50640,    526,     33,     11,  17198,     78,     43,      7,    679,\n",
      "             20,      8,  81434,   3231,  44412,    985,  12529,  25624,    198,\n",
      "            438,  22746,   5450,  58255,     41,   5158,     40,     11,    604,\n",
      "           9741,     24,   4235,   9714,     20,    198,     54,    526,   1901,\n",
      "          51932,  21313,    622,  28328,    833,    622,     11,   1163,    268,\n",
      "           1901,    320,    679,     19,     64,      8,  33025,   4876,    323,\n",
      "           1495,  53258,    198,  95711,   5450,     25,   2783,     45,  12852,\n",
      "          11541,    275,   2423,    261,     11,    604,  11068,     16,   4235,\n",
      "           6330,     16,    198,     54,    526,   1901,     11,  37120,    622,\n",
      "             11,  43758,    622,     11,  25507,   1901,    320,    679,     19,\n",
      "             65,      8,  33025,   4876,  40188,    555,    198,   1485,     75,\n",
      "           1113,    263,  69292,  39157,   5450,     25,   6157,  15836,  11541,\n",
      "            275,   2423,    261,     11,    604,   5037,     17,   4235,   5037,\n",
      "             24,    198,     55,  23332,     39,  44639,  69710,     44,  44639,\n",
      "           3524,     56,  51932,  17156,     55,      7,    679,     20,      8,\n",
      "           3246,     70,  56748,   7642,  20053,    336,  13025,   2590,    198,\n",
      "          45838,  52286,   4539,  95711,  17126,     55,    344,   1762,   1374,\n",
      "            277,     55,    344,     25,   3965,  22393,  21310,    198,  76065,\n",
      "          11865,  28365,  10602,     45,  51932,  18664,     44,  24412,    318,\n",
      "          23263,      7,    679,     19,      8,  42097,  23013,  22317,   2788,\n",
      "          25624,   2000,    198,  90153,   6108,   3488,  36864,     13,    763,\n",
      "             25,  55227,    315,    279,    220,    679,     19,  49843,   7058,\n",
      "            768,    389,  46763,   5528,    304,   5933,   4221,   8863,    320,\n",
      "           2783,     45,  12852,    705,  12086,    198,  22926,   4235,  13655,\n",
      "         128009, 128006,    882, 128007,    271,   7184,     11,   2728,    420,\n",
      "           3488,     25,  10699,    527,    279,  12283,    315,    279,   5684,\n",
      "           4710,    220,  21335,   1203,    279,   4320,   1193,    304,    264,\n",
      "          13325,   1160,   3645,     11,    369,   3187,     25,   2570,     32,\n",
      "           1882,     33,   7352,   1442,    499,   1541,    956,   1440,    279,\n",
      "           4320,     11,   1120,    471,    459,   4384,   1160,     13, 128009,\n",
      "         128006,  78191, 128007,    271,    681,  51697,    816,    518,    364,\n",
      "             43,  19260,   1901,    518,    364,  31192,    386,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Lin Y', 'Liu Z', 'Sun M']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Learning Knowledge Graph Embeddings\n",
      "with Type Regularizer\n",
      "BhushanKotnisandViviNastase\n",
      "HeidelbergUniversity\n",
      "Heidelberg,Germany69120\n",
      "{kotnis,nastase}@cl.uni-heidelberg.de\n",
      "ABSTRACT Thisresultsinamulti-graph,i.e. agraphwithdifferenttypesof\n",
      "Learningrelationsbasedonevidencefromknowledgerepositories linkswherealinktypecorrespondstoarelationtype.\n",
      "reliesonprocessingtheavailablerelationinstances. Knowledge KGsareknowntobeincomplete[10],i.e.,asignificantnumber\n",
      "repositoriesarenotbalancedintermsofrelationsorentities–there ofrelationsbetweenentitiesaremissing.Embeddingtheknowledge\n",
      "arerelationswithlessthan10butalsothousandsofinstances,and graphinacontinuousvectorspacehasbeensuccessfullyusedto\n",
      "entities involved in less than 10 but also thousands of relations. addressthisproblem[2,13,15].Suchmodelsrepresentthecompo-\n",
      "Manyrelations,however,havecleardomainandrange,whichwe nentsofthegraph,i.e.,theentitiesandrelations,usingrealvaluedla-\n",
      "hypothesize could help learn a better, more generalizing, model. tentfactorsthatencodethestructureoftheknowledgegraph.Forex-\n",
      "WeincludesuchinformationintheRESCALmodelintheform amplethelatentfactormodelshouldbeabletorecoverColognefrom\n",
      "ofaregularizationfactoraddedtothelossfunctionthattakesinto thelatentrepresentationsofMoselleandriver flowsThrough city.\n",
      "accountthetypes(categories)oftheentitiesthatappearasarguments ExamplesincludetheRESCAL[13]tensorfactorizationmodel,the\n",
      "torelationsintheknowledgebase.TestedonFreebase,afrequently TransE model [2] and their variations [9, 12]. We focus on the\n",
      "usedbenchmarkingdatasetforlink/pathpredictingtasks,wenote RESCALmodel,oneofthemostflexibleandwidelyusedmodels.\n",
      "increasedperformancecomparedtothebaselinemodelintermsof RESCALisabilinearmodelthatrepresentstriplesasapairwise\n",
      "meanreciprocalrankandhits@N,N=1,3,10. Furthermore,we interactionofsourceandtargetentitylatentfactors(embeddings)\n",
      "discoverscenariosthatsignificantlyimpacttheeffectivenessofthe throughamatrixthatrepresentsthelatentfactorsoftheconnecting\n",
      "typeregularizer. relation.Theentityandrelationrepresentationsinducedcanbeused\n",
      "to predict additional relations – edges – between known entities.\n",
      "KEYWORDS Table1listsafewexamplesofentitytypeinformationinFreebase.\n",
      "Existingknowledgegraphsareimbalanced–bothrelationand\n",
      "KnowledgeGraphs,GraphEmbedding,LinkPrediction\n",
      "entityfrequenciesvarywidely,asevidentfromthestatisticsonFree-\n",
      "ACMReferenceformat: base15kshowninFigure1.Sinceentityandrelationembeddings\n",
      "BhushanKotnisandViviNastase.2017.LearningKnowledgeGraphEm- arebasedontheconnectivitystructureofthegraph,itisreasonable\n",
      "beddings toaskwhatistheoutcomeoftheknowledgegraphembeddingfor\n",
      "withTypeRegularizer.InProceedingsofK-CAP2017:KnowledgeCapture\n",
      "entitiesandrelationswhichareunderrepresentedinthegraph,in\n",
      "Conference,Austin,TX,USA,December4–6,2017(K-CAP2017),6pages.\n",
      "particular,howgoodaretheyforthetaskoflinkprediction.\n",
      "DOI:10.1145/3148011.3154466\n",
      "ApproachessuchasRESCALtakeanextensionalviewofrela-\n",
      "tions–theyprocessthecollectionofinstanceswithoutknowledgeof\n",
      "1 INTRODUCTION higherlevelrulesorinformationabouttheserelations.Wehypothe-\n",
      "Knowledge–lexical,worldandcommon-sense–iscrucialfortasks sizethatprovidingthehigherlevel–intensional–viewintheform\n",
      "suchasautomatedtextcomprehensionandsummarization, ques- oftypesorcategoriesofrelationarguments,canleadtoimproved\n",
      "tionanswering,naturallanguagedialoguesystems.Tomakesuch resultsforthetaskoflinkprediction.Thismaybetrueparticularly\n",
      "knowledgeavailableforautomaticprocessing,themostcommon forknowledgegraphssuchasFreebasethathavestronglytypedrela-\n",
      "approachistoprovideitasacollectionofrelationtriples–entities tions,andalsoforlow-frequencyrelationsorforrelationsinvolving\n",
      "or concepts connected by a relation: e.g., (concept:city:London, low-frequencyentities.\n",
      "relation:country capital,concept:country:UK).Globally,suchcol- Inthisarticlewepresentexperimentalresultssupportingthehy-\n",
      "lectionscanbeviewedasknowledgegraphs(KGs),forexample pothesisthataugmentingsingle-relationmodelswithentitytypein-\n",
      "NELL [3], Freebase [1] and YAGO [16]. In such graphs, nodes formation,intheformofa‘Type’regularizer,leadstoimprovements\n",
      "(entities/concepts)maybeconnectedbydifferenttypesofrelations. inpredictingmissinglinks.Theresultsshowthateventhoughthe\n",
      "bilinearmodelinducesrepresentationsforallentitiesandrelations\n",
      "Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor together–soitimplicitlyusesthetypeinformationweprovideasa\n",
      "classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed\n",
      "separaterelation–thetyperegularizerwhichexplicitlyincludessuch\n",
      "forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation\n",
      "onthefirstpage.CopyrightsforcomponentsofthisworkownedbyothersthanACM informationforeachrelationleadstobetterresults.Furthermore,we\n",
      "mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish, notethepositiveimpactofincludingthetyperegularizerforrelations\n",
      "topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora\n",
      "involvinglow-frequencyentities,whereaslow-frequencyrelations\n",
      "fee.Requestpermissionsfrompermissions@acm.org.\n",
      "K-CAP2017,Austin,TX,USA arelessaffectedbythisaddedinformation. Wealsoanalyzethe\n",
      "©2017ACM. 978-1-4503-5553-7/17/12...$15.00\n",
      "DOI:10.1145/3148011.3154466\n",
      "8102\n",
      "raM\n",
      "2\n",
      "]IA.sc[\n",
      "2v87290.6071:viXra\n",
      "K-CAP2017,December4–6,2017,Austin,TX,USA BhushanKotnisandViviNastase\n",
      "SourceType Source PathorRelation Target TargetType\n",
      "film star wars episode IV produced by дeorдe lucas film producer\n",
      "person alexandre dumas people profession writer profession\n",
      "academic post professor profession people albert einstein person\n",
      "Table1:EntityTypeInformation:ExamplesofsourceandtargetentitytypesfromFreebaseusedinthetyperegularizer.\n",
      "104 Argument_1\n",
      "train\n",
      "103 dev\n",
      "test\n",
      "102\n",
      "101\n",
      "100\n",
      "0 2000 4000 6000 8000 10000 12000 14000\n",
      "104 Argument_2\n",
      "train\n",
      "103 dev\n",
      "test\n",
      "102\n",
      "101\n",
      "100\n",
      "0 2000 4000 6000 8000 10000 12000 14000\n",
      "105\n",
      "104\n",
      "103\n",
      "102\n",
      "101\n",
      "100\n",
      "0 200 400 600 800 1000 1200\n",
      "ycneuqerF\n",
      "entity information, in the form of latent factors, improves KBC\n",
      "performance.Thesourceandtargettypesarenotexplicitlyincluded.\n",
      "[4]makeusetheoftypeinformationandproduceavariationof\n",
      "RESCALtheycallTRESCAL–TypedRESCAL.Thetypeinformation\n",
      "isusedtoimprovetheefficiencyofthemodel,byreducingthesize\n",
      "oftheentitymatrixinthecomputationofthelossfunctiontoentities\n",
      "belongingtothedomainandrangeoftherelation.Theentitytype\n",
      "assuchisonlyimplicitlyincorporated,assomethingsharedbythe\n",
      "entitiessingledoutforcomputingthelossfunction.\n",
      "[5]buildson[11],andusesanRNNtomodelpathswhichincor-\n",
      "poratetypeinformationfortheentitiesalongthepath.Entitiesare\n",
      "representedasasumoftheirentitytypes,whicharelearnedduring\n",
      "training.Includingthisinformationleadstohigherperformance.\n",
      "Compared with these previous approaches, we add the entity\n",
      "typesexplicitlyinthemodel,andderivearepresentationforentities\n",
      "andtheirtypesconcurrently.Weanalyzetheimpactofusingsuch\n",
      "representationforlinkpredictionwithdifferentamountsoftraining\n",
      "data,tounderstandunderwhatconditionsthetypeinformationhas\n",
      "apositiveimpact.\n",
      "Relation\n",
      "train\n",
      "dev\n",
      "test 3 METHODS\n",
      "InthissectionwedescribetheRESCALmodelandshowhowthe\n",
      "typeregularizerwasaddedtoincludethetypeinformationforeach\n",
      "relationinthecomputationofthelossfunction.\n",
      "3.1 Definitions\n",
      "LetE,RbethesetofentitiesandrelationsintheKGrespectively.A\n",
      "knowledgegraphGisasetoftriples(s,r,t)wheres,t ∈E, r ∈R\n",
      "Figure 1: Statistics on argument and relation frequencies for\n",
      "andrelationr connectsstot.\n",
      "Freebase15k\n",
      "Theknowledgebasecompletion(KBC)taskisthetaskofclas-\n",
      "sifyingwhetherthetriple(s,r,t)isapartoftheknowledgegraph.\n",
      "Thiscanbedescribedas(s,r,?)or(?,r,t)wherethequestionmark\n",
      "representstheunknowncorrecttarget/sourceentityfromasetof\n",
      "effectsoftrainingdatasizeontheusefulnessofthetyperegularizer,\n",
      "candidateentities.\n",
      "andnotethatitsimpactgrowswiththeamountoftrainingdata.\n",
      "2 RELATEDWORK 3.2 RESCALModel\n",
      "Avarietyoflatentfactormodels[2,13–15]havebeendevelopedto TheRESCALmodel[13]weightstheinteractionofallpairwise\n",
      "represententitiesandrelationsinaknowledgegraph,andhavebeen latentfactorbetweenthesourceandtargetentityforpredictinga\n",
      "used to address the knowledge base completion (KBC) problem. relation. Itrepresentseveryentityasavector(x ∈Rd),andevery\n",
      "Mostlatentfactormodelsaretrainedoneitherknowledgegraph relationasamatrixW ∈ Rd×d. Thismodelrepresentsthetriple\n",
      "triples,ortriplesextractedfromopendomainknowledgeextraction (s,r,t)asascoregivenby\n",
      "tools [14]. A notable exception is the RNN model proposed by sc(s,r,t)=x sT Wr xt\n",
      "[11]thatlearnspathembeddingsforknowledgebasecompletion. Thisisequivalenttotensorfactorizationwhereeachrelationmatrix\n",
      "[7]proposeacompositionalobjectivefunctionoverlatentfactor isasliceofthetensor. Thesevectorsandmatricesarelearnedby\n",
      "models, whichistrainedonpathsaswellastriples. Formodels constructingalossfunctionthatcontraststhescoreofacorrecttriple\n",
      "thatarecompositional,[17]showsthatincorporatingintermediate toincorrectones.Hereweusethemax-marginlossdescribedinthe\n",
      "LearningKnowledgeGraphEmbeddings\n",
      "withTypeRegularizer K-CAP2017,December4–6,2017,Austin,TX,USA\n",
      "followingequation: WeobtainFreebasecategorydatafrom[6],andthentheentity\n",
      "typebymappingtheFreebaseentityidentifiertotheFreebasecat-\n",
      "N\n",
      "J(Θ)=(cid:213) (cid:213) mm(σ(sci),σ(sc i(cid:48))) (1) weg ho ir cy h. isT uh si es dre insu tl ht es ti rn ai1 n0 in1 g,3 s5 t3 agi en.s Ita tn isce ns oto uf st eh de dc ua rt ie ng go tr ey stre til mat eio.n\n",
      "i=1t(cid:48)∈N(t)\n",
      "(cid:20) (cid:21) 4.2 Implementation\n",
      "mm(σ(sci),σ(sc i(cid:48)))=max 0,1−σ(sci)+σ(sc i(cid:48))\n",
      "WeusetheAdam[8]SGDoptimizerfortrainingbecauseitaddresses\n",
      "theproblemofdecreasinglearningrateinAdaGrad.Weusemedian\n",
      "wherethereareNpositiveinstances,positiveandnegativeinstances\n",
      "arescoredassci =sc(si,ri,ti)andsc i(cid:48) =sc(si,ri,t i(cid:48)),respectively. g thr aa tdi ee nn tt itc yli ep mpi bn eg dt do inp gre sv he an vt eex up nl io ts ni ove rmg.ra Wdi een pt es ra fon rd mw ee da el xs ho ae un ss tu ivr ee\n",
      "N(t)isthesetofincorrecttargetsandσ isthesigmoidfunction.\n",
      "gridsearchfortheL2regularizeraswellasα onthevalidationset\n",
      "andwetunedthetrainingdurationusingearlystopping.Weuse100\n",
      "3.3 TheTypeRegularizer\n",
      "dimensionalentityvectorinallexperiments1.\n",
      "Weintroducearegularizertermwhichincorporatestypeinformation\n",
      "ofsourceandtargetentities. Letscat bethetypeforentitys and 4.3 EvaluationProcedure\n",
      "rcat therelationbetweensandscat.Dependingontheknowledge\n",
      "Forevaluationwefollowtheproceduredescribedin[15].Forevery\n",
      "resource,rcat couldbeis a(inanontology,forexample),cateдory\n",
      "testtriplewepredicteitherthesourceorthetarget,andnegative\n",
      "(inaresourcebuiltbasedonWikipedia),orothersuchrelationsthat\n",
      "instacesfortrainingandtestingareproducedbycorruptingpositive\n",
      "capturetheentitytype.Afewexamplesofentitytypescanbeseen ones:wereplaces(ort)ina(s,r,t)triplewithansn (ortn)thathas\n",
      "inTable1.Notethatentitytypeinformationisnotusedduringtest\n",
      "thesametypeass(ort)butdoesnotappearinapositiveinstance\n",
      "time. (sn,r,t) (or (s,r,tn)). For meaningful comparison, the negative\n",
      "Ifsisthesourceentityandt thetargetentityforqueryq,thenwe\n",
      "triplesthatoccurintrainingorvalidationdatasetsaspositivetriples\n",
      "definetheregularizerasinequation2,whereN(scat)andN(tcat)\n",
      "arefilteredout.Forfasterevaluation,insteadofusingallnegative\n",
      "aresetsof(negatives)forscat,tcat,whileT(scat),T(tcat)aresets\n",
      "triples,weproduce1000byrandomlysamplingentitiesfromthe\n",
      "ofcorrectcategoriesforsourcesandtargett respectively.mmisthe\n",
      "entireset.Wereportresultsintermsofhitsat1,3,10(HITS@1,3,10)\n",
      "maxmarginlossdescribedinequation(1).\n",
      "andmeanreciprocalrank(MRR)metrics.HitsatKistheproportion\n",
      "ofcorrectanswers(hits)inthefirstKrankedpredictions,whileMRR\n",
      "isthemeanofthereciprocaloftherankofthecorrectanswers.\n",
      "R(Θ,q):=\n",
      "(cid:213) (cid:18) (cid:48) (cid:19) 4.4 Results\n",
      "mm σsc(s,rcat,scat),σsc(s,rcat,s cat)\n",
      "(cid:48) Weusethebilinear(RESCAL)modelasabaseline.Asevidenced\n",
      "s cat∈N(scat)\n",
      "bytheresultsinTable2,addingthetyperegularizerimprovesper-\n",
      "scat∈T(scat)\n",
      "(cid:18) (cid:19) formance. It may be tempting to think that the performance im-\n",
      "+ (cid:213) mm σsc(t,rcat,tcat),σsc(t,rcat,t c(cid:48) at) (2) provementisnaturalsinceweareprovidingadditionalinformation\n",
      "(cid:48) throughthetyperegularizer.Wetestthisinfurtherexperiments.\n",
      "t cat∈N(tcat)\n",
      "tcat∈T(tcat)\n",
      "Metrics Bilinear Bilinear+TR\n",
      "Thecompleteobjectivefunctiontobeminimizedis MRR 0.343 0.3862\n",
      "N HITS@1 0.2451 0.304\n",
      "J(Θ)=(cid:213) (cid:213) mm(qi,ti,t i(cid:48) )+αR(Θ,qi) HITS@3 0.3804 0.4161\n",
      "i=1t i(cid:48)\n",
      "∈N(qi) HITS@10 0.5312 0.5408\n",
      "Table2: Evaluation: PerformanceComparisonbetweenbilin-\n",
      "wherethehyper-parameterα,α ≥ 0, controlstheimpactofthe\n",
      "earmodelwithandwithouttyperegularizer.\n",
      "regularizertermsandN(qi)isthesetofnegativetargetsforquery\n",
      "qi,whereqi correspondstoquery(si,ri,?). Wetesttheimpactofthetyperegularizerbyanalyzingitsperfor-\n",
      "manceondifferentsizesoftrainingdata.Wefirstgeneratemultiple\n",
      "4 EXPERIMENTS trainingdatasetsbyrandomlysampling25%,50%and75%ofthe\n",
      "4.1 Data triples. AsillustratedinTable3,whenusingonly25%to50%of\n",
      "thetrainingdata,theperformancedrops.Thetyperegularizeruses\n",
      "WecarryoutexperimentsonFB15K,asubsetoftheFreebaseknowl- categoryinformation,undercertaincircumstances(α =1)addingit\n",
      "edgegraphprovidedby[2]. Thisdatasetisastandardbenchmark\n",
      "isequivalenttoaddingapproximately100,000newtripleswithcate-\n",
      "datasetusedforevaluatinglinkpredictionalgorithms[2,12,18].\n",
      "goryrelationtothetrainingset.Thus,simplyaugmentingthemodel\n",
      "TheFB15Kdatasetconsistsof1345relationsand14,951entities.\n",
      "withadditionalinformationdoesnotalwaysimproveperformance.\n",
      "Thetraining,validationandtestsetconsistsof483,142,50,000and\n",
      "Thereasonbehindtheperformancedropwithlesstrainingdatais\n",
      "59,071triplesrespectively. TheFreebaserelationsdonotinclude\n",
      "notobvious,becauseaddingexternalinformationshouldhelpthe\n",
      "thecategoryrelation,thusthereisnooverlapbetweenthecategory\n",
      "triplesandFB15Ktriples. 1Codeisavailableathttps://github.com/bhushank/kge\n",
      "K-CAP2017,December4–6,2017,Austin,TX,USA BhushanKotnisandViviNastase\n",
      "%trainingdata Model MRR %Improvement RelationName Instances(train) Instances(test)\n",
      "100\n",
      "Bilinear 0.343 r1 /people/person/profession 11636 1384\n",
      "Bilinear+TR 0.3862 +12.59 r2 /music/genre/artists 5952 679\n",
      "Bilinear 0.3495 r3 /film/film/country 2407 280\n",
      "75\n",
      "Bilinear+TR 0.3552 +1.6 r4 /tv/tv program/genre 1010 100\n",
      "Bilinear 0.3457 Table4:Relationswithtrainandtestinstances\n",
      "50\n",
      "Bilinear+TR 0.3409 -1.3\n",
      "Bilinear 0.332\n",
      "25\n",
      "Bilinear+TR 0.3198 -3.67\n",
      "Table3: EffectoftrainingdatasizeonTR:Performancecom-\n",
      "parisonbetweenbilinearmodelswithandwithouttyperegular-\n",
      "izerfordifferentdatasetsizes.\n",
      "modellearnbetterembeddings. Wehypothesizethatthedropin\n",
      "performanceisbecausewhenfewernumberoftraininginstancesare\n",
      "available,thetyperegularizerleadsthesystemtolearnrelationsthat\n",
      "over-generalize. Themodelisbiasedtowardslearningcategories\n",
      "verywellforreducingtrainingloss.Thisresultsinembeddingsthat\n",
      "arebiasedtowardspredictingrelationsatthelevelofcategoriesand\n",
      "notindividualrelationsresultinginperformancedropfortherelation\n",
      "predictiontask.\n",
      "Figure3: MRRvs. PercentTrainDataformultiplerelations:\n",
      "NumberoftraininginstancesmodulateeffectofTypeRegular-\n",
      "izer.RelationslistedinTable4\n",
      "Fig. 3 shows the performance in terms of MRR (using Type\n",
      "Regularizer)forlinkpredictiononthesefourrelations.Theorange\n",
      "andbluelinesdenoterelations(r1,r2)with11,636and5952train-\n",
      "inginstancesrespectively,whiletheredandgreencurvesdenote\n",
      "relations(r3,r4)with2407and1010traininginstancesrespectively.\n",
      "Theredandgreencurves(therelationswithfewerinstances)showa\n",
      "largerchangeinMRRcomparedtotheorangeandbluecurves.This\n",
      "confirmsourhypothesisthattheTypeRegularizerismoresensitive\n",
      "forrelationswithasmallernumberoftraininginstances,andindi-\n",
      "N catesthattheembeddingslearnedforrelationswithlargernumber\n",
      "ofinstancesarelessbiasedtowardspredictingcategories.\n",
      "Figure 2: MRR vs. α: MRR drops with increasing We note that equation (2) has the same max margin structure\n",
      "strength of the type regularizer for models trained on 25% asthelossfunction,equation(1). Thereforeusingthisparticular\n",
      "(blue) and 100% (orange) of FB15K dataset. Plot for formulaforthetyperegularizerisequivalenttoaddingthecategory\n",
      "α =0.0001,0.001,0.01,0.01,1,5,10 relationasanadditionalsliceofthetensorfactorizedbyRESCAL,\n",
      "thenthehyperparameterαis1.Experimentshaveshownthoughthat\n",
      "We investigate this hypothesis by varying the value of α that finetuningα –andthisfine-tuningtheusageoftypeinformation–\n",
      "weighstheimportanceofthetyperegularizer(cf.equation1).We canleadtobetterresults.Morespecificallyitisequivalenttoadding\n",
      "plottheMeanReciprocalRankvs.thestrengthofthetyperegular- 101,353uniqueinstancesofcategoryrelation.\n",
      "izerformodeltrainedononly25%ofthetrainingdatainFig. 2. We also performed overall relation and entity analysis based\n",
      "Thehigherthestrengthofthetyperegularizer,thehigherthecost on their occurrence frequency. Looking at relations grouped by\n",
      "incurredformis-predictingthecategory. AsFig. 2shows,MRR theorder ofmagnitude(oom) oftheir occurrence frequency pre-\n",
      "fallssharplywithincreaseinα. Thiseffectisnotobservedinthe sentedinFigure4wenotethatlowfrequencyrelationsseemnot\n",
      "100% training data scenario. This suggests that adding category tobeaffectedbythetyperegularizer,andaremodeledbetterusing\n",
      "informationmayleadtoimprovedperformanceonlywhentheadded onlytheinstancesthemselves. Theresonforthisisthatverylow\n",
      "informationdoesnotseverelybiasthetrainingdata. frequency relations actually connect high frequency entities, e.g.\n",
      "Toinvestigatetheimpactoftrainingdatasizeonthetypereg- relation /award/hall of fame/discipline. On the other hand, high\n",
      "ularizerperformance,weanalyzeindetailtheperformanceofthe frequencyrelationshaveoveralllowerresultsthanotherrelations.\n",
      "systemforrelationswithadifferentnumberoftraininginstances. Thereasonforthisisthatinnumerouscases,oneofthearguments\n",
      "Table4listsfourrelationsweusedtolookintothisphenomenon. oftheserelationsisalowfrequencyentity.Forexample,thelives in\n",
      "LearningKnowledgeGraphEmbeddings\n",
      "withTypeRegularizer K-CAP2017,December4–6,2017,Austin,TX,USA\n",
      "OOM 0.0 OOM 1.0 OOM 2.0 OOM 3.0 OOM 4.0\n",
      "0.7\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "0.0\n",
      "25 50 75 100 25 50 75 100 25 50 75 100 25 50 75 100 25 50 75 100\n",
      "bilinear bilinear_tr\n",
      "OOM 0.0 OOM 1.0 OOM 2.0 OOM 3.0 OOM 4.0\n",
      "0.8\n",
      "0.7\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "0.0\n",
      "25 50 75 100 25 50 75 100 25 50 75 100 25 50 75 100 25 50 75 100\n",
      "bilinear bilinear_tr\n",
      "Figure4:MRRandHITS@10linkpredictionresultsgroupedbytheorderofmagnitudeofrelationfrequency,fordifferentamounts\n",
      "oftrainingdata.\n",
      "relation that connects a person with the city they live in, has as 5 CONCLUSION\n",
      "the”City”argumentanentitythatdoesnotappearinmanyother Weproposedatyperegularizerthatleveragesentitytypeinformation\n",
      "relations. forstate-of-the-artlatentfactormodelslikeRESCAL.Experiments\n",
      "To further clarify the reasons for variation in performance of onFreebaseFB15Kdatasetsuggestthataddingthetyperegular-\n",
      "relations,weanalyzethelinkpredictionresultsbasedontheorder izerimprovesperformanceontheknowledgebasecompletiontask.\n",
      "ofmagnitudeofentityfrequency,presentedinFigure5.Theresults Howeveraddingcategoryinformationmaynotimproveresultsfor\n",
      "inthiscasearemoreinlinewiththeexpectedoutcome–linksthat allrelations,particularlythosewithfewerpositiveinstanceswhere\n",
      "involvelowerfrequencyentitieshavelowerpredictionresults.The introducingcategoryinformationmayleadtoembeddingsthatarebi-\n",
      "typeinformationgenerallyhasapositiveimpactthroughout,except asedtowardscapturing/predictingcategoriesratherthanfinegrained\n",
      "medium-rangeentitieswhereitseemsthattypeinformationleadsto instances.Weplantostudytheimpactoftheaddedtypeinformation\n",
      "overgeneralization. fordatasetswheretherelationsarenotasstronglytypedasFreebase\n",
      "Usingthetyperegularizerasanadditionaltermswhoseweight –forgrammaticalcollocationinformationforexampleandinducing\n",
      "canbecalibratedusingtheα parametermakesiteasiertoadjustthe selectionalpreferences–andformorecomplex, pathprediction,\n",
      "influenceofthetypeinformationbasedonnodedegreesandrelation tasks.\n",
      "frequencies. Furthermore, byincorporatingthetypeinformation\n",
      "inthelossfunctionforeveryrelationasopposedtohavingitasa REFERENCES\n",
      "separaterelationintheknowledgegraphallowstheincorporationof\n",
      "[1] KurtBollacker,ColinEvans,PraveenParitosh,TimSturge,andJamieTaylor.\n",
      "therangeanddomaininformationforeachrelation,asopposedto 2008. Freebase: ACollaborativelyCreatedGraphDatabaseforStructuring\n",
      "modellingtheentitytypeoutsideofaparticularenvironment. HumanKnowledge.InProceedingsofthe2008ACMSIGMODInternational\n",
      "ConferenceonManagementofData(SIGMOD’08).ACM,NewYork,NY,USA,\n",
      "Itisinterestingtonotethatthebestresultsformediumtohigh 1247–1250. https://doi.org/10.1145/1376616.1376746\n",
      "frequencyentitiesandrelationsareobtainedwhenusingthefull [2] AntoineBordes,NicolasUsunier,AlbertoGarcia-Duran,JasonWeston,and\n",
      "Oksana Yakhnenko. 2013. Translating Embeddings for Modeling Multi-\n",
      "trainingdataandthetyperegularizer. Thisindicatesthatthetype\n",
      "relationalData. InAdvancesinNeuralInformationProcessingSystems26,\n",
      "regularizercanmitigatetheoverfittingtendencyofRESCAL,and C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Wein-\n",
      "produceamorerobustmodel. berger(Eds.).CurranAssociates,Inc.,2787–2795.http://papers.nips.cc/paper/\n",
      "5071-translating-embeddings-for-modeling-multi-relational-data.pdf\n",
      "[3] AndrewCarlson,JustinBetteridge,BryanKisiel,BurrSettles,EstevamR.Hr-\n",
      "uschka,andTomM.Mitchell.2010.TowardanArchitectureforNever-Ending\n",
      "LanguageLearning.InAAAI.\n",
      "K-CAP2017,December4–6,2017,Austin,TX,USA BhushanKotnisandViviNastase\n",
      "OOM 0.0 OOM 1.0 OOM 2.0 OOM 3.0\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "25 50 75 100 25 50 75 100 25 50 75 100 25 50 75 100\n",
      "bilinear bilinear_tr\n",
      "OOM 0.0 OOM 1.0 OOM 2.0 OOM 3.0\n",
      "0.7\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "25 50 75 100 25 50 75 100 25 50 75 100 25 50 75 100\n",
      "bilinear bilinear_tr\n",
      "Figure5: MRRandHITS@10linkpredictionresultsgroupedbytheorderofmagnitudeofentityfrequency,fordifferentamounts\n",
      "oftrainingdata.\n",
      "[4] Kai-WeiChang,Wen-tauYih,BishanYang,andChristopherMeek.2014.Typed [12] MaximilianNickel,LorenzoRosasco,andTomasoPoggio.2016.Holographic\n",
      "TensorDecompositionofKnowledgeBasesforRelationExtraction.InPro- EmbeddingsofKnowledgeGraphs.InProceedingsoftheThirtiethAAAICon-\n",
      "ceedingsofthe2014ConferenceonEmpiricalMethodsinNaturalLanguage ferenceonArtificialIntelligence(AAAI’16).AAAIPress,1955–1961. http:\n",
      "Processing(EMNLP).AssociationforComputationalLinguistics,1568–1579. //dl.acm.org/citation.cfm?id=3016100.3016172\n",
      "https://doi.org/10.3115/v1/D14-1165 [13] MaximilianNickel,VolkerTresp,andHans-PeterKriegel.2012. Factorizing\n",
      "[5] RajarshiDas,ArvindNeelakantan,DavidBelanger,andAndrewMcCallum.2016. YAGO:ScalableMachineLearningforLinkedData.InProceedingsofthe21st\n",
      "ChainsofReasoningoverEntities,Relations,andTextusingRecurrentNeural InternationalConferenceonWorldWideWeb(WWW’12).ACM,NewYork,NY,\n",
      "Networks.arXivpreprintarXiv:1607.01426(2016). USA,271–280. https://doi.org/10.1145/2187836.2187874\n",
      "[6] MattGardnerandTomMitchell.2015. EfficientandExpressiveKnowledge [14] SebastianRiedel,LiminYao,AndrewMcCallum,andM.BenjaminMarlin.2013.\n",
      "BaseCompletionUsingSubgraphFeatureExtraction.InProceedingsofthe2015 RelationExtractionwithMatrixFactorizationandUniversalSchemas.InProceed-\n",
      "ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.Association ingsofthe2013ConferenceoftheNorthAmericanChapteroftheAssociation\n",
      "forComputationalLinguistics,1488–1498.https://doi.org/10.18653/v1/D15-1173 forComputationalLinguistics:HumanLanguageTechnologies.Associationfor\n",
      "[7] KelvinGuu,JohnMiller,andPercyLiang.2015.TraversingKnowledgeGraphs ComputationalLinguistics,74–84.http://aclweb.org/anthology/N13-1008\n",
      "inVectorSpace.InProceedingsofthe2015ConferenceonEmpiricalMethods [15] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng.\n",
      "inNaturalLanguageProcessing.AssociationforComputationalLinguistics, 2013. Reasoning With Neural Tensor Networks for Knowledge Base\n",
      "318–327.https://doi.org/10.18653/v1/D15-1038 Completion. In Advances in Neural Information Processing Systems 26,\n",
      "[8] DiederikKingmaandJimmyBa.2014.Adam:Amethodforstochasticoptimiza- C. J. C. Burges, L. Bottou, M. Welling, Z.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,     42,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['FB15K']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: ComputationalLinguistics:HumanLanguageTechnologies.Associationfor\n",
      "[7] KelvinGuu,JohnMiller,andPercyLiang.2015.TraversingKnowledgeGraphs ComputationalLinguistics,74–84.http://aclweb.org/anthology/N13-1008\n",
      "inVectorSpace.InProceedingsofthe2015ConferenceonEmpiricalMethods [15] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng.\n",
      "inNaturalLanguageProcessing.AssociationforComputationalLinguistics, 2013. Reasoning With Neural Tensor Networks for Knowledge Base\n",
      "318–327.https://doi.org/10.18653/v1/D15-1038 Completion. In Advances in Neural Information Processing Systems 26,\n",
      "[8] DiederikKingmaandJimmyBa.2014.Adam:Amethodforstochasticoptimiza- C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Wein-\n",
      "tion.arXivpreprintarXiv:1412.6980(2014). berger(Eds.).CurranAssociates,Inc.,926–934. http://papers.nips.cc/paper/\n",
      "[9] YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,andXuanZhu.2015.Learning 5028-reasoning-with-neural-tensor-networks-for-knowledge-base-completion.\n",
      "EntityandRelationEmbeddingsforKnowledgeGraphCompletion.InProceed- pdf\n",
      "ingsoftheTwenty-NinthAAAIConferenceonArtificialIntelligence(AAAI’15). [16] FabianM.Suchanek,GjergjiKasneci,andGerhardWeikum.2007.Yago:ACore\n",
      "AAAIPress,2181–2187. http://dl.acm.org/citation.cfm?id=2886521.2886624 ofSemanticKnowledge.InProceedingsofthe16thInternationalConference\n",
      "[10] BonanMin,RalphGrishman,LiWan,ChangWang,andDavidGondek.2013. onWorldWideWeb(WWW’07).ACM,NewYork,NY,USA,697–706. https:\n",
      "DistantSupervisionforRelationExtractionwithanIncompleteKnowledgeBase. //doi.org/10.1145/1242572.1242667\n",
      "InProceedingsofthe2013ConferenceoftheNorthAmericanChapterofthe [17] KristinaToutanova,VictoriaLin,Wen-tauYih,HoifungPoon,andChrisQuirk.\n",
      "AssociationforComputationalLinguistics:HumanLanguageTechnologies.As- 2016.CompositionalLearningofEmbeddingsforRelationPathsinKnowledge\n",
      "sociationforComputationalLinguistics,777–782.http://aclweb.org/anthology/ BaseandText.InProceedingsofthe54thAnnualMeetingoftheAssociationfor\n",
      "N13-1095 ComputationalLinguistics(Volume1:LongPapers).AssociationforComputa-\n",
      "[11] ArvindNeelakantan,BenjaminRoth,andAndrewMcCallum.2015.Composi- tionalLinguistics,1434–1444.https://doi.org/10.18653/v1/P16-1136\n",
      "tionalVectorSpaceModelsforKnowledgeBaseCompletion.InProceedings [18] The´oTrouillon,ChristopherRDance,JohannesWelbl,SebastianRiedel,E´ric\n",
      "ofthe53rdAnnualMeetingoftheAssociationforComputationalLinguistics Gaussier,andGuillaumeBouchard.2017. KnowledgeGraphCompletionvia\n",
      "andthe7thInternationalJointConferenceonNaturalLanguageProcessing ComplexTensorFactorization.arXivpreprintarXiv:1702.06879(2017).\n",
      "(Volume1:LongPapers).AssociationforComputationalLinguistics,156–166.\n",
      "https://doi.org/10.3115/v1/P15-1016<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "          93028,     43,  13102,   5706,     25,  35075,  14126,  29356,   9268,\n",
      "          11108,   2168,    367,   2000,    198,     58,     22,     60,  92073,\n",
      "          17198,     84,     11,  13379,  89437,  51526,     47,   3035,     88,\n",
      "             43,  28323,     13,    679,     20,   8404,  22292,    287,  81434,\n",
      "          11461,     82,  93028,     43,  13102,   5706,     11,   5728,   4235,\n",
      "           5833,   7109,   1129,  48836,   2984,   2726,     14,  32329,   2508,\n",
      "          20906,   1032,     12,   1041,     23,    198,    258,   3866,  10115,\n",
      "           5450,  85438,    287,    708,     69,   1820,    679,     20,  92348,\n",
      "            263,  29831,  67966,  18337,    510,    868,     60,  12131,   2100,\n",
      "           9211,     11,  11824,  41287,  25507,     11,  26035,    423,  40623,\n",
      "             11,    323,  13929,  17030,    627,    258,  55381,  14126,  29992,\n",
      "          11108,   2168,    367,   2000,  59122,   1697,     43,  13102,   5706,\n",
      "             11,    220,    679,     18,     13,  27857,    287,   3161,  61577,\n",
      "          27127,  39810,    369,  33025,   5464,    198,  17592,   4235,  13817,\n",
      "             13,   2485,   1129,  48886,   2726,     14,    605,     13,   9714,\n",
      "           4331,   5574,     16,  15302,    868,     12,   6889,     23,  57350,\n",
      "             13,    763,  91958,    304,  61577,   8245,  29225,  15264,    220,\n",
      "           1627,    345,     58,     23,     60,    423,  22970,   1609,  34655,\n",
      "           1764,    438,  86755,  59927,     13,    679,     19,  70218,  56748,\n",
      "           4492,   2000,    267,  67054,  19680,  17528,     12,    356,     13,\n",
      "            622,     13,    356,     13,  12649,   4282,     11,    445,     13,\n",
      "          37330,    283,     11,    386,     13,    468,   6427,     11,   1901,\n",
      "             13,  24855,   1494,   2453,   5676,     11,    323,    735,     13,\n",
      "           1229,     13,  71613,   7058,  28491,  17126,     55,    344,   1762,\n",
      "           1374,    277,     55,    344,     25,   9335,     17,     13,  25169,\n",
      "             15,      7,    679,     19,    570,  10418,   1414,      7,   2782,\n",
      "             82,  36434,  17119,   6713,  30915,    988,     11,  40345,   2637,\n",
      "          26026,   4235,  24347,     13,   1795,   1129,  48393,   1276,   3153,\n",
      "          28912,   4420,   3271,   6018,     58,     24,     60,    816,   1201,\n",
      "           2192,  51697,  51932,   6151,     88,  10602,     43,  19260,  28112,\n",
      "          65966,    647,  31192,  30533,    526,     43,  19260,  51526,     55,\n",
      "          10602,     57,  17156,     13,    679,     20,   1236,  16933,    220,\n",
      "          17824,     23,   5621,   1525,    287,  27281,  41078,   4269,   2442,\n",
      "           3890,  57832,     82,  15548,  12934,  52286,  31113,  11733,  14723,\n",
      "            627,   3106,    438,  34890,  26566,  25624,   2000,  81434,  11461,\n",
      "          34290,   5450,  85438,     12,  13072,    198,    287,    708,     69,\n",
      "           1820,  76896,  11500,  48121,   6157,  15836,  92348,    263,   9470,\n",
      "          16895,   1090,   8677,   4444,   6157,     40,    529,    868,    570,\n",
      "            510,    845,     60,  19797,   1122,     44,    815,   1412,    276,\n",
      "           1247,  38406,     73,   2431,   7910,     42,  66636,  76832,  51526,\n",
      "          66497,  19221,   1687,   1609,    372,     13,   1049,     22,   7659,\n",
      "           6438,     25,   1741,    461,    198,  51207,   3378,    676,     11,\n",
      "          13302,     16,   4235,  13302,     22,     13,   1795,   1129,   8910,\n",
      "          15761,     76,   2726,   2971,   7709,    522,  21796,  20970,     28,\n",
      "          15287,  23181,     16,     13,  15287,  24199,     19,    315,  99031,\n",
      "          81434,   5450,  85438,    287,    708,     69,   1820,    845,    339,\n",
      "          34746,  92348,    198,     58,    605,     60,  13789,    276,   6349,\n",
      "          24412,  31323,   6600,    819,   1543,  31214,     72,     54,    276,\n",
      "             11,   1163,    526,     54,    526,  51526,  23083,     38,  17675,\n",
      "             74,     13,    679,     18,     13,    389,  10343,  62070,   6109,\n",
      "          14358,  19522,    529,   2589,    570,   1741,     44,     11,   3648,\n",
      "         100077,  22812,     56,     11,  25342,     11,  25388,   4235,  22457,\n",
      "             13,   3788,    512,     35,  11451,  10254,    651,   1854,   2000,\n",
      "          34890,    849,  27523,   4291,    276,  97798,  81434,   4066,     13,\n",
      "            443,  48886,   2726,     14,    605,     13,   8011,     20,     14,\n",
      "           8874,  15574,     17,     13,   8874,  15999,     22,    198,    644,\n",
      "          85438,    287,    708,     69,   1820,    679,     18,  92348,   1073,\n",
      "           1820,  26287,  29518,  26072,   1073,   1820,    510,   1114,     60,\n",
      "          27973,   2259,     51,    412,  86563,     11,  82056,  51697,  50640,\n",
      "            268,   2442,   2933,     56,   7141,  44639,     78,    333,   2234,\n",
      "             47,   9186,  51526,  32978,   2232,  14468,    627,  64561,   2000,\n",
      "          59122,   1697,     43,  13102,   5706,     25,  35075,  14126,  29356,\n",
      "           9268,  20855,     12,    220,    679,     21,   3034,    981,   3079,\n",
      "          48567,   1073,  26566,  25624,   2000,  34890,   1858,  16319,  81434,\n",
      "            198,     82,   2168,    367,   2000,  59122,   1697,     43,  13102,\n",
      "           5706,     11,  15831,   4235,  23833,   7109,   1129,  48836,   2984,\n",
      "           2726,     14,  32329,   2508,     14,   5464,    438,   1199,   5450,\n",
      "          85438,    287,    708,     69,   1820,   4370,    339,  81596,  65676,\n",
      "           1073,   1820,  64561,   2000,    198,     45,   1032,     12,   7743,\n",
      "             20,  93028,     43,  13102,   5706,  12692,   4765,     16,     25,\n",
      "           6720,     47,   9724,    570,  64561,   2000,  59122,     64,   7058,\n",
      "             58,    806,     60,   1676,     85,    485,   8989,    301,    587,\n",
      "          67289,   8324,    268,  26312,     49,   8942,  51526,  41598,  26353,\n",
      "           7368,    372,     13,    679,     20,   3034,    981,     72,     12,\n",
      "            259,   4001,     43,  13102,   5706,     11,  10290,     19,   4235,\n",
      "           8929,     19,     13,   2485,   1129,  48886,   2726,     14,    605,\n",
      "             13,   9714,   4331,   5574,     16,  16744,    845,     12,   8190,\n",
      "             21,    198,     83,   4001,   3866,  10115,  17399,   2000,  81434,\n",
      "           4066,  34290,   5450,  85438,    826,    510,    972,     60,    578,\n",
      "          29211,     78,  91635,  43588,     11,  75066,  37790,    685,  59962,\n",
      "           2319,  42256,  84616,   2067,     11,   1542,  37597,     49,   1142,\n",
      "            301,  43225,  29211,   2265,    198,   1073,   1820,   4331,   6634,\n",
      "          81596,  65676,   1073,   1820,  64561,   2000,  59122,   1697,     43,\n",
      "          13102,   5706,  94316,   1291,  51526,  17198,  99112,     33,   3102,\n",
      "            569,     13,    679,     22,     13,  33025,  11461,  34290,  20708,\n",
      "            198,    438,   1820,     22,    339,  34746,  42097,  92348,    263,\n",
      "          55381,  14126,  29992,  22872,  26404,  21316,   2065,  17126,     55,\n",
      "            344,   1762,   1374,    277,     55,    344,     25,   8258,     17,\n",
      "             13,  26661,   4643,      7,    679,     22,   4390,  12692,   4765,\n",
      "             16,     25,   6720,     47,   9724,    570,  64561,   2000,  59122,\n",
      "           1697,     43,  13102,   5706,     11,  10132,   4235,  11247,    627,\n",
      "           2485,   1129,  48886,   2726,     14,    605,     13,  15134,     20,\n",
      "           5574,     16,  16744,    868,     12,   4645,     21, 128009, 128006,\n",
      "            882, 128007,    271,   7184,     11,   2728,    420,   3488,     25,\n",
      "           3639,    527,    279,    836,    315,  30525,   1511,    304,    279,\n",
      "           5684,   4710,    220,  21335,   1203,    279,   4320,   1193,    304,\n",
      "            264,  13325,   1160,   3645,     11,    369,   3187,     25,   2570,\n",
      "             32,   1882,     33,   7352,   1442,    499,   1541,    956,   1440,\n",
      "            279,   4320,     11,   1120,    471,    459,   4384,   1160,     13,\n",
      "         128009, 128006,  78191, 128007,    271]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "          93028,     43,  13102,   5706,     25,  35075,  14126,  29356,   9268,\n",
      "          11108,   2168,    367,   2000,    198,     58,     22,     60,  92073,\n",
      "          17198,     84,     11,  13379,  89437,  51526,     47,   3035,     88,\n",
      "             43,  28323,     13,    679,     20,   8404,  22292,    287,  81434,\n",
      "          11461,     82,  93028,     43,  13102,   5706,     11,   5728,   4235,\n",
      "           5833,   7109,   1129,  48836,   2984,   2726,     14,  32329,   2508,\n",
      "          20906,   1032,     12,   1041,     23,    198,    258,   3866,  10115,\n",
      "           5450,  85438,    287,    708,     69,   1820,    679,     20,  92348,\n",
      "            263,  29831,  67966,  18337,    510,    868,     60,  12131,   2100,\n",
      "           9211,     11,  11824,  41287,  25507,     11,  26035,    423,  40623,\n",
      "             11,    323,  13929,  17030,    627,    258,  55381,  14126,  29992,\n",
      "          11108,   2168,    367,   2000,  59122,   1697,     43,  13102,   5706,\n",
      "             11,    220,    679,     18,     13,  27857,    287,   3161,  61577,\n",
      "          27127,  39810,    369,  33025,   5464,    198,  17592,   4235,  13817,\n",
      "             13,   2485,   1129,  48886,   2726,     14,    605,     13,   9714,\n",
      "           4331,   5574,     16,  15302,    868,     12,   6889,     23,  57350,\n",
      "             13,    763,  91958,    304,  61577,   8245,  29225,  15264,    220,\n",
      "           1627,    345,     58,     23,     60,    423,  22970,   1609,  34655,\n",
      "           1764,    438,  86755,  59927,     13,    679,     19,  70218,  56748,\n",
      "           4492,   2000,    267,  67054,  19680,  17528,     12,    356,     13,\n",
      "            622,     13,    356,     13,  12649,   4282,     11,    445,     13,\n",
      "          37330,    283,     11,    386,     13,    468,   6427,     11,   1901,\n",
      "             13,  24855,   1494,   2453,   5676,     11,    323,    735,     13,\n",
      "           1229,     13,  71613,   7058,  28491,  17126,     55,    344,   1762,\n",
      "           1374,    277,     55,    344,     25,   9335,     17,     13,  25169,\n",
      "             15,      7,    679,     19,    570,  10418,   1414,      7,   2782,\n",
      "             82,  36434,  17119,   6713,  30915,    988,     11,  40345,   2637,\n",
      "          26026,   4235,  24347,     13,   1795,   1129,  48393,   1276,   3153,\n",
      "          28912,   4420,   3271,   6018,     58,     24,     60,    816,   1201,\n",
      "           2192,  51697,  51932,   6151,     88,  10602,     43,  19260,  28112,\n",
      "          65966,    647,  31192,  30533,    526,     43,  19260,  51526,     55,\n",
      "          10602,     57,  17156,     13,    679,     20,   1236,  16933,    220,\n",
      "          17824,     23,   5621,   1525,    287,  27281,  41078,   4269,   2442,\n",
      "           3890,  57832,     82,  15548,  12934,  52286,  31113,  11733,  14723,\n",
      "            627,   3106,    438,  34890,  26566,  25624,   2000,  81434,  11461,\n",
      "          34290,   5450,  85438,     12,  13072,    198,    287,    708,     69,\n",
      "           1820,  76896,  11500,  48121,   6157,  15836,  92348,    263,   9470,\n",
      "          16895,   1090,   8677,   4444,   6157,     40,    529,    868,    570,\n",
      "            510,    845,     60,  19797,   1122,     44,    815,   1412,    276,\n",
      "           1247,  38406,     73,   2431,   7910,     42,  66636,  76832,  51526,\n",
      "          66497,  19221,   1687,   1609,    372,     13,   1049,     22,   7659,\n",
      "           6438,     25,   1741,    461,    198,  51207,   3378,    676,     11,\n",
      "          13302,     16,   4235,  13302,     22,     13,   1795,   1129,   8910,\n",
      "          15761,     76,   2726,   2971,   7709,    522,  21796,  20970,     28,\n",
      "          15287,  23181,     16,     13,  15287,  24199,     19,    315,  99031,\n",
      "          81434,   5450,  85438,    287,    708,     69,   1820,    845,    339,\n",
      "          34746,  92348,    198,     58,    605,     60,  13789,    276,   6349,\n",
      "          24412,  31323,   6600,    819,   1543,  31214,     72,     54,    276,\n",
      "             11,   1163,    526,     54,    526,  51526,  23083,     38,  17675,\n",
      "             74,     13,    679,     18,     13,    389,  10343,  62070,   6109,\n",
      "          14358,  19522,    529,   2589,    570,   1741,     44,     11,   3648,\n",
      "         100077,  22812,     56,     11,  25342,     11,  25388,   4235,  22457,\n",
      "             13,   3788,    512,     35,  11451,  10254,    651,   1854,   2000,\n",
      "          34890,    849,  27523,   4291,    276,  97798,  81434,   4066,     13,\n",
      "            443,  48886,   2726,     14,    605,     13,   8011,     20,     14,\n",
      "           8874,  15574,     17,     13,   8874,  15999,     22,    198,    644,\n",
      "          85438,    287,    708,     69,   1820,    679,     18,  92348,   1073,\n",
      "           1820,  26287,  29518,  26072,   1073,   1820,    510,   1114,     60,\n",
      "          27973,   2259,     51,    412,  86563,     11,  82056,  51697,  50640,\n",
      "            268,   2442,   2933,     56,   7141,  44639,     78,    333,   2234,\n",
      "             47,   9186,  51526,  32978,   2232,  14468,    627,  64561,   2000,\n",
      "          59122,   1697,     43,  13102,   5706,     25,  35075,  14126,  29356,\n",
      "           9268,  20855,     12,    220,    679,     21,   3034,    981,   3079,\n",
      "          48567,   1073,  26566,  25624,   2000,  34890,   1858,  16319,  81434,\n",
      "            198,     82,   2168,    367,   2000,  59122,   1697,     43,  13102,\n",
      "           5706,     11,  15831,   4235,  23833,   7109,   1129,  48836,   2984,\n",
      "           2726,     14,  32329,   2508,     14,   5464,    438,   1199,   5450,\n",
      "          85438,    287,    708,     69,   1820,   4370,    339,  81596,  65676,\n",
      "           1073,   1820,  64561,   2000,    198,     45,   1032,     12,   7743,\n",
      "             20,  93028,     43,  13102,   5706,  12692,   4765,     16,     25,\n",
      "           6720,     47,   9724,    570,  64561,   2000,  59122,     64,   7058,\n",
      "             58,    806,     60,   1676,     85,    485,   8989,    301,    587,\n",
      "          67289,   8324,    268,  26312,     49,   8942,  51526,  41598,  26353,\n",
      "           7368,    372,     13,    679,     20,   3034,    981,     72,     12,\n",
      "            259,   4001,     43,  13102,   5706,     11,  10290,     19,   4235,\n",
      "           8929,     19,     13,   2485,   1129,  48886,   2726,     14,    605,\n",
      "             13,   9714,   4331,   5574,     16,  16744,    845,     12,   8190,\n",
      "             21,    198,     83,   4001,   3866,  10115,  17399,   2000,  81434,\n",
      "           4066,  34290,   5450,  85438,    826,    510,    972,     60,    578,\n",
      "          29211,     78,  91635,  43588,     11,  75066,  37790,    685,  59962,\n",
      "           2319,  42256,  84616,   2067,     11,   1542,  37597,     49,   1142,\n",
      "            301,  43225,  29211,   2265,    198,   1073,   1820,   4331,   6634,\n",
      "          81596,  65676,   1073,   1820,  64561,   2000,  59122,   1697,     43,\n",
      "          13102,   5706,  94316,   1291,  51526,  17198,  99112,     33,   3102,\n",
      "            569,     13,    679,     22,     13,  33025,  11461,  34290,  20708,\n",
      "            198,    438,   1820,     22,    339,  34746,  42097,  92348,    263,\n",
      "          55381,  14126,  29992,  22872,  26404,  21316,   2065,  17126,     55,\n",
      "            344,   1762,   1374,    277,     55,    344,     25,   8258,     17,\n",
      "             13,  26661,   4643,      7,    679,     22,   4390,  12692,   4765,\n",
      "             16,     25,   6720,     47,   9724,    570,  64561,   2000,  59122,\n",
      "           1697,     43,  13102,   5706,     11,  10132,   4235,  11247,    627,\n",
      "           2485,   1129,  48886,   2726,     14,    605,     13,  15134,     20,\n",
      "           5574,     16,  16744,    868,     12,   4645,     21, 128009, 128006,\n",
      "            882, 128007,    271,   7184,     11,   2728,    420,   3488,     25,\n",
      "           3639,    527,    279,    836,    315,  30525,   1511,    304,    279,\n",
      "           5684,   4710,    220,  21335,   1203,    279,   4320,   1193,    304,\n",
      "            264,  13325,   1160,   3645,     11,    369,   3187,     25,   2570,\n",
      "             32,   1882,     33,   7352,   1442,    499,   1541,    956,   1440,\n",
      "            279,   4320,     11,   1120,    471,    459,   4384,   1160,     13,\n",
      "         128009, 128006,  78191, 128007,    271,   1318, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " []\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Learning Knowledge Graph Embeddings\n",
      "with Type Regularizer\n",
      "BhushanKotnisandViviNastase\n",
      "HeidelbergUniversity\n",
      "Heidelberg,Germany69120\n",
      "{kotnis,nastase}@cl.uni-heidelberg.de\n",
      "ABSTRACT Thisresultsinamulti-graph,i.e. agraphwithdifferenttypesof\n",
      "Learningrelationsbasedonevidencefromknowledgerepositories linkswherealinktypecorrespondstoarelationtype.\n",
      "reliesonprocessingtheavailablerelationinstances. Knowledge KGsareknowntobeincomplete[10],i.e.,asignificantnumber\n",
      "repositoriesarenotbalancedintermsofrelationsorentities–there ofrelationsbetweenentitiesaremissing.Embeddingtheknowledge\n",
      "arerelationswithlessthan10butalsothousandsofinstances,and graphinacontinuousvectorspacehasbeensuccessfullyusedto\n",
      "entities involved in less than 10 but also thousands of relations. addressthisproblem[2,13,15].Suchmodelsrepresentthecompo-\n",
      "Manyrelations,however,havecleardomainandrange,whichwe nentsofthegraph,i.e.,theentitiesandrelations,usingrealvaluedla-\n",
      "hypothesize could help learn a better, more generalizing, model. tentfactorsthatencodethestructureoftheknowledgegraph.Forex-\n",
      "WeincludesuchinformationintheRESCALmodelintheform amplethelatentfactormodelshouldbeabletorecoverColognefrom\n",
      "ofaregularizationfactoraddedtothelossfunctionthattakesinto thelatentrepresentationsofMoselleandriver flowsThrough city.\n",
      "accountthetypes(categories)oftheentitiesthatappearasarguments ExamplesincludetheRESCAL[13]tensorfactorizationmodel,the\n",
      "torelationsintheknowledgebase.TestedonFreebase,afrequently TransE model [2] and their variations [9, 12]. We focus on the\n",
      "usedbenchmarkingdatasetforlink/pathpredictingtasks,wenote RESCALmodel,oneofthemostflexibleandwidelyusedmodels.\n",
      "increasedperformancecomparedtothebaselinemodelintermsof RESCALisabilinearmodelthatrepresentstriplesasapairwise\n",
      "meanreciprocalrankandhits@N,N=1,3,10. Furthermore,we interactionofsourceandtargetentitylatentfactors(embeddings)\n",
      "discoverscenariosthatsignificantlyimpacttheeffectivenessofthe throughamatrixthatrepresentsthelatentfactorsoftheconnecting\n",
      "typeregularizer. relation.Theentityandrelationrepresentationsinducedcanbeused\n",
      "to predict additional relations – edges – between known entities.\n",
      "KEYWORDS Table1listsafewexamplesofentitytypeinformationinFreebase.\n",
      "Existingknowledgegraphsareimbalanced–bothrelationand\n",
      "KnowledgeGraphs,GraphEmbedding,LinkPrediction\n",
      "entityfrequenciesvarywidely,asevidentfromthestatisticsonFree-\n",
      "ACMReferenceformat: base15kshowninFigure1.Sinceentityandrelationembeddings\n",
      "BhushanKotnisandViviNastase.2017.LearningKnowledgeGraphEm- arebasedontheconnectivitystructureofthegraph,itisreasonable\n",
      "beddings toaskwhatistheoutcomeoftheknowledgegraphembeddingfor\n",
      "withTypeRegularizer.InProceedingsofK-CAP2017:KnowledgeCapture\n",
      "entitiesandrelationswhichareunderrepresentedinthegraph,in\n",
      "Conference,Austin,TX,USA,December4–6,2017(K-CAP2017),6pages.\n",
      "particular,howgoodaretheyforthetaskoflinkprediction.\n",
      "DOI:10.1145/3148011.3154466\n",
      "ApproachessuchasRESCALtakeanextensionalviewofrela-\n",
      "tions–theyprocessthecollectionofinstanceswithoutknowledgeof\n",
      "1 INTRODUCTION higherlevelrulesorinformationabouttheserelations.Wehypothe-\n",
      "Knowledge–lexical,worldandcommon-sense–iscrucialfortasks sizethatprovidingthehigherlevel–intensional–viewintheform\n",
      "suchasautomatedtextcomprehensionandsummarization, ques- oftypesorcategoriesofrelationarguments,canleadtoimproved\n",
      "tionanswering,naturallanguagedialoguesystems.Tomakesuch resultsforthetaskoflinkprediction.Thismaybetrueparticularly\n",
      "knowledgeavailableforautomaticprocessing,themostcommon forknowledgegraphssuchasFreebasethathavestronglytypedrela-\n",
      "approachistoprovideitasacollectionofrelationtriples–entities tions,andalsoforlow-frequencyrelationsorforrelationsinvolving\n",
      "or concepts connected by a relation: e.g., (concept:city:London, low-frequencyentities.\n",
      "relation:country capital,concept:country:UK).Globally,suchcol- Inthisarticlewepresentexperimentalresultssupportingthehy-\n",
      "lectionscanbeviewedasknowledgegraphs(KGs),forexample pothesisthataugmentingsingle-relationmodelswithentitytypein-\n",
      "NELL [3], Freebase [1] and YAGO [16]. In such graphs, nodes formation,intheformofa‘Type’regularizer,leadstoimprovements\n",
      "(entities/concepts)maybeconnectedbydifferenttypesofrelations. inpredictingmissinglinks.Theresultsshowthateventhoughthe\n",
      "bilinearmodelinducesrepresentationsforallentitiesandrelations\n",
      "Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor together–soitimplicitlyusesthetypeinformationweprovideasa\n",
      "classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed\n",
      "separaterelation–thetyperegularizerwhichexplicitlyincludessuch\n",
      "forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation\n",
      "onthefirstpage.CopyrightsforcomponentsofthisworkownedbyothersthanACM informationforeachrelationleadstobetterresults.Furthermore,we\n",
      "mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish, notethepositiveimpactofincludingthetyperegularizerforrelations\n",
      "topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora\n",
      "involvinglow-frequencyentities,whereaslow-frequencyrelations\n",
      "fee.Requestpermissionsfrompermissions@acm.org.\n",
      "K-CAP2017,Austin,TX,USA arelessaffectedbythisaddedinformation. Wealsoanalyzethe\n",
      "©2017ACM. 978-1-4503-5553-7/17/12...$15.00\n",
      "DOI:10.1145/3148011.3154466\n",
      "8102\n",
      "raM\n",
      "2\n",
      "]IA.sc[\n",
      "2v87290.6071:viXra\n",
      "K-CAP2017,December4–6,2017,Austin,TX,USA BhushanKotnisandViviNastase\n",
      "SourceType Source PathorRelation Target TargetType\n",
      "film star wars episode IV produced by дeorдe lucas film producer\n",
      "person alexandre dumas people profession writer profession\n",
      "academic post professor profession people albert einstein person\n",
      "Table1:EntityTypeInformation:ExamplesofsourceandtargetentitytypesfromFreebaseusedinthetyperegularizer.\n",
      "104 Argument_1\n",
      "train\n",
      "103 dev\n",
      "test\n",
      "102\n",
      "101\n",
      "100\n",
      "0 2000 4000 6000 8000 10000 12000 14000\n",
      "104 Argument_2\n",
      "train\n",
      "103 dev\n",
      "test\n",
      "102\n",
      "101\n",
      "100\n",
      "0 2000 4000 6000 8000 10000 12000 14000\n",
      "105\n",
      "104\n",
      "103\n",
      "102\n",
      "101\n",
      "100\n",
      "0 200 400 600 800 1000 1200\n",
      "ycneuqerF\n",
      "entity information, in the form of latent factors, improves KBC\n",
      "performance.Thesourceandtargettypesarenotexplicitlyincluded.\n",
      "[4]makeusetheoftypeinformationandproduceavariationof\n",
      "RESCALtheycallTRESCAL–TypedRESCAL.Thetypeinformation\n",
      "isusedtoimprovetheefficiencyofthemodel,byreducingthesize\n",
      "oftheentitymatrixinthecomputationofthelossfunctiontoentities\n",
      "belongingtothedomainandrangeoftherelation.Theentitytype\n",
      "assuchisonlyimplicitlyincorporated,assomethingsharedbythe\n",
      "entitiessingledoutforcomputingthelossfunction.\n",
      "[5]buildson[11],andusesanRNNtomodelpathswhichincor-\n",
      "poratetypeinformationfortheentitiesalongthepath.Entitiesare\n",
      "representedasasumoftheirentitytypes,whicharelearnedduring\n",
      "training.Includingthisinformationleadstohigherperformance.\n",
      "Compared with these previous approaches, we add the entity\n",
      "typesexplicitlyinthemodel,andderivearepresentationforentities\n",
      "andtheirtypesconcurrently.Weanalyzetheimpactofusingsuch\n",
      "representationforlinkpredictionwithdifferentamountsoftraining\n",
      "data,tounderstandunderwhatconditionsthetypeinformationhas\n",
      "apositiveimpact.\n",
      "Relation\n",
      "train\n",
      "dev\n",
      "test 3 METHODS\n",
      "InthissectionwedescribetheRESCALmodelandshowhowthe\n",
      "typeregularizerwasaddedtoincludethetypeinformationforeach\n",
      "relationinthecomputationofthelossfunction.\n",
      "3.1 Definitions\n",
      "LetE,RbethesetofentitiesandrelationsintheKGrespectively.A\n",
      "knowledgegraphGisasetoftriples(s,r,t)wheres,t ∈E, r ∈R\n",
      "Figure 1: Statistics on argument and relation frequencies for\n",
      "andrelationr connectsstot.\n",
      "Freebase15k\n",
      "Theknowledgebasecompletion(KBC)taskisthetaskofclas-\n",
      "sifyingwhetherthetriple(s,r,t)isapartoftheknowledgegraph.\n",
      "Thiscanbedescribedas(s,r,?)or(?,r,t)wherethequestionmark\n",
      "representstheunknowncorrecttarget/sourceentityfromasetof\n",
      "effectsoftrainingdatasizeontheusefulnessofthetyperegularizer,\n",
      "candidateentities.\n",
      "andnotethatitsimpactgrowswiththeamountoftrainingdata.\n",
      "2 RELATEDWORK 3.2 RESCALModel\n",
      "Avarietyoflatentfactormodels[2,13–15]havebeendevelopedto TheRESCALmodel[13]weightstheinteractionofallpairwise\n",
      "represententitiesandrelationsinaknowledgegraph,andhavebeen latentfactorbetweenthesourceandtargetentityforpredictinga\n",
      "used to address the knowledge base completion (KBC) problem. relation. Itrepresentseveryentityasavector(x ∈Rd),andevery\n",
      "Mostlatentfactormodelsaretrainedoneitherknowledgegraph relationasamatrixW ∈ Rd×d. Thismodelrepresentsthetriple\n",
      "triples,ortriplesextractedfromopendomainknowledgeextraction (s,r,t)asascoregivenby\n",
      "tools [14]. A notable exception is the RNN model proposed by sc(s,r,t)=x sT Wr xt\n",
      "[11]thatlearnspathembeddingsforknowledgebasecompletion. Thisisequivalenttotensorfactorizationwhereeachrelationmatrix\n",
      "[7]proposeacompositionalobjectivefunctionoverlatentfactor isasliceofthetensor. Thesevectorsandmatricesarelearnedby\n",
      "models, whichistrainedonpathsaswellastriples. Formodels constructingalossfunctionthatcontraststhescoreofacorrecttriple\n",
      "thatarecompositional,[17]showsthatincorporatingintermediate toincorrectones.Hereweusethemax-marginlossdescribedinthe\n",
      "LearningKnowledgeGraphEmbeddings\n",
      "withTypeRegularizer K-CAP2017,December4–6,2017,Austin,TX,USA\n",
      "followingequation: WeobtainFreebasecategorydatafrom[6],andthentheentity\n",
      "typebymappingtheFreebaseentityidentifiertotheFreebasecat-\n",
      "N\n",
      "J(Θ)=(cid:213) (cid:213) mm(σ(sci),σ(sc i(cid:48))) (1) weg ho ir cy h. isT uh si es dre insu tl ht es ti rn ai1 n0 in1 g,3 s5 t3 agi en.s Ita tn isce ns oto uf st eh de dc ua rt ie ng go tr ey stre til mat eio.n\n",
      "i=1t(cid:48)∈N(t)\n",
      "(cid:20) (cid:21) 4.2 Implementation\n",
      "mm(σ(sci),σ(sc i(cid:48)))=max 0,1−σ(sci)+σ(sc i(cid:48))\n",
      "WeusetheAdam[8]SGDoptimizerfortrainingbecauseitaddresses\n",
      "theproblemofdecreasinglearningrateinAdaGrad.Weusemedian\n",
      "wherethereareNpositiveinstances,positiveandnegativeinstances\n",
      "arescoredassci =sc(si,ri,ti)andsc i(cid:48) =sc(si,ri,t i(cid:48)),respectively. g thr aa tdi ee nn tt itc yli ep mpi bn eg dt do inp gre sv he an vt eex up nl io ts ni ove rmg.ra Wdi een pt es ra fon rd mw ee da el xs ho ae un ss tu ivr ee\n",
      "N(t)isthesetofincorrecttargetsandσ isthesigmoidfunction.\n",
      "gridsearchfortheL2regularizeraswellasα onthevalidationset\n",
      "andwetunedthetrainingdurationusingearlystopping.Weuse100\n",
      "3.3 TheTypeRegularizer\n",
      "dimensionalentityvectorinallexperiments1.\n",
      "Weintroducearegularizertermwhichincorporatestypeinformation\n",
      "ofsourceandtargetentities. Letscat bethetypeforentitys and 4.3 EvaluationProcedure\n",
      "rcat therelationbetweensandscat.Dependingontheknowledge\n",
      "Forevaluationwefollowtheproceduredescribedin[15].Forevery\n",
      "resource,rcat couldbeis a(inanontology,forexample),cateдory\n",
      "testtriplewepredicteitherthesourceorthetarget,andnegative\n",
      "(inaresourcebuiltbasedonWikipedia),orothersuchrelationsthat\n",
      "instacesfortrainingandtestingareproducedbycorruptingpositive\n",
      "capturetheentitytype.Afewexamplesofentitytypescanbeseen ones:wereplaces(ort)ina(s,r,t)triplewithansn (ortn)thathas\n",
      "inTable1.Notethatentitytypeinformationisnotusedduringtest\n",
      "thesametypeass(ort)butdoesnotappearinapositiveinstance\n",
      "time. (sn,r,t) (or (s,r,tn)). For meaningful comparison, the negative\n",
      "Ifsisthesourceentityandt thetargetentityforqueryq,thenwe\n",
      "triplesthatoccurintrainingorvalidationdatasetsaspositivetriples\n",
      "definetheregularizerasinequation2,whereN(scat)andN(tcat)\n",
      "arefilteredout.Forfasterevaluation,insteadofusingallnegative\n",
      "aresetsof(negatives)forscat,tcat,whileT(scat),T(tcat)aresets\n",
      "triples,weproduce1000byrandomlysamplingentitiesfromthe\n",
      "ofcorrectcategoriesforsourcesandtargett respectively.mmisthe\n",
      "entireset.Wereportresultsintermsofhitsat1,3,10(HITS@1,3,10)\n",
      "maxmarginlossdescribedinequation(1).\n",
      "andmeanreciprocalrank(MRR)metrics.HitsatKistheproportion\n",
      "ofcorrectanswers(hits)inthefirstKrankedpredictions,whileMRR\n",
      "isthemeanofthereciprocaloftherankofthecorrectanswers.\n",
      "R(Θ,q):=\n",
      "(cid:213) (cid:18) (cid:48) (cid:19) 4.4 Results\n",
      "mm σsc(s,rcat,scat),σsc(s,rcat,s cat)\n",
      "(cid:48) Weusethebilinear(RESCAL)modelasabaseline.Asevidenced\n",
      "s cat∈N(scat)\n",
      "bytheresultsinTable2,addingthetyperegularizerimprovesper-\n",
      "scat∈T(scat)\n",
      "(cid:18) (cid:19) formance. It may be tempting to think that the performance im-\n",
      "+ (cid:213) mm σsc(t,rcat,tcat),σsc(t,rcat,t c(cid:48) at) (2) provementisnaturalsinceweareprovidingadditionalinformation\n",
      "(cid:48) throughthetyperegularizer.Wetestthisinfurtherexperiments.\n",
      "t cat∈N(tcat)\n",
      "tcat∈T(tcat)\n",
      "Metrics Bilinear Bilinear+TR\n",
      "Thecompleteobjectivefunctiontobeminimizedis MRR 0.343 0.3862\n",
      "N HITS@1 0.2451 0.304\n",
      "J(Θ)=(cid:213) (cid:213) mm(qi,ti,t i(cid:48) )+αR(Θ,qi) HITS@3 0.3804 0.4161\n",
      "i=1t i(cid:48)\n",
      "∈N(qi) HITS@10 0.5312 0.5408\n",
      "Table2: Evaluation: PerformanceComparisonbetweenbilin-\n",
      "wherethehyper-parameterα,α ≥ 0, controlstheimpactofthe\n",
      "earmodelwithandwithouttyperegularizer.\n",
      "regularizertermsandN(qi)isthesetofnegativetargetsforquery\n",
      "qi,whereqi correspondstoquery(si,ri,?). Wetesttheimpactofthetyperegularizerbyanalyzingitsperfor-\n",
      "manceondifferentsizesoftrainingdata.Wefirstgeneratemultiple\n",
      "4 EXPERIMENTS trainingdatasetsbyrandomlysampling25%,50%and75%ofthe\n",
      "4.1 Data triples. AsillustratedinTable3,whenusingonly25%to50%of\n",
      "thetrainingdata,theperformancedrops.Thetyperegularizeruses\n",
      "WecarryoutexperimentsonFB15K,asubsetoftheFreebaseknowl- categoryinformation,undercertaincircumstances(α =1)addingit\n",
      "edgegraphprovidedby[2]. Thisdatasetisastandardbenchmark\n",
      "isequivalenttoaddingapproximately100,000newtripleswithcate-\n",
      "datasetusedforevaluatinglinkpredictionalgorithms[2,12,18].\n",
      "goryrelationtothetrainingset.Thus,simplyaugmentingthemodel\n",
      "TheFB15Kdatasetconsistsof1345relationsand14,951entities.\n",
      "withadditionalinformationdoesnotalwaysimproveperformance.\n",
      "Thetraining,validationandtestsetconsistsof483,142,50,000and\n",
      "Thereasonbehindtheperformancedropwithlesstrainingdatais\n",
      "59,071triplesrespectively. TheFreebaserelationsdonotinclude\n",
      "notobvious,becauseaddingexternalinformationshouldhelpthe\n",
      "thecategoryrelation,thusthereisnooverlapbetweenthecategory\n",
      "triplesandFB15Ktriples. 1Codeisavailableathttps://github.com/bhushank/kge\n",
      "K-CAP2017,December4–6,2017,Austin,TX,USA BhushanKotnisandViviNastase\n",
      "%trainingdata Model MRR %Improvement RelationName Instances(train) Instances(test)\n",
      "100\n",
      "Bilinear 0.343 r1 /people/person/profession 11636 1384\n",
      "Bilinear+TR 0.3862 +12.59 r2 /music/genre/artists 5952 679\n",
      "Bilinear 0.3495 r3 /film/film/country 2407 280\n",
      "75\n",
      "Bilinear+TR 0.3552 +1.6 r4 /tv/tv program/genre 1010 100\n",
      "Bilinear 0.3457 Table4:Relationswithtrainandtestinstances\n",
      "50\n",
      "Bilinear+TR 0.3409 -1.3\n",
      "Bilinear 0.332\n",
      "25\n",
      "Bilinear+TR 0.3198 -3.67\n",
      "Table3: EffectoftrainingdatasizeonTR:Performancecom-\n",
      "parisonbetweenbilinearmodelswithandwithouttyperegular-\n",
      "izerfordifferentdatasetsizes.\n",
      "modellearnbetterembeddings. Wehypothesizethatthedropin\n",
      "performanceisbecausewhenfewernumberoftraininginstancesare\n",
      "available,thetyperegularizerleadsthesystemtolearnrelationsthat\n",
      "over-generalize. Themodelisbiasedtowardslearningcategories\n",
      "verywellforreducingtrainingloss.Thisresultsinembeddingsthat\n",
      "arebiasedtowardspredictingrelationsatthelevelofcategoriesand\n",
      "notindividualrelationsresultinginperformancedropfortherelation\n",
      "predictiontask.\n",
      "Figure3: MRRvs. PercentTrainDataformultiplerelations:\n",
      "NumberoftraininginstancesmodulateeffectofTypeRegular-\n",
      "izer.RelationslistedinTable4\n",
      "Fig. 3 shows the performance in terms of MRR (using Type\n",
      "Regularizer)forlinkpredictiononthesefourrelations.Theorange\n",
      "andbluelinesdenoterelations(r1,r2)with11,636and5952train-\n",
      "inginstancesrespectively,whiletheredandgreencurvesdenote\n",
      "relations(r3,r4)with2407and1010traininginstancesrespectively.\n",
      "Theredandgreencurves(therelationswithfewerinstances)showa\n",
      "largerchangeinMRRcomparedtotheorangeandbluecurves.This\n",
      "confirmsourhypothesisthattheTypeRegularizerismoresensitive\n",
      "forrelationswithasmallernumberoftraininginstances,andindi-\n",
      "N catesthattheembeddingslearnedforrelationswithlargernumber\n",
      "ofinstancesarelessbiasedtowardspredictingcategories.\n",
      "Figure 2: MRR vs. α: MRR drops with increasing We note that equation (2) has the same max margin structure\n",
      "strength of the type regularizer for models trained on 25% asthelossfunction,equation(1). Thereforeusingthisparticular\n",
      "(blue) and 100% (orange) of FB15K dataset. Plot for formulaforthetyperegularizerisequivalenttoaddingthecategory\n",
      "α =0.0001,0.001,0.01,0.01,1,5,10 relationasanadditionalsliceofthetensorfactorizedbyRESCAL,\n",
      "thenthehyperparameterαis1.Experimentshaveshownthoughthat\n",
      "We investigate this hypothesis by varying the value of α that finetuningα –andthisfine-tuningtheusageoftypeinformation–\n",
      "weighstheimportanceofthetyperegularizer(cf.equation1).We canleadtobetterresults.Morespecificallyitisequivalenttoadding\n",
      "plottheMeanReciprocalRankvs.thestrengthofthetyperegular- 101,353uniqueinstancesofcategoryrelation.\n",
      "izerformodeltrainedononly25%ofthetrainingdatainFig. 2. We also performed overall relation and entity analysis based\n",
      "Thehigherthestrengthofthetyperegularizer,thehigherthecost on their occurrence frequency. Looking at relations grouped by\n",
      "incurredformis-predictingthecategory. AsFig. 2shows,MRR theorder ofmagnitude(oom) oftheir occurrence frequency pre-\n",
      "fallssharplywithincreaseinα. Thiseffectisnotobservedinthe sentedinFigure4wenotethatlowfrequencyrelationsseemnot\n",
      "100% training data scenario. This suggests that adding category tobeaffectedbythetyperegularizer,andaremodeledbetterusing\n",
      "informationmayleadtoimprovedperformanceonlywhentheadded onlytheinstancesthemselves. Theresonforthisisthatverylow\n",
      "informationdoesnotseverelybiasthetrainingdata. frequency relations actually connect high frequency entities, e.g.\n",
      "Toinvestigatetheimpactoftrainingdatasizeonthetypereg- relation /award/hall of fame/discipline. On the other hand, high\n",
      "ularizerperformance,weanalyzeindetailtheperformanceofthe frequencyrelationshaveoveralllowerresultsthanotherrelations.\n",
      "systemforrelationswithadifferentnumberoftraininginstances. Thereasonforthisisthatinnumerouscases,oneofthearguments\n",
      "Table4listsfourrelationsweusedtolookintothisphenomenon. oftheserelationsisalowfrequencyentity.Forexample,thelives in\n",
      "LearningKnowledgeGraphEmbeddings\n",
      "withTypeRegularizer K-CAP2017,December4–6,2017,Austin,TX,USA\n",
      "OOM 0.0 OOM 1.0 OOM 2.0 OOM 3.0 OOM 4.0\n",
      "0.7\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "0.0\n",
      "25 50 75 100 25 50 75 100 25 50 75 100 25 50 75 100 25 50 75 100\n",
      "bilinear bilinear_tr\n",
      "OOM 0.0 OOM 1.0 OOM 2.0 OOM 3.0 OOM 4.0\n",
      "0.8\n",
      "0.7\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "0.0\n",
      "25 50 75 100 25 50 75 100 25 50 75 100 25 50 75 100 25 50 75 100\n",
      "bilinear bilinear_tr\n",
      "Figure4:MRRandHITS@10linkpredictionresultsgroupedbytheorderofmagnitudeofrelationfrequency,fordifferentamounts\n",
      "oftrainingdata.\n",
      "relation that connects a person with the city they live in, has as 5 CONCLUSION\n",
      "the”City”argumentanentitythatdoesnotappearinmanyother Weproposedatyperegularizerthatleveragesentitytypeinformation\n",
      "relations. forstate-of-the-artlatentfactormodelslikeRESCAL.Experiments\n",
      "To further clarify the reasons for variation in performance of onFreebaseFB15Kdatasetsuggestthataddingthetyperegular-\n",
      "relations,weanalyzethelinkpredictionresultsbasedontheorder izerimprovesperformanceontheknowledgebasecompletiontask.\n",
      "ofmagnitudeofentityfrequency,presentedinFigure5.Theresults Howeveraddingcategoryinformationmaynotimproveresultsfor\n",
      "inthiscasearemoreinlinewiththeexpectedoutcome–linksthat allrelations,particularlythosewithfewerpositiveinstanceswhere\n",
      "involvelowerfrequencyentitieshavelowerpredictionresults.The introducingcategoryinformationmayleadtoembeddingsthatarebi-\n",
      "typeinformationgenerallyhasapositiveimpactthroughout,except asedtowardscapturing/predictingcategoriesratherthanfinegrained\n",
      "medium-rangeentitieswhereitseemsthattypeinformationleadsto instances.Weplantostudytheimpactoftheaddedtypeinformation\n",
      "overgeneralization. fordatasetswheretherelationsarenotasstronglytypedasFreebase\n",
      "Usingthetyperegularizerasanadditionaltermswhoseweight –forgrammaticalcollocationinformationforexampleandinducing\n",
      "canbecalibratedusingtheα parametermakesiteasiertoadjustthe selectionalpreferences–andformorecomplex, pathprediction,\n",
      "influenceofthetypeinformationbasedonnodedegreesandrelation tasks.\n",
      "frequencies. Furthermore, byincorporatingthetypeinformation\n",
      "inthelossfunctionforeveryrelationasopposedtohavingitasa REFERENCES\n",
      "separaterelationintheknowledgegraphallowstheincorporationof\n",
      "[1] KurtBollacker,ColinEvans,PraveenParitosh,TimSturge,andJamieTaylor.\n",
      "therangeanddomaininformationforeachrelation,asopposedto 2008. Freebase: ACollaborativelyCreatedGraphDatabaseforStructuring\n",
      "modellingtheentitytypeoutsideofaparticularenvironment. HumanKnowledge.InProceedingsofthe2008ACMSIGMODInternational\n",
      "ConferenceonManagementofData(SIGMOD’08).ACM,NewYork,NY,USA,\n",
      "Itisinterestingtonotethatthebestresultsformediumtohigh 1247–1250. https://doi.org/10.1145/1376616.1376746\n",
      "frequencyentitiesandrelationsareobtainedwhenusingthefull [2] AntoineBordes,NicolasUsunier,AlbertoGarcia-Duran,JasonWeston,and\n",
      "Oksana Yakhnenko. 2013. Translating Embeddings for Modeling Multi-\n",
      "trainingdataandthetyperegularizer. Thisindicatesthatthetype\n",
      "relationalData. InAdvancesinNeuralInformationProcessingSystems26,\n",
      "regularizercanmitigatetheoverfittingtendencyofRESCAL,and C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Wein-\n",
      "produceamorerobustmodel. berger(Eds.).CurranAssociates,Inc.,2787–2795.http://papers.nips.cc/paper/\n",
      "5071-translating-embeddings-for-modeling-multi-relational-data.pdf\n",
      "[3] AndrewCarlson,JustinBetteridge,BryanKisiel,BurrSettles,EstevamR.Hr-\n",
      "uschka,andTomM.Mitchell.2010.TowardanArchitectureforNever-Ending\n",
      "LanguageLearning.InAAAI.\n",
      "K-CAP2017,December4–6,2017,Austin,TX,USA BhushanKotnisandViviNastase\n",
      "OOM 0.0 OOM 1.0 OOM 2.0 OOM 3.0\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "25 50 75 100 25 50 75 100 25 50 75 100 25 50 75 100\n",
      "bilinear bilinear_tr\n",
      "OOM 0.0 OOM 1.0 OOM 2.0 OOM 3.0\n",
      "0.7\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "25 50 75 100 25 50 75 100 25 50 75 100 25 50 75 100\n",
      "bilinear bilinear_tr\n",
      "Figure5: MRRandHITS@10linkpredictionresultsgroupedbytheorderofmagnitudeofentityfrequency,fordifferentamounts\n",
      "oftrainingdata.\n",
      "[4] Kai-WeiChang,Wen-tauYih,BishanYang,andChristopherMeek.2014.Typed [12] MaximilianNickel,LorenzoRosasco,andTomasoPoggio.2016.Holographic\n",
      "TensorDecompositionofKnowledgeBasesforRelationExtraction.InPro- EmbeddingsofKnowledgeGraphs.InProceedingsoftheThirtiethAAAICon-\n",
      "ceedingsofthe2014ConferenceonEmpiricalMethodsinNaturalLanguage ferenceonArtificialIntelligence(AAAI’16).AAAIPress,1955–1961. http:\n",
      "Processing(EMNLP).AssociationforComputationalLinguistics,1568–1579. //dl.acm.org/citation.cfm?id=3016100.3016172\n",
      "https://doi.org/10.3115/v1/D14-1165 [13] MaximilianNickel,VolkerTresp,andHans-PeterKriegel.2012. Factorizing\n",
      "[5] RajarshiDas,ArvindNeelakantan,DavidBelanger,andAndrewMcCallum.2016. YAGO:ScalableMachineLearningforLinkedData.InProceedingsofthe21st\n",
      "ChainsofReasoningoverEntities,Relations,andTextusingRecurrentNeural InternationalConferenceonWorldWideWeb(WWW’12).ACM,NewYork,NY,\n",
      "Networks.arXivpreprintarXiv:1607.01426(2016). USA,271–280. https://doi.org/10.1145/2187836.2187874\n",
      "[6] MattGardnerandTomMitchell.2015. EfficientandExpressiveKnowledge [14] SebastianRiedel,LiminYao,AndrewMcCallum,andM.BenjaminMarlin.2013.\n",
      "BaseCompletionUsingSubgraphFeatureExtraction.InProceedingsofthe2015 RelationExtractionwithMatrixFactorizationandUniversalSchemas.InProceed-\n",
      "ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.Association ingsofthe2013ConferenceoftheNorthAmericanChapteroftheAssociation\n",
      "forComputationalLinguistics,1488–1498.https://doi.org/10.18653/v1/D15-1173 forComputationalLinguistics:HumanLanguageTechnologies.Associationfor\n",
      "[7] KelvinGuu,JohnMiller,andPercyLiang.2015.TraversingKnowledgeGraphs ComputationalLinguistics,74–84.http://aclweb.org/anthology/N13-1008\n",
      "inVectorSpace.InProceedingsofthe2015ConferenceonEmpiricalMethods [15] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng.\n",
      "inNaturalLanguageProcessing.AssociationforComputationalLinguistics, 2013. Reasoning With Neural Tensor Networks for Knowledge Base\n",
      "318–327.https://doi.org/10.18653/v1/D15-1038 Completion. In Advances in Neural Information Processing Systems 26,\n",
      "[8] DiederikKingmaandJimmyBa.2014.Adam:Amethodforstochasticoptimiza- C. J. C. Burges, L. Bottou, M. Welling, Z.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  62965,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Knowledge Base Completion (KBC)', 'Link Prediction']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: ComputationalLinguistics:HumanLanguageTechnologies.Associationfor\n",
      "[7] KelvinGuu,JohnMiller,andPercyLiang.2015.TraversingKnowledgeGraphs ComputationalLinguistics,74–84.http://aclweb.org/anthology/N13-1008\n",
      "inVectorSpace.InProceedingsofthe2015ConferenceonEmpiricalMethods [15] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng.\n",
      "inNaturalLanguageProcessing.AssociationforComputationalLinguistics, 2013. Reasoning With Neural Tensor Networks for Knowledge Base\n",
      "318–327.https://doi.org/10.18653/v1/D15-1038 Completion. In Advances in Neural Information Processing Systems 26,\n",
      "[8] DiederikKingmaandJimmyBa.2014.Adam:Amethodforstochasticoptimiza- C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Wein-\n",
      "tion.arXivpreprintarXiv:1412.6980(2014). berger(Eds.).CurranAssociates,Inc.,926–934. http://papers.nips.cc/paper/\n",
      "[9] YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,andXuanZhu.2015.Learning 5028-reasoning-with-neural-tensor-networks-for-knowledge-base-completion.\n",
      "EntityandRelationEmbeddingsforKnowledgeGraphCompletion.InProceed- pdf\n",
      "ingsoftheTwenty-NinthAAAIConferenceonArtificialIntelligence(AAAI’15). [16] FabianM.Suchanek,GjergjiKasneci,andGerhardWeikum.2007.Yago:ACore\n",
      "AAAIPress,2181–2187. http://dl.acm.org/citation.cfm?id=2886521.2886624 ofSemanticKnowledge.InProceedingsofthe16thInternationalConference\n",
      "[10] BonanMin,RalphGrishman,LiWan,ChangWang,andDavidGondek.2013. onWorldWideWeb(WWW’07).ACM,NewYork,NY,USA,697–706. https:\n",
      "DistantSupervisionforRelationExtractionwithanIncompleteKnowledgeBase. //doi.org/10.1145/1242572.1242667\n",
      "InProceedingsofthe2013ConferenceoftheNorthAmericanChapterofthe [17] KristinaToutanova,VictoriaLin,Wen-tauYih,HoifungPoon,andChrisQuirk.\n",
      "AssociationforComputationalLinguistics:HumanLanguageTechnologies.As- 2016.CompositionalLearningofEmbeddingsforRelationPathsinKnowledge\n",
      "sociationforComputationalLinguistics,777–782.http://aclweb.org/anthology/ BaseandText.InProceedingsofthe54thAnnualMeetingoftheAssociationfor\n",
      "N13-1095 ComputationalLinguistics(Volume1:LongPapers).AssociationforComputa-\n",
      "[11] ArvindNeelakantan,BenjaminRoth,andAndrewMcCallum.2015.Composi- tionalLinguistics,1434–1444.https://doi.org/10.18653/v1/P16-1136\n",
      "tionalVectorSpaceModelsforKnowledgeBaseCompletion.InProceedings [18] The´oTrouillon,ChristopherRDance,JohannesWelbl,SebastianRiedel,E´ric\n",
      "ofthe53rdAnnualMeetingoftheAssociationforComputationalLinguistics Gaussier,andGuillaumeBouchard.2017. KnowledgeGraphCompletionvia\n",
      "andthe7thInternationalJointConferenceonNaturalLanguageProcessing ComplexTensorFactorization.arXivpreprintarXiv:1702.06879(2017).\n",
      "(Volume1:LongPapers).AssociationforComputationalLinguistics,156–166.\n",
      "https://doi.org/10.3115/v1/P15-1016<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "          93028,     43,  13102,   5706,     25,  35075,  14126,  29356,   9268,\n",
      "          11108,   2168,    367,   2000,    198,     58,     22,     60,  92073,\n",
      "          17198,     84,     11,  13379,  89437,  51526,     47,   3035,     88,\n",
      "             43,  28323,     13,    679,     20,   8404,  22292,    287,  81434,\n",
      "          11461,     82,  93028,     43,  13102,   5706,     11,   5728,   4235,\n",
      "           5833,   7109,   1129,  48836,   2984,   2726,     14,  32329,   2508,\n",
      "          20906,   1032,     12,   1041,     23,    198,    258,   3866,  10115,\n",
      "           5450,  85438,    287,    708,     69,   1820,    679,     20,  92348,\n",
      "            263,  29831,  67966,  18337,    510,    868,     60,  12131,   2100,\n",
      "           9211,     11,  11824,  41287,  25507,     11,  26035,    423,  40623,\n",
      "             11,    323,  13929,  17030,    627,    258,  55381,  14126,  29992,\n",
      "          11108,   2168,    367,   2000,  59122,   1697,     43,  13102,   5706,\n",
      "             11,    220,    679,     18,     13,  27857,    287,   3161,  61577,\n",
      "          27127,  39810,    369,  33025,   5464,    198,  17592,   4235,  13817,\n",
      "             13,   2485,   1129,  48886,   2726,     14,    605,     13,   9714,\n",
      "           4331,   5574,     16,  15302,    868,     12,   6889,     23,  57350,\n",
      "             13,    763,  91958,    304,  61577,   8245,  29225,  15264,    220,\n",
      "           1627,    345,     58,     23,     60,    423,  22970,   1609,  34655,\n",
      "           1764,    438,  86755,  59927,     13,    679,     19,  70218,  56748,\n",
      "           4492,   2000,    267,  67054,  19680,  17528,     12,    356,     13,\n",
      "            622,     13,    356,     13,  12649,   4282,     11,    445,     13,\n",
      "          37330,    283,     11,    386,     13,    468,   6427,     11,   1901,\n",
      "             13,  24855,   1494,   2453,   5676,     11,    323,    735,     13,\n",
      "           1229,     13,  71613,   7058,  28491,  17126,     55,    344,   1762,\n",
      "           1374,    277,     55,    344,     25,   9335,     17,     13,  25169,\n",
      "             15,      7,    679,     19,    570,  10418,   1414,      7,   2782,\n",
      "             82,  36434,  17119,   6713,  30915,    988,     11,  40345,   2637,\n",
      "          26026,   4235,  24347,     13,   1795,   1129,  48393,   1276,   3153,\n",
      "          28912,   4420,   3271,   6018,     58,     24,     60,    816,   1201,\n",
      "           2192,  51697,  51932,   6151,     88,  10602,     43,  19260,  28112,\n",
      "          65966,    647,  31192,  30533,    526,     43,  19260,  51526,     55,\n",
      "          10602,     57,  17156,     13,    679,     20,   1236,  16933,    220,\n",
      "          17824,     23,   5621,   1525,    287,  27281,  41078,   4269,   2442,\n",
      "           3890,  57832,     82,  15548,  12934,  52286,  31113,  11733,  14723,\n",
      "            627,   3106,    438,  34890,  26566,  25624,   2000,  81434,  11461,\n",
      "          34290,   5450,  85438,     12,  13072,    198,    287,    708,     69,\n",
      "           1820,  76896,  11500,  48121,   6157,  15836,  92348,    263,   9470,\n",
      "          16895,   1090,   8677,   4444,   6157,     40,    529,    868,    570,\n",
      "            510,    845,     60,  19797,   1122,     44,    815,   1412,    276,\n",
      "           1247,  38406,     73,   2431,   7910,     42,  66636,  76832,  51526,\n",
      "          66497,  19221,   1687,   1609,    372,     13,   1049,     22,   7659,\n",
      "           6438,     25,   1741,    461,    198,  51207,   3378,    676,     11,\n",
      "          13302,     16,   4235,  13302,     22,     13,   1795,   1129,   8910,\n",
      "          15761,     76,   2726,   2971,   7709,    522,  21796,  20970,     28,\n",
      "          15287,  23181,     16,     13,  15287,  24199,     19,    315,  99031,\n",
      "          81434,   5450,  85438,    287,    708,     69,   1820,    845,    339,\n",
      "          34746,  92348,    198,     58,    605,     60,  13789,    276,   6349,\n",
      "          24412,  31323,   6600,    819,   1543,  31214,     72,     54,    276,\n",
      "             11,   1163,    526,     54,    526,  51526,  23083,     38,  17675,\n",
      "             74,     13,    679,     18,     13,    389,  10343,  62070,   6109,\n",
      "          14358,  19522,    529,   2589,    570,   1741,     44,     11,   3648,\n",
      "         100077,  22812,     56,     11,  25342,     11,  25388,   4235,  22457,\n",
      "             13,   3788,    512,     35,  11451,  10254,    651,   1854,   2000,\n",
      "          34890,    849,  27523,   4291,    276,  97798,  81434,   4066,     13,\n",
      "            443,  48886,   2726,     14,    605,     13,   8011,     20,     14,\n",
      "           8874,  15574,     17,     13,   8874,  15999,     22,    198,    644,\n",
      "          85438,    287,    708,     69,   1820,    679,     18,  92348,   1073,\n",
      "           1820,  26287,  29518,  26072,   1073,   1820,    510,   1114,     60,\n",
      "          27973,   2259,     51,    412,  86563,     11,  82056,  51697,  50640,\n",
      "            268,   2442,   2933,     56,   7141,  44639,     78,    333,   2234,\n",
      "             47,   9186,  51526,  32978,   2232,  14468,    627,  64561,   2000,\n",
      "          59122,   1697,     43,  13102,   5706,     25,  35075,  14126,  29356,\n",
      "           9268,  20855,     12,    220,    679,     21,   3034,    981,   3079,\n",
      "          48567,   1073,  26566,  25624,   2000,  34890,   1858,  16319,  81434,\n",
      "            198,     82,   2168,    367,   2000,  59122,   1697,     43,  13102,\n",
      "           5706,     11,  15831,   4235,  23833,   7109,   1129,  48836,   2984,\n",
      "           2726,     14,  32329,   2508,     14,   5464,    438,   1199,   5450,\n",
      "          85438,    287,    708,     69,   1820,   4370,    339,  81596,  65676,\n",
      "           1073,   1820,  64561,   2000,    198,     45,   1032,     12,   7743,\n",
      "             20,  93028,     43,  13102,   5706,  12692,   4765,     16,     25,\n",
      "           6720,     47,   9724,    570,  64561,   2000,  59122,     64,   7058,\n",
      "             58,    806,     60,   1676,     85,    485,   8989,    301,    587,\n",
      "          67289,   8324,    268,  26312,     49,   8942,  51526,  41598,  26353,\n",
      "           7368,    372,     13,    679,     20,   3034,    981,     72,     12,\n",
      "            259,   4001,     43,  13102,   5706,     11,  10290,     19,   4235,\n",
      "           8929,     19,     13,   2485,   1129,  48886,   2726,     14,    605,\n",
      "             13,   9714,   4331,   5574,     16,  16744,    845,     12,   8190,\n",
      "             21,    198,     83,   4001,   3866,  10115,  17399,   2000,  81434,\n",
      "           4066,  34290,   5450,  85438,    826,    510,    972,     60,    578,\n",
      "          29211,     78,  91635,  43588,     11,  75066,  37790,    685,  59962,\n",
      "           2319,  42256,  84616,   2067,     11,   1542,  37597,     49,   1142,\n",
      "            301,  43225,  29211,   2265,    198,   1073,   1820,   4331,   6634,\n",
      "          81596,  65676,   1073,   1820,  64561,   2000,  59122,   1697,     43,\n",
      "          13102,   5706,  94316,   1291,  51526,  17198,  99112,     33,   3102,\n",
      "            569,     13,    679,     22,     13,  33025,  11461,  34290,  20708,\n",
      "            198,    438,   1820,     22,    339,  34746,  42097,  92348,    263,\n",
      "          55381,  14126,  29992,  22872,  26404,  21316,   2065,  17126,     55,\n",
      "            344,   1762,   1374,    277,     55,    344,     25,   8258,     17,\n",
      "             13,  26661,   4643,      7,    679,     22,   4390,  12692,   4765,\n",
      "             16,     25,   6720,     47,   9724,    570,  64561,   2000,  59122,\n",
      "           1697,     43,  13102,   5706,     11,  10132,   4235,  11247,    627,\n",
      "           2485,   1129,  48886,   2726,     14,    605,     13,  15134,     20,\n",
      "           5574,     16,  16744,    868,     12,   4645,     21, 128009, 128006,\n",
      "            882, 128007,    271,   7184,     11,   2728,    420,   3488,     25,\n",
      "           3639,    527,    279,   9256,    430,    279,   1646,    374,  16572,\n",
      "            369,   4710,    220,  21335,   1203,    279,   4320,   1193,    304,\n",
      "            264,  13325,   1160,   3645,     11,    369,   3187,     25,   2570,\n",
      "             32,   1882,     33,   7352,   1442,    499,   1541,    956,   1440,\n",
      "            279,   4320,     11,   1120,    471,    459,   4384,   1160,     13,\n",
      "         128009, 128006,  78191, 128007,    271]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "          93028,     43,  13102,   5706,     25,  35075,  14126,  29356,   9268,\n",
      "          11108,   2168,    367,   2000,    198,     58,     22,     60,  92073,\n",
      "          17198,     84,     11,  13379,  89437,  51526,     47,   3035,     88,\n",
      "             43,  28323,     13,    679,     20,   8404,  22292,    287,  81434,\n",
      "          11461,     82,  93028,     43,  13102,   5706,     11,   5728,   4235,\n",
      "           5833,   7109,   1129,  48836,   2984,   2726,     14,  32329,   2508,\n",
      "          20906,   1032,     12,   1041,     23,    198,    258,   3866,  10115,\n",
      "           5450,  85438,    287,    708,     69,   1820,    679,     20,  92348,\n",
      "            263,  29831,  67966,  18337,    510,    868,     60,  12131,   2100,\n",
      "           9211,     11,  11824,  41287,  25507,     11,  26035,    423,  40623,\n",
      "             11,    323,  13929,  17030,    627,    258,  55381,  14126,  29992,\n",
      "          11108,   2168,    367,   2000,  59122,   1697,     43,  13102,   5706,\n",
      "             11,    220,    679,     18,     13,  27857,    287,   3161,  61577,\n",
      "          27127,  39810,    369,  33025,   5464,    198,  17592,   4235,  13817,\n",
      "             13,   2485,   1129,  48886,   2726,     14,    605,     13,   9714,\n",
      "           4331,   5574,     16,  15302,    868,     12,   6889,     23,  57350,\n",
      "             13,    763,  91958,    304,  61577,   8245,  29225,  15264,    220,\n",
      "           1627,    345,     58,     23,     60,    423,  22970,   1609,  34655,\n",
      "           1764,    438,  86755,  59927,     13,    679,     19,  70218,  56748,\n",
      "           4492,   2000,    267,  67054,  19680,  17528,     12,    356,     13,\n",
      "            622,     13,    356,     13,  12649,   4282,     11,    445,     13,\n",
      "          37330,    283,     11,    386,     13,    468,   6427,     11,   1901,\n",
      "             13,  24855,   1494,   2453,   5676,     11,    323,    735,     13,\n",
      "           1229,     13,  71613,   7058,  28491,  17126,     55,    344,   1762,\n",
      "           1374,    277,     55,    344,     25,   9335,     17,     13,  25169,\n",
      "             15,      7,    679,     19,    570,  10418,   1414,      7,   2782,\n",
      "             82,  36434,  17119,   6713,  30915,    988,     11,  40345,   2637,\n",
      "          26026,   4235,  24347,     13,   1795,   1129,  48393,   1276,   3153,\n",
      "          28912,   4420,   3271,   6018,     58,     24,     60,    816,   1201,\n",
      "           2192,  51697,  51932,   6151,     88,  10602,     43,  19260,  28112,\n",
      "          65966,    647,  31192,  30533,    526,     43,  19260,  51526,     55,\n",
      "          10602,     57,  17156,     13,    679,     20,   1236,  16933,    220,\n",
      "          17824,     23,   5621,   1525,    287,  27281,  41078,   4269,   2442,\n",
      "           3890,  57832,     82,  15548,  12934,  52286,  31113,  11733,  14723,\n",
      "            627,   3106,    438,  34890,  26566,  25624,   2000,  81434,  11461,\n",
      "          34290,   5450,  85438,     12,  13072,    198,    287,    708,     69,\n",
      "           1820,  76896,  11500,  48121,   6157,  15836,  92348,    263,   9470,\n",
      "          16895,   1090,   8677,   4444,   6157,     40,    529,    868,    570,\n",
      "            510,    845,     60,  19797,   1122,     44,    815,   1412,    276,\n",
      "           1247,  38406,     73,   2431,   7910,     42,  66636,  76832,  51526,\n",
      "          66497,  19221,   1687,   1609,    372,     13,   1049,     22,   7659,\n",
      "           6438,     25,   1741,    461,    198,  51207,   3378,    676,     11,\n",
      "          13302,     16,   4235,  13302,     22,     13,   1795,   1129,   8910,\n",
      "          15761,     76,   2726,   2971,   7709,    522,  21796,  20970,     28,\n",
      "          15287,  23181,     16,     13,  15287,  24199,     19,    315,  99031,\n",
      "          81434,   5450,  85438,    287,    708,     69,   1820,    845,    339,\n",
      "          34746,  92348,    198,     58,    605,     60,  13789,    276,   6349,\n",
      "          24412,  31323,   6600,    819,   1543,  31214,     72,     54,    276,\n",
      "             11,   1163,    526,     54,    526,  51526,  23083,     38,  17675,\n",
      "             74,     13,    679,     18,     13,    389,  10343,  62070,   6109,\n",
      "          14358,  19522,    529,   2589,    570,   1741,     44,     11,   3648,\n",
      "         100077,  22812,     56,     11,  25342,     11,  25388,   4235,  22457,\n",
      "             13,   3788,    512,     35,  11451,  10254,    651,   1854,   2000,\n",
      "          34890,    849,  27523,   4291,    276,  97798,  81434,   4066,     13,\n",
      "            443,  48886,   2726,     14,    605,     13,   8011,     20,     14,\n",
      "           8874,  15574,     17,     13,   8874,  15999,     22,    198,    644,\n",
      "          85438,    287,    708,     69,   1820,    679,     18,  92348,   1073,\n",
      "           1820,  26287,  29518,  26072,   1073,   1820,    510,   1114,     60,\n",
      "          27973,   2259,     51,    412,  86563,     11,  82056,  51697,  50640,\n",
      "            268,   2442,   2933,     56,   7141,  44639,     78,    333,   2234,\n",
      "             47,   9186,  51526,  32978,   2232,  14468,    627,  64561,   2000,\n",
      "          59122,   1697,     43,  13102,   5706,     25,  35075,  14126,  29356,\n",
      "           9268,  20855,     12,    220,    679,     21,   3034,    981,   3079,\n",
      "          48567,   1073,  26566,  25624,   2000,  34890,   1858,  16319,  81434,\n",
      "            198,     82,   2168,    367,   2000,  59122,   1697,     43,  13102,\n",
      "           5706,     11,  15831,   4235,  23833,   7109,   1129,  48836,   2984,\n",
      "           2726,     14,  32329,   2508,     14,   5464,    438,   1199,   5450,\n",
      "          85438,    287,    708,     69,   1820,   4370,    339,  81596,  65676,\n",
      "           1073,   1820,  64561,   2000,    198,     45,   1032,     12,   7743,\n",
      "             20,  93028,     43,  13102,   5706,  12692,   4765,     16,     25,\n",
      "           6720,     47,   9724,    570,  64561,   2000,  59122,     64,   7058,\n",
      "             58,    806,     60,   1676,     85,    485,   8989,    301,    587,\n",
      "          67289,   8324,    268,  26312,     49,   8942,  51526,  41598,  26353,\n",
      "           7368,    372,     13,    679,     20,   3034,    981,     72,     12,\n",
      "            259,   4001,     43,  13102,   5706,     11,  10290,     19,   4235,\n",
      "           8929,     19,     13,   2485,   1129,  48886,   2726,     14,    605,\n",
      "             13,   9714,   4331,   5574,     16,  16744,    845,     12,   8190,\n",
      "             21,    198,     83,   4001,   3866,  10115,  17399,   2000,  81434,\n",
      "           4066,  34290,   5450,  85438,    826,    510,    972,     60,    578,\n",
      "          29211,     78,  91635,  43588,     11,  75066,  37790,    685,  59962,\n",
      "           2319,  42256,  84616,   2067,     11,   1542,  37597,     49,   1142,\n",
      "            301,  43225,  29211,   2265,    198,   1073,   1820,   4331,   6634,\n",
      "          81596,  65676,   1073,   1820,  64561,   2000,  59122,   1697,     43,\n",
      "          13102,   5706,  94316,   1291,  51526,  17198,  99112,     33,   3102,\n",
      "            569,     13,    679,     22,     13,  33025,  11461,  34290,  20708,\n",
      "            198,    438,   1820,     22,    339,  34746,  42097,  92348,    263,\n",
      "          55381,  14126,  29992,  22872,  26404,  21316,   2065,  17126,     55,\n",
      "            344,   1762,   1374,    277,     55,    344,     25,   8258,     17,\n",
      "             13,  26661,   4643,      7,    679,     22,   4390,  12692,   4765,\n",
      "             16,     25,   6720,     47,   9724,    570,  64561,   2000,  59122,\n",
      "           1697,     43,  13102,   5706,     11,  10132,   4235,  11247,    627,\n",
      "           2485,   1129,  48886,   2726,     14,    605,     13,  15134,     20,\n",
      "           5574,     16,  16744,    868,     12,   4645,     21, 128009, 128006,\n",
      "            882, 128007,    271,   7184,     11,   2728,    420,   3488,     25,\n",
      "           3639,    527,    279,   9256,    430,    279,   1646,    374,  16572,\n",
      "            369,   4710,    220,  21335,   1203,    279,   4320,   1193,    304,\n",
      "            264,  13325,   1160,   3645,     11,    369,   3187,     25,   2570,\n",
      "             32,   1882,     33,   7352,   1442,    499,   1541,    956,   1440,\n",
      "            279,   4320,     11,   1120,    471,    459,   4384,   1160,     13,\n",
      "         128009, 128006,  78191, 128007,    271,    681,  81434,   5464,  57350,\n",
      "            518,    364,  34890,  95606,    518,    364,  26197,    287,    663,\n",
      "         128009]], device='cuda:0')\n",
      "Decoded output:\n",
      " ['Knowledge Base Completion', 'Relation Extraction', 'Reasoning']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Learning Knowledge Graph Embeddings\n",
      "with Type Regularizer\n",
      "BhushanKotnisandViviNastase\n",
      "HeidelbergUniversity\n",
      "Heidelberg,Germany69120\n",
      "{kotnis,nastase}@cl.uni-heidelberg.de\n",
      "ABSTRACT Thisresultsinamulti-graph,i.e. agraphwithdifferenttypesof\n",
      "Learningrelationsbasedonevidencefromknowledgerepositories linkswherealinktypecorrespondstoarelationtype.\n",
      "reliesonprocessingtheavailablerelationinstances. Knowledge KGsareknowntobeincomplete[10],i.e.,asignificantnumber\n",
      "repositoriesarenotbalancedintermsofrelationsorentities–there ofrelationsbetweenentitiesaremissing.Embeddingtheknowledge\n",
      "arerelationswithlessthan10butalsothousandsofinstances,and graphinacontinuousvectorspacehasbeensuccessfullyusedto\n",
      "entities involved in less than 10 but also thousands of relations. addressthisproblem[2,13,15].Suchmodelsrepresentthecompo-\n",
      "Manyrelations,however,havecleardomainandrange,whichwe nentsofthegraph,i.e.,theentitiesandrelations,usingrealvaluedla-\n",
      "hypothesize could help learn a better, more generalizing, model. tentfactorsthatencodethestructureoftheknowledgegraph.Forex-\n",
      "WeincludesuchinformationintheRESCALmodelintheform amplethelatentfactormodelshouldbeabletorecoverColognefrom\n",
      "ofaregularizationfactoraddedtothelossfunctionthattakesinto thelatentrepresentationsofMoselleandriver flowsThrough city.\n",
      "accountthetypes(categories)oftheentitiesthatappearasarguments ExamplesincludetheRESCAL[13]tensorfactorizationmodel,the\n",
      "torelationsintheknowledgebase.TestedonFreebase,afrequently TransE model [2] and their variations [9, 12]. We focus on the\n",
      "usedbenchmarkingdatasetforlink/pathpredictingtasks,wenote RESCALmodel,oneofthemostflexibleandwidelyusedmodels.\n",
      "increasedperformancecomparedtothebaselinemodelintermsof RESCALisabilinearmodelthatrepresentstriplesasapairwise\n",
      "meanreciprocalrankandhits@N,N=1,3,10. Furthermore,we interactionofsourceandtargetentitylatentfactors(embeddings)\n",
      "discoverscenariosthatsignificantlyimpacttheeffectivenessofthe throughamatrixthatrepresentsthelatentfactorsoftheconnecting\n",
      "typeregularizer. relation.Theentityandrelationrepresentationsinducedcanbeused\n",
      "to predict additional relations – edges – between known entities.\n",
      "KEYWORDS Table1listsafewexamplesofentitytypeinformationinFreebase.\n",
      "Existingknowledgegraphsareimbalanced–bothrelationand\n",
      "KnowledgeGraphs,GraphEmbedding,LinkPrediction\n",
      "entityfrequenciesvarywidely,asevidentfromthestatisticsonFree-\n",
      "ACMReferenceformat: base15kshowninFigure1.Sinceentityandrelationembeddings\n",
      "BhushanKotnisandViviNastase.2017.LearningKnowledgeGraphEm- arebasedontheconnectivitystructureofthegraph,itisreasonable\n",
      "beddings toaskwhatistheoutcomeoftheknowledgegraphembeddingfor\n",
      "withTypeRegularizer.InProceedingsofK-CAP2017:KnowledgeCapture\n",
      "entitiesandrelationswhichareunderrepresentedinthegraph,in\n",
      "Conference,Austin,TX,USA,December4–6,2017(K-CAP2017),6pages.\n",
      "particular,howgoodaretheyforthetaskoflinkprediction.\n",
      "DOI:10.1145/3148011.3154466\n",
      "ApproachessuchasRESCALtakeanextensionalviewofrela-\n",
      "tions–theyprocessthecollectionofinstanceswithoutknowledgeof\n",
      "1 INTRODUCTION higherlevelrulesorinformationabouttheserelations.Wehypothe-\n",
      "Knowledge–lexical,worldandcommon-sense–iscrucialfortasks sizethatprovidingthehigherlevel–intensional–viewintheform\n",
      "suchasautomatedtextcomprehensionandsummarization, ques- oftypesorcategoriesofrelationarguments,canleadtoimproved\n",
      "tionanswering,naturallanguagedialoguesystems.Tomakesuch resultsforthetaskoflinkprediction.Thismaybetrueparticularly\n",
      "knowledgeavailableforautomaticprocessing,themostcommon forknowledgegraphssuchasFreebasethathavestronglytypedrela-\n",
      "approachistoprovideitasacollectionofrelationtriples–entities tions,andalsoforlow-frequencyrelationsorforrelationsinvolving\n",
      "or concepts connected by a relation: e.g., (concept:city:London, low-frequencyentities.\n",
      "relation:country capital,concept:country:UK).Globally,suchcol- Inthisarticlewepresentexperimentalresultssupportingthehy-\n",
      "lectionscanbeviewedasknowledgegraphs(KGs),forexample pothesisthataugmentingsingle-relationmodelswithentitytypein-\n",
      "NELL [3], Freebase [1] and YAGO [16]. In such graphs, nodes formation,intheformofa‘Type’regularizer,leadstoimprovements\n",
      "(entities/concepts)maybeconnectedbydifferenttypesofrelations. inpredictingmissinglinks.Theresultsshowthateventhoughthe\n",
      "bilinearmodelinducesrepresentationsforallentitiesandrelations\n",
      "Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor together–soitimplicitlyusesthetypeinformationweprovideasa\n",
      "classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed\n",
      "separaterelation–thetyperegularizerwhichexplicitlyincludessuch\n",
      "forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation\n",
      "onthefirstpage.CopyrightsforcomponentsofthisworkownedbyothersthanACM informationforeachrelationleadstobetterresults.Furthermore,we\n",
      "mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish, notethepositiveimpactofincludingthetyperegularizerforrelations\n",
      "topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora\n",
      "involvinglow-frequencyentities,whereaslow-frequencyrelations\n",
      "fee.Requestpermissionsfrompermissions@acm.org.\n",
      "K-CAP2017,Austin,TX,USA arelessaffectedbythisaddedinformation. Wealsoanalyzethe\n",
      "©2017ACM. 978-1-4503-5553-7/17/12...$15.00\n",
      "DOI:10.1145/3148011.3154466\n",
      "8102\n",
      "raM\n",
      "2\n",
      "]IA.sc[\n",
      "2v87290.6071:viXra\n",
      "K-CAP2017,December4–6,2017,Austin,TX,USA BhushanKotnisandViviNastase\n",
      "SourceType Source PathorRelation Target TargetType\n",
      "film star wars episode IV produced by дeorдe lucas film producer\n",
      "person alexandre dumas people profession writer profession\n",
      "academic post professor profession people albert einstein person\n",
      "Table1:EntityTypeInformation:ExamplesofsourceandtargetentitytypesfromFreebaseusedinthetyperegularizer.\n",
      "104 Argument_1\n",
      "train\n",
      "103 dev\n",
      "test\n",
      "102\n",
      "101\n",
      "100\n",
      "0 2000 4000 6000 8000 10000 12000 14000\n",
      "104 Argument_2\n",
      "train\n",
      "103 dev\n",
      "test\n",
      "102\n",
      "101\n",
      "100\n",
      "0 2000 4000 6000 8000 10000 12000 14000\n",
      "105\n",
      "104\n",
      "103\n",
      "102\n",
      "101\n",
      "100\n",
      "0 200 400 600 800 1000 1200\n",
      "ycneuqerF\n",
      "entity information, in the form of latent factors, improves KBC\n",
      "performance.Thesourceandtargettypesarenotexplicitlyincluded.\n",
      "[4]makeusetheoftypeinformationandproduceavariationof\n",
      "RESCALtheycallTRESCAL–TypedRESCAL.Thetypeinformation\n",
      "isusedtoimprovetheefficiencyofthemodel,byreducingthesize\n",
      "oftheentitymatrixinthecomputationofthelossfunctiontoentities\n",
      "belongingtothedomainandrangeoftherelation.Theentitytype\n",
      "assuchisonlyimplicitlyincorporated,assomethingsharedbythe\n",
      "entitiessingledoutforcomputingthelossfunction.\n",
      "[5]buildson[11],andusesanRNNtomodelpathswhichincor-\n",
      "poratetypeinformationfortheentitiesalongthepath.Entitiesare\n",
      "representedasasumoftheirentitytypes,whicharelearnedduring\n",
      "training.Includingthisinformationleadstohigherperformance.\n",
      "Compared with these previous approaches, we add the entity\n",
      "typesexplicitlyinthemodel,andderivearepresentationforentities\n",
      "andtheirtypesconcurrently.Weanalyzetheimpactofusingsuch\n",
      "representationforlinkpredictionwithdifferentamountsoftraining\n",
      "data,tounderstandunderwhatconditionsthetypeinformationhas\n",
      "apositiveimpact.\n",
      "Relation\n",
      "train\n",
      "dev\n",
      "test 3 METHODS\n",
      "InthissectionwedescribetheRESCALmodelandshowhowthe\n",
      "typeregularizerwasaddedtoincludethetypeinformationforeach\n",
      "relationinthecomputationofthelossfunction.\n",
      "3.1 Definitions\n",
      "LetE,RbethesetofentitiesandrelationsintheKGrespectively.A\n",
      "knowledgegraphGisasetoftriples(s,r,t)wheres,t ∈E, r ∈R\n",
      "Figure 1: Statistics on argument and relation frequencies for\n",
      "andrelationr connectsstot.\n",
      "Freebase15k\n",
      "Theknowledgebasecompletion(KBC)taskisthetaskofclas-\n",
      "sifyingwhetherthetriple(s,r,t)isapartoftheknowledgegraph.\n",
      "Thiscanbedescribedas(s,r,?)or(?,r,t)wherethequestionmark\n",
      "representstheunknowncorrecttarget/sourceentityfromasetof\n",
      "effectsoftrainingdatasizeontheusefulnessofthetyperegularizer,\n",
      "candidateentities.\n",
      "andnotethatitsimpactgrowswiththeamountoftrainingdata.\n",
      "2 RELATEDWORK 3.2 RESCALModel\n",
      "Avarietyoflatentfactormodels[2,13–15]havebeendevelopedto TheRESCALmodel[13]weightstheinteractionofallpairwise\n",
      "represententitiesandrelationsinaknowledgegraph,andhavebeen latentfactorbetweenthesourceandtargetentityforpredictinga\n",
      "used to address the knowledge base completion (KBC) problem. relation. Itrepresentseveryentityasavector(x ∈Rd),andevery\n",
      "Mostlatentfactormodelsaretrainedoneitherknowledgegraph relationasamatrixW ∈ Rd×d. Thismodelrepresentsthetriple\n",
      "triples,ortriplesextractedfromopendomainknowledgeextraction (s,r,t)asascoregivenby\n",
      "tools [14]. A notable exception is the RNN model proposed by sc(s,r,t)=x sT Wr xt\n",
      "[11]thatlearnspathembeddingsforknowledgebasecompletion. Thisisequivalenttotensorfactorizationwhereeachrelationmatrix\n",
      "[7]proposeacompositionalobjectivefunctionoverlatentfactor isasliceofthetensor. Thesevectorsandmatricesarelearnedby\n",
      "models, whichistrainedonpathsaswellastriples. Formodels constructingalossfunctionthatcontraststhescoreofacorrecttriple\n",
      "thatarecompositional,[17]showsthatincorporatingintermediate toincorrectones.Hereweusethemax-marginlossdescribedinthe\n",
      "LearningKnowledgeGraphEmbeddings\n",
      "withTypeRegularizer K-CAP2017,December4–6,2017,Austin,TX,USA\n",
      "followingequation: WeobtainFreebasecategorydatafrom[6],andthentheentity\n",
      "typebymappingtheFreebaseentityidentifiertotheFreebasecat-\n",
      "N\n",
      "J(Θ)=(cid:213) (cid:213) mm(σ(sci),σ(sc i(cid:48))) (1) weg ho ir cy h. isT uh si es dre insu tl ht es ti rn ai1 n0 in1 g,3 s5 t3 agi en.s Ita tn isce ns oto uf st eh de dc ua rt ie ng go tr ey stre til mat eio.n\n",
      "i=1t(cid:48)∈N(t)\n",
      "(cid:20) (cid:21) 4.2 Implementation\n",
      "mm(σ(sci),σ(sc i(cid:48)))=max 0,1−σ(sci)+σ(sc i(cid:48))\n",
      "WeusetheAdam[8]SGDoptimizerfortrainingbecauseitaddresses\n",
      "theproblemofdecreasinglearningrateinAdaGrad.Weusemedian\n",
      "wherethereareNpositiveinstances,positiveandnegativeinstances\n",
      "arescoredassci =sc(si,ri,ti)andsc i(cid:48) =sc(si,ri,t i(cid:48)),respectively. g thr aa tdi ee nn tt itc yli ep mpi bn eg dt do inp gre sv he an vt eex up nl io ts ni ove rmg.ra Wdi een pt es ra fon rd mw ee da el xs ho ae un ss tu ivr ee\n",
      "N(t)isthesetofincorrecttargetsandσ isthesigmoidfunction.\n",
      "gridsearchfortheL2regularizeraswellasα onthevalidationset\n",
      "andwetunedthetrainingdurationusingearlystopping.Weuse100\n",
      "3.3 TheTypeRegularizer\n",
      "dimensionalentityvectorinallexperiments1.\n",
      "Weintroducearegularizertermwhichincorporatestypeinformation\n",
      "ofsourceandtargetentities. Letscat bethetypeforentitys and 4.3 EvaluationProcedure\n",
      "rcat therelationbetweensandscat.Dependingontheknowledge\n",
      "Forevaluationwefollowtheproceduredescribedin[15].Forevery\n",
      "resource,rcat couldbeis a(inanontology,forexample),cateдory\n",
      "testtriplewepredicteitherthesourceorthetarget,andnegative\n",
      "(inaresourcebuiltbasedonWikipedia),orothersuchrelationsthat\n",
      "instacesfortrainingandtestingareproducedbycorruptingpositive\n",
      "capturetheentitytype.Afewexamplesofentitytypescanbeseen ones:wereplaces(ort)ina(s,r,t)triplewithansn (ortn)thathas\n",
      "inTable1.Notethatentitytypeinformationisnotusedduringtest\n",
      "thesametypeass(ort)butdoesnotappearinapositiveinstance\n",
      "time. (sn,r,t) (or (s,r,tn)). For meaningful comparison, the negative\n",
      "Ifsisthesourceentityandt thetargetentityforqueryq,thenwe\n",
      "triplesthatoccurintrainingorvalidationdatasetsaspositivetriples\n",
      "definetheregularizerasinequation2,whereN(scat)andN(tcat)\n",
      "arefilteredout.Forfasterevaluation,insteadofusingallnegative\n",
      "aresetsof(negatives)forscat,tcat,whileT(scat),T(tcat)aresets\n",
      "triples,weproduce1000byrandomlysamplingentitiesfromthe\n",
      "ofcorrectcategoriesforsourcesandtargett respectively.mmisthe\n",
      "entireset.Wereportresultsintermsofhitsat1,3,10(HITS@1,3,10)\n",
      "maxmarginlossdescribedinequation(1).\n",
      "andmeanreciprocalrank(MRR)metrics.HitsatKistheproportion\n",
      "ofcorrectanswers(hits)inthefirstKrankedpredictions,whileMRR\n",
      "isthemeanofthereciprocaloftherankofthecorrectanswers.\n",
      "R(Θ,q):=\n",
      "(cid:213) (cid:18) (cid:48) (cid:19) 4.4 Results\n",
      "mm σsc(s,rcat,scat),σsc(s,rcat,s cat)\n",
      "(cid:48) Weusethebilinear(RESCAL)modelasabaseline.Asevidenced\n",
      "s cat∈N(scat)\n",
      "bytheresultsinTable2,addingthetyperegularizerimprovesper-\n",
      "scat∈T(scat)\n",
      "(cid:18) (cid:19) formance. It may be tempting to think that the performance im-\n",
      "+ (cid:213) mm σsc(t,rcat,tcat),σsc(t,rcat,t c(cid:48) at) (2) provementisnaturalsinceweareprovidingadditionalinformation\n",
      "(cid:48) throughthetyperegularizer.Wetestthisinfurtherexperiments.\n",
      "t cat∈N(tcat)\n",
      "tcat∈T(tcat)\n",
      "Metrics Bilinear Bilinear+TR\n",
      "Thecompleteobjectivefunctiontobeminimizedis MRR 0.343 0.3862\n",
      "N HITS@1 0.2451 0.304\n",
      "J(Θ)=(cid:213) (cid:213) mm(qi,ti,t i(cid:48) )+αR(Θ,qi) HITS@3 0.3804 0.4161\n",
      "i=1t i(cid:48)\n",
      "∈N(qi) HITS@10 0.5312 0.5408\n",
      "Table2: Evaluation: PerformanceComparisonbetweenbilin-\n",
      "wherethehyper-parameterα,α ≥ 0, controlstheimpactofthe\n",
      "earmodelwithandwithouttyperegularizer.\n",
      "regularizertermsandN(qi)isthesetofnegativetargetsforquery\n",
      "qi,whereqi correspondstoquery(si,ri,?). Wetesttheimpactofthetyperegularizerbyanalyzingitsperfor-\n",
      "manceondifferentsizesoftrainingdata.Wefirstgeneratemultiple\n",
      "4 EXPERIMENTS trainingdatasetsbyrandomlysampling25%,50%and75%ofthe\n",
      "4.1 Data triples. AsillustratedinTable3,whenusingonly25%to50%of\n",
      "thetrainingdata,theperformancedrops.Thetyperegularizeruses\n",
      "WecarryoutexperimentsonFB15K,asubsetoftheFreebaseknowl- categoryinformation,undercertaincircumstances(α =1)addingit\n",
      "edgegraphprovidedby[2]. Thisdatasetisastandardbenchmark\n",
      "isequivalenttoaddingapproximately100,000newtripleswithcate-\n",
      "datasetusedforevaluatinglinkpredictionalgorithms[2,12,18].\n",
      "goryrelationtothetrainingset.Thus,simplyaugmentingthemodel\n",
      "TheFB15Kdatasetconsistsof1345relationsand14,951entities.\n",
      "withadditionalinformationdoesnotalwaysimproveperformance.\n",
      "Thetraining,validationandtestsetconsistsof483,142,50,000and\n",
      "Thereasonbehindtheperformancedropwithlesstrainingdatais\n",
      "59,071triplesrespectively. TheFreebaserelationsdonotinclude\n",
      "notobvious,becauseaddingexternalinformationshouldhelpthe\n",
      "thecategoryrelation,thusthereisnooverlapbetweenthecategory\n",
      "triplesandFB15Ktriples. 1Codeisavailableathttps://github.com/bhushank/kge\n",
      "K-CAP2017,December4–6,2017,Austin,TX,USA BhushanKotnisandViviNastase\n",
      "%trainingdata Model MRR %Improvement RelationName Instances(train) Instances(test)\n",
      "100\n",
      "Bilinear 0.343 r1 /people/person/profession 11636 1384\n",
      "Bilinear+TR 0.3862 +12.59 r2 /music/genre/artists 5952 679\n",
      "Bilinear 0.3495 r3 /film/film/country 2407 280\n",
      "75\n",
      "Bilinear+TR 0.3552 +1.6 r4 /tv/tv program/genre 1010 100\n",
      "Bilinear 0.3457 Table4:Relationswithtrainandtestinstances\n",
      "50\n",
      "Bilinear+TR 0.3409 -1.3\n",
      "Bilinear 0.332\n",
      "25\n",
      "Bilinear+TR 0.3198 -3.67\n",
      "Table3: EffectoftrainingdatasizeonTR:Performancecom-\n",
      "parisonbetweenbilinearmodelswithandwithouttyperegular-\n",
      "izerfordifferentdatasetsizes.\n",
      "modellearnbetterembeddings. Wehypothesizethatthedropin\n",
      "performanceisbecausewhenfewernumberoftraininginstancesare\n",
      "available,thetyperegularizerleadsthesystemtolearnrelationsthat\n",
      "over-generalize. Themodelisbiasedtowardslearningcategories\n",
      "verywellforreducingtrainingloss.Thisresultsinembeddingsthat\n",
      "arebiasedtowardspredictingrelationsatthelevelofcategoriesand\n",
      "notindividualrelationsresultinginperformancedropfortherelation\n",
      "predictiontask.\n",
      "Figure3: MRRvs. PercentTrainDataformultiplerelations:\n",
      "NumberoftraininginstancesmodulateeffectofTypeRegular-\n",
      "izer.RelationslistedinTable4\n",
      "Fig. 3 shows the performance in terms of MRR (using Type\n",
      "Regularizer)forlinkpredictiononthesefourrelations.Theorange\n",
      "andbluelinesdenoterelations(r1,r2)with11,636and5952train-\n",
      "inginstancesrespectively,whiletheredandgreencurvesdenote\n",
      "relations(r3,r4)with2407and1010traininginstancesrespectively.\n",
      "Theredandgreencurves(therelationswithfewerinstances)showa\n",
      "largerchangeinMRRcomparedtotheorangeandbluecurves.This\n",
      "confirmsourhypothesisthattheTypeRegularizerismoresensitive\n",
      "forrelationswithasmallernumberoftraininginstances,andindi-\n",
      "N catesthattheembeddingslearnedforrelationswithlargernumber\n",
      "ofinstancesarelessbiasedtowardspredictingcategories.\n",
      "Figure 2: MRR vs. α: MRR drops with increasing We note that equation (2) has the same max margin structure\n",
      "strength of the type regularizer for models trained on 25% asthelossfunction,equation(1). Thereforeusingthisparticular\n",
      "(blue) and 100% (orange) of FB15K dataset. Plot for formulaforthetyperegularizerisequivalenttoaddingthecategory\n",
      "α =0.0001,0.001,0.01,0.01,1,5,10 relationasanadditionalsliceofthetensorfactorizedbyRESCAL,\n",
      "thenthehyperparameterαis1.Experimentshaveshownthoughthat\n",
      "We investigate this hypothesis by varying the value of α that finetuningα –andthisfine-tuningtheusageoftypeinformation–\n",
      "weighstheimportanceofthetyperegularizer(cf.equation1).We canleadtobetterresults.Morespecificallyitisequivalenttoadding\n",
      "plottheMeanReciprocalRankvs.thestrengthofthetyperegular- 101,353uniqueinstancesofcategoryrelation.\n",
      "izerformodeltrainedononly25%ofthetrainingdatainFig. 2. We also performed overall relation and entity analysis based\n",
      "Thehigherthestrengthofthetyperegularizer,thehigherthecost on their occurrence frequency. Looking at relations grouped by\n",
      "incurredformis-predictingthecategory. AsFig. 2shows,MRR theorder ofmagnitude(oom) oftheir occurrence frequency pre-\n",
      "fallssharplywithincreaseinα. Thiseffectisnotobservedinthe sentedinFigure4wenotethatlowfrequencyrelationsseemnot\n",
      "100% training data scenario. This suggests that adding category tobeaffectedbythetyperegularizer,andaremodeledbetterusing\n",
      "informationmayleadtoimprovedperformanceonlywhentheadded onlytheinstancesthemselves. Theresonforthisisthatverylow\n",
      "informationdoesnotseverelybiasthetrainingdata. frequency relations actually connect high frequency entities, e.g.\n",
      "Toinvestigatetheimpactoftrainingdatasizeonthetypereg- relation /award/hall of fame/discipline. On the other hand, high\n",
      "ularizerperformance,weanalyzeindetailtheperformanceofthe frequencyrelationshaveoveralllowerresultsthanotherrelations.\n",
      "systemforrelationswithadifferentnumberoftraininginstances. Thereasonforthisisthatinnumerouscases,oneofthearguments\n",
      "Table4listsfourrelationsweusedtolookintothisphenomenon. oftheserelationsisalowfrequencyentity.Forexample,thelives in\n",
      "LearningKnowledgeGraphEmbeddings\n",
      "withTypeRegularizer K-CAP2017,December4–6,2017,Austin,TX,USA\n",
      "OOM 0.0 OOM 1.0 OOM 2.0 OOM 3.0 OOM 4.0\n",
      "0.7\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "0.0\n",
      "25 50 75 100 25 50 75 100 25 50 75 100 25 50 75 100 25 50 75 100\n",
      "bilinear bilinear_tr\n",
      "OOM 0.0 OOM 1.0 OOM 2.0 OOM 3.0 OOM 4.0\n",
      "0.8\n",
      "0.7\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "0.0\n",
      "25 50 75 100 25 50 75 100 25 50 75 100 25 50 75 100 25 50 75 100\n",
      "bilinear bilinear_tr\n",
      "Figure4:MRRandHITS@10linkpredictionresultsgroupedbytheorderofmagnitudeofrelationfrequency,fordifferentamounts\n",
      "oftrainingdata.\n",
      "relation that connects a person with the city they live in, has as 5 CONCLUSION\n",
      "the”City”argumentanentitythatdoesnotappearinmanyother Weproposedatyperegularizerthatleveragesentitytypeinformation\n",
      "relations. forstate-of-the-artlatentfactormodelslikeRESCAL.Experiments\n",
      "To further clarify the reasons for variation in performance of onFreebaseFB15Kdatasetsuggestthataddingthetyperegular-\n",
      "relations,weanalyzethelinkpredictionresultsbasedontheorder izerimprovesperformanceontheknowledgebasecompletiontask.\n",
      "ofmagnitudeofentityfrequency,presentedinFigure5.Theresults Howeveraddingcategoryinformationmaynotimproveresultsfor\n",
      "inthiscasearemoreinlinewiththeexpectedoutcome–linksthat allrelations,particularlythosewithfewerpositiveinstanceswhere\n",
      "involvelowerfrequencyentitieshavelowerpredictionresults.The introducingcategoryinformationmayleadtoembeddingsthatarebi-\n",
      "typeinformationgenerallyhasapositiveimpactthroughout,except asedtowardscapturing/predictingcategoriesratherthanfinegrained\n",
      "medium-rangeentitieswhereitseemsthattypeinformationleadsto instances.Weplantostudytheimpactoftheaddedtypeinformation\n",
      "overgeneralization. fordatasetswheretherelationsarenotasstronglytypedasFreebase\n",
      "Usingthetyperegularizerasanadditionaltermswhoseweight –forgrammaticalcollocationinformationforexampleandinducing\n",
      "canbecalibratedusingtheα parametermakesiteasiertoadjustthe selectionalpreferences–andformorecomplex, pathprediction,\n",
      "influenceofthetypeinformationbasedonnodedegreesandrelation tasks.\n",
      "frequencies. Furthermore, byincorporatingthetypeinformation\n",
      "inthelossfunctionforeveryrelationasopposedtohavingitasa REFERENCES\n",
      "separaterelationintheknowledgegraphallowstheincorporationof\n",
      "[1] KurtBollacker,ColinEvans,PraveenParitosh,TimSturge,andJamieTaylor.\n",
      "therangeanddomaininformationforeachrelation,asopposedto 2008. Freebase: ACollaborativelyCreatedGraphDatabaseforStructuring\n",
      "modellingtheentitytypeoutsideofaparticularenvironment. HumanKnowledge.InProceedingsofthe2008ACMSIGMODInternational\n",
      "ConferenceonManagementofData(SIGMOD’08).ACM,NewYork,NY,USA,\n",
      "Itisinterestingtonotethatthebestresultsformediumtohigh 1247–1250. https://doi.org/10.1145/1376616.1376746\n",
      "frequencyentitiesandrelationsareobtainedwhenusingthefull [2] AntoineBordes,NicolasUsunier,AlbertoGarcia-Duran,JasonWeston,and\n",
      "Oksana Yakhnenko. 2013. Translating Embeddings for Modeling Multi-\n",
      "trainingdataandthetyperegularizer. Thisindicatesthatthetype\n",
      "relationalData. InAdvancesinNeuralInformationProcessingSystems26,\n",
      "regularizercanmitigatetheoverfittingtendencyofRESCAL,and C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Wein-\n",
      "produceamorerobustmodel. berger(Eds.).CurranAssociates,Inc.,2787–2795.http://papers.nips.cc/paper/\n",
      "5071-translating-embeddings-for-modeling-multi-relational-data.pdf\n",
      "[3] AndrewCarlson,JustinBetteridge,BryanKisiel,BurrSettles,EstevamR.Hr-\n",
      "uschka,andTomM.Mitchell.2010.TowardanArchitectureforNever-Ending\n",
      "LanguageLearning.InAAAI.\n",
      "K-CAP2017,December4–6,2017,Austin,TX,USA BhushanKotnisandViviNastase\n",
      "OOM 0.0 OOM 1.0 OOM 2.0 OOM 3.0\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "25 50 75 100 25 50 75 100 25 50 75 100 25 50 75 100\n",
      "bilinear bilinear_tr\n",
      "OOM 0.0 OOM 1.0 OOM 2.0 OOM 3.0\n",
      "0.7\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "25 50 75 100 25 50 75 100 25 50 75 100 25 50 75 100\n",
      "bilinear bilinear_tr\n",
      "Figure5: MRRandHITS@10linkpredictionresultsgroupedbytheorderofmagnitudeofentityfrequency,fordifferentamounts\n",
      "oftrainingdata.\n",
      "[4] Kai-WeiChang,Wen-tauYih,BishanYang,andChristopherMeek.2014.Typed [12] MaximilianNickel,LorenzoRosasco,andTomasoPoggio.2016.Holographic\n",
      "TensorDecompositionofKnowledgeBasesforRelationExtraction.InPro- EmbeddingsofKnowledgeGraphs.InProceedingsoftheThirtiethAAAICon-\n",
      "ceedingsofthe2014ConferenceonEmpiricalMethodsinNaturalLanguage ferenceonArtificialIntelligence(AAAI’16).AAAIPress,1955–1961. http:\n",
      "Processing(EMNLP).AssociationforComputationalLinguistics,1568–1579. //dl.acm.org/citation.cfm?id=3016100.3016172\n",
      "https://doi.org/10.3115/v1/D14-1165 [13] MaximilianNickel,VolkerTresp,andHans-PeterKriegel.2012. Factorizing\n",
      "[5] RajarshiDas,ArvindNeelakantan,DavidBelanger,andAndrewMcCallum.2016. YAGO:ScalableMachineLearningforLinkedData.InProceedingsofthe21st\n",
      "ChainsofReasoningoverEntities,Relations,andTextusingRecurrentNeural InternationalConferenceonWorldWideWeb(WWW’12).ACM,NewYork,NY,\n",
      "Networks.arXivpreprintarXiv:1607.01426(2016). USA,271–280. https://doi.org/10.1145/2187836.2187874\n",
      "[6] MattGardnerandTomMitchell.2015. EfficientandExpressiveKnowledge [14] SebastianRiedel,LiminYao,AndrewMcCallum,andM.BenjaminMarlin.2013.\n",
      "BaseCompletionUsingSubgraphFeatureExtraction.InProceedingsofthe2015 RelationExtractionwithMatrixFactorizationandUniversalSchemas.InProceed-\n",
      "ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.Association ingsofthe2013ConferenceoftheNorthAmericanChapteroftheAssociation\n",
      "forComputationalLinguistics,1488–1498.https://doi.org/10.18653/v1/D15-1173 forComputationalLinguistics:HumanLanguageTechnologies.Associationfor\n",
      "[7] KelvinGuu,JohnMiller,andPercyLiang.2015.TraversingKnowledgeGraphs ComputationalLinguistics,74–84.http://aclweb.org/anthology/N13-1008\n",
      "inVectorSpace.InProceedingsofthe2015ConferenceonEmpiricalMethods [15] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng.\n",
      "inNaturalLanguageProcessing.AssociationforComputationalLinguistics, 2013. Reasoning With Neural Tensor Networks for Knowledge Base\n",
      "318–327.https://doi.org/10.18653/v1/D15-1038 Completion. In Advances in Neural Information Processing Systems 26,\n",
      "[8] DiederikKingmaandJimmyBa.2014.Adam:Amethodforstochasticoptimiza- C. J. C. Burges, L. Bottou, M. Welling, Z.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,    521,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Bhushan Kotnis', 'Vivi Nastase']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: ComputationalLinguistics:HumanLanguageTechnologies.Associationfor\n",
      "[7] KelvinGuu,JohnMiller,andPercyLiang.2015.TraversingKnowledgeGraphs ComputationalLinguistics,74–84.http://aclweb.org/anthology/N13-1008\n",
      "inVectorSpace.InProceedingsofthe2015ConferenceonEmpiricalMethods [15] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng.\n",
      "inNaturalLanguageProcessing.AssociationforComputationalLinguistics, 2013. Reasoning With Neural Tensor Networks for Knowledge Base\n",
      "318–327.https://doi.org/10.18653/v1/D15-1038 Completion. In Advances in Neural Information Processing Systems 26,\n",
      "[8] DiederikKingmaandJimmyBa.2014.Adam:Amethodforstochasticoptimiza- C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Wein-\n",
      "tion.arXivpreprintarXiv:1412.6980(2014). berger(Eds.).CurranAssociates,Inc.,926–934. http://papers.nips.cc/paper/\n",
      "[9] YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,andXuanZhu.2015.Learning 5028-reasoning-with-neural-tensor-networks-for-knowledge-base-completion.\n",
      "EntityandRelationEmbeddingsforKnowledgeGraphCompletion.InProceed- pdf\n",
      "ingsoftheTwenty-NinthAAAIConferenceonArtificialIntelligence(AAAI’15). [16] FabianM.Suchanek,GjergjiKasneci,andGerhardWeikum.2007.Yago:ACore\n",
      "AAAIPress,2181–2187. http://dl.acm.org/citation.cfm?id=2886521.2886624 ofSemanticKnowledge.InProceedingsofthe16thInternationalConference\n",
      "[10] BonanMin,RalphGrishman,LiWan,ChangWang,andDavidGondek.2013. onWorldWideWeb(WWW’07).ACM,NewYork,NY,USA,697–706. https:\n",
      "DistantSupervisionforRelationExtractionwithanIncompleteKnowledgeBase. //doi.org/10.1145/1242572.1242667\n",
      "InProceedingsofthe2013ConferenceoftheNorthAmericanChapterofthe [17] KristinaToutanova,VictoriaLin,Wen-tauYih,HoifungPoon,andChrisQuirk.\n",
      "AssociationforComputationalLinguistics:HumanLanguageTechnologies.As- 2016.CompositionalLearningofEmbeddingsforRelationPathsinKnowledge\n",
      "sociationforComputationalLinguistics,777–782.http://aclweb.org/anthology/ BaseandText.InProceedingsofthe54thAnnualMeetingoftheAssociationfor\n",
      "N13-1095 ComputationalLinguistics(Volume1:LongPapers).AssociationforComputa-\n",
      "[11] ArvindNeelakantan,BenjaminRoth,andAndrewMcCallum.2015.Composi- tionalLinguistics,1434–1444.https://doi.org/10.18653/v1/P16-1136\n",
      "tionalVectorSpaceModelsforKnowledgeBaseCompletion.InProceedings [18] The´oTrouillon,ChristopherRDance,JohannesWelbl,SebastianRiedel,E´ric\n",
      "ofthe53rdAnnualMeetingoftheAssociationforComputationalLinguistics Gaussier,andGuillaumeBouchard.2017. KnowledgeGraphCompletionvia\n",
      "andthe7thInternationalJointConferenceonNaturalLanguageProcessing ComplexTensorFactorization.arXivpreprintarXiv:1702.06879(2017).\n",
      "(Volume1:LongPapers).AssociationforComputationalLinguistics,156–166.\n",
      "https://doi.org/10.3115/v1/P15-1016<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "          93028,     43,  13102,   5706,     25,  35075,  14126,  29356,   9268,\n",
      "          11108,   2168,    367,   2000,    198,     58,     22,     60,  92073,\n",
      "          17198,     84,     11,  13379,  89437,  51526,     47,   3035,     88,\n",
      "             43,  28323,     13,    679,     20,   8404,  22292,    287,  81434,\n",
      "          11461,     82,  93028,     43,  13102,   5706,     11,   5728,   4235,\n",
      "           5833,   7109,   1129,  48836,   2984,   2726,     14,  32329,   2508,\n",
      "          20906,   1032,     12,   1041,     23,    198,    258,   3866,  10115,\n",
      "           5450,  85438,    287,    708,     69,   1820,    679,     20,  92348,\n",
      "            263,  29831,  67966,  18337,    510,    868,     60,  12131,   2100,\n",
      "           9211,     11,  11824,  41287,  25507,     11,  26035,    423,  40623,\n",
      "             11,    323,  13929,  17030,    627,    258,  55381,  14126,  29992,\n",
      "          11108,   2168,    367,   2000,  59122,   1697,     43,  13102,   5706,\n",
      "             11,    220,    679,     18,     13,  27857,    287,   3161,  61577,\n",
      "          27127,  39810,    369,  33025,   5464,    198,  17592,   4235,  13817,\n",
      "             13,   2485,   1129,  48886,   2726,     14,    605,     13,   9714,\n",
      "           4331,   5574,     16,  15302,    868,     12,   6889,     23,  57350,\n",
      "             13,    763,  91958,    304,  61577,   8245,  29225,  15264,    220,\n",
      "           1627,    345,     58,     23,     60,    423,  22970,   1609,  34655,\n",
      "           1764,    438,  86755,  59927,     13,    679,     19,  70218,  56748,\n",
      "           4492,   2000,    267,  67054,  19680,  17528,     12,    356,     13,\n",
      "            622,     13,    356,     13,  12649,   4282,     11,    445,     13,\n",
      "          37330,    283,     11,    386,     13,    468,   6427,     11,   1901,\n",
      "             13,  24855,   1494,   2453,   5676,     11,    323,    735,     13,\n",
      "           1229,     13,  71613,   7058,  28491,  17126,     55,    344,   1762,\n",
      "           1374,    277,     55,    344,     25,   9335,     17,     13,  25169,\n",
      "             15,      7,    679,     19,    570,  10418,   1414,      7,   2782,\n",
      "             82,  36434,  17119,   6713,  30915,    988,     11,  40345,   2637,\n",
      "          26026,   4235,  24347,     13,   1795,   1129,  48393,   1276,   3153,\n",
      "          28912,   4420,   3271,   6018,     58,     24,     60,    816,   1201,\n",
      "           2192,  51697,  51932,   6151,     88,  10602,     43,  19260,  28112,\n",
      "          65966,    647,  31192,  30533,    526,     43,  19260,  51526,     55,\n",
      "          10602,     57,  17156,     13,    679,     20,   1236,  16933,    220,\n",
      "          17824,     23,   5621,   1525,    287,  27281,  41078,   4269,   2442,\n",
      "           3890,  57832,     82,  15548,  12934,  52286,  31113,  11733,  14723,\n",
      "            627,   3106,    438,  34890,  26566,  25624,   2000,  81434,  11461,\n",
      "          34290,   5450,  85438,     12,  13072,    198,    287,    708,     69,\n",
      "           1820,  76896,  11500,  48121,   6157,  15836,  92348,    263,   9470,\n",
      "          16895,   1090,   8677,   4444,   6157,     40,    529,    868,    570,\n",
      "            510,    845,     60,  19797,   1122,     44,    815,   1412,    276,\n",
      "           1247,  38406,     73,   2431,   7910,     42,  66636,  76832,  51526,\n",
      "          66497,  19221,   1687,   1609,    372,     13,   1049,     22,   7659,\n",
      "           6438,     25,   1741,    461,    198,  51207,   3378,    676,     11,\n",
      "          13302,     16,   4235,  13302,     22,     13,   1795,   1129,   8910,\n",
      "          15761,     76,   2726,   2971,   7709,    522,  21796,  20970,     28,\n",
      "          15287,  23181,     16,     13,  15287,  24199,     19,    315,  99031,\n",
      "          81434,   5450,  85438,    287,    708,     69,   1820,    845,    339,\n",
      "          34746,  92348,    198,     58,    605,     60,  13789,    276,   6349,\n",
      "          24412,  31323,   6600,    819,   1543,  31214,     72,     54,    276,\n",
      "             11,   1163,    526,     54,    526,  51526,  23083,     38,  17675,\n",
      "             74,     13,    679,     18,     13,    389,  10343,  62070,   6109,\n",
      "          14358,  19522,    529,   2589,    570,   1741,     44,     11,   3648,\n",
      "         100077,  22812,     56,     11,  25342,     11,  25388,   4235,  22457,\n",
      "             13,   3788,    512,     35,  11451,  10254,    651,   1854,   2000,\n",
      "          34890,    849,  27523,   4291,    276,  97798,  81434,   4066,     13,\n",
      "            443,  48886,   2726,     14,    605,     13,   8011,     20,     14,\n",
      "           8874,  15574,     17,     13,   8874,  15999,     22,    198,    644,\n",
      "          85438,    287,    708,     69,   1820,    679,     18,  92348,   1073,\n",
      "           1820,  26287,  29518,  26072,   1073,   1820,    510,   1114,     60,\n",
      "          27973,   2259,     51,    412,  86563,     11,  82056,  51697,  50640,\n",
      "            268,   2442,   2933,     56,   7141,  44639,     78,    333,   2234,\n",
      "             47,   9186,  51526,  32978,   2232,  14468,    627,  64561,   2000,\n",
      "          59122,   1697,     43,  13102,   5706,     25,  35075,  14126,  29356,\n",
      "           9268,  20855,     12,    220,    679,     21,   3034,    981,   3079,\n",
      "          48567,   1073,  26566,  25624,   2000,  34890,   1858,  16319,  81434,\n",
      "            198,     82,   2168,    367,   2000,  59122,   1697,     43,  13102,\n",
      "           5706,     11,  15831,   4235,  23833,   7109,   1129,  48836,   2984,\n",
      "           2726,     14,  32329,   2508,     14,   5464,    438,   1199,   5450,\n",
      "          85438,    287,    708,     69,   1820,   4370,    339,  81596,  65676,\n",
      "           1073,   1820,  64561,   2000,    198,     45,   1032,     12,   7743,\n",
      "             20,  93028,     43,  13102,   5706,  12692,   4765,     16,     25,\n",
      "           6720,     47,   9724,    570,  64561,   2000,  59122,     64,   7058,\n",
      "             58,    806,     60,   1676,     85,    485,   8989,    301,    587,\n",
      "          67289,   8324,    268,  26312,     49,   8942,  51526,  41598,  26353,\n",
      "           7368,    372,     13,    679,     20,   3034,    981,     72,     12,\n",
      "            259,   4001,     43,  13102,   5706,     11,  10290,     19,   4235,\n",
      "           8929,     19,     13,   2485,   1129,  48886,   2726,     14,    605,\n",
      "             13,   9714,   4331,   5574,     16,  16744,    845,     12,   8190,\n",
      "             21,    198,     83,   4001,   3866,  10115,  17399,   2000,  81434,\n",
      "           4066,  34290,   5450,  85438,    826,    510,    972,     60,    578,\n",
      "          29211,     78,  91635,  43588,     11,  75066,  37790,    685,  59962,\n",
      "           2319,  42256,  84616,   2067,     11,   1542,  37597,     49,   1142,\n",
      "            301,  43225,  29211,   2265,    198,   1073,   1820,   4331,   6634,\n",
      "          81596,  65676,   1073,   1820,  64561,   2000,  59122,   1697,     43,\n",
      "          13102,   5706,  94316,   1291,  51526,  17198,  99112,     33,   3102,\n",
      "            569,     13,    679,     22,     13,  33025,  11461,  34290,  20708,\n",
      "            198,    438,   1820,     22,    339,  34746,  42097,  92348,    263,\n",
      "          55381,  14126,  29992,  22872,  26404,  21316,   2065,  17126,     55,\n",
      "            344,   1762,   1374,    277,     55,    344,     25,   8258,     17,\n",
      "             13,  26661,   4643,      7,    679,     22,   4390,  12692,   4765,\n",
      "             16,     25,   6720,     47,   9724,    570,  64561,   2000,  59122,\n",
      "           1697,     43,  13102,   5706,     11,  10132,   4235,  11247,    627,\n",
      "           2485,   1129,  48886,   2726,     14,    605,     13,  15134,     20,\n",
      "           5574,     16,  16744,    868,     12,   4645,     21, 128009, 128006,\n",
      "            882, 128007,    271,   7184,     11,   2728,    420,   3488,     25,\n",
      "          10699,    527,    279,  12283,    315,    279,   5684,   4710,    220,\n",
      "          21335,   1203,    279,   4320,   1193,    304,    264,  13325,   1160,\n",
      "           3645,     11,    369,   3187,     25,   2570,     32,   1882,     33,\n",
      "           7352,   1442,    499,   1541,    956,   1440,    279,   4320,     11,\n",
      "           1120,    471,    459,   4384,   1160,     13, 128009, 128006,  78191,\n",
      "         128007,    271]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "          93028,     43,  13102,   5706,     25,  35075,  14126,  29356,   9268,\n",
      "          11108,   2168,    367,   2000,    198,     58,     22,     60,  92073,\n",
      "          17198,     84,     11,  13379,  89437,  51526,     47,   3035,     88,\n",
      "             43,  28323,     13,    679,     20,   8404,  22292,    287,  81434,\n",
      "          11461,     82,  93028,     43,  13102,   5706,     11,   5728,   4235,\n",
      "           5833,   7109,   1129,  48836,   2984,   2726,     14,  32329,   2508,\n",
      "          20906,   1032,     12,   1041,     23,    198,    258,   3866,  10115,\n",
      "           5450,  85438,    287,    708,     69,   1820,    679,     20,  92348,\n",
      "            263,  29831,  67966,  18337,    510,    868,     60,  12131,   2100,\n",
      "           9211,     11,  11824,  41287,  25507,     11,  26035,    423,  40623,\n",
      "             11,    323,  13929,  17030,    627,    258,  55381,  14126,  29992,\n",
      "          11108,   2168,    367,   2000,  59122,   1697,     43,  13102,   5706,\n",
      "             11,    220,    679,     18,     13,  27857,    287,   3161,  61577,\n",
      "          27127,  39810,    369,  33025,   5464,    198,  17592,   4235,  13817,\n",
      "             13,   2485,   1129,  48886,   2726,     14,    605,     13,   9714,\n",
      "           4331,   5574,     16,  15302,    868,     12,   6889,     23,  57350,\n",
      "             13,    763,  91958,    304,  61577,   8245,  29225,  15264,    220,\n",
      "           1627,    345,     58,     23,     60,    423,  22970,   1609,  34655,\n",
      "           1764,    438,  86755,  59927,     13,    679,     19,  70218,  56748,\n",
      "           4492,   2000,    267,  67054,  19680,  17528,     12,    356,     13,\n",
      "            622,     13,    356,     13,  12649,   4282,     11,    445,     13,\n",
      "          37330,    283,     11,    386,     13,    468,   6427,     11,   1901,\n",
      "             13,  24855,   1494,   2453,   5676,     11,    323,    735,     13,\n",
      "           1229,     13,  71613,   7058,  28491,  17126,     55,    344,   1762,\n",
      "           1374,    277,     55,    344,     25,   9335,     17,     13,  25169,\n",
      "             15,      7,    679,     19,    570,  10418,   1414,      7,   2782,\n",
      "             82,  36434,  17119,   6713,  30915,    988,     11,  40345,   2637,\n",
      "          26026,   4235,  24347,     13,   1795,   1129,  48393,   1276,   3153,\n",
      "          28912,   4420,   3271,   6018,     58,     24,     60,    816,   1201,\n",
      "           2192,  51697,  51932,   6151,     88,  10602,     43,  19260,  28112,\n",
      "          65966,    647,  31192,  30533,    526,     43,  19260,  51526,     55,\n",
      "          10602,     57,  17156,     13,    679,     20,   1236,  16933,    220,\n",
      "          17824,     23,   5621,   1525,    287,  27281,  41078,   4269,   2442,\n",
      "           3890,  57832,     82,  15548,  12934,  52286,  31113,  11733,  14723,\n",
      "            627,   3106,    438,  34890,  26566,  25624,   2000,  81434,  11461,\n",
      "          34290,   5450,  85438,     12,  13072,    198,    287,    708,     69,\n",
      "           1820,  76896,  11500,  48121,   6157,  15836,  92348,    263,   9470,\n",
      "          16895,   1090,   8677,   4444,   6157,     40,    529,    868,    570,\n",
      "            510,    845,     60,  19797,   1122,     44,    815,   1412,    276,\n",
      "           1247,  38406,     73,   2431,   7910,     42,  66636,  76832,  51526,\n",
      "          66497,  19221,   1687,   1609,    372,     13,   1049,     22,   7659,\n",
      "           6438,     25,   1741,    461,    198,  51207,   3378,    676,     11,\n",
      "          13302,     16,   4235,  13302,     22,     13,   1795,   1129,   8910,\n",
      "          15761,     76,   2726,   2971,   7709,    522,  21796,  20970,     28,\n",
      "          15287,  23181,     16,     13,  15287,  24199,     19,    315,  99031,\n",
      "          81434,   5450,  85438,    287,    708,     69,   1820,    845,    339,\n",
      "          34746,  92348,    198,     58,    605,     60,  13789,    276,   6349,\n",
      "          24412,  31323,   6600,    819,   1543,  31214,     72,     54,    276,\n",
      "             11,   1163,    526,     54,    526,  51526,  23083,     38,  17675,\n",
      "             74,     13,    679,     18,     13,    389,  10343,  62070,   6109,\n",
      "          14358,  19522,    529,   2589,    570,   1741,     44,     11,   3648,\n",
      "         100077,  22812,     56,     11,  25342,     11,  25388,   4235,  22457,\n",
      "             13,   3788,    512,     35,  11451,  10254,    651,   1854,   2000,\n",
      "          34890,    849,  27523,   4291,    276,  97798,  81434,   4066,     13,\n",
      "            443,  48886,   2726,     14,    605,     13,   8011,     20,     14,\n",
      "           8874,  15574,     17,     13,   8874,  15999,     22,    198,    644,\n",
      "          85438,    287,    708,     69,   1820,    679,     18,  92348,   1073,\n",
      "           1820,  26287,  29518,  26072,   1073,   1820,    510,   1114,     60,\n",
      "          27973,   2259,     51,    412,  86563,     11,  82056,  51697,  50640,\n",
      "            268,   2442,   2933,     56,   7141,  44639,     78,    333,   2234,\n",
      "             47,   9186,  51526,  32978,   2232,  14468,    627,  64561,   2000,\n",
      "          59122,   1697,     43,  13102,   5706,     25,  35075,  14126,  29356,\n",
      "           9268,  20855,     12,    220,    679,     21,   3034,    981,   3079,\n",
      "          48567,   1073,  26566,  25624,   2000,  34890,   1858,  16319,  81434,\n",
      "            198,     82,   2168,    367,   2000,  59122,   1697,     43,  13102,\n",
      "           5706,     11,  15831,   4235,  23833,   7109,   1129,  48836,   2984,\n",
      "           2726,     14,  32329,   2508,     14,   5464,    438,   1199,   5450,\n",
      "          85438,    287,    708,     69,   1820,   4370,    339,  81596,  65676,\n",
      "           1073,   1820,  64561,   2000,    198,     45,   1032,     12,   7743,\n",
      "             20,  93028,     43,  13102,   5706,  12692,   4765,     16,     25,\n",
      "           6720,     47,   9724,    570,  64561,   2000,  59122,     64,   7058,\n",
      "             58,    806,     60,   1676,     85,    485,   8989,    301,    587,\n",
      "          67289,   8324,    268,  26312,     49,   8942,  51526,  41598,  26353,\n",
      "           7368,    372,     13,    679,     20,   3034,    981,     72,     12,\n",
      "            259,   4001,     43,  13102,   5706,     11,  10290,     19,   4235,\n",
      "           8929,     19,     13,   2485,   1129,  48886,   2726,     14,    605,\n",
      "             13,   9714,   4331,   5574,     16,  16744,    845,     12,   8190,\n",
      "             21,    198,     83,   4001,   3866,  10115,  17399,   2000,  81434,\n",
      "           4066,  34290,   5450,  85438,    826,    510,    972,     60,    578,\n",
      "          29211,     78,  91635,  43588,     11,  75066,  37790,    685,  59962,\n",
      "           2319,  42256,  84616,   2067,     11,   1542,  37597,     49,   1142,\n",
      "            301,  43225,  29211,   2265,    198,   1073,   1820,   4331,   6634,\n",
      "          81596,  65676,   1073,   1820,  64561,   2000,  59122,   1697,     43,\n",
      "          13102,   5706,  94316,   1291,  51526,  17198,  99112,     33,   3102,\n",
      "            569,     13,    679,     22,     13,  33025,  11461,  34290,  20708,\n",
      "            198,    438,   1820,     22,    339,  34746,  42097,  92348,    263,\n",
      "          55381,  14126,  29992,  22872,  26404,  21316,   2065,  17126,     55,\n",
      "            344,   1762,   1374,    277,     55,    344,     25,   8258,     17,\n",
      "             13,  26661,   4643,      7,    679,     22,   4390,  12692,   4765,\n",
      "             16,     25,   6720,     47,   9724,    570,  64561,   2000,  59122,\n",
      "           1697,     43,  13102,   5706,     11,  10132,   4235,  11247,    627,\n",
      "           2485,   1129,  48886,   2726,     14,    605,     13,  15134,     20,\n",
      "           5574,     16,  16744,    868,     12,   4645,     21, 128009, 128006,\n",
      "            882, 128007,    271,   7184,     11,   2728,    420,   3488,     25,\n",
      "          10699,    527,    279,  12283,    315,    279,   5684,   4710,    220,\n",
      "          21335,   1203,    279,   4320,   1193,    304,    264,  13325,   1160,\n",
      "           3645,     11,    369,   3187,     25,   2570,     32,   1882,     33,\n",
      "           7352,   1442,    499,   1541,    956,   1440,    279,   4320,     11,\n",
      "           1120,    471,    459,   4384,   1160,     13, 128009, 128006,  78191,\n",
      "         128007,    271,    681,  85699,   9799,   4673,     84,    518,    364,\n",
      "          13379,  17472,    518,    364,     47,   3035,     88,  14851,    526,\n",
      "            663, 128009]], device='cuda:0')\n",
      "Decoded output:\n",
      " ['Kelvin Guu', 'John Miller', 'Percy Liang']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Analysis of the Impact of Negative Sampling\n",
      "on Link Prediction in Knowledge Graphs\n",
      "BhushanKotnisandViviNastase\n",
      "InstituteforComputationalLinguistics,\n",
      "UniversityofHeidelberg\n",
      "Heidelberg,Germany\n",
      "{kotnis,nastase}@cl.uni-heidelberg.de\n",
      "ABSTRACT YAGO[25]areexamplesoflargeknowledgegraphsthatcontain\n",
      "Knowledge graphs are large, useful, but incomplete knowledge millionsofentitiesandfacts.Factsarerepresentedastriples,each\n",
      "repositories.Theyencodeknowledgethroughentitiesandrelations consistingoftwoentitiesconnectedbyabinaryrelation,e.g.,(con-\n",
      "whichdefineeachotherthroughtheconnectivestructureofthe cept:city:London,relation:country capital,concept:country:UK).Here\n",
      "graph. Thishasinspiredmethodsforthejointembeddingofen- entitiessuchasLondonandUKarerepresentedasnodesandthe\n",
      "titiesandrelationsincontinuouslow-dimensionalvectorspaces, relationcountry capital isrepresentedasabinarylinkthatcon-\n",
      "thatcanbeusedtoinducenewedgesinthegraph,i.e.,linkpredic- nectsthesenodes.Thesametwonodesmaybeconnectedbymore\n",
      "tioninknowledgegraphs. Learningtheserepresentationsrelies thanonetypeofrelation,makingtheKGamulti-graph.KGshave\n",
      "oncontrastingpositiveinstanceswithnegativeones.Knowledge foundapplicationsinquestionansweringsystems[15],evaluating\n",
      "graphsincludeonlypositiverelationinstances,leavingthedoor trustworthinessofwebcontent[8],andwebsearch[7].\n",
      "openforavarietyofmethodsforselectingnegativeexamples.We AlthoughKGssuchasFreebaseconsistofmillionsofentities\n",
      "presentanempiricalstudyontheimpactofnegativesamplingon andbillionsoffacts, theyarestillincomplete[28]whichlimits\n",
      "thelearnedembeddings,assessedthroughthetaskoflinkpredic- theirapplication. However,itispossibletoinfernew(missing)\n",
      "tion.Weusestate-of-the-artknowledgegraphembeddingmethods factsfromknownfacts.Recently,latentfactormodelsthatcapture\n",
      "–Rescal,TransE,DistMultandComplEX–andevaluateonbench- globalpatternsfromtheKGhavereceivedconsiderableattention.\n",
      "markdatasets–FB15kandWN18.Wecomparewellknownmeth- Theylearnarepresentationofthegraphinacontinuousvector\n",
      "odsfornegativesamplingandproposetwonewembeddingbased spacebyinducingembeddingsthatcapturethegraphstructure.\n",
      "samplingmethods.Wenoteamarkeddifferenceintheimpactof PredictingnewedgestoautomaticallyaddnewfactstoaKG\n",
      "thesesamplingmethodsonthetwodatasets,withthe”traditional” helpsbypassthetextanalysisstageandbootstrapnewknowledge\n",
      "corruptingpositivesmethodleadingtobestresultsonWN18,while basedonwhatisalreadycapturedintheKG.Similartootherprob-\n",
      "embeddingbasedmethodsbenefitFB15k. lemsinprocessingnaturallanguage,suchasparsing,dataconsists\n",
      "(almost)exclusivelyofpositiveinstances.Asolutiontothisissue\n",
      "CCSCONCEPTS isusingimplicitnegativeevidence, wherebyinstancesthathave\n",
      "notbeenobservedareconsiderednegatives,andareusedforcon-\n",
      "•Informationsystems→Questionanswering;Retrievaltasks\n",
      "trastiveestimation[23],wheretheaimistorankobservedinstances\n",
      "andgoals;Informationretrieval;\n",
      "higherthannegative(unobserved)ones.Negativeinstancescanbe\n",
      "generatedusingavarietyofmethods.\n",
      "KEYWORDS\n",
      "Inthisarticlewepresenttheresultsofourinvestigationonthe\n",
      "knowledge graphs, negative sampling, embedding models, link\n",
      "impactofseveralnegativesamplingmethodsonstate-of-the-art\n",
      "prediction\n",
      "knowledgegraphembeddingmodels.Additionallyweproposetwo\n",
      "ACMReferenceformat: negativesamplingstrategiesforfinetuningthemodel.Understand-\n",
      "BhushanKotnisandViviNastase.2018.AnalysisoftheImpactofNegative ing the impact of negative instance sampling will have at least\n",
      "Sampling twouses:providingthebasisforchoosingthenegativesampling\n",
      "onLinkPredictioninKnowledgeGraphs.InProceedingsofWorkshopon methodtobuildthebestmodelforagivenmethod,andallowingus\n",
      "KnowledgeBaseConstruction,ReasoningandMining,LosAngeles,California toplaceintherightcontextresultsreportedintheliteraturethat\n",
      "USA,Feb2018(KBCOM’18),14pages.\n",
      "wereproducedwhileusingdifferentnegativesamplingmethods.\n",
      "DOI:10.1145/nnnnnnn.nnnnnnn\n",
      "2 LINKPREDICTIONINKNOWLEDGE\n",
      "1 INTRODUCTION GRAPHS\n",
      "Muchofhumanknowledgecanbeformalizedintermsofrealworld KnowledgegraphsKG =(E,R)containknowledgeintheformof\n",
      "entities,abstractconcepts,categoriesandtherelationsbetween relationtriples(s,r,t),wheres,t ∈ Eareentities,andr ∈ Risa\n",
      "them. Agraphstructure–aknowledgegraph(KG)–isanatu- relation.Theseknowledgegraphsarenotcomplete,andadditional\n",
      "ral candidate for representing this. NELL [5], Freebase [3] and links(facts)canbeinferred,basedontheideathatsimilarnodes\n",
      "havesimilarrelations–e.g.allcountrieshaveacapitalcity.\n",
      "KBCOM’18,LosAngeles,CaliforniaUSA TheKGcanbeencodedusingdifferentmodelingtechniques,\n",
      "2018.978-x-xxxx-xxxx-x/YY/MM...$15.00\n",
      "DOI:10.1145/nnnnnnn.nnnnnnn whichresultsinencodingsforboththeentitiesandtherelations.\n",
      "8102\n",
      "raM\n",
      "2\n",
      "]IA.sc[\n",
      "2v61860.8071:viXra\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "Avarietyoftechniqueshavebeenproposed[4,14,20,21,24,29]. wherexs, xr, xt areddimensionalvectors,andd(x)iseitherthe\n",
      "ThesemethodslearnamodelfortheprocessedKGasalargeset L1 orL2-normofx. WeuseTransEwithL2-norm. Forlearning\n",
      "ofparameters,inducedbasedonoptimizingalossfunctionwith embeddings,weusemax-marginloss(1).\n",
      "respecttopositiveandnegativeinstancesoflinksrepresenting ComparedtoRescal,TransEhasmuchfewerparameters,but\n",
      "differentrelations.MethodssuchasRescal[21]andNeuralTensor itismorelimitedinthevarietyofrelationsitcanmodel,asthe\n",
      "Networks[24]learnmillionsofparametersthatmakesthemmore translationoperationassumes1:1relations.\n",
      "flexible, enablingthemtomodelwellavarietyofrelations, but\n",
      "atthecostofincreasedcomputationalcomplexityandpotential 2.3 DistMult\n",
      "overfitting.TransE[4],DistMult[29]learnsimplermodels(withfar DistMult [29] is a special case of the Rescal model, where the\n",
      "fewerparameters)andareeasiertotrainbutareunabletomodel relationmatrixisassumedtobediagonal.Thisresultsinasparse\n",
      "certaintypesofrelationssuchasmany-to-one(TransE)andasym- relationmatrixandconsequentlyfewerparameters.Howeverthis\n",
      "metricrelations(DistMult).Recentworksuchas[20]achievethe simplicityresultsinthereductionofmodelingpower.TheDistMult\n",
      "modelingpowerofRescalwithasmallernumberofparameters modelissymmetricandhencecanonlymodelsymmetricrelations.\n",
      "bycompressingthetensorproduct.Complexvaluedembeddings However,DistMultperformswellonFB15Kbenchmarkdataset,\n",
      "(ComplEx)[27]extendtheDistMulttomodelantisymmetricrela- sincethetestdatacontainsonlyafewinstancesofasymmetric\n",
      "tionsbyusingcomplexvaluedembeddings. triples.TheDistMultscoringfunctionisgivenby\n",
      "[12]showedthatmostlatentfactormodelscanbemodifiedto\n",
      "learn from paths rather than individual triples which improves sc(s,r,t)=x sT Diag(Wr)xt\n",
      "performance.RecurrentNeuralNetworksthatlearnpathrepresen- Thiscanalsobewrittenasathreewayinnerproduct\n",
      "tationshavealsobeenusedforlinkprediction[6,18]. Allthese\n",
      "modelsrequirenegativesamplesduringtraining.\n",
      "sc(s,r,t)=(cid:104)xs,xr,xt(cid:105)\n",
      "resW pee ctf to ocu lis nkou prr ea dn ica tl iy os nis ino kn nofo wu lr eds gta et ge r-o apf- ht sh :e C-a or mt pm lEe xth,o Dd iss tMw uit lh t, w Rdh.er Ae s(cid:104)x bs e, fox rr e,x wt e(cid:105) u= se(cid:205) ti hx es mix ar ri gx it ni a lon sd sx (r 1)= foD ri la eg ar(W nir n) ga tn hd ex ses, vx er c, tx ot rs.∈\n",
      "Rescal,TransE.ComplExperformsaswellastheHolographic\n",
      "Embedding(HolE)model,soHolEwasnotincluded1. 2.4 ComplEx\n",
      "TheComplExmodel[27]performssparsetensorfactorizationof\n",
      "2.1 Rescal\n",
      "theKGinthecomplexdomain.Nodesandrelationsaremodeledby\n",
      "TheRescalmodel[21,22]weighstheinteractionofallpairwise ddimensionalvectorswitharealandimaginarypart(Re(x),Im(x)).\n",
      "latentfactorsbetweenthesourceandtargetentityforpredicting ThisallowsComplExtomodelanti-symmetricrelationssincethe\n",
      "a relation. It represents every entity as ad dimensional vector threewaydotproduct(innerproduct)inthecomplexdomainis\n",
      "(x ∈ Rd),andeveryrelationasad ×d matrixW ∈ Rd×d. This notsymmetric. ComplExcanbeseenasDistMultwithcomplex\n",
      "modelrepresentsthetriple(s,r,t)asascoregivenby embeddings.ThescorefunctionofComplExisgivenby:\n",
      "sc(s,r,t)=x sT Wr xt sc(s,r,t)=Re((cid:104)xs,xr,x¯ t(cid:105))\n",
      "Thesevectorsandmatricesarelearnedusingalossfunctionthat =(cid:104)Re(xs),Re(xr),Re(xt)(cid:105)+(cid:104)Im(xs),Re(xr),Im(xt)(cid:105)\n",
      "contraststhescoreofacorrecttripletoincorrectones.Commonly +(cid:104)Re(xs),Im(xr),Im(xt)(cid:105)−(cid:104)Im(xs),Im(xr),Re(xt)(cid:105)\n",
      "usedlossfunctionsincludecross-entropyloss[26],binarynegative\n",
      "[27]trainedComplExwithnegativelog-likelihood.Tomaintainthe\n",
      "loglikelihood[27], andmax-marginloss[12,20]whichweuse\n",
      "sameexperimentalconditionsforassessingtheefficacyofnegative\n",
      "here:\n",
      "sampling,wetrainComplExwithmaxmarginloss(1).\n",
      "N\n",
      "L(θ)=(cid:213) (cid:213) [1−sci +s c(cid:48) i]+ (1)\n",
      "3 NEGATIVESAMPLING\n",
      "i t(cid:48)∈N(t)\n",
      "KnowledgeGraphscaptureknowledgeas<entity,relation,entity>\n",
      "s taci rg= ets sc.( Ssi im,ri il, at ri) tra in pd les sc(cid:48) air= eusc s( es di, wri h,t ei(cid:48) r) e.N th( et) reis lat th ioe nse at no df ti an rc go er tr ae rc et t cr oi npl te as in,w oi nth lye pn oti st ii te is vem ia np sp tae nd ct eo s.n Wod he is l, ea on nd er -e cl la at si son cls at so sie fid cg ae tis o. nK sG os\n",
      "-\n",
      "shared,butthesourceentityisincorrect. lutions have been around for some time [17], for inducing KG\n",
      "embeddings,usingnegativeinstancesleadstobettermodels.\n",
      "2.2 TransE\n",
      "Negativeinstancesarenotmarkedinaknowledgegraph.The\n",
      "TransE[4]interpretsrelationsasatranslationoperationfromthe taskoflinkpredictionhasmuchincommonwithothertasksin\n",
      "sourcetothetargetmediatedbytherelation.Morespecifically,it NLP where (most of) the observed data consists of positive in-\n",
      "embedsatriplespatiallysuchthatthesourcevectorcantravelto stances. [23]proposedcontrastiveestimation,wherebyinstances\n",
      "thetargetvectorthroughtherelationvector,i.e.,xs +xr ≈xt.The thatwereproducedbyperturbingtheobservedones(andthatthem-\n",
      "scoringfunctionsc(s,r,t)forTransEisgivenby selveshavenotbeenobserved)willserveasnegativeinstances,and\n",
      "theaimistorankobservedinstanceshigherthantheunobserved\n",
      "sc(s,r,t)=−d(xs +xr −xt)\n",
      "(”negative”)ones. Inneuralprobabilisticlanguagemodels,nega-\n",
      "tivesamplingwasfirstproposedin[1]asimportancesampling.A\n",
      "1AndalsobecauseHolEisverysimilartoComplEx.Thiswasverifiedthroughpersonal\n",
      "correspondencewithanauthoroftheComplExpaper. samplingsolutionthatwasmorestablethanimportancesampling\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA\n",
      "wasintroducedby[16],whobuiltuponthenoise-contrastiveesti- produced,wesupplementthissetwithrandomlyproducednegative\n",
      "mation[10].Intheseapproachesnegativesamplesaredrawnfrom samples.\n",
      "anon-parametricnoisedistribution.\n",
      "Forknowledgegraphsinparticulartherearemanydifferent 3.3 TypedSampling: T\n",
      "waystoproducenegativeinstancesbasedonthegraphstructure.\n",
      "KnowledgegraphssuchasFreeBaseandNELL[5]havestrongly\n",
      "Wepresentanoverviewoftechniquesforproducingnegativein-\n",
      "typed relations. For example, a relationborn in holds between\n",
      "stancesfromaknowledgegraph,andweevaluatetheirimpacton\n",
      "entitiesoftypepersonandentitiesoftypecity.Relevantnegative\n",
      "knowledgegraphcompletion,orlinkprediction.\n",
      "candidates(sourcesortargets)canbeminedbyconstrainingthe\n",
      "entitiestobelongtothesametypeasthatofthesource(ortarget).\n",
      "3.1 Randomsampling: R Thiscanhelpbypasstheproblemmentionedforthecorruptmethod,\n",
      "whensomerelationsinthedatasethaveveryfewinstances.\n",
      "The simplest form of sampling negative instances is to assume\n",
      "Foreveryrelationr :S →T,\n",
      "aclosedworldhypothesisandconsideranytriplethatdoesnot\n",
      "appearintheKGasanegativeinstance.Let\n",
      "ifSr,t ={s|shastypeSt}andTr,t ={t|t hastypeTt},\n",
      "K =K+={(si,ri,ti)|yi =1;i =1,2,···,N} w ini st th anS ct ea sn wd ilR lt coth ne sisd to om fa trin ipa len sdrangerespectivelyofr,negative\n",
      "denotethecompleteknowledgegraph,whereyi = 1represents\n",
      "(s(cid:48),r,t),s(cid:48)∈Sand(s,r,t(cid:48)),t(cid:48)∈T,\n",
      "thepresenceofatriple(si,ri,ti)(apositiveinstance)andyi =0\n",
      "suchthat\n",
      "representsabsence.Accordingtotheclosedworldassumption,the (s(cid:48),r,t)(cid:60)Rand(s,r,t(cid:48))(cid:60)K+.\n",
      "set Ko −fn =eg {a (st iiv,re is,K ti− )|yis i g =iv 0e ;n i =by 1,2,···,N} We Ifth anen ensa tim typ hle asn ms n ou rem thbe ar no of nn ee tg ya pt eiv (ee.gsa.m Alp bl ee rs tf Ero inm steth ines he astr ti yp ple es s.\n",
      "SincetheKGisincompletethissetcontainspositivetriplesnot\n",
      "presentintheKG.Furthermorethissetmightbeverylargebecause\n",
      "person,scientist),weincludeitinSr,t (orTr,t)ifoneofitstypes\n",
      "theincorrectfacts(O(N2))faroutnumberthecorrectones. m froa mtch Fe rs eeS bt a( so er rT et l) a. tW ioneo mb et ta ain dac ta ate reg lo er ay sed dat ia nf [o 9r ],th ae ndFr te he eb ea nse tid tyat ta ys pe et\n",
      "Asimplesolutiontothescalabilityproblemisrandomlysam-\n",
      "bymappingtheFreebaseentityidentifiertotheFreebasecategory.\n",
      "plingasmallnumberofsamplesfromK−.Givenapositivetriple\n",
      "This results in 101,353 instances of the category relation which\n",
      "(s,r,t)wegeneratens negativetriplesbysamplingns targeten-\n",
      "isusedinthetrainingstagetoproducetypednegativesamples.\n",
      "titiesfromtheentitysetE.Sincethesamplingisrandom,wedo\n",
      "DomainandrangetypesforFreebaserelationsareprovidedby\n",
      "notcheckwhetherthesampledtriplesarepresentinthetrainand\n",
      "Freebaseitself.Afewexamplesofentitiesandtypesareincluded\n",
      "developmentset,becausetheprobabilitytheyarepresentinK+is\n",
      "inTable1.\n",
      "negligible.Thesameprocedureisusedtogeneratenegativesource\n",
      "WedonotusetypedsamplingforWordnet.Thehypernym/hyponym\n",
      "entities.\n",
      "relationsarethedefactotyperelationsinWordNet,butarehier-\n",
      "Thenegativesproducedbyrandomsamplingmaynotbevery\n",
      "archicalratherthanamappingontoagivensmallsetofpredeter-\n",
      "useful: for the positive triple (Tom Cruise, starred in, Top Gun), minedtypesasinFreebase.\n",
      "negativetargetssuchasLondonorMount Everestseemirrelevant.\n",
      "Relevantnegativetargetsshouldincludeentitiesthataremovies, 3.4 RelationalSampling: REL\n",
      "suchasTerminator,Inception.Toobtainsuchnegativesitisneces-\n",
      "Althoughtypedorcorruptrelationsamplingcangeneraterelevant\n",
      "sarytoconstrainthesetofentitiesfromwhichsamplesaredrawn.\n",
      "negativecandidates,duetotheincompletenessoftheKG,someof\n",
      "Weexploresuchconstraintsinthefollowingsections.\n",
      "thesecandidatescouldbeunknownpositives. Ifweassumethat\n",
      "sourcetargetpairsparticipateinonlyonerelation,thensampling\n",
      "3.2 Corruptingpositiveinstances: C targets(sources)thatareconnectedtothecurrentsource(target)\n",
      "throughrelationsotherthanthecurrentrelationcanyieldtrue\n",
      "Weuseamethoddescribedin[24]thatgeneratesnegativeinstances\n",
      "negatives.Thisisacommonprocedureinmulti-classlearning.\n",
      "bycorruptingpositiveinstances:foreveryrelationr,Socheretal.\n",
      "[24]collectthesets\n",
      "Moreformally,forpositivetriple(s,r,t)thenegativecandidate\n",
      "S\n",
      "={s|(s,r,∗)∈K+}andT ={t|(∗,r,t)∈K+}, sourcesetisS− = {s|(s,r(cid:48),t(cid:48)), ∀r(cid:48) ∈ R,r(cid:48) (cid:44)r}andtargetset\n",
      "andproducesetsofcorruptedtriples T−={t|(s(cid:48),r(cid:48),t), ∀r(cid:48) ∈ R,r(cid:48)(cid:44)r}.Asbefore,aftercomputingS\n",
      "S(cid:48)={(s(cid:48),r,t)|s(cid:48)∈S,(s(cid:48),r,t)(cid:60)K+}and andTwefilteroutpositivetriplesfromtrainanddevelopmentset\n",
      "T(cid:48)={(s,r,t(cid:48))|t(cid:48)∈T,(s,r,t(cid:48))(cid:60)K+}. andsampleanumberns ofnegativesamples.\n",
      "DuringtrainingK+consistsoftriplesfromtraininganddevelop-\n",
      "3.5 NearestNeighborsampling: NN\n",
      "mentset.Wesampleanumberns ofnegativesamplesfromS(cid:48)and\n",
      "T(cid:48).Suchamethodproducesnegativeinstancesthatarecloserto Mostnegativesamplingmethodsgeneratenegativesamplesbased\n",
      "thepositiveonesthanthoseproducedthroughrandomsampling. oneithertheclosedworldassumption,functionalconstraintssuch\n",
      "Anissuewiththismethodisthatforrelationswithveryfew astypeconstraints,andtripleperturbation[19]. Weintroducea\n",
      "positiveinstances,therewillnotbealargeenoughpoolofsource negativesamplingmethodwhichusesapre-trainedembedding\n",
      "andtargetcandidatestocorruptthepositiveinstances. Thedata modelforgeneratingnegativesamples.Wenamethispre-trained\n",
      "analysis shows that this is an issue for the FB15k dataset. For embeddingmodelthe‘negativesamplingmodel’.Weusethenega-\n",
      "relationswherenotenoughcorruptednegativeinstancescanbe tivesamplingmodeltogeneratenegativetargets(sources)thatare\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "SourceType Source Relation Target TargetType\n",
      "film star wars episode IV produced by дeorдe lucas film producer\n",
      "person alexandre dumas people profession writer profession\n",
      "academic post professor profession people albert einstein award winner\n",
      "Table 1: Entity Types in Freebase: Examples of source and target entity types from Freebase used for generating negative\n",
      "samples.\n",
      "closetothepositivetarget(source)invectorspace.Thiswouldhelp neighborsampler,weusethenegativesamplingmodelforobtaining\n",
      "themodellearntodiscriminatebetweenpositivesandnegatives thepredictedvectorandentityembeddings.Thenegativesampling\n",
      "verysimilartothepositives. modelisnotupdated.\n",
      "Forapositivetriple(s,r,t),withxt thevectorrepresentationof Givenapositivetriple(s,r,t)weobtainthepredictedvector\n",
      "t obtainedfromthenegativesamplingmodel,thesetofnegative vt = x sT Wr wherexs, Wr are entity and relation embeddings\n",
      "samplesarethetopnsnearestneighborsofxt (thatarenotpositive) ofsources andrelationr obtainedusingthenegativesampling\n",
      "obtainedfromthenegativesamplingmodel.Thenegativesampling model. Notethatvt maynotbethesameasxt,thetargetentity\n",
      "modelmaybedifferentthanthemodelthatisbeingtrained.We representation.Thesetof(target)negativesamplesarethetopns\n",
      "usetheRescalmodeltrainedwith100typed(T)negativesamples nearestneighborsofthepredictedvectorvt.Algorithm2describes\n",
      "asanegativesamplingmodelfortheFB15Kdataset. Notethat theprocedureforasingletriple,inpracticeweuseabatchandthe\n",
      "theRescalmodelparametersarefrozen(notupdated),itissimply BallTreeisbuiltonlyonce.\n",
      "usedforgeneratingnegativesthatareusedfortraininganother\n",
      "model.Algorithm1describestheprocedureforasingletriple.In\n",
      "Algorithm2:NearMissSamplingusingRescalnegativesam-\n",
      "practiceweuseabatchoftriplesandthenearestneighborsearch\n",
      "pler\n",
      "isperformedusingtheBallTreealgorithmwhichisbuiltonlyonce\n",
      "sincethenegativesamplingmodelisnotupdated.\n",
      "Input :Triple(s,r,t),EntitySetE,Positivesourceandtargets\n",
      "Ps andPt,NegativeSamplingEmbeddingModelfn,\n",
      "Algorithm1:Algorithm1NearestNeighborSampling\n",
      "Numberofnegativesamplesns\n",
      "Output:Setofns negativesamples\n",
      "Input :Triple(s,r,t),EntitySetE,Positivesourceandtargets Ns ←E\\Ps, Nt ←E\\Pt;\n",
      "P Ns ua mn bd eP rt o, fN ne eg ga at ti iv ve eS sa am mp pl li en sg nE smbeddingModelfn, X Inns iti← alizf e(N ths e), KX bnt a← lltrf e( eN wt i) th;\n",
      "Xs andXt ;\n",
      "XO IN nns su itt i← ←p alu izEt f e: (\\S N tP he s st e, )o, KN Xf t bn nt as ← ← lln te E rg f e\\a ( ePt Ni wtv t;e i) th;sa Xm spl ae ns\n",
      "dXt ;\n",
      "v Sxs s ←← ← nx ef sT an r( W es s) r t,,x nvt et i← g← hbWf on rr( sr (x v), t sW,; nr um←n =nf sn );(r)n ;\n",
      "xt ← fn(t);\n",
      "n n T re← turn nea Sr,Test neighbors(vt,num=ns);\n",
      "xs ← fn(s);\n",
      "S ←nearest neighbors(xs,num=ns);\n",
      "T ←nearest neighbors(xt,num=ns); Likenearestneighborsampling,nearmisssamplingisalsocom-\n",
      "returnS,T putationallyexpensive,soinsteadoflearningfromrandomlyini-\n",
      "tializedparameterswetuneapre-trainedmodelfor5epochs.\n",
      "Nearestneighborsamplingiscomputationallyexpensivecom- 4 DATA\n",
      "paredtothemethodsdiscussedinprevioussections.Thisisbecause\n",
      "WeevaluatetheimpactofnegativesamplingontheFreebasedataset\n",
      "asearchoverallentitiesneedstobeperformedforsourceandtarget\n",
      "(FB15k)andontheWordNetdataset(WN18)introducedby[4].\n",
      "entitiesforeverytriple. Thereforeweuseamodeltrainedusing\n",
      "Theyareverydifferentincoverage–FB15kcontainsmostlynamed\n",
      "typednegativesamplingmethodsforFreebaseandcorruptedsam-\n",
      "entitiesconnectedthroughstronglytypedrelations,whileWN18\n",
      "plingforWordnettoinitializetheparametersandthenfinetune\n",
      "contains mostly common nouns connected through lexical and\n",
      "themodelusingnearestneighborsamplingfor5epochs.\n",
      "semanticrelations.DatasetdetailsareincludedinTable2.\n",
      "3.6 NearMisssampling: nmiss\n",
      "4.1 FB15k\n",
      "Thenearestneighborsamplergeneratesnegativesthataresimilar\n",
      "FB15k[4]consistsofapproximately15,000entitiesand1345rela-\n",
      "topositivesinvectorspace.Someofthosenegativesmayberanked\n",
      "tions.Weusethesplitsuppliedbythedataset:483,142train,50,000\n",
      "higherthanthepositives.Exposingsuchhighlyrankednegatives\n",
      "validationand59,071positivetestinstances.\n",
      "totheclassifiercanhelpthemodellearnabetterdiscriminator.We\n",
      "namethissettingasnearmisssampling,becausethegenerated\n",
      "negativesaretoprankedcandidateswhichmakesitdifficultfor Dataset |E| |R| Training Development Test\n",
      "themodeltoclassifythemasnegatives(nearmisses).Togenerate FB15K 14,951 1345 483,142 50000 59071\n",
      "highlyrankednegatives, wecollectthetopns targets(sources) WN18 40,943 18 141,442 5000 5000\n",
      "closest to the predicted target (source) vector. Like the nearest Table 2: Dataset Details: |E| = # of entities, |R| = # of rela-\n",
      "tions.\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA\n",
      "Figure1:FB15kdatasetfrequencystatistics Figure2:WordNet18datasetfrequencystatistics\n",
      "Thetrainingdatacontainsrelationsthathavehighvariation Fromagraphstructurepointofview,WN18nodeshavelow\n",
      "inthenumberofinstances–39%oftherelationshaveatmost connectivity–theaveragedegreeontheentiredatasetisapprox-\n",
      "10instances,whilethemostfrequentrelation2hasalmost16000. imately 1.2, and on the training data alone approximately 3.45.\n",
      "Thisdisparityisalsoreflectedinthedistributionofnodedegrees Thistranslatesintosparseradjacencymatricesforfactorization,\n",
      "– 12% of the entities have degree equal or less than 10 (appear comparedtoFreebase.\n",
      "inatmost10instances). TheaveragedegreeofanodeinFB15k WordNetcontainslexicalandsemanticrelations. Lexicalrela-\n",
      "isapproximately13.2overall,and32.4onthetrainingdata. The tions–suchasderivationally related formconnectlemmasfrom\n",
      "distributionofrelationsandnodedegreesispresentedinFigure1. differentpartsofspeechthataremorphologicallyconnected.The\n",
      "ThetypeofrelationsincludedinFreebaseconnectnamedentities. semanticrelationscoveris arelations(hypernym/hyponym,in-\n",
      "They are extrinsic relations, in that they do not hold based on stancehypernym/hyponym),threetypesofpart ofrelations(mem-\n",
      "theintrinsicpropertiesoftheconnectedentities,butaredueto ber,substanceandpart). ThesemanticrelationsinWordNetare\n",
      "externalcircumstances.Forexample,thepeople professionrelation intrinsic,astheyreflectorarisefromintrinsicpropertiesofthe\n",
      "connecting people and their professions are not determined by connectedentities.Forexample,acatis aanimal,andcathas part\n",
      "intrinsicpropertiesofpeopleandprofessions.RelationsinFreeBase pawsnotbecauseofexternalcircumstances,butbecauseofwhata\n",
      "arestronglytyped–thedomainandrangeoftherelationsaretypes, catis.ComparedtoFreeBase,WordNetrelationsarenottyped–\n",
      "e.g.thecountry capitalrelationconnectscountriesandcities. thereisnocleardomainandrangefortheWordNetrelations.\n",
      "4.2 WN18 5 EXPERIMENTS\n",
      "ThisdatasetconsistsofasubsetofrelationsfromtheWordNetlexi- 5.1 Implementation\n",
      "caldatabase3,splitintotraining,developmentandtesting:141442/\n",
      "ForfaircomparisonwereimplementedRescal,TransE,DistMult,\n",
      "5000/5000. Thereare18relations. Thereislessvariationinthe\n",
      "ComplExusingPyTorch,andtestedthemusingthesameexperi-\n",
      "numberofinstancesperrelationcomparedtotheFB15k,ascanbe\n",
      "mentalsetting:sameloss(max-marginloss),embeddingsize(100),\n",
      "seeninFigure2.Thereisonerelationwithlessthan100instances\n",
      "anddata.WeusetheAdam[13]SGDoptimizerfortrainingbecause\n",
      "(similar to),whilethemostfrequentrelations(hypernym,hyponym)\n",
      "itaddressestheproblemofdecreasinglearningrateinAdaGrad.\n",
      "haveapproximately35,000.\n",
      "Weensurethatentityembeddingsforallthemodelshaveunitnorm.\n",
      "2/award/awardnominee/awardnominations./award/awardnomination/awardnominee\n",
      "Weperformedexhaustiverandomizedgridsearch[2]fortheL2\n",
      "3https://wordnet.princeton.edu/ regularizeronthevalidationsetforallmodelsandwetunedthe\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "Model lr λ 5.4 Results\n",
      "Freebase WepresenttheresultsoflinkpredictiononFB15kandWN18in\n",
      "ComplEx 0.001 1.31E-06 termsofMRRinFigures3and4forns ∈{1,2,5,10,20,50,100}for\n",
      "DistMult 0.001 4.93E-06 eachpositiveinstance.\n",
      "Rescal 0.001 0.0002084 Theresultsshowthatthedifferentsamplingmethodshavedif-\n",
      "TransE 0.001 0.00024036 ferenteffectsonthetwodatasets. Sincelinkpredictionisbased\n",
      "Wordnet exclusivelyontheembeddingofthegraphs,differencesinperfor-\n",
      "ComplEx(ns ∈{1,2,5}) 0.005 2.82E-05 mancearecausedbythedifferentstructure(e.g. differentnode\n",
      "ComplEx(ns >=10) 0.01 2.82E-05 degreeswhicharereflectedinthesparsityoftherelationadjacency\n",
      "DistMult(ns ∈{1,2,5})<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,    972,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['FB15k', 'WN18']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Figures3and4forns ∈{1,2,5,10,20,50,100}for\n",
      "DistMult 0.001 4.93E-06 eachpositiveinstance.\n",
      "Rescal 0.001 0.0002084 Theresultsshowthatthedifferentsamplingmethodshavedif-\n",
      "TransE 0.001 0.00024036 ferenteffectsonthetwodatasets. Sincelinkpredictionisbased\n",
      "Wordnet exclusivelyontheembeddingofthegraphs,differencesinperfor-\n",
      "ComplEx(ns ∈{1,2,5}) 0.005 2.82E-05 mancearecausedbythedifferentstructure(e.g. differentnode\n",
      "ComplEx(ns >=10) 0.01 2.82E-05 degreeswhicharereflectedinthesparsityoftherelationadjacency\n",
      "DistMult(ns ∈{1,2,5}) 0.005 3.12E-06 matrices) and the different nature of the relations – typed and\n",
      "DistMult(ns >=10) 0.01 3.12E-06 extrinsicinFB15k,nottypedand(mostly)intrinsicinWordNet.\n",
      "Rescal(ns ∈{1,2,5}) 0.005 7.48E-05 Assuggestedbyworkonlearningstatisticalmodelsthrough\n",
      "Rescal(ns >=10) 0.01 7.48E-05 noisecontrastiveestimation[11], selectingdifficultnegativein-\n",
      "TransE(ns ∈{1,2,5}) 0.005 0.0001863777692 stancesproducesbettermodels:nearmisssamplingleadstobetter\n",
      "TransE(ns >=10) 0.01 0.0001863777692 resultsonFB15kformostembeddingsmethods. Thereasonem-\n",
      "Table3:Parametervalues beddingbasedsamplingworkswellonFreeBaseisprimarilybe-\n",
      "causethenegativesamplesgeneratedbythepre-trainedembedding\n",
      "modelareveryclosetothediscriminatorboundary.Forexample,\n",
      "trainingdurationusingearlystopping.Thelearningrate(lr)and\n",
      "thenearmisssamplinginvolvesgeneratingnegativetargetentities\n",
      "λ(theL2 normcoefficient)arepresentedinTable3. Thecodeis\n",
      "thatarehighlyrankedbytheembeddingmodel. Theseentities\n",
      "availableinGithub4.\n",
      "arelikelytobehighlyrankedbythemodelthatisbeingtrained.\n",
      "ThedifferentmethodsfornegativesamplingdescribedinSection\n",
      "Thereforeprovidingtheseentitiesasnegativesallowsthesystem\n",
      "3wereusedtoproducenegativeinstancesfortraining.InFB15K\n",
      "tolearnamodelthatranksthembelowthepositivetargetusing\n",
      "somerelationsdonothaveenoughsourcesortargetstogenerate\n",
      "themax-marginloss.Notethatthesamplesgeneratedbytheem-\n",
      "negativetriplesbycorruptingpositivetriples. Ifthenumberof\n",
      "beddingmodelareclosetoeachotherinvectorspaceduetothe\n",
      "generatedtriplesarelessthantherequired(ns),wecompletethe\n",
      "abilityoftheembeddingmodeltoclusterentities.Thereforealmost\n",
      "setofnegativesampleswithrandomlygeneratedtriples.\n",
      "allthegeneratednegativesamplesareclosetothediscriminator\n",
      "Forthenearestneighborandnearmisssettings,weusedthe\n",
      "boundary. Wetreatedthenegativesamplingmodel(pre-trained\n",
      "bestperformingmodelforinitializingtheparameters,andusedthe\n",
      "model)asahyperparameter. WefoundthattheRESCALmodel\n",
      "Rescalmodeltunedontypednegativesamples(100negativesam-\n",
      "workedbest.Wespeculatethatthismightbeduetothesuperior\n",
      "ples)asthenegativesamplingmodelforFB15KandRescaltrained\n",
      "abilityofRESCALmodeltoclustersimilarentities.\n",
      "bycorruptingpositivesamples(100negativesamples)forWN18.\n",
      "Corruptingpositiveinstances,themethodmostfrequentlyused\n",
      "forlinkprediction,istheleastcompetitiveonFB15k,butfitsWord-\n",
      "5.2 Testdata Netwell,particularlyforRescal. DistMultisnotverysensitive\n",
      "The test data is the same across all experiments. The negative tothetypeofnegativesamplingonWN18,exceptforthenearest\n",
      "instancesfor thetestdataweregeneratedasdescribedin[4]– neighbormethodwithwhichitdoesnotperformwell.\n",
      "corruptingpositiveinstancesusingallentitiesofthedictionary Tounderstandwhycorruptingpositiveinstancesworksbeston\n",
      "insteadofthecorrectsourceandtarget,withoutsampling. WordNet,welookatthedataandthegraphstatistics.TheWN18\n",
      "Alsofollowingtheprocedureof[4],weusethefilteredsetting: datasethas18relationswhilewithFB15khasabout1495relations.\n",
      "thenegativesamplesaddedtothetrainingdataarefilteredwith DuetoperrelationdatasparsityinFB15K,seeFig.1and2,negative\n",
      "respecttothetestdatatoavoid(known)falsenegativesintraining. samplingusingcorruptedtriplesworkspoorlyforFB15K,asitoften\n",
      "hastofallbackonrandomsamplingwhennotenoughpositive\n",
      "5.3 Evaluationmetrics instanceswithasharedsource/targetareavailablefor”corruption”.\n",
      "Corruptsamplingworksbetterinaninstancerichenvironment.\n",
      "Forevaluationweusethemeanreciprocalrank(MRR)andhits@K\n",
      "Apartfromdatasparsity,thenatureofWordNetandFreebase\n",
      "thatarecommonlyusedforlinkprediction.\n",
      "relationsmayalsoaffecttheperformanceofnegativesampling\n",
      "ForalistofNanswersforlinkprediction,themeanreciprocal\n",
      "methods.WordNetrelationshaveopenendedrangesanddomains\n",
      "rank(MRR)andhits@karedefinedas:\n",
      "whileFreebaserelationshavetypedrangesanddomains.Embed-\n",
      "dingbasedmethods,suchasthenearmisssamplingmethodwe\n",
      "MRR= 1 (cid:205)N 1 hits@K = |{i|ranki<K}| implemented,workonthebasisofclusteringsimilarentities,and\n",
      "N i=1ranki N donotfunctionwellforWordNetwheretherelationsdonothave\n",
      "whereranki istherankofthepositiveinstanceipredictedbythe\n",
      "domainsandrangesthatreflectconceptual/semanticclusters.\n",
      "modelwithrespecttothenegativesamples. ForFB15kweuse\n",
      "Wehavediscussedthedifferencesinperformanceofsampling\n",
      "hits@10,forWN18,hits@1.\n",
      "methodsforthetwoKGsused.Therearealsodifferenceswithre-\n",
      "specttothelinkpredictionmethods.Randomsamplingworksbest\n",
      "forTransE.Thismaybesurprisingatfirst,butisunderstandable\n",
      "4https://github.com/bhushank/kge-rl\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA\n",
      "complex MRR distmult MRR rescal MRR transE MRR\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "complex hits@10 distmult hits@10 rescal hits@10 transE hits@10\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0 100 101 102 100 101 102 100 101 102 100 101 102\n",
      "corrupt nmiss nn random relational typed\n",
      "Figure3:LinkpredictiononFB15k,evaluatedintermsofMRRforns ∈{1,2,5,10,20,50,100}onalogarithmicscale.\n",
      "complex MRR distmult MRR rescal MRR transE MRR\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "complex hits@1 distmult hits@1 rescal hits@1 transE hits@1\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0 100 101 102 100 101 102 100 101 102 100 101 102\n",
      "corrupt nmiss nn random relational\n",
      "Figure4:LinkpredictiononWN18,evaluatedintermsofMRRforns ∈{1,2,5,10,20,50,100}onalogarithmicscale.\n",
      "consideringthatthetheoreticalmodelbehindTransEassumes1:1 Yangetal.[29] Negativesampling\n",
      "relations.Providingitwithnegativeentitiesthatareclose(using MRR HITS@10 neg.sampling MRR HITS@10\n",
      "typed,corruptedorembeddingmethods)doesnotresultinimprove- FB15k\n",
      "mentbecausethenegativeentitiesgeneratedusingtyped,corrupt DistMult 0.35 57.7 nearmiss 0.46 70.64\n",
      "orembeddingsareclosetoeachotherinvectorspaceandthemodel Rescal 0.31 51.9 nearmiss 0.42 64.34\n",
      "willultimatelybeunabletodistinguishbetweenthem.Thisisnot TransE 0.32 53.9 nearmiss 0.37 62.97\n",
      "thecasewhendoingrandomsampling,whenTransEisnotper- WN18\n",
      "turbedbytooclosenegatives.ComplExandDistMultperformwell DistMult 0.83 94.2 corrupt 0.82 94.06\n",
      "withbothnearmissandnearestneighboursamplingonFB15k. Rescal 0.89 92.8 corrupt 0.92 93.91\n",
      "Rescalperformsbestwithnearmisssamplingonthisdata,and TransE 0.38 90.9 corrupt 0.40 86.98\n",
      "withcorruptingpositivesamplesforWordNet.Formiddle-range Table4:SotAresultsusingamax-marginlossfunctionand\n",
      "ns relationalsamplingperformsbest. corruptingpositiveinstancesvs. thebestperformingnega-\n",
      "AsdescribedinSection4,thetrainingdataforbothmethods tivesampling.\n",
      "variesquiteabitintermsofthefrequencyoftherelationscovered.\n",
      "Freebaseismoreextreme,inthatapproximately39%oftherelations\n",
      "haveatmost10positiveinstancestotrainon. Weanalyzedthe entityrepresentations,wecannotethattheperformanceonlink\n",
      "effectsofnegativesamplingondifferentslicesofthedata,splitby predictionfortheserelationswithveryfewinstancesvariesmuch\n",
      "theorderofmagnitude(oom)ofthefrequencyoftherelationsin withthenegativesamplingmethod.Overall,thebestresultsareob-\n",
      "thetrainingdata.Moreprecisely,wegrouprelationsintosetsGn tainedwiththesamesamplingmethodasfortheirmorepopulous\n",
      "indexedbytheorderofmagnituden: counterparts,butforspecificrangesofthenumberofgenerated\n",
      "Gn ={r|10n < freq(n,trainingdata)<=10(n+1)}5. negative samples other methods would work best (e.g. nearest\n",
      "Freebasehas5slices(0..4)andWordNet4(1..4).Theresults(as neighborandrelationalsamplingforWordNetdata).\n",
      "MRRandhits@K)forslicesrepresentingrelationswithOOM2 Thereportedexperimentswereperformedusingthemaxmargin\n",
      "ormorecloselymirrortheoverallresults.Theresultsforthelow lossfunction.InTable4weincludethestateoftheartresultson\n",
      "frequencyrelationsareshowninFigures5and6.Thehits@Kscore DistMult, Rescal andTransEobtainedwith amax marginloss\n",
      "aresimilartotheMRRones,sowedonotincludethem6. functionreportedin[29]andcorruptingtripes,tocomparewith\n",
      "Whiletheresultsonthelowfrequencyrelationscannotbeana- theresultsobtainedwiththebestnegativesamplingmethodforthe\n",
      "lyzedseparatelyfromtheotherrelationsbecausetheembeddings dataset.Slightdifferencesinthelearningrateandλaccountforthe\n",
      "processreliesonprocessingandinducingjointlyallrelationand differencesinperformancewhenusingcorruptpositiveinstances\n",
      "asnegativesamplesfortheWN18dataset.\n",
      "Recently, [27] used the log-likelihood objective, which leads\n",
      "5WeincluderelationsthathaveonlyoneinstanceinG0.\n",
      "6Thecompletesetofplotsaccompaniesthecodeandwillbeshared. toimprovementsoverthepublishedresultsforthemethodsthey\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "corrupt nn relational corrupt nn relational\n",
      "nmiss random typed nmiss random\n",
      "complex complex complex complex\n",
      "0.60 0.60\n",
      "0.55 0.55 1.0 1.0\n",
      "0.50 0.50\n",
      "0.8 0.8\n",
      "0.45 0.45\n",
      "0.40 0.40 0.6 0.6\n",
      "0.35 0.35\n",
      "0.4 0.4\n",
      "0.30 0.30\n",
      "0.25 0.25 0.2 0.2\n",
      "0.20 0.20\n",
      "distmult distmult distmult distmult\n",
      "0.6 0.6 1.1 1.1\n",
      "0.5 0.5 1.0 1.0\n",
      "0.4 0.4 0.9 0.9\n",
      "0.3 0.3 0.8 0.8\n",
      "0.2 0.2 0.7 0.7\n",
      "0.1 0.1 0.6 0.6\n",
      "rescal rescal rescal rescal\n",
      "0.6 0.6\n",
      "1.0 1.0\n",
      "0.5 0.5\n",
      "0.8 0.8\n",
      "0.4 0.4\n",
      "0.6 0.6\n",
      "0.3 0.3\n",
      "0.4 0.4\n",
      "0.2 0.2\n",
      "0.2 0.2\n",
      "0.1 0.1\n",
      "transE transE transE transE\n",
      "0.6 0.6 0.6 0.6\n",
      "0.5 0.5 0.5 0.5\n",
      "0.4 0.4 0.4 0.4\n",
      "0.3 0.3 0.3 0.3\n",
      "0.2 0.2 0.2 0.2\n",
      "0.1 0.1 0.1 0.1\n",
      "0.0 0.0 0.0 0.0\n",
      "100 101 102 100 101 102 100 101 102 100 101 102\n",
      "Figure5: ResultsonrelationswithOOM0and1inFB15k Figure6: ResultsonrelationswithOOM1and2inWN18\n",
      "(MRRs) (MRRs)\n",
      "compared(TransE,ComplEx,HolE,DistMult).Weplantoanalyze\n",
      "sampling worked best for Freebase withmost of the graphem-\n",
      "thenegativesamplingmethodswhileusingthisnewlossfunction.\n",
      "beddingmethods,whilecorruptingpositivetriplesleadstobest\n",
      "resultsonWordNet. Thenewlyproposednearmissandnearest\n",
      "6 CONCLUSION\n",
      "neighbornegativesamplingworkbestforFreebase,forthreeoutof\n",
      "Wereportananalysisoftheimpactofsixnegativesamplingmeth- thefourgraphembeddingsmethods.Fromanalysisofdatasets,we\n",
      "odsontheperformanceoflinkpredictioninknowledgegraphs,for furtherconcludedthatembeddingbasednegativesamplingisvery\n",
      "fourmethodsforgraphembedding–ComplEx,DistMult,Rescal, usefulforcombatingdatasparsity,whilecorruptsamplingworks\n",
      "TransE.Theanalysisisperformedwithrespecttotwodatasets–a bestinthedatarichscenario.Thenatureoftherelationsinthese\n",
      "subsetofFreebase(FB15k)andasubsetofWordNet(WN18)–that graphs(typedwithrespecttotheirdomainandrangevs.open)as\n",
      "areverydifferentinthetypeofknowledgetheycover. wellasthestatisticsoftheknowledgegraph(numberofpositive\n",
      "Theresultsindicatethatdifferentapproachestonegativesam- instancesperrelation)explainthedifferentbehaviourwithrespect\n",
      "plingworkbestforthetworesources. Theproposednearmiss tonegativesampling.\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA\n",
      "REFERENCES\n",
      "[22] MaximilianNickel,VolkerTresp,andHans-PeterKriegel.2012. Factorizing\n",
      "[1] YoshuaBengioandJean-Se´bastienSene´cal.2008.Adaptiveimportancesampling YAGO:ScalableMachineLearningforLinkedData.InProceedingsofthe21st\n",
      "toacceleratetrainingofaneuralprobabilisticlanguagemodel.IEEETransactions InternationalConferenceonWorldWideWeb(WWW’12).ACM,NewYork,NY,\n",
      "onNeuralNetworks4,19(2008),713–722. USA,271–280. https://doi.org/10.1145/2187836.2187874\n",
      "[2] JamesBergstraandYoshuaBengio.2012.RandomSearchforHyper-parameter [23] NoahASmithandJasonEisner.2005. Contrastiveestimation:Traininglog-\n",
      "Optimization. J.Mach.Learn.Res.13(Feb.2012),281–305. http://dl.acm.org/ linearmodelsonunlabeleddata.InProceedingsofthe43rdAnnualMeeting\n",
      "citation.cfm?id=2188385.2188395 onAssociationforComputationalLinguistics.AssociationforComputational\n",
      "[3] KurtBollacker,ColinEvans,PraveenParitosh,TimSturge,andJamieTaylor.2008. Linguistics,354–362.\n",
      "Freebase:ACollaborativelyCreatedGraphDatabaseforStructuringHuman [24] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng.\n",
      "Knowledge.InProceedingsofthe2008ACMSIGMODInternationalConference 2013. Reasoning With Neural Tensor Networks for Knowledge Base\n",
      "onManagementofData(SIGMOD’08).ACM,NewYork,NY,USA,1247–1250. Completion. In Advances in Neural Information Processing Systems 26,\n",
      "https://doi.org/10.1145/1376616.1376746 C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Wein-\n",
      "[4] AntoineBordes,NicolasUsunier,AlbertoGarcia-Duran,JasonWeston,and berger(Eds.).CurranAssociates,Inc.,926–934. http://papers.nips.cc/paper/\n",
      "Oksana Yakhnenko. 2013. Translating Embeddings for Modeling Multi- 5028-reasoning-with-neural-tensor-networks-for-knowledge-base-completion.\n",
      "relational Data. In Advances in Neural Information Processing Systems 26, pdf\n",
      "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Wein- [25] FabianM.Suchanek,GjergjiKasneci,andGerhardWeikum.2007. YAGO:A\n",
      "berger(Eds.).CurranAssociates,Inc.,2787–2795.http://papers.nips.cc/paper/ CoreofSemanticKnowledge.InProceedingsofthe16thInternationalConference\n",
      "5071-translating-embeddings-for-modeling-multi-relational-data.pdf onWorldWideWeb(WWW’07).ACM,NewYork,NY,USA,697–706. https:\n",
      "[5] AndrewCarlson,JustinBetteridge,BryanKisiel,BurrSettles,EstevamR.Hr- //doi.org/10.1145/1242572.1242667\n",
      "uschka,andTomM.Mitchell.2010.TowardanArchitectureforNever-Ending [26] KristinaToutanova,VictoriaLin,Wen-tauYih,HoifungPoon,andChrisQuirk.\n",
      "LanguageLearning.InAAAI. 2016.CompositionalLearningofEmbeddingsforRelationPathsinKnowledge\n",
      "[6] RajarshiDas,ArvindNeelakantan,DavidBelanger,andAndrewMcCallum.2016. BaseandText.InProceedingsofthe54thAnnualMeetingoftheAssociationfor\n",
      "ChainsofReasoningoverEntities,Relations,andTextusingRecurrentNeural ComputationalLinguistics(Volume1:LongPapers).AssociationforComputational\n",
      "Networks.arXivpreprintarXiv:1607.01426(2016). Linguistics,1434–1444.https://doi.org/10.18653/v1/P16-1136\n",
      "[7] XinDong,EvgeniyGabrilovich,GeremyHeitz,WilkoHorn,NiLao,Kevin [27] The´oTrouillon,ChristopherRDance,JohannesWelbl,SebastianRiedel,E´ric\n",
      "Murphy,ThomasStrohmann,ShaohuaSun,andWeiZhang.2014.Knowledge Gaussier,andGuillaumeBouchard.2017. KnowledgeGraphCompletionvia\n",
      "vault:aweb-scaleapproachtoprobabilisticknowledgefusion.InKDD. ComplexTensorFactorization.arXivpreprintarXiv:1702.06879(2017).\n",
      "[8] XinLunaDong,EvgeniyGabrilovich,KevinMurphy,VanDang,WilkoHorn, [28] RobertWest,EvgeniyGabrilovich,KevinMurphy,ShaohuaSun,RahulGupta,\n",
      "CamilloLugaresi,ShaohuaSun,andWeiZhang.2015.Knowledge-basedTrust: andDekangLin.2014.KnowledgeBaseCompletionviaSearch-basedQuestion\n",
      "EstimatingtheTrustworthinessofWebSources.Proc.VLDBEndow.8,9(May Answering.InProceedingsofthe23rdInternationalConferenceonWorldWide\n",
      "2015),938–949. https://doi.org/10.14778/2777598.2777603 Web(WWW’14).ACM,NewYork,NY,USA,515–526. https://doi.org/10.1145/\n",
      "[9] MattGardnerandTomMitchell.2015. EfficientandExpressiveKnowledge 2566486.2568032\n",
      "BaseCompletionUsingSubgraphFeatureExtraction.InProceedingsofthe2015 [29] BishanYang,Wen-tauYih,XiaodongHe,JianfengGao,andLiDeng.2015.\n",
      "ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.Associationfor Embeddingentitiesandrelationsforlearningandinferenceinknowledgebases.\n",
      "ComputationalLinguistics,1488–1498.https://doi.org/10.18653/v1/D15-1173 InProceedingsofthe2015InternationalConferenceonRepresentationLearning.\n",
      "[10] MichaelGutmannandAapoHyvarinen.2012.Noise-contrastiveestimationof\n",
      "unnormalizedstatisticalmod-els,withapplicationstonaturalimagestatistics.\n",
      "TheJournalofMachineLearningResearch13(2012),307fi?!361.\n",
      "[11] MichaelGutmannandAapoHyva¨rinen.2010. Noise-contrastiveestimation:\n",
      "Anewestimationprincipleforunnormalizedstatisticalmodels.InProceedings\n",
      "oftheThirteenthInternationalConferenceonArtificialIntelligenceandStatistics.\n",
      "297–304.\n",
      "[12] KelvinGuu,JohnMiller,andPercyLiang.2015.TraversingKnowledgeGraphs\n",
      "inVectorSpace.InProceedingsofthe2015ConferenceonEmpiricalMethodsin\n",
      "NaturalLanguageProcessing.AssociationforComputationalLinguistics,318–327.\n",
      "https://doi.org/10.18653/v1/D15-1038\n",
      "[13] DiederikKingmaandJimmyBa.2014.Adam:Amethodforstochasticoptimiza-\n",
      "tion.arXivpreprintarXiv:1412.6980(2014).\n",
      "[14] YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,andXuanZhu.2015.Learning\n",
      "EntityandRelationEmbeddingsforKnowledgeGraphCompletion.InProceed-\n",
      "ingsoftheTwenty-NinthAAAIConferenceonArtificialIntelligence(AAAI’15).\n",
      "AAAIPress,2181–2187. http://dl.acm.org/citation.cfm?id=2886521.2886624\n",
      "[15] AlexanderMiller,AdamFisch,JesseDodge,Amir-HosseinKarimi,Antoine\n",
      "Bordes,andJasonWeston.2016. Key-ValueMemoryNetworksforDirectly\n",
      "ReadingDocuments.InProceedingsofthe2016ConferenceonEmpiricalMethods\n",
      "inNaturalLanguageProcessing.AssociationforComputationalLinguistics,1400–\n",
      "1409.http://aclweb.org/anthology/D16-1147\n",
      "[16] AndriyMnihandYeeWhyeTeh.2012.Afastandsimplealgorithmfortraining\n",
      "neuralprobabilisticlanguagemodels.InProc.ofICML.\n",
      "[17] M.Moya,M.Koch,andLHostetler.1993. One-classclassifiernetworksfor\n",
      "targetrecognitionapplications.InProc.oftheWorldCongressonNeuralNetworks.\n",
      "InternationalNeuralNetworkSociety,INNS,Portland,OR.,797fi?!801.\n",
      "[18] ArvindNeelakantan, BenjaminRoth, andAndrewMcCallum.2015. Com-\n",
      "positionalVectorSpaceModelsforKnowledgeBaseCompletion.InProceed-\n",
      "ingsofthe53rdAnnualMeetingoftheAssociationforComputationalLinguis-\n",
      "ticsandthe7thInternationalJointConferenceonNaturalLanguageProcessing\n",
      "(Volume1:LongPapers).AssociationforComputationalLinguistics,156–166.\n",
      "https://doi.org/10.3115/v1/P15-1016\n",
      "[19] M.Nickel,K.Murphy,V.Tresp,andE.Gabrilovich.2016.AReviewofRelational\n",
      "MachineLearningforKnowledgeGraphs.Proc.IEEE104,1(Jan2016),11–33.\n",
      "https://doi.org/10.1109/JPROC.2015.2483592\n",
      "[20] MaximilianNickel,LorenzoRosasco,andTomasoPoggio.2016. Holographic\n",
      "EmbeddingsofKnowledgeGraphs.InProceedingsoftheThirtiethAAAIConfer-\n",
      "enceonArtificialIntelligence(AAAI’16).AAAIPress,1955–1961.http://dl.acm.\n",
      "org/citation.cfm?id=3016100.3016172\n",
      "[21] MaximilianNickel,VolkerTresp,andHans-PeterKriegel.2011.AThree-Way\n",
      "ModelforCollectiveLearningonMulti-RelationalData.InICML.\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "PerformanceanalysisonFreebase(FB15k).\n",
      "complex MRR distmult MRR rescal MRR transE MRR\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "complex hits@10 distmult hits@10 rescal hits@10 transE hits@10\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "100 101 102 100 101 102 100 101 102 100 101 102\n",
      "corrupt nmiss nn random relational typed\n",
      "Figure7:PerformanceonFB15kintermsofMRRandHits@10\n",
      "PerformanceanalysisonWordNet(WN18).\n",
      "complex MRR distmult MRR rescal MRR transE MRR\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "complex hits@1 distmult hits@1 rescal hits@1 transE hits@1\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "100 101 102 100 101 102 100 101 102 100 101 102\n",
      "corrupt nmiss nn random relational\n",
      "Figure8:PerformanceonWN18intermsofMRRandHits@1\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "xelpmoc\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "tlumtsid\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "lacser\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "Esnart\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "depyt\n",
      "lanoitaler\n",
      "modnar\n",
      "nn\n",
      "ssimn\n",
      "tpurroc\n",
      "Figure9:PerformanceonFB15kintermsofMRRforrelationswithdifferentordersofmagnitude\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "xelpmoc\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "tlumtsid\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "lacser\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "Esnart\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "depyt\n",
      "lanoitaler\n",
      "modnar\n",
      "nn\n",
      "ssimn\n",
      "tpurroc\n",
      "Figure10:PerformanceonFB15kintermsofHits@10forrelationswithdifferentordersofmagnitude\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb201<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,    972,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['FB15k', 'WN18']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "Esnart\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "depyt\n",
      "lanoitaler\n",
      "modnar\n",
      "nn\n",
      "ssimn\n",
      "tpurroc\n",
      "Figure10:PerformanceonFB15kintermsofHits@10forrelationswithdifferentordersofmagnitude\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "xelpmoc\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "tlumtsid\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "lacser\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "Esnart\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "lanoitaler\n",
      "modnar\n",
      "nn\n",
      "ssimn\n",
      "tpurroc\n",
      "Figure11:PerformanceonWN18intermsofMRRforrelationswithdifferentordersofmagnitude\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "xelpmoc\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "tlumtsid\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "lacser\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "Esnart\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "lanoitaler\n",
      "modnar\n",
      "nn\n",
      "ssimn\n",
      "tpurroc\n",
      "Figure12:PerformanceonWN18intermsofHits@1forrelationswithdifferentordersofmagnitude<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,    972,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['FB15k', 'WN18']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Analysis of the Impact of Negative Sampling\n",
      "on Link Prediction in Knowledge Graphs\n",
      "BhushanKotnisandViviNastase\n",
      "InstituteforComputationalLinguistics,\n",
      "UniversityofHeidelberg\n",
      "Heidelberg,Germany\n",
      "{kotnis,nastase}@cl.uni-heidelberg.de\n",
      "ABSTRACT YAGO[25]areexamplesoflargeknowledgegraphsthatcontain\n",
      "Knowledge graphs are large, useful, but incomplete knowledge millionsofentitiesandfacts.Factsarerepresentedastriples,each\n",
      "repositories.Theyencodeknowledgethroughentitiesandrelations consistingoftwoentitiesconnectedbyabinaryrelation,e.g.,(con-\n",
      "whichdefineeachotherthroughtheconnectivestructureofthe cept:city:London,relation:country capital,concept:country:UK).Here\n",
      "graph. Thishasinspiredmethodsforthejointembeddingofen- entitiessuchasLondonandUKarerepresentedasnodesandthe\n",
      "titiesandrelationsincontinuouslow-dimensionalvectorspaces, relationcountry capital isrepresentedasabinarylinkthatcon-\n",
      "thatcanbeusedtoinducenewedgesinthegraph,i.e.,linkpredic- nectsthesenodes.Thesametwonodesmaybeconnectedbymore\n",
      "tioninknowledgegraphs. Learningtheserepresentationsrelies thanonetypeofrelation,makingtheKGamulti-graph.KGshave\n",
      "oncontrastingpositiveinstanceswithnegativeones.Knowledge foundapplicationsinquestionansweringsystems[15],evaluating\n",
      "graphsincludeonlypositiverelationinstances,leavingthedoor trustworthinessofwebcontent[8],andwebsearch[7].\n",
      "openforavarietyofmethodsforselectingnegativeexamples.We AlthoughKGssuchasFreebaseconsistofmillionsofentities\n",
      "presentanempiricalstudyontheimpactofnegativesamplingon andbillionsoffacts, theyarestillincomplete[28]whichlimits\n",
      "thelearnedembeddings,assessedthroughthetaskoflinkpredic- theirapplication. However,itispossibletoinfernew(missing)\n",
      "tion.Weusestate-of-the-artknowledgegraphembeddingmethods factsfromknownfacts.Recently,latentfactormodelsthatcapture\n",
      "–Rescal,TransE,DistMultandComplEX–andevaluateonbench- globalpatternsfromtheKGhavereceivedconsiderableattention.\n",
      "markdatasets–FB15kandWN18.Wecomparewellknownmeth- Theylearnarepresentationofthegraphinacontinuousvector\n",
      "odsfornegativesamplingandproposetwonewembeddingbased spacebyinducingembeddingsthatcapturethegraphstructure.\n",
      "samplingmethods.Wenoteamarkeddifferenceintheimpactof PredictingnewedgestoautomaticallyaddnewfactstoaKG\n",
      "thesesamplingmethodsonthetwodatasets,withthe”traditional” helpsbypassthetextanalysisstageandbootstrapnewknowledge\n",
      "corruptingpositivesmethodleadingtobestresultsonWN18,while basedonwhatisalreadycapturedintheKG.Similartootherprob-\n",
      "embeddingbasedmethodsbenefitFB15k. lemsinprocessingnaturallanguage,suchasparsing,dataconsists\n",
      "(almost)exclusivelyofpositiveinstances.Asolutiontothisissue\n",
      "CCSCONCEPTS isusingimplicitnegativeevidence, wherebyinstancesthathave\n",
      "notbeenobservedareconsiderednegatives,andareusedforcon-\n",
      "•Informationsystems→Questionanswering;Retrievaltasks\n",
      "trastiveestimation[23],wheretheaimistorankobservedinstances\n",
      "andgoals;Informationretrieval;\n",
      "higherthannegative(unobserved)ones.Negativeinstancescanbe\n",
      "generatedusingavarietyofmethods.\n",
      "KEYWORDS\n",
      "Inthisarticlewepresenttheresultsofourinvestigationonthe\n",
      "knowledge graphs, negative sampling, embedding models, link\n",
      "impactofseveralnegativesamplingmethodsonstate-of-the-art\n",
      "prediction\n",
      "knowledgegraphembeddingmodels.Additionallyweproposetwo\n",
      "ACMReferenceformat: negativesamplingstrategiesforfinetuningthemodel.Understand-\n",
      "BhushanKotnisandViviNastase.2018.AnalysisoftheImpactofNegative ing the impact of negative instance sampling will have at least\n",
      "Sampling twouses:providingthebasisforchoosingthenegativesampling\n",
      "onLinkPredictioninKnowledgeGraphs.InProceedingsofWorkshopon methodtobuildthebestmodelforagivenmethod,andallowingus\n",
      "KnowledgeBaseConstruction,ReasoningandMining,LosAngeles,California toplaceintherightcontextresultsreportedintheliteraturethat\n",
      "USA,Feb2018(KBCOM’18),14pages.\n",
      "wereproducedwhileusingdifferentnegativesamplingmethods.\n",
      "DOI:10.1145/nnnnnnn.nnnnnnn\n",
      "2 LINKPREDICTIONINKNOWLEDGE\n",
      "1 INTRODUCTION GRAPHS\n",
      "Muchofhumanknowledgecanbeformalizedintermsofrealworld KnowledgegraphsKG =(E,R)containknowledgeintheformof\n",
      "entities,abstractconcepts,categoriesandtherelationsbetween relationtriples(s,r,t),wheres,t ∈ Eareentities,andr ∈ Risa\n",
      "them. Agraphstructure–aknowledgegraph(KG)–isanatu- relation.Theseknowledgegraphsarenotcomplete,andadditional\n",
      "ral candidate for representing this. NELL [5], Freebase [3] and links(facts)canbeinferred,basedontheideathatsimilarnodes\n",
      "havesimilarrelations–e.g.allcountrieshaveacapitalcity.\n",
      "KBCOM’18,LosAngeles,CaliforniaUSA TheKGcanbeencodedusingdifferentmodelingtechniques,\n",
      "2018.978-x-xxxx-xxxx-x/YY/MM...$15.00\n",
      "DOI:10.1145/nnnnnnn.nnnnnnn whichresultsinencodingsforboththeentitiesandtherelations.\n",
      "8102\n",
      "raM\n",
      "2\n",
      "]IA.sc[\n",
      "2v61860.8071:viXra\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "Avarietyoftechniqueshavebeenproposed[4,14,20,21,24,29]. wherexs, xr, xt areddimensionalvectors,andd(x)iseitherthe\n",
      "ThesemethodslearnamodelfortheprocessedKGasalargeset L1 orL2-normofx. WeuseTransEwithL2-norm. Forlearning\n",
      "ofparameters,inducedbasedonoptimizingalossfunctionwith embeddings,weusemax-marginloss(1).\n",
      "respecttopositiveandnegativeinstancesoflinksrepresenting ComparedtoRescal,TransEhasmuchfewerparameters,but\n",
      "differentrelations.MethodssuchasRescal[21]andNeuralTensor itismorelimitedinthevarietyofrelationsitcanmodel,asthe\n",
      "Networks[24]learnmillionsofparametersthatmakesthemmore translationoperationassumes1:1relations.\n",
      "flexible, enablingthemtomodelwellavarietyofrelations, but\n",
      "atthecostofincreasedcomputationalcomplexityandpotential 2.3 DistMult\n",
      "overfitting.TransE[4],DistMult[29]learnsimplermodels(withfar DistMult [29] is a special case of the Rescal model, where the\n",
      "fewerparameters)andareeasiertotrainbutareunabletomodel relationmatrixisassumedtobediagonal.Thisresultsinasparse\n",
      "certaintypesofrelationssuchasmany-to-one(TransE)andasym- relationmatrixandconsequentlyfewerparameters.Howeverthis\n",
      "metricrelations(DistMult).Recentworksuchas[20]achievethe simplicityresultsinthereductionofmodelingpower.TheDistMult\n",
      "modelingpowerofRescalwithasmallernumberofparameters modelissymmetricandhencecanonlymodelsymmetricrelations.\n",
      "bycompressingthetensorproduct.Complexvaluedembeddings However,DistMultperformswellonFB15Kbenchmarkdataset,\n",
      "(ComplEx)[27]extendtheDistMulttomodelantisymmetricrela- sincethetestdatacontainsonlyafewinstancesofasymmetric\n",
      "tionsbyusingcomplexvaluedembeddings. triples.TheDistMultscoringfunctionisgivenby\n",
      "[12]showedthatmostlatentfactormodelscanbemodifiedto\n",
      "learn from paths rather than individual triples which improves sc(s,r,t)=x sT Diag(Wr)xt\n",
      "performance.RecurrentNeuralNetworksthatlearnpathrepresen- Thiscanalsobewrittenasathreewayinnerproduct\n",
      "tationshavealsobeenusedforlinkprediction[6,18]. Allthese\n",
      "modelsrequirenegativesamplesduringtraining.\n",
      "sc(s,r,t)=(cid:104)xs,xr,xt(cid:105)\n",
      "resW pee ctf to ocu lis nkou prr ea dn ica tl iy os nis ino kn nofo wu lr eds gta et ge r-o apf- ht sh :e C-a or mt pm lEe xth,o Dd iss tMw uit lh t, w Rdh.er Ae s(cid:104)x bs e, fox rr e,x wt e(cid:105) u= se(cid:205) ti hx es mix ar ri gx it ni a lon sd sx (r 1)= foD ri la eg ar(W nir n) ga tn hd ex ses, vx er c, tx ot rs.∈\n",
      "Rescal,TransE.ComplExperformsaswellastheHolographic\n",
      "Embedding(HolE)model,soHolEwasnotincluded1. 2.4 ComplEx\n",
      "TheComplExmodel[27]performssparsetensorfactorizationof\n",
      "2.1 Rescal\n",
      "theKGinthecomplexdomain.Nodesandrelationsaremodeledby\n",
      "TheRescalmodel[21,22]weighstheinteractionofallpairwise ddimensionalvectorswitharealandimaginarypart(Re(x),Im(x)).\n",
      "latentfactorsbetweenthesourceandtargetentityforpredicting ThisallowsComplExtomodelanti-symmetricrelationssincethe\n",
      "a relation. It represents every entity as ad dimensional vector threewaydotproduct(innerproduct)inthecomplexdomainis\n",
      "(x ∈ Rd),andeveryrelationasad ×d matrixW ∈ Rd×d. This notsymmetric. ComplExcanbeseenasDistMultwithcomplex\n",
      "modelrepresentsthetriple(s,r,t)asascoregivenby embeddings.ThescorefunctionofComplExisgivenby:\n",
      "sc(s,r,t)=x sT Wr xt sc(s,r,t)=Re((cid:104)xs,xr,x¯ t(cid:105))\n",
      "Thesevectorsandmatricesarelearnedusingalossfunctionthat =(cid:104)Re(xs),Re(xr),Re(xt)(cid:105)+(cid:104)Im(xs),Re(xr),Im(xt)(cid:105)\n",
      "contraststhescoreofacorrecttripletoincorrectones.Commonly +(cid:104)Re(xs),Im(xr),Im(xt)(cid:105)−(cid:104)Im(xs),Im(xr),Re(xt)(cid:105)\n",
      "usedlossfunctionsincludecross-entropyloss[26],binarynegative\n",
      "[27]trainedComplExwithnegativelog-likelihood.Tomaintainthe\n",
      "loglikelihood[27], andmax-marginloss[12,20]whichweuse\n",
      "sameexperimentalconditionsforassessingtheefficacyofnegative\n",
      "here:\n",
      "sampling,wetrainComplExwithmaxmarginloss(1).\n",
      "N\n",
      "L(θ)=(cid:213) (cid:213) [1−sci +s c(cid:48) i]+ (1)\n",
      "3 NEGATIVESAMPLING\n",
      "i t(cid:48)∈N(t)\n",
      "KnowledgeGraphscaptureknowledgeas<entity,relation,entity>\n",
      "s taci rg= ets sc.( Ssi im,ri il, at ri) tra in pd les sc(cid:48) air= eusc s( es di, wri h,t ei(cid:48) r) e.N th( et) reis lat th ioe nse at no df ti an rc go er tr ae rc et t cr oi npl te as in,w oi nth lye pn oti st ii te is vem ia np sp tae nd ct eo s.n Wod he is l, ea on nd er -e cl la at si son cls at so sie fid cg ae tis o. nK sG os\n",
      "-\n",
      "shared,butthesourceentityisincorrect. lutions have been around for some time [17], for inducing KG\n",
      "embeddings,usingnegativeinstancesleadstobettermodels.\n",
      "2.2 TransE\n",
      "Negativeinstancesarenotmarkedinaknowledgegraph.The\n",
      "TransE[4]interpretsrelationsasatranslationoperationfromthe taskoflinkpredictionhasmuchincommonwithothertasksin\n",
      "sourcetothetargetmediatedbytherelation.Morespecifically,it NLP where (most of) the observed data consists of positive in-\n",
      "embedsatriplespatiallysuchthatthesourcevectorcantravelto stances. [23]proposedcontrastiveestimation,wherebyinstances\n",
      "thetargetvectorthroughtherelationvector,i.e.,xs +xr ≈xt.The thatwereproducedbyperturbingtheobservedones(andthatthem-\n",
      "scoringfunctionsc(s,r,t)forTransEisgivenby selveshavenotbeenobserved)willserveasnegativeinstances,and\n",
      "theaimistorankobservedinstanceshigherthantheunobserved\n",
      "sc(s,r,t)=−d(xs +xr −xt)\n",
      "(”negative”)ones. Inneuralprobabilisticlanguagemodels,nega-\n",
      "tivesamplingwasfirstproposedin[1]asimportancesampling.A\n",
      "1AndalsobecauseHolEisverysimilartoComplEx.Thiswasverifiedthroughpersonal\n",
      "correspondencewithanauthoroftheComplExpaper. samplingsolutionthatwasmorestablethanimportancesampling\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA\n",
      "wasintroducedby[16],whobuiltuponthenoise-contrastiveesti- produced,wesupplementthissetwithrandomlyproducednegative\n",
      "mation[10].Intheseapproachesnegativesamplesaredrawnfrom samples.\n",
      "anon-parametricnoisedistribution.\n",
      "Forknowledgegraphsinparticulartherearemanydifferent 3.3 TypedSampling: T\n",
      "waystoproducenegativeinstancesbasedonthegraphstructure.\n",
      "KnowledgegraphssuchasFreeBaseandNELL[5]havestrongly\n",
      "Wepresentanoverviewoftechniquesforproducingnegativein-\n",
      "typed relations. For example, a relationborn in holds between\n",
      "stancesfromaknowledgegraph,andweevaluatetheirimpacton\n",
      "entitiesoftypepersonandentitiesoftypecity.Relevantnegative\n",
      "knowledgegraphcompletion,orlinkprediction.\n",
      "candidates(sourcesortargets)canbeminedbyconstrainingthe\n",
      "entitiestobelongtothesametypeasthatofthesource(ortarget).\n",
      "3.1 Randomsampling: R Thiscanhelpbypasstheproblemmentionedforthecorruptmethod,\n",
      "whensomerelationsinthedatasethaveveryfewinstances.\n",
      "The simplest form of sampling negative instances is to assume\n",
      "Foreveryrelationr :S →T,\n",
      "aclosedworldhypothesisandconsideranytriplethatdoesnot\n",
      "appearintheKGasanegativeinstance.Let\n",
      "ifSr,t ={s|shastypeSt}andTr,t ={t|t hastypeTt},\n",
      "K =K+={(si,ri,ti)|yi =1;i =1,2,···,N} w ini st th anS ct ea sn wd ilR lt coth ne sisd to om fa trin ipa len sdrangerespectivelyofr,negative\n",
      "denotethecompleteknowledgegraph,whereyi = 1represents\n",
      "(s(cid:48),r,t),s(cid:48)∈Sand(s,r,t(cid:48)),t(cid:48)∈T,\n",
      "thepresenceofatriple(si,ri,ti)(apositiveinstance)andyi =0\n",
      "suchthat\n",
      "representsabsence.Accordingtotheclosedworldassumption,the (s(cid:48),r,t)(cid:60)Rand(s,r,t(cid:48))(cid:60)K+.\n",
      "set Ko −fn =eg {a (st iiv,re is,K ti− )|yis i g =iv 0e ;n i =by 1,2,···,N} We Ifth anen ensa tim typ hle asn ms n ou rem thbe ar no of nn ee tg ya pt eiv (ee.gsa.m Alp bl ee rs tf Ero inm steth ines he astr ti yp ple es s.\n",
      "SincetheKGisincompletethissetcontainspositivetriplesnot\n",
      "presentintheKG.Furthermorethissetmightbeverylargebecause\n",
      "person,scientist),weincludeitinSr,t (orTr,t)ifoneofitstypes\n",
      "theincorrectfacts(O(N2))faroutnumberthecorrectones. m froa mtch Fe rs eeS bt a( so er rT et l) a. tW ioneo mb et ta ain dac ta ate reg lo er ay sed dat ia nf [o 9r ],th ae ndFr te he eb ea nse tid tyat ta ys pe et\n",
      "Asimplesolutiontothescalabilityproblemisrandomlysam-\n",
      "bymappingtheFreebaseentityidentifiertotheFreebasecategory.\n",
      "plingasmallnumberofsamplesfromK−.Givenapositivetriple\n",
      "This results in 101,353 instances of the category relation which\n",
      "(s,r,t)wegeneratens negativetriplesbysamplingns targeten-\n",
      "isusedinthetrainingstagetoproducetypednegativesamples.\n",
      "titiesfromtheentitysetE.Sincethesamplingisrandom,wedo\n",
      "DomainandrangetypesforFreebaserelationsareprovidedby\n",
      "notcheckwhetherthesampledtriplesarepresentinthetrainand\n",
      "Freebaseitself.Afewexamplesofentitiesandtypesareincluded\n",
      "developmentset,becausetheprobabilitytheyarepresentinK+is\n",
      "inTable1.\n",
      "negligible.Thesameprocedureisusedtogeneratenegativesource\n",
      "WedonotusetypedsamplingforWordnet.Thehypernym/hyponym\n",
      "entities.\n",
      "relationsarethedefactotyperelationsinWordNet,butarehier-\n",
      "Thenegativesproducedbyrandomsamplingmaynotbevery\n",
      "archicalratherthanamappingontoagivensmallsetofpredeter-\n",
      "useful: for the positive triple (Tom Cruise, starred in, Top Gun), minedtypesasinFreebase.\n",
      "negativetargetssuchasLondonorMount Everestseemirrelevant.\n",
      "Relevantnegativetargetsshouldincludeentitiesthataremovies, 3.4 RelationalSampling: REL\n",
      "suchasTerminator,Inception.Toobtainsuchnegativesitisneces-\n",
      "Althoughtypedorcorruptrelationsamplingcangeneraterelevant\n",
      "sarytoconstrainthesetofentitiesfromwhichsamplesaredrawn.\n",
      "negativecandidates,duetotheincompletenessoftheKG,someof\n",
      "Weexploresuchconstraintsinthefollowingsections.\n",
      "thesecandidatescouldbeunknownpositives. Ifweassumethat\n",
      "sourcetargetpairsparticipateinonlyonerelation,thensampling\n",
      "3.2 Corruptingpositiveinstances: C targets(sources)thatareconnectedtothecurrentsource(target)\n",
      "throughrelationsotherthanthecurrentrelationcanyieldtrue\n",
      "Weuseamethoddescribedin[24]thatgeneratesnegativeinstances\n",
      "negatives.Thisisacommonprocedureinmulti-classlearning.\n",
      "bycorruptingpositiveinstances:foreveryrelationr,Socheretal.\n",
      "[24]collectthesets\n",
      "Moreformally,forpositivetriple(s,r,t)thenegativecandidate\n",
      "S\n",
      "={s|(s,r,∗)∈K+}andT ={t|(∗,r,t)∈K+}, sourcesetisS− = {s|(s,r(cid:48),t(cid:48)), ∀r(cid:48) ∈ R,r(cid:48) (cid:44)r}andtargetset\n",
      "andproducesetsofcorruptedtriples T−={t|(s(cid:48),r(cid:48),t), ∀r(cid:48) ∈ R,r(cid:48)(cid:44)r}.Asbefore,aftercomputingS\n",
      "S(cid:48)={(s(cid:48),r,t)|s(cid:48)∈S,(s(cid:48),r,t)(cid:60)K+}and andTwefilteroutpositivetriplesfromtrainanddevelopmentset\n",
      "T(cid:48)={(s,r,t(cid:48))|t(cid:48)∈T,(s,r,t(cid:48))(cid:60)K+}. andsampleanumberns ofnegativesamples.\n",
      "DuringtrainingK+consistsoftriplesfromtraininganddevelop-\n",
      "3.5 NearestNeighborsampling: NN\n",
      "mentset.Wesampleanumberns ofnegativesamplesfromS(cid:48)and\n",
      "T(cid:48).Suchamethodproducesnegativeinstancesthatarecloserto Mostnegativesamplingmethodsgeneratenegativesamplesbased\n",
      "thepositiveonesthanthoseproducedthroughrandomsampling. oneithertheclosedworldassumption,functionalconstraintssuch\n",
      "Anissuewiththismethodisthatforrelationswithveryfew astypeconstraints,andtripleperturbation[19]. Weintroducea\n",
      "positiveinstances,therewillnotbealargeenoughpoolofsource negativesamplingmethodwhichusesapre-trainedembedding\n",
      "andtargetcandidatestocorruptthepositiveinstances. Thedata modelforgeneratingnegativesamples.Wenamethispre-trained\n",
      "analysis shows that this is an issue for the FB15k dataset. For embeddingmodelthe‘negativesamplingmodel’.Weusethenega-\n",
      "relationswherenotenoughcorruptednegativeinstancescanbe tivesamplingmodeltogeneratenegativetargets(sources)thatare\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "SourceType Source Relation Target TargetType\n",
      "film star wars episode IV produced by дeorдe lucas film producer\n",
      "person alexandre dumas people profession writer profession\n",
      "academic post professor profession people albert einstein award winner\n",
      "Table 1: Entity Types in Freebase: Examples of source and target entity types from Freebase used for generating negative\n",
      "samples.\n",
      "closetothepositivetarget(source)invectorspace.Thiswouldhelp neighborsampler,weusethenegativesamplingmodelforobtaining\n",
      "themodellearntodiscriminatebetweenpositivesandnegatives thepredictedvectorandentityembeddings.Thenegativesampling\n",
      "verysimilartothepositives. modelisnotupdated.\n",
      "Forapositivetriple(s,r,t),withxt thevectorrepresentationof Givenapositivetriple(s,r,t)weobtainthepredictedvector\n",
      "t obtainedfromthenegativesamplingmodel,thesetofnegative vt = x sT Wr wherexs, Wr are entity and relation embeddings\n",
      "samplesarethetopnsnearestneighborsofxt (thatarenotpositive) ofsources andrelationr obtainedusingthenegativesampling\n",
      "obtainedfromthenegativesamplingmodel.Thenegativesampling model. Notethatvt maynotbethesameasxt,thetargetentity\n",
      "modelmaybedifferentthanthemodelthatisbeingtrained.We representation.Thesetof(target)negativesamplesarethetopns\n",
      "usetheRescalmodeltrainedwith100typed(T)negativesamples nearestneighborsofthepredictedvectorvt.Algorithm2describes\n",
      "asanegativesamplingmodelfortheFB15Kdataset. Notethat theprocedureforasingletriple,inpracticeweuseabatchandthe\n",
      "theRescalmodelparametersarefrozen(notupdated),itissimply BallTreeisbuiltonlyonce.\n",
      "usedforgeneratingnegativesthatareusedfortraininganother\n",
      "model.Algorithm1describestheprocedureforasingletriple.In\n",
      "Algorithm2:NearMissSamplingusingRescalnegativesam-\n",
      "practiceweuseabatchoftriplesandthenearestneighborsearch\n",
      "pler\n",
      "isperformedusingtheBallTreealgorithmwhichisbuiltonlyonce\n",
      "sincethenegativesamplingmodelisnotupdated.\n",
      "Input :Triple(s,r,t),EntitySetE,Positivesourceandtargets\n",
      "Ps andPt,NegativeSamplingEmbeddingModelfn,\n",
      "Algorithm1:Algorithm1NearestNeighborSampling\n",
      "Numberofnegativesamplesns\n",
      "Output:Setofns negativesamples\n",
      "Input :Triple(s,r,t),EntitySetE,Positivesourceandtargets Ns ←E\\Ps, Nt ←E\\Pt;\n",
      "P Ns ua mn bd eP rt o, fN ne eg ga at ti iv ve eS sa am mp pl li en sg nE smbeddingModelfn, X Inns iti← alizf e(N ths e), KX bnt a← lltrf e( eN wt i) th;\n",
      "Xs andXt ;\n",
      "XO IN nns su itt i← ←p alu izEt f e: (\\S N tP he s st e, )o, KN Xf t bn nt as ← ← lln te E rg f e\\a ( ePt Ni wtv t;e i) th;sa Xm spl ae ns\n",
      "dXt ;\n",
      "v Sxs s ←← ← nx ef sT an r( W es s) r t,,x nvt et i← g← hbWf on rr( sr (x v), t sW,; nr um←n =nf sn );(r)n ;\n",
      "xt ← fn(t);\n",
      "n n T re← turn nea Sr,Test neighbors(vt,num=ns);\n",
      "xs ← fn(s);\n",
      "S ←nearest neighbors(xs,num=ns);\n",
      "T ←nearest neighbors(xt,num=ns); Likenearestneighborsampling,nearmisssamplingisalsocom-\n",
      "returnS,T putationallyexpensive,soinsteadoflearningfromrandomlyini-\n",
      "tializedparameterswetuneapre-trainedmodelfor5epochs.\n",
      "Nearestneighborsamplingiscomputationallyexpensivecom- 4 DATA\n",
      "paredtothemethodsdiscussedinprevioussections.Thisisbecause\n",
      "WeevaluatetheimpactofnegativesamplingontheFreebasedataset\n",
      "asearchoverallentitiesneedstobeperformedforsourceandtarget\n",
      "(FB15k)andontheWordNetdataset(WN18)introducedby[4].\n",
      "entitiesforeverytriple. Thereforeweuseamodeltrainedusing\n",
      "Theyareverydifferentincoverage–FB15kcontainsmostlynamed\n",
      "typednegativesamplingmethodsforFreebaseandcorruptedsam-\n",
      "entitiesconnectedthroughstronglytypedrelations,whileWN18\n",
      "plingforWordnettoinitializetheparametersandthenfinetune\n",
      "contains mostly common nouns connected through lexical and\n",
      "themodelusingnearestneighborsamplingfor5epochs.\n",
      "semanticrelations.DatasetdetailsareincludedinTable2.\n",
      "3.6 NearMisssampling: nmiss\n",
      "4.1 FB15k\n",
      "Thenearestneighborsamplergeneratesnegativesthataresimilar\n",
      "FB15k[4]consistsofapproximately15,000entitiesand1345rela-\n",
      "topositivesinvectorspace.Someofthosenegativesmayberanked\n",
      "tions.Weusethesplitsuppliedbythedataset:483,142train,50,000\n",
      "higherthanthepositives.Exposingsuchhighlyrankednegatives\n",
      "validationand59,071positivetestinstances.\n",
      "totheclassifiercanhelpthemodellearnabetterdiscriminator.We\n",
      "namethissettingasnearmisssampling,becausethegenerated\n",
      "negativesaretoprankedcandidateswhichmakesitdifficultfor Dataset |E| |R| Training Development Test\n",
      "themodeltoclassifythemasnegatives(nearmisses).Togenerate FB15K 14,951 1345 483,142 50000 59071\n",
      "highlyrankednegatives, wecollectthetopns targets(sources) WN18 40,943 18 141,442 5000 5000\n",
      "closest to the predicted target (source) vector. Like the nearest Table 2: Dataset Details: |E| = # of entities, |R| = # of rela-\n",
      "tions.\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA\n",
      "Figure1:FB15kdatasetfrequencystatistics Figure2:WordNet18datasetfrequencystatistics\n",
      "Thetrainingdatacontainsrelationsthathavehighvariation Fromagraphstructurepointofview,WN18nodeshavelow\n",
      "inthenumberofinstances–39%oftherelationshaveatmost connectivity–theaveragedegreeontheentiredatasetisapprox-\n",
      "10instances,whilethemostfrequentrelation2hasalmost16000. imately 1.2, and on the training data alone approximately 3.45.\n",
      "Thisdisparityisalsoreflectedinthedistributionofnodedegrees Thistranslatesintosparseradjacencymatricesforfactorization,\n",
      "– 12% of the entities have degree equal or less than 10 (appear comparedtoFreebase.\n",
      "inatmost10instances). TheaveragedegreeofanodeinFB15k WordNetcontainslexicalandsemanticrelations. Lexicalrela-\n",
      "isapproximately13.2overall,and32.4onthetrainingdata. The tions–suchasderivationally related formconnectlemmasfrom\n",
      "distributionofrelationsandnodedegreesispresentedinFigure1. differentpartsofspeechthataremorphologicallyconnected.The\n",
      "ThetypeofrelationsincludedinFreebaseconnectnamedentities. semanticrelationscoveris arelations(hypernym/hyponym,in-\n",
      "They are extrinsic relations, in that they do not hold based on stancehypernym/hyponym),threetypesofpart ofrelations(mem-\n",
      "theintrinsicpropertiesoftheconnectedentities,butaredueto ber,substanceandpart). ThesemanticrelationsinWordNetare\n",
      "externalcircumstances.Forexample,thepeople professionrelation intrinsic,astheyreflectorarisefromintrinsicpropertiesofthe\n",
      "connecting people and their professions are not determined by connectedentities.Forexample,acatis aanimal,andcathas part\n",
      "intrinsicpropertiesofpeopleandprofessions.RelationsinFreeBase pawsnotbecauseofexternalcircumstances,butbecauseofwhata\n",
      "arestronglytyped–thedomainandrangeoftherelationsaretypes, catis.ComparedtoFreeBase,WordNetrelationsarenottyped–\n",
      "e.g.thecountry capitalrelationconnectscountriesandcities. thereisnocleardomainandrangefortheWordNetrelations.\n",
      "4.2 WN18 5 EXPERIMENTS\n",
      "ThisdatasetconsistsofasubsetofrelationsfromtheWordNetlexi- 5.1 Implementation\n",
      "caldatabase3,splitintotraining,developmentandtesting:141442/\n",
      "ForfaircomparisonwereimplementedRescal,TransE,DistMult,\n",
      "5000/5000. Thereare18relations. Thereislessvariationinthe\n",
      "ComplExusingPyTorch,andtestedthemusingthesameexperi-\n",
      "numberofinstancesperrelationcomparedtotheFB15k,ascanbe\n",
      "mentalsetting:sameloss(max-marginloss),embeddingsize(100),\n",
      "seeninFigure2.Thereisonerelationwithlessthan100instances\n",
      "anddata.WeusetheAdam[13]SGDoptimizerfortrainingbecause\n",
      "(similar to),whilethemostfrequentrelations(hypernym,hyponym)\n",
      "itaddressestheproblemofdecreasinglearningrateinAdaGrad.\n",
      "haveapproximately35,000.\n",
      "Weensurethatentityembeddingsforallthemodelshaveunitnorm.\n",
      "2/award/awardnominee/awardnominations./award/awardnomination/awardnominee\n",
      "Weperformedexhaustiverandomizedgridsearch[2]fortheL2\n",
      "3https://wordnet.princeton.edu/ regularizeronthevalidationsetforallmodelsandwetunedthe\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "Model lr λ 5.4 Results\n",
      "Freebase WepresenttheresultsoflinkpredictiononFB15kandWN18in\n",
      "ComplEx 0.001 1.31E-06 termsofMRRinFigures3and4forns ∈{1,2,5,10,20,50,100}for\n",
      "DistMult 0.001 4.93E-06 eachpositiveinstance.\n",
      "Rescal 0.001 0.0002084 Theresultsshowthatthedifferentsamplingmethodshavedif-\n",
      "TransE 0.001 0.00024036 ferenteffectsonthetwodatasets. Sincelinkpredictionisbased\n",
      "Wordnet exclusivelyontheembeddingofthegraphs,differencesinperfor-\n",
      "ComplEx(ns ∈{1,2,5}) 0.005 2.82E-05 mancearecausedbythedifferentstructure(e.g. differentnode\n",
      "ComplEx(ns >=10) 0.01 2.82E-05 degreeswhicharereflectedinthesparsityoftherelationadjacency\n",
      "DistMult(ns ∈{1,2,5})<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  57350,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Link Prediction', 'Knowledge Graph Completion']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Figures3and4forns ∈{1,2,5,10,20,50,100}for\n",
      "DistMult 0.001 4.93E-06 eachpositiveinstance.\n",
      "Rescal 0.001 0.0002084 Theresultsshowthatthedifferentsamplingmethodshavedif-\n",
      "TransE 0.001 0.00024036 ferenteffectsonthetwodatasets. Sincelinkpredictionisbased\n",
      "Wordnet exclusivelyontheembeddingofthegraphs,differencesinperfor-\n",
      "ComplEx(ns ∈{1,2,5}) 0.005 2.82E-05 mancearecausedbythedifferentstructure(e.g. differentnode\n",
      "ComplEx(ns >=10) 0.01 2.82E-05 degreeswhicharereflectedinthesparsityoftherelationadjacency\n",
      "DistMult(ns ∈{1,2,5}) 0.005 3.12E-06 matrices) and the different nature of the relations – typed and\n",
      "DistMult(ns >=10) 0.01 3.12E-06 extrinsicinFB15k,nottypedand(mostly)intrinsicinWordNet.\n",
      "Rescal(ns ∈{1,2,5}) 0.005 7.48E-05 Assuggestedbyworkonlearningstatisticalmodelsthrough\n",
      "Rescal(ns >=10) 0.01 7.48E-05 noisecontrastiveestimation[11], selectingdifficultnegativein-\n",
      "TransE(ns ∈{1,2,5}) 0.005 0.0001863777692 stancesproducesbettermodels:nearmisssamplingleadstobetter\n",
      "TransE(ns >=10) 0.01 0.0001863777692 resultsonFB15kformostembeddingsmethods. Thereasonem-\n",
      "Table3:Parametervalues beddingbasedsamplingworkswellonFreeBaseisprimarilybe-\n",
      "causethenegativesamplesgeneratedbythepre-trainedembedding\n",
      "modelareveryclosetothediscriminatorboundary.Forexample,\n",
      "trainingdurationusingearlystopping.Thelearningrate(lr)and\n",
      "thenearmisssamplinginvolvesgeneratingnegativetargetentities\n",
      "λ(theL2 normcoefficient)arepresentedinTable3. Thecodeis\n",
      "thatarehighlyrankedbytheembeddingmodel. Theseentities\n",
      "availableinGithub4.\n",
      "arelikelytobehighlyrankedbythemodelthatisbeingtrained.\n",
      "ThedifferentmethodsfornegativesamplingdescribedinSection\n",
      "Thereforeprovidingtheseentitiesasnegativesallowsthesystem\n",
      "3wereusedtoproducenegativeinstancesfortraining.InFB15K\n",
      "tolearnamodelthatranksthembelowthepositivetargetusing\n",
      "somerelationsdonothaveenoughsourcesortargetstogenerate\n",
      "themax-marginloss.Notethatthesamplesgeneratedbytheem-\n",
      "negativetriplesbycorruptingpositivetriples. Ifthenumberof\n",
      "beddingmodelareclosetoeachotherinvectorspaceduetothe\n",
      "generatedtriplesarelessthantherequired(ns),wecompletethe\n",
      "abilityoftheembeddingmodeltoclusterentities.Thereforealmost\n",
      "setofnegativesampleswithrandomlygeneratedtriples.\n",
      "allthegeneratednegativesamplesareclosetothediscriminator\n",
      "Forthenearestneighborandnearmisssettings,weusedthe\n",
      "boundary. Wetreatedthenegativesamplingmodel(pre-trained\n",
      "bestperformingmodelforinitializingtheparameters,andusedthe\n",
      "model)asahyperparameter. WefoundthattheRESCALmodel\n",
      "Rescalmodeltunedontypednegativesamples(100negativesam-\n",
      "workedbest.Wespeculatethatthismightbeduetothesuperior\n",
      "ples)asthenegativesamplingmodelforFB15KandRescaltrained\n",
      "abilityofRESCALmodeltoclustersimilarentities.\n",
      "bycorruptingpositivesamples(100negativesamples)forWN18.\n",
      "Corruptingpositiveinstances,themethodmostfrequentlyused\n",
      "forlinkprediction,istheleastcompetitiveonFB15k,butfitsWord-\n",
      "5.2 Testdata Netwell,particularlyforRescal. DistMultisnotverysensitive\n",
      "The test data is the same across all experiments. The negative tothetypeofnegativesamplingonWN18,exceptforthenearest\n",
      "instancesfor thetestdataweregeneratedasdescribedin[4]– neighbormethodwithwhichitdoesnotperformwell.\n",
      "corruptingpositiveinstancesusingallentitiesofthedictionary Tounderstandwhycorruptingpositiveinstancesworksbeston\n",
      "insteadofthecorrectsourceandtarget,withoutsampling. WordNet,welookatthedataandthegraphstatistics.TheWN18\n",
      "Alsofollowingtheprocedureof[4],weusethefilteredsetting: datasethas18relationswhilewithFB15khasabout1495relations.\n",
      "thenegativesamplesaddedtothetrainingdataarefilteredwith DuetoperrelationdatasparsityinFB15K,seeFig.1and2,negative\n",
      "respecttothetestdatatoavoid(known)falsenegativesintraining. samplingusingcorruptedtriplesworkspoorlyforFB15K,asitoften\n",
      "hastofallbackonrandomsamplingwhennotenoughpositive\n",
      "5.3 Evaluationmetrics instanceswithasharedsource/targetareavailablefor”corruption”.\n",
      "Corruptsamplingworksbetterinaninstancerichenvironment.\n",
      "Forevaluationweusethemeanreciprocalrank(MRR)andhits@K\n",
      "Apartfromdatasparsity,thenatureofWordNetandFreebase\n",
      "thatarecommonlyusedforlinkprediction.\n",
      "relationsmayalsoaffecttheperformanceofnegativesampling\n",
      "ForalistofNanswersforlinkprediction,themeanreciprocal\n",
      "methods.WordNetrelationshaveopenendedrangesanddomains\n",
      "rank(MRR)andhits@karedefinedas:\n",
      "whileFreebaserelationshavetypedrangesanddomains.Embed-\n",
      "dingbasedmethods,suchasthenearmisssamplingmethodwe\n",
      "MRR= 1 (cid:205)N 1 hits@K = |{i|ranki<K}| implemented,workonthebasisofclusteringsimilarentities,and\n",
      "N i=1ranki N donotfunctionwellforWordNetwheretherelationsdonothave\n",
      "whereranki istherankofthepositiveinstanceipredictedbythe\n",
      "domainsandrangesthatreflectconceptual/semanticclusters.\n",
      "modelwithrespecttothenegativesamples. ForFB15kweuse\n",
      "Wehavediscussedthedifferencesinperformanceofsampling\n",
      "hits@10,forWN18,hits@1.\n",
      "methodsforthetwoKGsused.Therearealsodifferenceswithre-\n",
      "specttothelinkpredictionmethods.Randomsamplingworksbest\n",
      "forTransE.Thismaybesurprisingatfirst,butisunderstandable\n",
      "4https://github.com/bhushank/kge-rl\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA\n",
      "complex MRR distmult MRR rescal MRR transE MRR\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "complex hits@10 distmult hits@10 rescal hits@10 transE hits@10\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0 100 101 102 100 101 102 100 101 102 100 101 102\n",
      "corrupt nmiss nn random relational typed\n",
      "Figure3:LinkpredictiononFB15k,evaluatedintermsofMRRforns ∈{1,2,5,10,20,50,100}onalogarithmicscale.\n",
      "complex MRR distmult MRR rescal MRR transE MRR\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "complex hits@1 distmult hits@1 rescal hits@1 transE hits@1\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0 100 101 102 100 101 102 100 101 102 100 101 102\n",
      "corrupt nmiss nn random relational\n",
      "Figure4:LinkpredictiononWN18,evaluatedintermsofMRRforns ∈{1,2,5,10,20,50,100}onalogarithmicscale.\n",
      "consideringthatthetheoreticalmodelbehindTransEassumes1:1 Yangetal.[29] Negativesampling\n",
      "relations.Providingitwithnegativeentitiesthatareclose(using MRR HITS@10 neg.sampling MRR HITS@10\n",
      "typed,corruptedorembeddingmethods)doesnotresultinimprove- FB15k\n",
      "mentbecausethenegativeentitiesgeneratedusingtyped,corrupt DistMult 0.35 57.7 nearmiss 0.46 70.64\n",
      "orembeddingsareclosetoeachotherinvectorspaceandthemodel Rescal 0.31 51.9 nearmiss 0.42 64.34\n",
      "willultimatelybeunabletodistinguishbetweenthem.Thisisnot TransE 0.32 53.9 nearmiss 0.37 62.97\n",
      "thecasewhendoingrandomsampling,whenTransEisnotper- WN18\n",
      "turbedbytooclosenegatives.ComplExandDistMultperformwell DistMult 0.83 94.2 corrupt 0.82 94.06\n",
      "withbothnearmissandnearestneighboursamplingonFB15k. Rescal 0.89 92.8 corrupt 0.92 93.91\n",
      "Rescalperformsbestwithnearmisssamplingonthisdata,and TransE 0.38 90.9 corrupt 0.40 86.98\n",
      "withcorruptingpositivesamplesforWordNet.Formiddle-range Table4:SotAresultsusingamax-marginlossfunctionand\n",
      "ns relationalsamplingperformsbest. corruptingpositiveinstancesvs. thebestperformingnega-\n",
      "AsdescribedinSection4,thetrainingdataforbothmethods tivesampling.\n",
      "variesquiteabitintermsofthefrequencyoftherelationscovered.\n",
      "Freebaseismoreextreme,inthatapproximately39%oftherelations\n",
      "haveatmost10positiveinstancestotrainon. Weanalyzedthe entityrepresentations,wecannotethattheperformanceonlink\n",
      "effectsofnegativesamplingondifferentslicesofthedata,splitby predictionfortheserelationswithveryfewinstancesvariesmuch\n",
      "theorderofmagnitude(oom)ofthefrequencyoftherelationsin withthenegativesamplingmethod.Overall,thebestresultsareob-\n",
      "thetrainingdata.Moreprecisely,wegrouprelationsintosetsGn tainedwiththesamesamplingmethodasfortheirmorepopulous\n",
      "indexedbytheorderofmagnituden: counterparts,butforspecificrangesofthenumberofgenerated\n",
      "Gn ={r|10n < freq(n,trainingdata)<=10(n+1)}5. negative samples other methods would work best (e.g. nearest\n",
      "Freebasehas5slices(0..4)andWordNet4(1..4).Theresults(as neighborandrelationalsamplingforWordNetdata).\n",
      "MRRandhits@K)forslicesrepresentingrelationswithOOM2 Thereportedexperimentswereperformedusingthemaxmargin\n",
      "ormorecloselymirrortheoverallresults.Theresultsforthelow lossfunction.InTable4weincludethestateoftheartresultson\n",
      "frequencyrelationsareshowninFigures5and6.Thehits@Kscore DistMult, Rescal andTransEobtainedwith amax marginloss\n",
      "aresimilartotheMRRones,sowedonotincludethem6. functionreportedin[29]andcorruptingtripes,tocomparewith\n",
      "Whiletheresultsonthelowfrequencyrelationscannotbeana- theresultsobtainedwiththebestnegativesamplingmethodforthe\n",
      "lyzedseparatelyfromtheotherrelationsbecausetheembeddings dataset.Slightdifferencesinthelearningrateandλaccountforthe\n",
      "processreliesonprocessingandinducingjointlyallrelationand differencesinperformancewhenusingcorruptpositiveinstances\n",
      "asnegativesamplesfortheWN18dataset.\n",
      "Recently, [27] used the log-likelihood objective, which leads\n",
      "5WeincluderelationsthathaveonlyoneinstanceinG0.\n",
      "6Thecompletesetofplotsaccompaniesthecodeandwillbeshared. toimprovementsoverthepublishedresultsforthemethodsthey\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "corrupt nn relational corrupt nn relational\n",
      "nmiss random typed nmiss random\n",
      "complex complex complex complex\n",
      "0.60 0.60\n",
      "0.55 0.55 1.0 1.0\n",
      "0.50 0.50\n",
      "0.8 0.8\n",
      "0.45 0.45\n",
      "0.40 0.40 0.6 0.6\n",
      "0.35 0.35\n",
      "0.4 0.4\n",
      "0.30 0.30\n",
      "0.25 0.25 0.2 0.2\n",
      "0.20 0.20\n",
      "distmult distmult distmult distmult\n",
      "0.6 0.6 1.1 1.1\n",
      "0.5 0.5 1.0 1.0\n",
      "0.4 0.4 0.9 0.9\n",
      "0.3 0.3 0.8 0.8\n",
      "0.2 0.2 0.7 0.7\n",
      "0.1 0.1 0.6 0.6\n",
      "rescal rescal rescal rescal\n",
      "0.6 0.6\n",
      "1.0 1.0\n",
      "0.5 0.5\n",
      "0.8 0.8\n",
      "0.4 0.4\n",
      "0.6 0.6\n",
      "0.3 0.3\n",
      "0.4 0.4\n",
      "0.2 0.2\n",
      "0.2 0.2\n",
      "0.1 0.1\n",
      "transE transE transE transE\n",
      "0.6 0.6 0.6 0.6\n",
      "0.5 0.5 0.5 0.5\n",
      "0.4 0.4 0.4 0.4\n",
      "0.3 0.3 0.3 0.3\n",
      "0.2 0.2 0.2 0.2\n",
      "0.1 0.1 0.1 0.1\n",
      "0.0 0.0 0.0 0.0\n",
      "100 101 102 100 101 102 100 101 102 100 101 102\n",
      "Figure5: ResultsonrelationswithOOM0and1inFB15k Figure6: ResultsonrelationswithOOM1and2inWN18\n",
      "(MRRs) (MRRs)\n",
      "compared(TransE,ComplEx,HolE,DistMult).Weplantoanalyze\n",
      "sampling worked best for Freebase withmost of the graphem-\n",
      "thenegativesamplingmethodswhileusingthisnewlossfunction.\n",
      "beddingmethods,whilecorruptingpositivetriplesleadstobest\n",
      "resultsonWordNet. Thenewlyproposednearmissandnearest\n",
      "6 CONCLUSION\n",
      "neighbornegativesamplingworkbestforFreebase,forthreeoutof\n",
      "Wereportananalysisoftheimpactofsixnegativesamplingmeth- thefourgraphembeddingsmethods.Fromanalysisofdatasets,we\n",
      "odsontheperformanceoflinkpredictioninknowledgegraphs,for furtherconcludedthatembeddingbasednegativesamplingisvery\n",
      "fourmethodsforgraphembedding–ComplEx,DistMult,Rescal, usefulforcombatingdatasparsity,whilecorruptsamplingworks\n",
      "TransE.Theanalysisisperformedwithrespecttotwodatasets–a bestinthedatarichscenario.Thenatureoftherelationsinthese\n",
      "subsetofFreebase(FB15k)andasubsetofWordNet(WN18)–that graphs(typedwithrespecttotheirdomainandrangevs.open)as\n",
      "areverydifferentinthetypeofknowledgetheycover. wellasthestatisticsoftheknowledgegraph(numberofpositive\n",
      "Theresultsindicatethatdifferentapproachestonegativesam- instancesperrelation)explainthedifferentbehaviourwithrespect\n",
      "plingworkbestforthetworesources. Theproposednearmiss tonegativesampling.\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA\n",
      "REFERENCES\n",
      "[22] MaximilianNickel,VolkerTresp,andHans-PeterKriegel.2012. Factorizing\n",
      "[1] YoshuaBengioandJean-Se´bastienSene´cal.2008.Adaptiveimportancesampling YAGO:ScalableMachineLearningforLinkedData.InProceedingsofthe21st\n",
      "toacceleratetrainingofaneuralprobabilisticlanguagemodel.IEEETransactions InternationalConferenceonWorldWideWeb(WWW’12).ACM,NewYork,NY,\n",
      "onNeuralNetworks4,19(2008),713–722. USA,271–280. https://doi.org/10.1145/2187836.2187874\n",
      "[2] JamesBergstraandYoshuaBengio.2012.RandomSearchforHyper-parameter [23] NoahASmithandJasonEisner.2005. Contrastiveestimation:Traininglog-\n",
      "Optimization. J.Mach.Learn.Res.13(Feb.2012),281–305. http://dl.acm.org/ linearmodelsonunlabeleddata.InProceedingsofthe43rdAnnualMeeting\n",
      "citation.cfm?id=2188385.2188395 onAssociationforComputationalLinguistics.AssociationforComputational\n",
      "[3] KurtBollacker,ColinEvans,PraveenParitosh,TimSturge,andJamieTaylor.2008. Linguistics,354–362.\n",
      "Freebase:ACollaborativelyCreatedGraphDatabaseforStructuringHuman [24] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng.\n",
      "Knowledge.InProceedingsofthe2008ACMSIGMODInternationalConference 2013. Reasoning With Neural Tensor Networks for Knowledge Base\n",
      "onManagementofData(SIGMOD’08).ACM,NewYork,NY,USA,1247–1250. Completion. In Advances in Neural Information Processing Systems 26,\n",
      "https://doi.org/10.1145/1376616.1376746 C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Wein-\n",
      "[4] AntoineBordes,NicolasUsunier,AlbertoGarcia-Duran,JasonWeston,and berger(Eds.).CurranAssociates,Inc.,926–934. http://papers.nips.cc/paper/\n",
      "Oksana Yakhnenko. 2013. Translating Embeddings for Modeling Multi- 5028-reasoning-with-neural-tensor-networks-for-knowledge-base-completion.\n",
      "relational Data. In Advances in Neural Information Processing Systems 26, pdf\n",
      "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Wein- [25] FabianM.Suchanek,GjergjiKasneci,andGerhardWeikum.2007. YAGO:A\n",
      "berger(Eds.).CurranAssociates,Inc.,2787–2795.http://papers.nips.cc/paper/ CoreofSemanticKnowledge.InProceedingsofthe16thInternationalConference\n",
      "5071-translating-embeddings-for-modeling-multi-relational-data.pdf onWorldWideWeb(WWW’07).ACM,NewYork,NY,USA,697–706. https:\n",
      "[5] AndrewCarlson,JustinBetteridge,BryanKisiel,BurrSettles,EstevamR.Hr- //doi.org/10.1145/1242572.1242667\n",
      "uschka,andTomM.Mitchell.2010.TowardanArchitectureforNever-Ending [26] KristinaToutanova,VictoriaLin,Wen-tauYih,HoifungPoon,andChrisQuirk.\n",
      "LanguageLearning.InAAAI. 2016.CompositionalLearningofEmbeddingsforRelationPathsinKnowledge\n",
      "[6] RajarshiDas,ArvindNeelakantan,DavidBelanger,andAndrewMcCallum.2016. BaseandText.InProceedingsofthe54thAnnualMeetingoftheAssociationfor\n",
      "ChainsofReasoningoverEntities,Relations,andTextusingRecurrentNeural ComputationalLinguistics(Volume1:LongPapers).AssociationforComputational\n",
      "Networks.arXivpreprintarXiv:1607.01426(2016). Linguistics,1434–1444.https://doi.org/10.18653/v1/P16-1136\n",
      "[7] XinDong,EvgeniyGabrilovich,GeremyHeitz,WilkoHorn,NiLao,Kevin [27] The´oTrouillon,ChristopherRDance,JohannesWelbl,SebastianRiedel,E´ric\n",
      "Murphy,ThomasStrohmann,ShaohuaSun,andWeiZhang.2014.Knowledge Gaussier,andGuillaumeBouchard.2017. KnowledgeGraphCompletionvia\n",
      "vault:aweb-scaleapproachtoprobabilisticknowledgefusion.InKDD. ComplexTensorFactorization.arXivpreprintarXiv:1702.06879(2017).\n",
      "[8] XinLunaDong,EvgeniyGabrilovich,KevinMurphy,VanDang,WilkoHorn, [28] RobertWest,EvgeniyGabrilovich,KevinMurphy,ShaohuaSun,RahulGupta,\n",
      "CamilloLugaresi,ShaohuaSun,andWeiZhang.2015.Knowledge-basedTrust: andDekangLin.2014.KnowledgeBaseCompletionviaSearch-basedQuestion\n",
      "EstimatingtheTrustworthinessofWebSources.Proc.VLDBEndow.8,9(May Answering.InProceedingsofthe23rdInternationalConferenceonWorldWide\n",
      "2015),938–949. https://doi.org/10.14778/2777598.2777603 Web(WWW’14).ACM,NewYork,NY,USA,515–526. https://doi.org/10.1145/\n",
      "[9] MattGardnerandTomMitchell.2015. EfficientandExpressiveKnowledge 2566486.2568032\n",
      "BaseCompletionUsingSubgraphFeatureExtraction.InProceedingsofthe2015 [29] BishanYang,Wen-tauYih,XiaodongHe,JianfengGao,andLiDeng.2015.\n",
      "ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.Associationfor Embeddingentitiesandrelationsforlearningandinferenceinknowledgebases.\n",
      "ComputationalLinguistics,1488–1498.https://doi.org/10.18653/v1/D15-1173 InProceedingsofthe2015InternationalConferenceonRepresentationLearning.\n",
      "[10] MichaelGutmannandAapoHyvarinen.2012.Noise-contrastiveestimationof\n",
      "unnormalizedstatisticalmod-els,withapplicationstonaturalimagestatistics.\n",
      "TheJournalofMachineLearningResearch13(2012),307fi?!361.\n",
      "[11] MichaelGutmannandAapoHyva¨rinen.2010. Noise-contrastiveestimation:\n",
      "Anewestimationprincipleforunnormalizedstatisticalmodels.InProceedings\n",
      "oftheThirteenthInternationalConferenceonArtificialIntelligenceandStatistics.\n",
      "297–304.\n",
      "[12] KelvinGuu,JohnMiller,andPercyLiang.2015.TraversingKnowledgeGraphs\n",
      "inVectorSpace.InProceedingsofthe2015ConferenceonEmpiricalMethodsin\n",
      "NaturalLanguageProcessing.AssociationforComputationalLinguistics,318–327.\n",
      "https://doi.org/10.18653/v1/D15-1038\n",
      "[13] DiederikKingmaandJimmyBa.2014.Adam:Amethodforstochasticoptimiza-\n",
      "tion.arXivpreprintarXiv:1412.6980(2014).\n",
      "[14] YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,andXuanZhu.2015.Learning\n",
      "EntityandRelationEmbeddingsforKnowledgeGraphCompletion.InProceed-\n",
      "ingsoftheTwenty-NinthAAAIConferenceonArtificialIntelligence(AAAI’15).\n",
      "AAAIPress,2181–2187. http://dl.acm.org/citation.cfm?id=2886521.2886624\n",
      "[15] AlexanderMiller,AdamFisch,JesseDodge,Amir-HosseinKarimi,Antoine\n",
      "Bordes,andJasonWeston.2016. Key-ValueMemoryNetworksforDirectly\n",
      "ReadingDocuments.InProceedingsofthe2016ConferenceonEmpiricalMethods\n",
      "inNaturalLanguageProcessing.AssociationforComputationalLinguistics,1400–\n",
      "1409.http://aclweb.org/anthology/D16-1147\n",
      "[16] AndriyMnihandYeeWhyeTeh.2012.Afastandsimplealgorithmfortraining\n",
      "neuralprobabilisticlanguagemodels.InProc.ofICML.\n",
      "[17] M.Moya,M.Koch,andLHostetler.1993. One-classclassifiernetworksfor\n",
      "targetrecognitionapplications.InProc.oftheWorldCongressonNeuralNetworks.\n",
      "InternationalNeuralNetworkSociety,INNS,Portland,OR.,797fi?!801.\n",
      "[18] ArvindNeelakantan, BenjaminRoth, andAndrewMcCallum.2015. Com-\n",
      "positionalVectorSpaceModelsforKnowledgeBaseCompletion.InProceed-\n",
      "ingsofthe53rdAnnualMeetingoftheAssociationforComputationalLinguis-\n",
      "ticsandthe7thInternationalJointConferenceonNaturalLanguageProcessing\n",
      "(Volume1:LongPapers).AssociationforComputationalLinguistics,156–166.\n",
      "https://doi.org/10.3115/v1/P15-1016\n",
      "[19] M.Nickel,K.Murphy,V.Tresp,andE.Gabrilovich.2016.AReviewofRelational\n",
      "MachineLearningforKnowledgeGraphs.Proc.IEEE104,1(Jan2016),11–33.\n",
      "https://doi.org/10.1109/JPROC.2015.2483592\n",
      "[20] MaximilianNickel,LorenzoRosasco,andTomasoPoggio.2016. Holographic\n",
      "EmbeddingsofKnowledgeGraphs.InProceedingsoftheThirtiethAAAIConfer-\n",
      "enceonArtificialIntelligence(AAAI’16).AAAIPress,1955–1961.http://dl.acm.\n",
      "org/citation.cfm?id=3016100.3016172\n",
      "[21] MaximilianNickel,VolkerTresp,andHans-PeterKriegel.2011.AThree-Way\n",
      "ModelforCollectiveLearningonMulti-RelationalData.InICML.\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "PerformanceanalysisonFreebase(FB15k).\n",
      "complex MRR distmult MRR rescal MRR transE MRR\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "complex hits@10 distmult hits@10 rescal hits@10 transE hits@10\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "100 101 102 100 101 102 100 101 102 100 101 102\n",
      "corrupt nmiss nn random relational typed\n",
      "Figure7:PerformanceonFB15kintermsofMRRandHits@10\n",
      "PerformanceanalysisonWordNet(WN18).\n",
      "complex MRR distmult MRR rescal MRR transE MRR\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "complex hits@1 distmult hits@1 rescal hits@1 transE hits@1\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "100 101 102 100 101 102 100 101 102 100 101 102\n",
      "corrupt nmiss nn random relational\n",
      "Figure8:PerformanceonWN18intermsofMRRandHits@1\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "xelpmoc\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "tlumtsid\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "lacser\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "Esnart\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "depyt\n",
      "lanoitaler\n",
      "modnar\n",
      "nn\n",
      "ssimn\n",
      "tpurroc\n",
      "Figure9:PerformanceonFB15kintermsofMRRforrelationswithdifferentordersofmagnitude\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "xelpmoc\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "tlumtsid\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "lacser\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "Esnart\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "depyt\n",
      "lanoitaler\n",
      "modnar\n",
      "nn\n",
      "ssimn\n",
      "tpurroc\n",
      "Figure10:PerformanceonFB15kintermsofHits@10forrelationswithdifferentordersofmagnitude\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb201<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   9954,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Link prediction', 'Knowledge graph completion']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "Esnart\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "depyt\n",
      "lanoitaler\n",
      "modnar\n",
      "nn\n",
      "ssimn\n",
      "tpurroc\n",
      "Figure10:PerformanceonFB15kintermsofHits@10forrelationswithdifferentordersofmagnitude\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "xelpmoc\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "tlumtsid\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "lacser\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "Esnart\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "lanoitaler\n",
      "modnar\n",
      "nn\n",
      "ssimn\n",
      "tpurroc\n",
      "Figure11:PerformanceonWN18intermsofMRRforrelationswithdifferentordersofmagnitude\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "xelpmoc\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "tlumtsid\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "lacser\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "Esnart\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "lanoitaler\n",
      "modnar\n",
      "nn\n",
      "ssimn\n",
      "tpurroc\n",
      "Figure12:PerformanceonWN18intermsofHits@1forrelationswithdifferentordersofmagnitude<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,     82,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Link Prediction in Knowledge Graphs']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Analysis of the Impact of Negative Sampling\n",
      "on Link Prediction in Knowledge Graphs\n",
      "BhushanKotnisandViviNastase\n",
      "InstituteforComputationalLinguistics,\n",
      "UniversityofHeidelberg\n",
      "Heidelberg,Germany\n",
      "{kotnis,nastase}@cl.uni-heidelberg.de\n",
      "ABSTRACT YAGO[25]areexamplesoflargeknowledgegraphsthatcontain\n",
      "Knowledge graphs are large, useful, but incomplete knowledge millionsofentitiesandfacts.Factsarerepresentedastriples,each\n",
      "repositories.Theyencodeknowledgethroughentitiesandrelations consistingoftwoentitiesconnectedbyabinaryrelation,e.g.,(con-\n",
      "whichdefineeachotherthroughtheconnectivestructureofthe cept:city:London,relation:country capital,concept:country:UK).Here\n",
      "graph. Thishasinspiredmethodsforthejointembeddingofen- entitiessuchasLondonandUKarerepresentedasnodesandthe\n",
      "titiesandrelationsincontinuouslow-dimensionalvectorspaces, relationcountry capital isrepresentedasabinarylinkthatcon-\n",
      "thatcanbeusedtoinducenewedgesinthegraph,i.e.,linkpredic- nectsthesenodes.Thesametwonodesmaybeconnectedbymore\n",
      "tioninknowledgegraphs. Learningtheserepresentationsrelies thanonetypeofrelation,makingtheKGamulti-graph.KGshave\n",
      "oncontrastingpositiveinstanceswithnegativeones.Knowledge foundapplicationsinquestionansweringsystems[15],evaluating\n",
      "graphsincludeonlypositiverelationinstances,leavingthedoor trustworthinessofwebcontent[8],andwebsearch[7].\n",
      "openforavarietyofmethodsforselectingnegativeexamples.We AlthoughKGssuchasFreebaseconsistofmillionsofentities\n",
      "presentanempiricalstudyontheimpactofnegativesamplingon andbillionsoffacts, theyarestillincomplete[28]whichlimits\n",
      "thelearnedembeddings,assessedthroughthetaskoflinkpredic- theirapplication. However,itispossibletoinfernew(missing)\n",
      "tion.Weusestate-of-the-artknowledgegraphembeddingmethods factsfromknownfacts.Recently,latentfactormodelsthatcapture\n",
      "–Rescal,TransE,DistMultandComplEX–andevaluateonbench- globalpatternsfromtheKGhavereceivedconsiderableattention.\n",
      "markdatasets–FB15kandWN18.Wecomparewellknownmeth- Theylearnarepresentationofthegraphinacontinuousvector\n",
      "odsfornegativesamplingandproposetwonewembeddingbased spacebyinducingembeddingsthatcapturethegraphstructure.\n",
      "samplingmethods.Wenoteamarkeddifferenceintheimpactof PredictingnewedgestoautomaticallyaddnewfactstoaKG\n",
      "thesesamplingmethodsonthetwodatasets,withthe”traditional” helpsbypassthetextanalysisstageandbootstrapnewknowledge\n",
      "corruptingpositivesmethodleadingtobestresultsonWN18,while basedonwhatisalreadycapturedintheKG.Similartootherprob-\n",
      "embeddingbasedmethodsbenefitFB15k. lemsinprocessingnaturallanguage,suchasparsing,dataconsists\n",
      "(almost)exclusivelyofpositiveinstances.Asolutiontothisissue\n",
      "CCSCONCEPTS isusingimplicitnegativeevidence, wherebyinstancesthathave\n",
      "notbeenobservedareconsiderednegatives,andareusedforcon-\n",
      "•Informationsystems→Questionanswering;Retrievaltasks\n",
      "trastiveestimation[23],wheretheaimistorankobservedinstances\n",
      "andgoals;Informationretrieval;\n",
      "higherthannegative(unobserved)ones.Negativeinstancescanbe\n",
      "generatedusingavarietyofmethods.\n",
      "KEYWORDS\n",
      "Inthisarticlewepresenttheresultsofourinvestigationonthe\n",
      "knowledge graphs, negative sampling, embedding models, link\n",
      "impactofseveralnegativesamplingmethodsonstate-of-the-art\n",
      "prediction\n",
      "knowledgegraphembeddingmodels.Additionallyweproposetwo\n",
      "ACMReferenceformat: negativesamplingstrategiesforfinetuningthemodel.Understand-\n",
      "BhushanKotnisandViviNastase.2018.AnalysisoftheImpactofNegative ing the impact of negative instance sampling will have at least\n",
      "Sampling twouses:providingthebasisforchoosingthenegativesampling\n",
      "onLinkPredictioninKnowledgeGraphs.InProceedingsofWorkshopon methodtobuildthebestmodelforagivenmethod,andallowingus\n",
      "KnowledgeBaseConstruction,ReasoningandMining,LosAngeles,California toplaceintherightcontextresultsreportedintheliteraturethat\n",
      "USA,Feb2018(KBCOM’18),14pages.\n",
      "wereproducedwhileusingdifferentnegativesamplingmethods.\n",
      "DOI:10.1145/nnnnnnn.nnnnnnn\n",
      "2 LINKPREDICTIONINKNOWLEDGE\n",
      "1 INTRODUCTION GRAPHS\n",
      "Muchofhumanknowledgecanbeformalizedintermsofrealworld KnowledgegraphsKG =(E,R)containknowledgeintheformof\n",
      "entities,abstractconcepts,categoriesandtherelationsbetween relationtriples(s,r,t),wheres,t ∈ Eareentities,andr ∈ Risa\n",
      "them. Agraphstructure–aknowledgegraph(KG)–isanatu- relation.Theseknowledgegraphsarenotcomplete,andadditional\n",
      "ral candidate for representing this. NELL [5], Freebase [3] and links(facts)canbeinferred,basedontheideathatsimilarnodes\n",
      "havesimilarrelations–e.g.allcountrieshaveacapitalcity.\n",
      "KBCOM’18,LosAngeles,CaliforniaUSA TheKGcanbeencodedusingdifferentmodelingtechniques,\n",
      "2018.978-x-xxxx-xxxx-x/YY/MM...$15.00\n",
      "DOI:10.1145/nnnnnnn.nnnnnnn whichresultsinencodingsforboththeentitiesandtherelations.\n",
      "8102\n",
      "raM\n",
      "2\n",
      "]IA.sc[\n",
      "2v61860.8071:viXra\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "Avarietyoftechniqueshavebeenproposed[4,14,20,21,24,29]. wherexs, xr, xt areddimensionalvectors,andd(x)iseitherthe\n",
      "ThesemethodslearnamodelfortheprocessedKGasalargeset L1 orL2-normofx. WeuseTransEwithL2-norm. Forlearning\n",
      "ofparameters,inducedbasedonoptimizingalossfunctionwith embeddings,weusemax-marginloss(1).\n",
      "respecttopositiveandnegativeinstancesoflinksrepresenting ComparedtoRescal,TransEhasmuchfewerparameters,but\n",
      "differentrelations.MethodssuchasRescal[21]andNeuralTensor itismorelimitedinthevarietyofrelationsitcanmodel,asthe\n",
      "Networks[24]learnmillionsofparametersthatmakesthemmore translationoperationassumes1:1relations.\n",
      "flexible, enablingthemtomodelwellavarietyofrelations, but\n",
      "atthecostofincreasedcomputationalcomplexityandpotential 2.3 DistMult\n",
      "overfitting.TransE[4],DistMult[29]learnsimplermodels(withfar DistMult [29] is a special case of the Rescal model, where the\n",
      "fewerparameters)andareeasiertotrainbutareunabletomodel relationmatrixisassumedtobediagonal.Thisresultsinasparse\n",
      "certaintypesofrelationssuchasmany-to-one(TransE)andasym- relationmatrixandconsequentlyfewerparameters.Howeverthis\n",
      "metricrelations(DistMult).Recentworksuchas[20]achievethe simplicityresultsinthereductionofmodelingpower.TheDistMult\n",
      "modelingpowerofRescalwithasmallernumberofparameters modelissymmetricandhencecanonlymodelsymmetricrelations.\n",
      "bycompressingthetensorproduct.Complexvaluedembeddings However,DistMultperformswellonFB15Kbenchmarkdataset,\n",
      "(ComplEx)[27]extendtheDistMulttomodelantisymmetricrela- sincethetestdatacontainsonlyafewinstancesofasymmetric\n",
      "tionsbyusingcomplexvaluedembeddings. triples.TheDistMultscoringfunctionisgivenby\n",
      "[12]showedthatmostlatentfactormodelscanbemodifiedto\n",
      "learn from paths rather than individual triples which improves sc(s,r,t)=x sT Diag(Wr)xt\n",
      "performance.RecurrentNeuralNetworksthatlearnpathrepresen- Thiscanalsobewrittenasathreewayinnerproduct\n",
      "tationshavealsobeenusedforlinkprediction[6,18]. Allthese\n",
      "modelsrequirenegativesamplesduringtraining.\n",
      "sc(s,r,t)=(cid:104)xs,xr,xt(cid:105)\n",
      "resW pee ctf to ocu lis nkou prr ea dn ica tl iy os nis ino kn nofo wu lr eds gta et ge r-o apf- ht sh :e C-a or mt pm lEe xth,o Dd iss tMw uit lh t, w Rdh.er Ae s(cid:104)x bs e, fox rr e,x wt e(cid:105) u= se(cid:205) ti hx es mix ar ri gx it ni a lon sd sx (r 1)= foD ri la eg ar(W nir n) ga tn hd ex ses, vx er c, tx ot rs.∈\n",
      "Rescal,TransE.ComplExperformsaswellastheHolographic\n",
      "Embedding(HolE)model,soHolEwasnotincluded1. 2.4 ComplEx\n",
      "TheComplExmodel[27]performssparsetensorfactorizationof\n",
      "2.1 Rescal\n",
      "theKGinthecomplexdomain.Nodesandrelationsaremodeledby\n",
      "TheRescalmodel[21,22]weighstheinteractionofallpairwise ddimensionalvectorswitharealandimaginarypart(Re(x),Im(x)).\n",
      "latentfactorsbetweenthesourceandtargetentityforpredicting ThisallowsComplExtomodelanti-symmetricrelationssincethe\n",
      "a relation. It represents every entity as ad dimensional vector threewaydotproduct(innerproduct)inthecomplexdomainis\n",
      "(x ∈ Rd),andeveryrelationasad ×d matrixW ∈ Rd×d. This notsymmetric. ComplExcanbeseenasDistMultwithcomplex\n",
      "modelrepresentsthetriple(s,r,t)asascoregivenby embeddings.ThescorefunctionofComplExisgivenby:\n",
      "sc(s,r,t)=x sT Wr xt sc(s,r,t)=Re((cid:104)xs,xr,x¯ t(cid:105))\n",
      "Thesevectorsandmatricesarelearnedusingalossfunctionthat =(cid:104)Re(xs),Re(xr),Re(xt)(cid:105)+(cid:104)Im(xs),Re(xr),Im(xt)(cid:105)\n",
      "contraststhescoreofacorrecttripletoincorrectones.Commonly +(cid:104)Re(xs),Im(xr),Im(xt)(cid:105)−(cid:104)Im(xs),Im(xr),Re(xt)(cid:105)\n",
      "usedlossfunctionsincludecross-entropyloss[26],binarynegative\n",
      "[27]trainedComplExwithnegativelog-likelihood.Tomaintainthe\n",
      "loglikelihood[27], andmax-marginloss[12,20]whichweuse\n",
      "sameexperimentalconditionsforassessingtheefficacyofnegative\n",
      "here:\n",
      "sampling,wetrainComplExwithmaxmarginloss(1).\n",
      "N\n",
      "L(θ)=(cid:213) (cid:213) [1−sci +s c(cid:48) i]+ (1)\n",
      "3 NEGATIVESAMPLING\n",
      "i t(cid:48)∈N(t)\n",
      "KnowledgeGraphscaptureknowledgeas<entity,relation,entity>\n",
      "s taci rg= ets sc.( Ssi im,ri il, at ri) tra in pd les sc(cid:48) air= eusc s( es di, wri h,t ei(cid:48) r) e.N th( et) reis lat th ioe nse at no df ti an rc go er tr ae rc et t cr oi npl te as in,w oi nth lye pn oti st ii te is vem ia np sp tae nd ct eo s.n Wod he is l, ea on nd er -e cl la at si son cls at so sie fid cg ae tis o. nK sG os\n",
      "-\n",
      "shared,butthesourceentityisincorrect. lutions have been around for some time [17], for inducing KG\n",
      "embeddings,usingnegativeinstancesleadstobettermodels.\n",
      "2.2 TransE\n",
      "Negativeinstancesarenotmarkedinaknowledgegraph.The\n",
      "TransE[4]interpretsrelationsasatranslationoperationfromthe taskoflinkpredictionhasmuchincommonwithothertasksin\n",
      "sourcetothetargetmediatedbytherelation.Morespecifically,it NLP where (most of) the observed data consists of positive in-\n",
      "embedsatriplespatiallysuchthatthesourcevectorcantravelto stances. [23]proposedcontrastiveestimation,wherebyinstances\n",
      "thetargetvectorthroughtherelationvector,i.e.,xs +xr ≈xt.The thatwereproducedbyperturbingtheobservedones(andthatthem-\n",
      "scoringfunctionsc(s,r,t)forTransEisgivenby selveshavenotbeenobserved)willserveasnegativeinstances,and\n",
      "theaimistorankobservedinstanceshigherthantheunobserved\n",
      "sc(s,r,t)=−d(xs +xr −xt)\n",
      "(”negative”)ones. Inneuralprobabilisticlanguagemodels,nega-\n",
      "tivesamplingwasfirstproposedin[1]asimportancesampling.A\n",
      "1AndalsobecauseHolEisverysimilartoComplEx.Thiswasverifiedthroughpersonal\n",
      "correspondencewithanauthoroftheComplExpaper. samplingsolutionthatwasmorestablethanimportancesampling\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA\n",
      "wasintroducedby[16],whobuiltuponthenoise-contrastiveesti- produced,wesupplementthissetwithrandomlyproducednegative\n",
      "mation[10].Intheseapproachesnegativesamplesaredrawnfrom samples.\n",
      "anon-parametricnoisedistribution.\n",
      "Forknowledgegraphsinparticulartherearemanydifferent 3.3 TypedSampling: T\n",
      "waystoproducenegativeinstancesbasedonthegraphstructure.\n",
      "KnowledgegraphssuchasFreeBaseandNELL[5]havestrongly\n",
      "Wepresentanoverviewoftechniquesforproducingnegativein-\n",
      "typed relations. For example, a relationborn in holds between\n",
      "stancesfromaknowledgegraph,andweevaluatetheirimpacton\n",
      "entitiesoftypepersonandentitiesoftypecity.Relevantnegative\n",
      "knowledgegraphcompletion,orlinkprediction.\n",
      "candidates(sourcesortargets)canbeminedbyconstrainingthe\n",
      "entitiestobelongtothesametypeasthatofthesource(ortarget).\n",
      "3.1 Randomsampling: R Thiscanhelpbypasstheproblemmentionedforthecorruptmethod,\n",
      "whensomerelationsinthedatasethaveveryfewinstances.\n",
      "The simplest form of sampling negative instances is to assume\n",
      "Foreveryrelationr :S →T,\n",
      "aclosedworldhypothesisandconsideranytriplethatdoesnot\n",
      "appearintheKGasanegativeinstance.Let\n",
      "ifSr,t ={s|shastypeSt}andTr,t ={t|t hastypeTt},\n",
      "K =K+={(si,ri,ti)|yi =1;i =1,2,···,N} w ini st th anS ct ea sn wd ilR lt coth ne sisd to om fa trin ipa len sdrangerespectivelyofr,negative\n",
      "denotethecompleteknowledgegraph,whereyi = 1represents\n",
      "(s(cid:48),r,t),s(cid:48)∈Sand(s,r,t(cid:48)),t(cid:48)∈T,\n",
      "thepresenceofatriple(si,ri,ti)(apositiveinstance)andyi =0\n",
      "suchthat\n",
      "representsabsence.Accordingtotheclosedworldassumption,the (s(cid:48),r,t)(cid:60)Rand(s,r,t(cid:48))(cid:60)K+.\n",
      "set Ko −fn =eg {a (st iiv,re is,K ti− )|yis i g =iv 0e ;n i =by 1,2,···,N} We Ifth anen ensa tim typ hle asn ms n ou rem thbe ar no of nn ee tg ya pt eiv (ee.gsa.m Alp bl ee rs tf Ero inm steth ines he astr ti yp ple es s.\n",
      "SincetheKGisincompletethissetcontainspositivetriplesnot\n",
      "presentintheKG.Furthermorethissetmightbeverylargebecause\n",
      "person,scientist),weincludeitinSr,t (orTr,t)ifoneofitstypes\n",
      "theincorrectfacts(O(N2))faroutnumberthecorrectones. m froa mtch Fe rs eeS bt a( so er rT et l) a. tW ioneo mb et ta ain dac ta ate reg lo er ay sed dat ia nf [o 9r ],th ae ndFr te he eb ea nse tid tyat ta ys pe et\n",
      "Asimplesolutiontothescalabilityproblemisrandomlysam-\n",
      "bymappingtheFreebaseentityidentifiertotheFreebasecategory.\n",
      "plingasmallnumberofsamplesfromK−.Givenapositivetriple\n",
      "This results in 101,353 instances of the category relation which\n",
      "(s,r,t)wegeneratens negativetriplesbysamplingns targeten-\n",
      "isusedinthetrainingstagetoproducetypednegativesamples.\n",
      "titiesfromtheentitysetE.Sincethesamplingisrandom,wedo\n",
      "DomainandrangetypesforFreebaserelationsareprovidedby\n",
      "notcheckwhetherthesampledtriplesarepresentinthetrainand\n",
      "Freebaseitself.Afewexamplesofentitiesandtypesareincluded\n",
      "developmentset,becausetheprobabilitytheyarepresentinK+is\n",
      "inTable1.\n",
      "negligible.Thesameprocedureisusedtogeneratenegativesource\n",
      "WedonotusetypedsamplingforWordnet.Thehypernym/hyponym\n",
      "entities.\n",
      "relationsarethedefactotyperelationsinWordNet,butarehier-\n",
      "Thenegativesproducedbyrandomsamplingmaynotbevery\n",
      "archicalratherthanamappingontoagivensmallsetofpredeter-\n",
      "useful: for the positive triple (Tom Cruise, starred in, Top Gun), minedtypesasinFreebase.\n",
      "negativetargetssuchasLondonorMount Everestseemirrelevant.\n",
      "Relevantnegativetargetsshouldincludeentitiesthataremovies, 3.4 RelationalSampling: REL\n",
      "suchasTerminator,Inception.Toobtainsuchnegativesitisneces-\n",
      "Althoughtypedorcorruptrelationsamplingcangeneraterelevant\n",
      "sarytoconstrainthesetofentitiesfromwhichsamplesaredrawn.\n",
      "negativecandidates,duetotheincompletenessoftheKG,someof\n",
      "Weexploresuchconstraintsinthefollowingsections.\n",
      "thesecandidatescouldbeunknownpositives. Ifweassumethat\n",
      "sourcetargetpairsparticipateinonlyonerelation,thensampling\n",
      "3.2 Corruptingpositiveinstances: C targets(sources)thatareconnectedtothecurrentsource(target)\n",
      "throughrelationsotherthanthecurrentrelationcanyieldtrue\n",
      "Weuseamethoddescribedin[24]thatgeneratesnegativeinstances\n",
      "negatives.Thisisacommonprocedureinmulti-classlearning.\n",
      "bycorruptingpositiveinstances:foreveryrelationr,Socheretal.\n",
      "[24]collectthesets\n",
      "Moreformally,forpositivetriple(s,r,t)thenegativecandidate\n",
      "S\n",
      "={s|(s,r,∗)∈K+}andT ={t|(∗,r,t)∈K+}, sourcesetisS− = {s|(s,r(cid:48),t(cid:48)), ∀r(cid:48) ∈ R,r(cid:48) (cid:44)r}andtargetset\n",
      "andproducesetsofcorruptedtriples T−={t|(s(cid:48),r(cid:48),t), ∀r(cid:48) ∈ R,r(cid:48)(cid:44)r}.Asbefore,aftercomputingS\n",
      "S(cid:48)={(s(cid:48),r,t)|s(cid:48)∈S,(s(cid:48),r,t)(cid:60)K+}and andTwefilteroutpositivetriplesfromtrainanddevelopmentset\n",
      "T(cid:48)={(s,r,t(cid:48))|t(cid:48)∈T,(s,r,t(cid:48))(cid:60)K+}. andsampleanumberns ofnegativesamples.\n",
      "DuringtrainingK+consistsoftriplesfromtraininganddevelop-\n",
      "3.5 NearestNeighborsampling: NN\n",
      "mentset.Wesampleanumberns ofnegativesamplesfromS(cid:48)and\n",
      "T(cid:48).Suchamethodproducesnegativeinstancesthatarecloserto Mostnegativesamplingmethodsgeneratenegativesamplesbased\n",
      "thepositiveonesthanthoseproducedthroughrandomsampling. oneithertheclosedworldassumption,functionalconstraintssuch\n",
      "Anissuewiththismethodisthatforrelationswithveryfew astypeconstraints,andtripleperturbation[19]. Weintroducea\n",
      "positiveinstances,therewillnotbealargeenoughpoolofsource negativesamplingmethodwhichusesapre-trainedembedding\n",
      "andtargetcandidatestocorruptthepositiveinstances. Thedata modelforgeneratingnegativesamples.Wenamethispre-trained\n",
      "analysis shows that this is an issue for the FB15k dataset. For embeddingmodelthe‘negativesamplingmodel’.Weusethenega-\n",
      "relationswherenotenoughcorruptednegativeinstancescanbe tivesamplingmodeltogeneratenegativetargets(sources)thatare\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "SourceType Source Relation Target TargetType\n",
      "film star wars episode IV produced by дeorдe lucas film producer\n",
      "person alexandre dumas people profession writer profession\n",
      "academic post professor profession people albert einstein award winner\n",
      "Table 1: Entity Types in Freebase: Examples of source and target entity types from Freebase used for generating negative\n",
      "samples.\n",
      "closetothepositivetarget(source)invectorspace.Thiswouldhelp neighborsampler,weusethenegativesamplingmodelforobtaining\n",
      "themodellearntodiscriminatebetweenpositivesandnegatives thepredictedvectorandentityembeddings.Thenegativesampling\n",
      "verysimilartothepositives. modelisnotupdated.\n",
      "Forapositivetriple(s,r,t),withxt thevectorrepresentationof Givenapositivetriple(s,r,t)weobtainthepredictedvector\n",
      "t obtainedfromthenegativesamplingmodel,thesetofnegative vt = x sT Wr wherexs, Wr are entity and relation embeddings\n",
      "samplesarethetopnsnearestneighborsofxt (thatarenotpositive) ofsources andrelationr obtainedusingthenegativesampling\n",
      "obtainedfromthenegativesamplingmodel.Thenegativesampling model. Notethatvt maynotbethesameasxt,thetargetentity\n",
      "modelmaybedifferentthanthemodelthatisbeingtrained.We representation.Thesetof(target)negativesamplesarethetopns\n",
      "usetheRescalmodeltrainedwith100typed(T)negativesamples nearestneighborsofthepredictedvectorvt.Algorithm2describes\n",
      "asanegativesamplingmodelfortheFB15Kdataset. Notethat theprocedureforasingletriple,inpracticeweuseabatchandthe\n",
      "theRescalmodelparametersarefrozen(notupdated),itissimply BallTreeisbuiltonlyonce.\n",
      "usedforgeneratingnegativesthatareusedfortraininganother\n",
      "model.Algorithm1describestheprocedureforasingletriple.In\n",
      "Algorithm2:NearMissSamplingusingRescalnegativesam-\n",
      "practiceweuseabatchoftriplesandthenearestneighborsearch\n",
      "pler\n",
      "isperformedusingtheBallTreealgorithmwhichisbuiltonlyonce\n",
      "sincethenegativesamplingmodelisnotupdated.\n",
      "Input :Triple(s,r,t),EntitySetE,Positivesourceandtargets\n",
      "Ps andPt,NegativeSamplingEmbeddingModelfn,\n",
      "Algorithm1:Algorithm1NearestNeighborSampling\n",
      "Numberofnegativesamplesns\n",
      "Output:Setofns negativesamples\n",
      "Input :Triple(s,r,t),EntitySetE,Positivesourceandtargets Ns ←E\\Ps, Nt ←E\\Pt;\n",
      "P Ns ua mn bd eP rt o, fN ne eg ga at ti iv ve eS sa am mp pl li en sg nE smbeddingModelfn, X Inns iti← alizf e(N ths e), KX bnt a← lltrf e( eN wt i) th;\n",
      "Xs andXt ;\n",
      "XO IN nns su itt i← ←p alu izEt f e: (\\S N tP he s st e, )o, KN Xf t bn nt as ← ← lln te E rg f e\\a ( ePt Ni wtv t;e i) th;sa Xm spl ae ns\n",
      "dXt ;\n",
      "v Sxs s ←← ← nx ef sT an r( W es s) r t,,x nvt et i← g← hbWf on rr( sr (x v), t sW,; nr um←n =nf sn );(r)n ;\n",
      "xt ← fn(t);\n",
      "n n T re← turn nea Sr,Test neighbors(vt,num=ns);\n",
      "xs ← fn(s);\n",
      "S ←nearest neighbors(xs,num=ns);\n",
      "T ←nearest neighbors(xt,num=ns); Likenearestneighborsampling,nearmisssamplingisalsocom-\n",
      "returnS,T putationallyexpensive,soinsteadoflearningfromrandomlyini-\n",
      "tializedparameterswetuneapre-trainedmodelfor5epochs.\n",
      "Nearestneighborsamplingiscomputationallyexpensivecom- 4 DATA\n",
      "paredtothemethodsdiscussedinprevioussections.Thisisbecause\n",
      "WeevaluatetheimpactofnegativesamplingontheFreebasedataset\n",
      "asearchoverallentitiesneedstobeperformedforsourceandtarget\n",
      "(FB15k)andontheWordNetdataset(WN18)introducedby[4].\n",
      "entitiesforeverytriple. Thereforeweuseamodeltrainedusing\n",
      "Theyareverydifferentincoverage–FB15kcontainsmostlynamed\n",
      "typednegativesamplingmethodsforFreebaseandcorruptedsam-\n",
      "entitiesconnectedthroughstronglytypedrelations,whileWN18\n",
      "plingforWordnettoinitializetheparametersandthenfinetune\n",
      "contains mostly common nouns connected through lexical and\n",
      "themodelusingnearestneighborsamplingfor5epochs.\n",
      "semanticrelations.DatasetdetailsareincludedinTable2.\n",
      "3.6 NearMisssampling: nmiss\n",
      "4.1 FB15k\n",
      "Thenearestneighborsamplergeneratesnegativesthataresimilar\n",
      "FB15k[4]consistsofapproximately15,000entitiesand1345rela-\n",
      "topositivesinvectorspace.Someofthosenegativesmayberanked\n",
      "tions.Weusethesplitsuppliedbythedataset:483,142train,50,000\n",
      "higherthanthepositives.Exposingsuchhighlyrankednegatives\n",
      "validationand59,071positivetestinstances.\n",
      "totheclassifiercanhelpthemodellearnabetterdiscriminator.We\n",
      "namethissettingasnearmisssampling,becausethegenerated\n",
      "negativesaretoprankedcandidateswhichmakesitdifficultfor Dataset |E| |R| Training Development Test\n",
      "themodeltoclassifythemasnegatives(nearmisses).Togenerate FB15K 14,951 1345 483,142 50000 59071\n",
      "highlyrankednegatives, wecollectthetopns targets(sources) WN18 40,943 18 141,442 5000 5000\n",
      "closest to the predicted target (source) vector. Like the nearest Table 2: Dataset Details: |E| = # of entities, |R| = # of rela-\n",
      "tions.\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA\n",
      "Figure1:FB15kdatasetfrequencystatistics Figure2:WordNet18datasetfrequencystatistics\n",
      "Thetrainingdatacontainsrelationsthathavehighvariation Fromagraphstructurepointofview,WN18nodeshavelow\n",
      "inthenumberofinstances–39%oftherelationshaveatmost connectivity–theaveragedegreeontheentiredatasetisapprox-\n",
      "10instances,whilethemostfrequentrelation2hasalmost16000. imately 1.2, and on the training data alone approximately 3.45.\n",
      "Thisdisparityisalsoreflectedinthedistributionofnodedegrees Thistranslatesintosparseradjacencymatricesforfactorization,\n",
      "– 12% of the entities have degree equal or less than 10 (appear comparedtoFreebase.\n",
      "inatmost10instances). TheaveragedegreeofanodeinFB15k WordNetcontainslexicalandsemanticrelations. Lexicalrela-\n",
      "isapproximately13.2overall,and32.4onthetrainingdata. The tions–suchasderivationally related formconnectlemmasfrom\n",
      "distributionofrelationsandnodedegreesispresentedinFigure1. differentpartsofspeechthataremorphologicallyconnected.The\n",
      "ThetypeofrelationsincludedinFreebaseconnectnamedentities. semanticrelationscoveris arelations(hypernym/hyponym,in-\n",
      "They are extrinsic relations, in that they do not hold based on stancehypernym/hyponym),threetypesofpart ofrelations(mem-\n",
      "theintrinsicpropertiesoftheconnectedentities,butaredueto ber,substanceandpart). ThesemanticrelationsinWordNetare\n",
      "externalcircumstances.Forexample,thepeople professionrelation intrinsic,astheyreflectorarisefromintrinsicpropertiesofthe\n",
      "connecting people and their professions are not determined by connectedentities.Forexample,acatis aanimal,andcathas part\n",
      "intrinsicpropertiesofpeopleandprofessions.RelationsinFreeBase pawsnotbecauseofexternalcircumstances,butbecauseofwhata\n",
      "arestronglytyped–thedomainandrangeoftherelationsaretypes, catis.ComparedtoFreeBase,WordNetrelationsarenottyped–\n",
      "e.g.thecountry capitalrelationconnectscountriesandcities. thereisnocleardomainandrangefortheWordNetrelations.\n",
      "4.2 WN18 5 EXPERIMENTS\n",
      "ThisdatasetconsistsofasubsetofrelationsfromtheWordNetlexi- 5.1 Implementation\n",
      "caldatabase3,splitintotraining,developmentandtesting:141442/\n",
      "ForfaircomparisonwereimplementedRescal,TransE,DistMult,\n",
      "5000/5000. Thereare18relations. Thereislessvariationinthe\n",
      "ComplExusingPyTorch,andtestedthemusingthesameexperi-\n",
      "numberofinstancesperrelationcomparedtotheFB15k,ascanbe\n",
      "mentalsetting:sameloss(max-marginloss),embeddingsize(100),\n",
      "seeninFigure2.Thereisonerelationwithlessthan100instances\n",
      "anddata.WeusetheAdam[13]SGDoptimizerfortrainingbecause\n",
      "(similar to),whilethemostfrequentrelations(hypernym,hyponym)\n",
      "itaddressestheproblemofdecreasinglearningrateinAdaGrad.\n",
      "haveapproximately35,000.\n",
      "Weensurethatentityembeddingsforallthemodelshaveunitnorm.\n",
      "2/award/awardnominee/awardnominations./award/awardnomination/awardnominee\n",
      "Weperformedexhaustiverandomizedgridsearch[2]fortheL2\n",
      "3https://wordnet.princeton.edu/ regularizeronthevalidationsetforallmodelsandwetunedthe\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "Model lr λ 5.4 Results\n",
      "Freebase WepresenttheresultsoflinkpredictiononFB15kandWN18in\n",
      "ComplEx 0.001 1.31E-06 termsofMRRinFigures3and4forns ∈{1,2,5,10,20,50,100}for\n",
      "DistMult 0.001 4.93E-06 eachpositiveinstance.\n",
      "Rescal 0.001 0.0002084 Theresultsshowthatthedifferentsamplingmethodshavedif-\n",
      "TransE 0.001 0.00024036 ferenteffectsonthetwodatasets. Sincelinkpredictionisbased\n",
      "Wordnet exclusivelyontheembeddingofthegraphs,differencesinperfor-\n",
      "ComplEx(ns ∈{1,2,5}) 0.005 2.82E-05 mancearecausedbythedifferentstructure(e.g. differentnode\n",
      "ComplEx(ns >=10) 0.01 2.82E-05 degreeswhicharereflectedinthesparsityoftherelationadjacency\n",
      "DistMult(ns ∈{1,2,5})<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,    521,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Bhushan Kotnis', 'Vivi Nastase']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Figures3and4forns ∈{1,2,5,10,20,50,100}for\n",
      "DistMult 0.001 4.93E-06 eachpositiveinstance.\n",
      "Rescal 0.001 0.0002084 Theresultsshowthatthedifferentsamplingmethodshavedif-\n",
      "TransE 0.001 0.00024036 ferenteffectsonthetwodatasets. Sincelinkpredictionisbased\n",
      "Wordnet exclusivelyontheembeddingofthegraphs,differencesinperfor-\n",
      "ComplEx(ns ∈{1,2,5}) 0.005 2.82E-05 mancearecausedbythedifferentstructure(e.g. differentnode\n",
      "ComplEx(ns >=10) 0.01 2.82E-05 degreeswhicharereflectedinthesparsityoftherelationadjacency\n",
      "DistMult(ns ∈{1,2,5}) 0.005 3.12E-06 matrices) and the different nature of the relations – typed and\n",
      "DistMult(ns >=10) 0.01 3.12E-06 extrinsicinFB15k,nottypedand(mostly)intrinsicinWordNet.\n",
      "Rescal(ns ∈{1,2,5}) 0.005 7.48E-05 Assuggestedbyworkonlearningstatisticalmodelsthrough\n",
      "Rescal(ns >=10) 0.01 7.48E-05 noisecontrastiveestimation[11], selectingdifficultnegativein-\n",
      "TransE(ns ∈{1,2,5}) 0.005 0.0001863777692 stancesproducesbettermodels:nearmisssamplingleadstobetter\n",
      "TransE(ns >=10) 0.01 0.0001863777692 resultsonFB15kformostembeddingsmethods. Thereasonem-\n",
      "Table3:Parametervalues beddingbasedsamplingworkswellonFreeBaseisprimarilybe-\n",
      "causethenegativesamplesgeneratedbythepre-trainedembedding\n",
      "modelareveryclosetothediscriminatorboundary.Forexample,\n",
      "trainingdurationusingearlystopping.Thelearningrate(lr)and\n",
      "thenearmisssamplinginvolvesgeneratingnegativetargetentities\n",
      "λ(theL2 normcoefficient)arepresentedinTable3. Thecodeis\n",
      "thatarehighlyrankedbytheembeddingmodel. Theseentities\n",
      "availableinGithub4.\n",
      "arelikelytobehighlyrankedbythemodelthatisbeingtrained.\n",
      "ThedifferentmethodsfornegativesamplingdescribedinSection\n",
      "Thereforeprovidingtheseentitiesasnegativesallowsthesystem\n",
      "3wereusedtoproducenegativeinstancesfortraining.InFB15K\n",
      "tolearnamodelthatranksthembelowthepositivetargetusing\n",
      "somerelationsdonothaveenoughsourcesortargetstogenerate\n",
      "themax-marginloss.Notethatthesamplesgeneratedbytheem-\n",
      "negativetriplesbycorruptingpositivetriples. Ifthenumberof\n",
      "beddingmodelareclosetoeachotherinvectorspaceduetothe\n",
      "generatedtriplesarelessthantherequired(ns),wecompletethe\n",
      "abilityoftheembeddingmodeltoclusterentities.Thereforealmost\n",
      "setofnegativesampleswithrandomlygeneratedtriples.\n",
      "allthegeneratednegativesamplesareclosetothediscriminator\n",
      "Forthenearestneighborandnearmisssettings,weusedthe\n",
      "boundary. Wetreatedthenegativesamplingmodel(pre-trained\n",
      "bestperformingmodelforinitializingtheparameters,andusedthe\n",
      "model)asahyperparameter. WefoundthattheRESCALmodel\n",
      "Rescalmodeltunedontypednegativesamples(100negativesam-\n",
      "workedbest.Wespeculatethatthismightbeduetothesuperior\n",
      "ples)asthenegativesamplingmodelforFB15KandRescaltrained\n",
      "abilityofRESCALmodeltoclustersimilarentities.\n",
      "bycorruptingpositivesamples(100negativesamples)forWN18.\n",
      "Corruptingpositiveinstances,themethodmostfrequentlyused\n",
      "forlinkprediction,istheleastcompetitiveonFB15k,butfitsWord-\n",
      "5.2 Testdata Netwell,particularlyforRescal. DistMultisnotverysensitive\n",
      "The test data is the same across all experiments. The negative tothetypeofnegativesamplingonWN18,exceptforthenearest\n",
      "instancesfor thetestdataweregeneratedasdescribedin[4]– neighbormethodwithwhichitdoesnotperformwell.\n",
      "corruptingpositiveinstancesusingallentitiesofthedictionary Tounderstandwhycorruptingpositiveinstancesworksbeston\n",
      "insteadofthecorrectsourceandtarget,withoutsampling. WordNet,welookatthedataandthegraphstatistics.TheWN18\n",
      "Alsofollowingtheprocedureof[4],weusethefilteredsetting: datasethas18relationswhilewithFB15khasabout1495relations.\n",
      "thenegativesamplesaddedtothetrainingdataarefilteredwith DuetoperrelationdatasparsityinFB15K,seeFig.1and2,negative\n",
      "respecttothetestdatatoavoid(known)falsenegativesintraining. samplingusingcorruptedtriplesworkspoorlyforFB15K,asitoften\n",
      "hastofallbackonrandomsamplingwhennotenoughpositive\n",
      "5.3 Evaluationmetrics instanceswithasharedsource/targetareavailablefor”corruption”.\n",
      "Corruptsamplingworksbetterinaninstancerichenvironment.\n",
      "Forevaluationweusethemeanreciprocalrank(MRR)andhits@K\n",
      "Apartfromdatasparsity,thenatureofWordNetandFreebase\n",
      "thatarecommonlyusedforlinkprediction.\n",
      "relationsmayalsoaffecttheperformanceofnegativesampling\n",
      "ForalistofNanswersforlinkprediction,themeanreciprocal\n",
      "methods.WordNetrelationshaveopenendedrangesanddomains\n",
      "rank(MRR)andhits@karedefinedas:\n",
      "whileFreebaserelationshavetypedrangesanddomains.Embed-\n",
      "dingbasedmethods,suchasthenearmisssamplingmethodwe\n",
      "MRR= 1 (cid:205)N 1 hits@K = |{i|ranki<K}| implemented,workonthebasisofclusteringsimilarentities,and\n",
      "N i=1ranki N donotfunctionwellforWordNetwheretherelationsdonothave\n",
      "whereranki istherankofthepositiveinstanceipredictedbythe\n",
      "domainsandrangesthatreflectconceptual/semanticclusters.\n",
      "modelwithrespecttothenegativesamples. ForFB15kweuse\n",
      "Wehavediscussedthedifferencesinperformanceofsampling\n",
      "hits@10,forWN18,hits@1.\n",
      "methodsforthetwoKGsused.Therearealsodifferenceswithre-\n",
      "specttothelinkpredictionmethods.Randomsamplingworksbest\n",
      "forTransE.Thismaybesurprisingatfirst,butisunderstandable\n",
      "4https://github.com/bhushank/kge-rl\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA\n",
      "complex MRR distmult MRR rescal MRR transE MRR\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "complex hits@10 distmult hits@10 rescal hits@10 transE hits@10\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0 100 101 102 100 101 102 100 101 102 100 101 102\n",
      "corrupt nmiss nn random relational typed\n",
      "Figure3:LinkpredictiononFB15k,evaluatedintermsofMRRforns ∈{1,2,5,10,20,50,100}onalogarithmicscale.\n",
      "complex MRR distmult MRR rescal MRR transE MRR\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "complex hits@1 distmult hits@1 rescal hits@1 transE hits@1\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0 100 101 102 100 101 102 100 101 102 100 101 102\n",
      "corrupt nmiss nn random relational\n",
      "Figure4:LinkpredictiononWN18,evaluatedintermsofMRRforns ∈{1,2,5,10,20,50,100}onalogarithmicscale.\n",
      "consideringthatthetheoreticalmodelbehindTransEassumes1:1 Yangetal.[29] Negativesampling\n",
      "relations.Providingitwithnegativeentitiesthatareclose(using MRR HITS@10 neg.sampling MRR HITS@10\n",
      "typed,corruptedorembeddingmethods)doesnotresultinimprove- FB15k\n",
      "mentbecausethenegativeentitiesgeneratedusingtyped,corrupt DistMult 0.35 57.7 nearmiss 0.46 70.64\n",
      "orembeddingsareclosetoeachotherinvectorspaceandthemodel Rescal 0.31 51.9 nearmiss 0.42 64.34\n",
      "willultimatelybeunabletodistinguishbetweenthem.Thisisnot TransE 0.32 53.9 nearmiss 0.37 62.97\n",
      "thecasewhendoingrandomsampling,whenTransEisnotper- WN18\n",
      "turbedbytooclosenegatives.ComplExandDistMultperformwell DistMult 0.83 94.2 corrupt 0.82 94.06\n",
      "withbothnearmissandnearestneighboursamplingonFB15k. Rescal 0.89 92.8 corrupt 0.92 93.91\n",
      "Rescalperformsbestwithnearmisssamplingonthisdata,and TransE 0.38 90.9 corrupt 0.40 86.98\n",
      "withcorruptingpositivesamplesforWordNet.Formiddle-range Table4:SotAresultsusingamax-marginlossfunctionand\n",
      "ns relationalsamplingperformsbest. corruptingpositiveinstancesvs. thebestperformingnega-\n",
      "AsdescribedinSection4,thetrainingdataforbothmethods tivesampling.\n",
      "variesquiteabitintermsofthefrequencyoftherelationscovered.\n",
      "Freebaseismoreextreme,inthatapproximately39%oftherelations\n",
      "haveatmost10positiveinstancestotrainon. Weanalyzedthe entityrepresentations,wecannotethattheperformanceonlink\n",
      "effectsofnegativesamplingondifferentslicesofthedata,splitby predictionfortheserelationswithveryfewinstancesvariesmuch\n",
      "theorderofmagnitude(oom)ofthefrequencyoftherelationsin withthenegativesamplingmethod.Overall,thebestresultsareob-\n",
      "thetrainingdata.Moreprecisely,wegrouprelationsintosetsGn tainedwiththesamesamplingmethodasfortheirmorepopulous\n",
      "indexedbytheorderofmagnituden: counterparts,butforspecificrangesofthenumberofgenerated\n",
      "Gn ={r|10n < freq(n,trainingdata)<=10(n+1)}5. negative samples other methods would work best (e.g. nearest\n",
      "Freebasehas5slices(0..4)andWordNet4(1..4).Theresults(as neighborandrelationalsamplingforWordNetdata).\n",
      "MRRandhits@K)forslicesrepresentingrelationswithOOM2 Thereportedexperimentswereperformedusingthemaxmargin\n",
      "ormorecloselymirrortheoverallresults.Theresultsforthelow lossfunction.InTable4weincludethestateoftheartresultson\n",
      "frequencyrelationsareshowninFigures5and6.Thehits@Kscore DistMult, Rescal andTransEobtainedwith amax marginloss\n",
      "aresimilartotheMRRones,sowedonotincludethem6. functionreportedin[29]andcorruptingtripes,tocomparewith\n",
      "Whiletheresultsonthelowfrequencyrelationscannotbeana- theresultsobtainedwiththebestnegativesamplingmethodforthe\n",
      "lyzedseparatelyfromtheotherrelationsbecausetheembeddings dataset.Slightdifferencesinthelearningrateandλaccountforthe\n",
      "processreliesonprocessingandinducingjointlyallrelationand differencesinperformancewhenusingcorruptpositiveinstances\n",
      "asnegativesamplesfortheWN18dataset.\n",
      "Recently, [27] used the log-likelihood objective, which leads\n",
      "5WeincluderelationsthathaveonlyoneinstanceinG0.\n",
      "6Thecompletesetofplotsaccompaniesthecodeandwillbeshared. toimprovementsoverthepublishedresultsforthemethodsthey\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "corrupt nn relational corrupt nn relational\n",
      "nmiss random typed nmiss random\n",
      "complex complex complex complex\n",
      "0.60 0.60\n",
      "0.55 0.55 1.0 1.0\n",
      "0.50 0.50\n",
      "0.8 0.8\n",
      "0.45 0.45\n",
      "0.40 0.40 0.6 0.6\n",
      "0.35 0.35\n",
      "0.4 0.4\n",
      "0.30 0.30\n",
      "0.25 0.25 0.2 0.2\n",
      "0.20 0.20\n",
      "distmult distmult distmult distmult\n",
      "0.6 0.6 1.1 1.1\n",
      "0.5 0.5 1.0 1.0\n",
      "0.4 0.4 0.9 0.9\n",
      "0.3 0.3 0.8 0.8\n",
      "0.2 0.2 0.7 0.7\n",
      "0.1 0.1 0.6 0.6\n",
      "rescal rescal rescal rescal\n",
      "0.6 0.6\n",
      "1.0 1.0\n",
      "0.5 0.5\n",
      "0.8 0.8\n",
      "0.4 0.4\n",
      "0.6 0.6\n",
      "0.3 0.3\n",
      "0.4 0.4\n",
      "0.2 0.2\n",
      "0.2 0.2\n",
      "0.1 0.1\n",
      "transE transE transE transE\n",
      "0.6 0.6 0.6 0.6\n",
      "0.5 0.5 0.5 0.5\n",
      "0.4 0.4 0.4 0.4\n",
      "0.3 0.3 0.3 0.3\n",
      "0.2 0.2 0.2 0.2\n",
      "0.1 0.1 0.1 0.1\n",
      "0.0 0.0 0.0 0.0\n",
      "100 101 102 100 101 102 100 101 102 100 101 102\n",
      "Figure5: ResultsonrelationswithOOM0and1inFB15k Figure6: ResultsonrelationswithOOM1and2inWN18\n",
      "(MRRs) (MRRs)\n",
      "compared(TransE,ComplEx,HolE,DistMult).Weplantoanalyze\n",
      "sampling worked best for Freebase withmost of the graphem-\n",
      "thenegativesamplingmethodswhileusingthisnewlossfunction.\n",
      "beddingmethods,whilecorruptingpositivetriplesleadstobest\n",
      "resultsonWordNet. Thenewlyproposednearmissandnearest\n",
      "6 CONCLUSION\n",
      "neighbornegativesamplingworkbestforFreebase,forthreeoutof\n",
      "Wereportananalysisoftheimpactofsixnegativesamplingmeth- thefourgraphembeddingsmethods.Fromanalysisofdatasets,we\n",
      "odsontheperformanceoflinkpredictioninknowledgegraphs,for furtherconcludedthatembeddingbasednegativesamplingisvery\n",
      "fourmethodsforgraphembedding–ComplEx,DistMult,Rescal, usefulforcombatingdatasparsity,whilecorruptsamplingworks\n",
      "TransE.Theanalysisisperformedwithrespecttotwodatasets–a bestinthedatarichscenario.Thenatureoftherelationsinthese\n",
      "subsetofFreebase(FB15k)andasubsetofWordNet(WN18)–that graphs(typedwithrespecttotheirdomainandrangevs.open)as\n",
      "areverydifferentinthetypeofknowledgetheycover. wellasthestatisticsoftheknowledgegraph(numberofpositive\n",
      "Theresultsindicatethatdifferentapproachestonegativesam- instancesperrelation)explainthedifferentbehaviourwithrespect\n",
      "plingworkbestforthetworesources. Theproposednearmiss tonegativesampling.\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA\n",
      "REFERENCES\n",
      "[22] MaximilianNickel,VolkerTresp,andHans-PeterKriegel.2012. Factorizing\n",
      "[1] YoshuaBengioandJean-Se´bastienSene´cal.2008.Adaptiveimportancesampling YAGO:ScalableMachineLearningforLinkedData.InProceedingsofthe21st\n",
      "toacceleratetrainingofaneuralprobabilisticlanguagemodel.IEEETransactions InternationalConferenceonWorldWideWeb(WWW’12).ACM,NewYork,NY,\n",
      "onNeuralNetworks4,19(2008),713–722. USA,271–280. https://doi.org/10.1145/2187836.2187874\n",
      "[2] JamesBergstraandYoshuaBengio.2012.RandomSearchforHyper-parameter [23] NoahASmithandJasonEisner.2005. Contrastiveestimation:Traininglog-\n",
      "Optimization. J.Mach.Learn.Res.13(Feb.2012),281–305. http://dl.acm.org/ linearmodelsonunlabeleddata.InProceedingsofthe43rdAnnualMeeting\n",
      "citation.cfm?id=2188385.2188395 onAssociationforComputationalLinguistics.AssociationforComputational\n",
      "[3] KurtBollacker,ColinEvans,PraveenParitosh,TimSturge,andJamieTaylor.2008. Linguistics,354–362.\n",
      "Freebase:ACollaborativelyCreatedGraphDatabaseforStructuringHuman [24] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng.\n",
      "Knowledge.InProceedingsofthe2008ACMSIGMODInternationalConference 2013. Reasoning With Neural Tensor Networks for Knowledge Base\n",
      "onManagementofData(SIGMOD’08).ACM,NewYork,NY,USA,1247–1250. Completion. In Advances in Neural Information Processing Systems 26,\n",
      "https://doi.org/10.1145/1376616.1376746 C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Wein-\n",
      "[4] AntoineBordes,NicolasUsunier,AlbertoGarcia-Duran,JasonWeston,and berger(Eds.).CurranAssociates,Inc.,926–934. http://papers.nips.cc/paper/\n",
      "Oksana Yakhnenko. 2013. Translating Embeddings for Modeling Multi- 5028-reasoning-with-neural-tensor-networks-for-knowledge-base-completion.\n",
      "relational Data. In Advances in Neural Information Processing Systems 26, pdf\n",
      "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Wein- [25] FabianM.Suchanek,GjergjiKasneci,andGerhardWeikum.2007. YAGO:A\n",
      "berger(Eds.).CurranAssociates,Inc.,2787–2795.http://papers.nips.cc/paper/ CoreofSemanticKnowledge.InProceedingsofthe16thInternationalConference\n",
      "5071-translating-embeddings-for-modeling-multi-relational-data.pdf onWorldWideWeb(WWW’07).ACM,NewYork,NY,USA,697–706. https:\n",
      "[5] AndrewCarlson,JustinBetteridge,BryanKisiel,BurrSettles,EstevamR.Hr- //doi.org/10.1145/1242572.1242667\n",
      "uschka,andTomM.Mitchell.2010.TowardanArchitectureforNever-Ending [26] KristinaToutanova,VictoriaLin,Wen-tauYih,HoifungPoon,andChrisQuirk.\n",
      "LanguageLearning.InAAAI. 2016.CompositionalLearningofEmbeddingsforRelationPathsinKnowledge\n",
      "[6] RajarshiDas,ArvindNeelakantan,DavidBelanger,andAndrewMcCallum.2016. BaseandText.InProceedingsofthe54thAnnualMeetingoftheAssociationfor\n",
      "ChainsofReasoningoverEntities,Relations,andTextusingRecurrentNeural ComputationalLinguistics(Volume1:LongPapers).AssociationforComputational\n",
      "Networks.arXivpreprintarXiv:1607.01426(2016). Linguistics,1434–1444.https://doi.org/10.18653/v1/P16-1136\n",
      "[7] XinDong,EvgeniyGabrilovich,GeremyHeitz,WilkoHorn,NiLao,Kevin [27] The´oTrouillon,ChristopherRDance,JohannesWelbl,SebastianRiedel,E´ric\n",
      "Murphy,ThomasStrohmann,ShaohuaSun,andWeiZhang.2014.Knowledge Gaussier,andGuillaumeBouchard.2017. KnowledgeGraphCompletionvia\n",
      "vault:aweb-scaleapproachtoprobabilisticknowledgefusion.InKDD. ComplexTensorFactorization.arXivpreprintarXiv:1702.06879(2017).\n",
      "[8] XinLunaDong,EvgeniyGabrilovich,KevinMurphy,VanDang,WilkoHorn, [28] RobertWest,EvgeniyGabrilovich,KevinMurphy,ShaohuaSun,RahulGupta,\n",
      "CamilloLugaresi,ShaohuaSun,andWeiZhang.2015.Knowledge-basedTrust: andDekangLin.2014.KnowledgeBaseCompletionviaSearch-basedQuestion\n",
      "EstimatingtheTrustworthinessofWebSources.Proc.VLDBEndow.8,9(May Answering.InProceedingsofthe23rdInternationalConferenceonWorldWide\n",
      "2015),938–949. https://doi.org/10.14778/2777598.2777603 Web(WWW’14).ACM,NewYork,NY,USA,515–526. https://doi.org/10.1145/\n",
      "[9] MattGardnerandTomMitchell.2015. EfficientandExpressiveKnowledge 2566486.2568032\n",
      "BaseCompletionUsingSubgraphFeatureExtraction.InProceedingsofthe2015 [29] BishanYang,Wen-tauYih,XiaodongHe,JianfengGao,andLiDeng.2015.\n",
      "ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.Associationfor Embeddingentitiesandrelationsforlearningandinferenceinknowledgebases.\n",
      "ComputationalLinguistics,1488–1498.https://doi.org/10.18653/v1/D15-1173 InProceedingsofthe2015InternationalConferenceonRepresentationLearning.\n",
      "[10] MichaelGutmannandAapoHyvarinen.2012.Noise-contrastiveestimationof\n",
      "unnormalizedstatisticalmod-els,withapplicationstonaturalimagestatistics.\n",
      "TheJournalofMachineLearningResearch13(2012),307fi?!361.\n",
      "[11] MichaelGutmannandAapoHyva¨rinen.2010. Noise-contrastiveestimation:\n",
      "Anewestimationprincipleforunnormalizedstatisticalmodels.InProceedings\n",
      "oftheThirteenthInternationalConferenceonArtificialIntelligenceandStatistics.\n",
      "297–304.\n",
      "[12] KelvinGuu,JohnMiller,andPercyLiang.2015.TraversingKnowledgeGraphs\n",
      "inVectorSpace.InProceedingsofthe2015ConferenceonEmpiricalMethodsin\n",
      "NaturalLanguageProcessing.AssociationforComputationalLinguistics,318–327.\n",
      "https://doi.org/10.18653/v1/D15-1038\n",
      "[13] DiederikKingmaandJimmyBa.2014.Adam:Amethodforstochasticoptimiza-\n",
      "tion.arXivpreprintarXiv:1412.6980(2014).\n",
      "[14] YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,andXuanZhu.2015.Learning\n",
      "EntityandRelationEmbeddingsforKnowledgeGraphCompletion.InProceed-\n",
      "ingsoftheTwenty-NinthAAAIConferenceonArtificialIntelligence(AAAI’15).\n",
      "AAAIPress,2181–2187. http://dl.acm.org/citation.cfm?id=2886521.2886624\n",
      "[15] AlexanderMiller,AdamFisch,JesseDodge,Amir-HosseinKarimi,Antoine\n",
      "Bordes,andJasonWeston.2016. Key-ValueMemoryNetworksforDirectly\n",
      "ReadingDocuments.InProceedingsofthe2016ConferenceonEmpiricalMethods\n",
      "inNaturalLanguageProcessing.AssociationforComputationalLinguistics,1400–\n",
      "1409.http://aclweb.org/anthology/D16-1147\n",
      "[16] AndriyMnihandYeeWhyeTeh.2012.Afastandsimplealgorithmfortraining\n",
      "neuralprobabilisticlanguagemodels.InProc.ofICML.\n",
      "[17] M.Moya,M.Koch,andLHostetler.1993. One-classclassifiernetworksfor\n",
      "targetrecognitionapplications.InProc.oftheWorldCongressonNeuralNetworks.\n",
      "InternationalNeuralNetworkSociety,INNS,Portland,OR.,797fi?!801.\n",
      "[18] ArvindNeelakantan, BenjaminRoth, andAndrewMcCallum.2015. Com-\n",
      "positionalVectorSpaceModelsforKnowledgeBaseCompletion.InProceed-\n",
      "ingsofthe53rdAnnualMeetingoftheAssociationforComputationalLinguis-\n",
      "ticsandthe7thInternationalJointConferenceonNaturalLanguageProcessing\n",
      "(Volume1:LongPapers).AssociationforComputationalLinguistics,156–166.\n",
      "https://doi.org/10.3115/v1/P15-1016\n",
      "[19] M.Nickel,K.Murphy,V.Tresp,andE.Gabrilovich.2016.AReviewofRelational\n",
      "MachineLearningforKnowledgeGraphs.Proc.IEEE104,1(Jan2016),11–33.\n",
      "https://doi.org/10.1109/JPROC.2015.2483592\n",
      "[20] MaximilianNickel,LorenzoRosasco,andTomasoPoggio.2016. Holographic\n",
      "EmbeddingsofKnowledgeGraphs.InProceedingsoftheThirtiethAAAIConfer-\n",
      "enceonArtificialIntelligence(AAAI’16).AAAIPress,1955–1961.http://dl.acm.\n",
      "org/citation.cfm?id=3016100.3016172\n",
      "[21] MaximilianNickel,VolkerTresp,andHans-PeterKriegel.2011.AThree-Way\n",
      "ModelforCollectiveLearningonMulti-RelationalData.InICML.\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "PerformanceanalysisonFreebase(FB15k).\n",
      "complex MRR distmult MRR rescal MRR transE MRR\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "complex hits@10 distmult hits@10 rescal hits@10 transE hits@10\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "100 101 102 100 101 102 100 101 102 100 101 102\n",
      "corrupt nmiss nn random relational typed\n",
      "Figure7:PerformanceonFB15kintermsofMRRandHits@10\n",
      "PerformanceanalysisonWordNet(WN18).\n",
      "complex MRR distmult MRR rescal MRR transE MRR\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "complex hits@1 distmult hits@1 rescal hits@1 transE hits@1\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "100 101 102 100 101 102 100 101 102 100 101 102\n",
      "corrupt nmiss nn random relational\n",
      "Figure8:PerformanceonWN18intermsofMRRandHits@1\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "xelpmoc\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "tlumtsid\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "lacser\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "Esnart\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "depyt\n",
      "lanoitaler\n",
      "modnar\n",
      "nn\n",
      "ssimn\n",
      "tpurroc\n",
      "Figure9:PerformanceonFB15kintermsofMRRforrelationswithdifferentordersofmagnitude\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "xelpmoc\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "tlumtsid\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "lacser\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "Esnart\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "depyt\n",
      "lanoitaler\n",
      "modnar\n",
      "nn\n",
      "ssimn\n",
      "tpurroc\n",
      "Figure10:PerformanceonFB15kintermsofHits@10forrelationswithdifferentordersofmagnitude\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb201<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,    521,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Bhushan Kotnis', 'Vivi Nastase']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.0\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "Esnart\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "depyt\n",
      "lanoitaler\n",
      "modnar\n",
      "nn\n",
      "ssimn\n",
      "tpurroc\n",
      "Figure10:PerformanceonFB15kintermsofHits@10forrelationswithdifferentordersofmagnitude\n",
      "AnalysisoftheImpactofNegativeSampling\n",
      "onLinkPredictioninKnowledgeGraphs KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "xelpmoc\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "tlumtsid\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "lacser\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "Esnart\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "lanoitaler\n",
      "modnar\n",
      "nn\n",
      "ssimn\n",
      "tpurroc\n",
      "Figure11:PerformanceonWN18intermsofMRRforrelationswithdifferentordersofmagnitude\n",
      "KBCOM’18,Feb2018,LosAngeles,CaliforniaUSA BhushanKotnisandViviNastase\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "xelpmoc\n",
      "xelpmoc\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "tlumtsid\n",
      "tlumtsid\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "lacser\n",
      "lacser\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "0.4\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.3\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.2\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "0.1\n",
      "moo\n",
      "--\n",
      "Esnart\n",
      "Esnart\n",
      "0.1 8.0 6.0 4.0 2.0 0.0\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "201\n",
      "101\n",
      "001\n",
      "lanoitaler\n",
      "modnar\n",
      "nn\n",
      "ssimn\n",
      "tpurroc\n",
      "Figure12:PerformanceonWN18intermsofHits@1forrelationswithdifferentordersofmagnitude<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,    521,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Bhushan Kotnis', 'Vivi Nastase']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: DeepPath: A Reinforcement Learning Method for\n",
      "Knowledge Graph Reasoning\n",
      "WenhanXiong and ThienHoang and WilliamYangWang\n",
      "DepartmentofComputerScience\n",
      "UniversityofCalifornia,SantaBarbara\n",
      "SantaBarbara,CA93106USA\n",
      "{xwhan,william}@cs.ucsb.edu, thienhoang@umail.ucsb.edu\n",
      "Abstract of learning explicit inference formulas, given a\n",
      "large KG. For example, if the KG includes the\n",
      "Westudytheproblemoflearningtoreason\n",
      "beliefs such as Neymar plays for Barcelona, and\n",
      "in large scale knowledge graphs (KGs).\n",
      "Barcelona are in the La Liga league, then ma-\n",
      "More specifically, we describe a novel re-\n",
      "chines should be able to learn the following for-\n",
      "inforcementlearningframeworkforlearn-\n",
      "mula: playerPlaysForTeam(P,T) ∧ teamPlaysIn-\n",
      "ing multi-hop relational paths: we use a\n",
      "League(T,L) ⇒ playerPlaysInLeague(P,L). In the\n",
      "policy-based agent with continuous states\n",
      "testing time, by plugging in the learned formulas,\n",
      "based on knowledge graph embeddings,\n",
      "the system should be able to automatically infer\n",
      "which reasons in a KG vector space by\n",
      "the missing link between a pair of entities. This\n",
      "sampling the most promising relation to\n",
      "kind of reasoning machine will potentially serve\n",
      "extend its path. In contrast to prior work,\n",
      "as an essential components of complex QA sys-\n",
      "our approach includes a reward function\n",
      "tems.\n",
      "thattakestheaccuracy,diversity,andef-\n",
      "In recent years, the Path-Ranking Algorithm\n",
      "ficiency into consideration. Experimen-\n",
      "(PRA) (Lao et al., 2010, 2011a) emerges as a\n",
      "tally, we show that our proposed method\n",
      "promising method for learning inference paths in\n",
      "outperforms a path-ranking based algo-\n",
      "largeKGs. PRAusesarandom-walkwithrestarts\n",
      "rithm and knowledge graph embedding\n",
      "based inference mechanism to perform multiple\n",
      "methods on Freebase and Never-Ending\n",
      "bounded depth-first search processes to find rela-\n",
      "LanguageLearningdatasets.1\n",
      "tionalpaths. Coupledwithelastic-netbasedlearn-\n",
      "ing, PRA then picks more plausible paths using\n",
      "1 Introduction\n",
      "supervised learning. However, PRA operates in\n",
      "Deep neural networks for acoustic modeling in a fully discrete space, which makes it difficult to\n",
      "speech recognitionIn recent years, deep learn- evaluateandcomparesimilarentitiesandrelations\n",
      "ing techniques have obtained many state-of-the- inaKG.\n",
      "artresultsinvariousclassificationandrecognition In this work, we propose a novel approach\n",
      "problems (Krizhevsky et al., 2012; Hinton et al., for controllable multi-hop reasoning: we frame\n",
      "2012;Kim,2014). However,complexnaturallan- the path learning process as reinforcement learn-\n",
      "guage processing problems often require multi- ing (RL). In contrast to PRA, we use translation-\n",
      "ple inter-related decisions, and empowering deep based knowledge based embedding method (Bor-\n",
      "learningmodelswiththeabilityoflearningtorea- des et al., 2013) to encode the continuous state of\n",
      "son is still a challenging issue. To handle com- our RL agent, which reasons in the vector space\n",
      "plex queries where there are no obvious answers, environment of the knowledge graph. The agent\n",
      "intelligent machines must be able to reason with takes incremental steps by sampling a relation to\n",
      "existing resources, and learn to infer an unknown extend its path. To better guide the RL agent for\n",
      "answer. learning relational paths, we use policy gradient\n",
      "More specifically, we situate our study in the training (Mnih et al., 2015) with a novel reward\n",
      "context of multi-hop reasoning, which is the task function that jointly encourages accuracy, diver-\n",
      "sity,andefficiency. Empirically,weshowthatour\n",
      "1CodeandtheNELLdatasetareavailableathttps://\n",
      "github.com/xwhan/DeepPath. method outperforms PRA and embedding based\n",
      "8102\n",
      "luJ\n",
      "7\n",
      "]LC.sc[\n",
      "3v09660.7071:viXra\n",
      "methods on a Freebase and a Never-Ending Lan- therefore it does not scale. Note that many of the\n",
      "guage Learning (Carlson et al., 2010a) dataset. recentKGreasoningmethods(Neelakantanetal.,\n",
      "Ourcontributionsarethree-fold: 2015; Das et al., 2017) still rely on first learning\n",
      "the PRA paths, which only operates in a discrete\n",
      "• We are the first to consider reinforcement\n",
      "space. Comparing to PRA, our method reasons\n",
      "learning(RL)methodsforlearningrelational\n",
      "in a continuous space, and by incorporating vari-\n",
      "pathsinknowledgegraphs;\n",
      "ous criteria in the reward function, our reinforce-\n",
      "ment learning (RL) framework has better control\n",
      "• Our learning method uses a complex reward\n",
      "andmoreflexibilityoverthepath-findingprocess.\n",
      "function that considers accuracy, efficiency,\n",
      "Neural symbolic machine (Liang et al., 2016)\n",
      "and path diversity simultaneously, offering\n",
      "is a more recent work on KG reasoning, which\n",
      "bettercontrolandmoreflexibilityinthepath-\n",
      "also applies reinforcement learning but has a dif-\n",
      "findingprocess;\n",
      "ferent flavor from our work. NSM learns to com-\n",
      "• We show that our method can scale up to poseprogramsthatcanfindanswerstonaturallan-\n",
      "large scale knowledge graphs, outperform- guage questions, while our RL model tries to add\n",
      "ingPRAandKGembeddingmethodsintwo new facts to knowledge graph (KG) by reasoning\n",
      "tasks. on existing KG triples. In order to get answers,\n",
      "NSMlearnstogenerateasequenceofactionsthat\n",
      "In the next section, we outline related work in\n",
      "canbecombinedasaexecutableprogram. Theac-\n",
      "path-findingandembeddingmethodsinKGs. We\n",
      "tionspaceinNSMisasetofpredefinedtokens. In\n",
      "describe the proposed method in Section 3. We\n",
      "ourframework,thegoalistofindreasoningpaths,\n",
      "show experimental results in Section 4. Finally,\n",
      "thustheactionspaceisrelationspaceintheKG.A\n",
      "weconcludeinSection5.\n",
      "similar framework (Johnson et al., 2017) has also\n",
      "2 RelatedWork beenappliedtovisualreasoningtasks.\n",
      "ThePath-RankingAlgorithm(PRA)method(Lao 3 Methodology\n",
      "et al., 2011b) is a primary path-finding approach\n",
      "that uses random walk with restart strategies for Inthissection,wedescribeindetailourRL-based\n",
      "multi-hop reasoning. Gardner et al. (2013; 2014) framework for multi-hop relation reasoning. The\n",
      "proposeamodificationtoPRAthatcomputesfea- specific task of relation reasoning is to find re-\n",
      "ture similarity in the vector space. Wang and liable predictive paths between entity pairs. We\n",
      "Cohen (2015) introduce a recursive random walk formulate the path finding problem as a sequen-\n",
      "approach for integrating the background KG and tialdecisionmakingproblemwhichcanbesolved\n",
      "text—the method performs structure learning of with a RL agent. We first describe the environ-\n",
      "logic programs and information extraction from ment and the policy-based RL agent. By interact-\n",
      "text at the same time. A potential bottleneck for ingwiththeenvironmentdesignedaroundtheKG,\n",
      "randomwalkinferenceisthatsupernodesconnect- the agent learns to pick the promising reasoning\n",
      "ing to large amount of formulas will create huge paths. Thenwedescribethetrainingprocedureof\n",
      "fan-out areas that significantly slow down the in- ourRLmodel. Afterthat,wedescribeanefficient\n",
      "ferenceandaffecttheaccuracy. path-constrainedsearchalgorithmforrelationrea-\n",
      "Toutanovaetal.(2015)provideaconvolutional soningwiththepathsfoundbytheRLagent.\n",
      "neural network solution to multi-hop reasoning.\n",
      "3.1 ReinforcementLearningforRelation\n",
      "TheybuildaCNNmodelbasedonlexicalizedde-\n",
      "Reasoning\n",
      "pendencypaths,whichsuffersfromtheerrorprop-\n",
      "agationissueduetoparseerrors. Guuetal.(2015) The RL system consists of two parts (see Fig-\n",
      "usesKGembeddingstoanswerpathqueries. Zeng ure 1). The first part is the external environment\n",
      "et al. (2014) described a CNN model for rela- E which specifies the dynamics of the interaction\n",
      "tional extraction, but it does not explicitly model between the agent and the KG. This environment\n",
      "therelationalpaths. Neelakantanetal.(2015)pro- is modeled as a Markov decision process (MDP).\n",
      "posearecurrentneuralnetworksmodelformodel- A tuple < S,A,P,R > is defined to represent\n",
      "ingrelationalpathsinknowledgebasecompletion the MDP, where S is the continuous state space,\n",
      "(KBC),butittrainstoomanyseparatemodels,and A = {a,a,...,a } is the set of all available ac-\n",
      "1 2 n\n",
      "Query Node: Band of Brothers The KG Environment Policy Based Agent\n",
      "Reason Task: tvProgramLanguage\n",
      "English\n",
      "State\n",
      "Next State\n",
      "Caesars ReLU\n",
      "Entertain… personLanguages Actor\n",
      "Reward\n",
      "serviceLocation-1 Neal profession ReLU\n",
      "McDonough\n",
      "United Reason Step\n",
      "States Tom Hanks\n",
      "Softmax\n",
      "castActor\n",
      "countryOfOrigin awardWorkWinner 𝛑(a|s)\n",
      "Graham writtenBy Band of music Michael\n",
      "Yost Brothers Kamen\n",
      "tvProgramCreator... tvProgramGenre\n",
      "HBO Mini-Series\n",
      "Figure1:OverviewofourRLmodel.Left:TheKGenvironmentEmodeledbyaMDP.Thedottedarrows(partially)showthe\n",
      "existingrelationlinksintheKGandtheboldarrowsshowthereasoningpathsfoundbytheRLagent. −1denotestheinverse\n",
      "ofanrelation. Right: Thestructureofthepolicynetworkagent. Ateachstep,byinteractingwiththeenvironment,theagent\n",
      "learnstopickarelationlinktoextendthereasoningpaths.\n",
      "tions, P(S = s(cid:48)|S = s,A = a) is the transi- Beginningwiththesourceentitye,theagentuse\n",
      "t+1 t t s\n",
      "tion probability matrix, and R(s,a) is the reward the policy network to pick the most promising\n",
      "functionofevery(s,a)pairs. relation to extend its path at each step until it\n",
      "reaches the target entity e. To keep the output\n",
      "The second part of the system, the RL t\n",
      "dimension of the policy network consistent, the\n",
      "agent, is represented as a policy network\n",
      "action space is defined as all the relations in the\n",
      "π (s,a) = p(a|s;θ) which maps the state vec-\n",
      "θ\n",
      "KG.\n",
      "tor to a stochastic policy. The neural network\n",
      "parameters θ are updated using stochastic gra-\n",
      "States The entities and relations in a KG are\n",
      "dient descent. Compared to Deep Q Network\n",
      "naturally discrete atomic symbols. Since exist-\n",
      "(DQN) (Mnih et al., 2013), policy-based RL\n",
      "ing practical KGs like Freebase (Bollacker et al.,\n",
      "methods turn out to be more appropriate for our\n",
      "2008)andNELL(Carlsonetal.,2010b)oftenhave\n",
      "knowledge graph scenario. One reason is that\n",
      "huge amounts of triples. It is impossible to di-\n",
      "for the path finding problem in KG, the action\n",
      "rectly model all the symbolic atoms in states. To\n",
      "space can be very large due to complexity of the\n",
      "capture the semantic information of these sym-\n",
      "relationgraph. Thiscanleadtopoorconvergence\n",
      "bols,weusetranslation-basedembeddingssuchas\n",
      "properties for DQN. Besides, instead of learning\n",
      "TransE (Bordes et al., 2013) and TransH (Wang\n",
      "a greedy policy which is common in value-based\n",
      "etal.,2014)torepresenttheentitiesandrelations.\n",
      "methods like DQN, the policy network is able to\n",
      "These embeddings map all the symbols to a low-\n",
      "learn a stochastic policy which prevent the agent\n",
      "dimensionalvectorspace. Inourframework,each\n",
      "fromgettingstuckatanintermediatestate. Before\n",
      "statecapturestheagent’spositionintheKG.After\n",
      "we describe the structure of our policy network,\n",
      "takinganaction,theagentwillmovefromoneen-\n",
      "we first describe the components (actions, states,\n",
      "titytoanother. Thesetwoarelinkedbytheaction\n",
      "rewards)oftheRLenvironment.\n",
      "(relation)justtakenbytheagent. Thestatevector\n",
      "atsteptisgivenasfollows:\n",
      "Actions Given the entity pairs (e,e ) with\n",
      "s t\n",
      "s = (e,e −e )\n",
      "relation r, we want the agent to find the most t t target t\n",
      "informative paths linking these entity pairs. where e denotes the embeddings of the current\n",
      "t\n",
      "entitynodeande denotestheembeddingsof betweenthecurrentpathandtheexistingones:\n",
      "target\n",
      "the target entity. At the initial state, e = e.\n",
      "t source\n",
      "We do not incorporate the reasoning relation in\n",
      "|F|\n",
      "the state, because the embedding ofthe reasoning 1 (cid:88)\n",
      "r = − cos(p,p )\n",
      "relation remain constant during path finding, DIVERSITY |F| i\n",
      "i=1\n",
      "which is not helpful in training. However, we\n",
      "find out that by training the RL agent using a set\n",
      "of positive samples for one particular relation, where p =\n",
      "(cid:80)n\n",
      "i=1r i represents the path embed-\n",
      "the agent can successfully discover the relation dingfortherelationchainr 1 → r 2 →... → r n.\n",
      "semantics. Policy Network We use a fully-connected neu-\n",
      "ral network to parameterize the policy function\n",
      "RewardsThereareafewfactorsthatcontributeto π(s;θ) that maps the state vector s to a proba-\n",
      "thequalityofthepathsfoundbytheRLagent. To bility distribution over all possible actions. The\n",
      "encourage the agent to find predictive paths, our neuralnetworkconsistsoftwohiddenlayers,each\n",
      "rewardfunctionsincludethefollowingscoringcri- followed by a rectifier nonlinearity layer (ReLU).\n",
      "teria: The output layer is normalized using a softmax\n",
      "Global accuracy: For our environment settings, function(seeFigure1).\n",
      "the number of actions that can be taken by the\n",
      "agent can be very large. In other words, there are\n",
      "3.2 TrainingPipeline\n",
      "muchmoreincorrectsequentialdecisionsthanthe\n",
      "correct ones. The number of these incorrect de-\n",
      "In practice, one big challenge of KG reasoning is\n",
      "cision sequences can increase exponentially with\n",
      "that the relation set can be quite large. For a typ-\n",
      "the length of the path. In view of this challenge,\n",
      "ical KG, the RL agent is often faced with hun-\n",
      "the first reward function we add to the RL model\n",
      "dreds (thousands) of possible actions. In other\n",
      "isdefinedasfollows:\n",
      "words, the output layer of the policy network of-\n",
      "(cid:40)\n",
      "+1, ifthepathreachese ten has a large dimension. Due to the complexity\n",
      "target\n",
      "r =\n",
      "GLOBAL of the relation graph and the large action space,\n",
      "−1, otherwise\n",
      "if we directly train the RL model by trial and er-\n",
      "the agent is given an offline positive reward +1 if rors, which is typical for RL algorithms, the RL\n",
      "itreachesthetargetafterasequenceofactions. model will show very poor convergence proper-\n",
      "Path efficiency: For the relation reasoning task, ties. After a long-time training, the agents fails\n",
      "we observe that short paths tend to provide more to find any valuable path. To tackle this prob-\n",
      "reliable reasoning evidence than longer paths. lem,westartourtrainingwithasupervisedpolicy\n",
      "Shorter chains of relations can also improve the whichisinspiredbytheimitationlearningpipeline\n",
      "efficiency of the reasoning by limiting the length used by AlphaGo (Silver et al., 2016). In the Go\n",
      "oftheRL’sinteractionswiththeenvironment. The game, the player is facing nearly 250 possible le-\n",
      "efficiencyrewardisdefinedasfollows: galmovesateachstep. Directlytrainingtheagent\n",
      "to pick actions from the original action space can\n",
      "1\n",
      "r = beadifficulttask. AlphaGofirsttrainasupervised\n",
      "EFFICIENCY length(p)\n",
      "policy network using experts moves. In our case,\n",
      "thesupervisedpolicyistrainedwitharandomized\n",
      "where path p is defined as a sequence of relations\n",
      "breadth-firstsearch(BFS).\n",
      "r → r →... → r.\n",
      "1 2 n\n",
      "Pathdiversity: Wetraintheagenttofindpathsus- Supervised Policy Learning For each relation,\n",
      "ingpositivesamplesforeachrelation. Thesetrain- we use a subset of all the positive samples (en-\n",
      "ingsample(e,e )havesimilarstaterep- titypairs)tolearnthesupervisedpolicy. Foreach\n",
      "source target\n",
      "resentations in the vector space. The agent tends positive sample (e,e ), a two-side BFS\n",
      "source target\n",
      "to find paths with similar syntax and semantics. is conducted to find same correct paths between\n",
      "These paths often contains redundant information the entities. For each path p with a sequence of\n",
      "sincesomeofthemmaybecorrelated. Toencour- relations r → r →... → r, we update the pa-\n",
      "1 2 n\n",
      "agetheagenttofinddiversepaths,wedefineadi- rameters θ to maximize the expected cumulative\n",
      "versityrewardfunctionusingthecosinesimilarity reward using Monte-Carlo Policy Gradient (RE-\n",
      "INFORCE)(Williams,1992): Algorithm 1: Retraining Procedure with re-\n",
      "(cid:88) wardfunctions\n",
      "J(θ) = E ( R )\n",
      "a∼π(a|s;θ) st,at Restoreparametersθ fromsupervisedpolicy;\n",
      "1\n",
      "t\n",
      "forepisode←1toN do\n",
      "(cid:88)(cid:88) 2\n",
      "= π(a|s ;θ)R (1)\n",
      "t st,at\n",
      "3\n",
      "Initializestatevectors\n",
      "t\n",
      "← s\n",
      "0\n",
      "t a∈A Initializeepisodelengthsteps ← 0\n",
      "4\n",
      "where J(θ) is the expected total rewards for one 5\n",
      "whilenum steps < max lengthdo\n",
      "Randomlysampleactiona ∼ π(a|s )\n",
      "episode. For supervised learning, we give a re- 6 t\n",
      "ward of +1 for each step of a successful episode. 7\n",
      "ObserverewardR t,nextstates\n",
      "t+1\n",
      "// if the step fails\n",
      "By plugging in the paths found by the BFS, the\n",
      "ifR = −1then\n",
      "approximated gradient used to update the policy 8 t\n",
      "Save< s,a >toM\n",
      "networkisshownbelow: 9 t neg\n",
      "ifsuccessorsteps = max length\n",
      "10\n",
      "(cid:88)(cid:88)\n",
      "∇ J(θ) = π(a|s ;θ)∇ logπ(a|s ;θ) then\n",
      "θ t θ t\n",
      "t a∈A 11 break\n",
      "≈ ∇\n",
      "(cid:88)\n",
      "logπ(a = r |s ;θ) (2) 12\n",
      "Incrementnum steps\n",
      "θ t t\n",
      "t // penalize failed steps\n",
      "Updateθ using\n",
      "wherer belongstothepathp. 13\n",
      "t (cid:80)\n",
      "g ∝ ∇ logπ(a = r |s ;θ)(−1)\n",
      "However, the vanilla BFS is a biased search al- θ Mneg t t\n",
      "gorithm which prefers short paths. When plug- ifsuccessthen\n",
      "ging in these biased paths, it becomes difficult 14 R total ← λ 1r GLOBAL+λ 2r EFFICIENCY+\n",
      "for the agent to find longer paths which may po- λ 3r\n",
      "DIVERSITY\n",
      "tentially be useful. We want the paths to be 15 Updateθ using\n",
      "(cid:80)\n",
      "controlled only by the defined reward functions. g ∝ ∇ θ tlogπ(a = r t|s t;θ)R total\n",
      "To prevent the biased search, we adopt a sim-\n",
      "ple trick to add some random mechanisms to the\n",
      "BFS. Instead of directly searching the path be-\n",
      "boundmax length. Theepisodeendsiftheagent\n",
      "tween e and e, we randomly pick a in-\n",
      "source target failstoreachthetargetentitywithinmax length\n",
      "termediate node e and then conduct two BFS\n",
      "inter steps. After each episode, the policy network is\n",
      "between(e,e )and(e,e ). The\n",
      "source inter inter target updatedusingthefollowinggradient:\n",
      "concatenatedpathsareusedtotraintheagent. The\n",
      "supervised learning saves the agent great efforts (cid:88)\n",
      "∇ J(θ) = ∇ logπ(a = r |s ;θ)R (3)\n",
      "θ θ t t total\n",
      "learning from failed actions. With the learned ex-\n",
      "t\n",
      "perience, we then train the agent to find desirable\n",
      "paths. where R is the linear combination of the de-\n",
      "total\n",
      "Retraining with Rewards To find the reasoning fined reward functions. The detail of the retrain\n",
      "paths controlled by the reward functions, we use process is shown in Algorithm 1. In practice, θ is\n",
      "reward functions to retrain the supervised policy updated using the Adam Optimizer (Kingma and\n",
      "network. Foreachrelation,thereasoningwithone Ba,2014)withL 2 regularization.\n",
      "entitypairistreatedasoneepisode. Startingwith\n",
      "3.3 Bi-directionalPath-constrainedSearch\n",
      "the source node e, the agent picks a relation\n",
      "source\n",
      "accordingtothestochasticpolicyπ(a|s),whichis Given an entity pair, the reasoning paths learned\n",
      "a probability distribution over all relations, to ex- by the RL agent can be used as logical formulas\n",
      "tenditsreasoningpath. Thisrelationlinkmaylead to predict the relation link. Each formula is veri-\n",
      "to a new entity, or it may lead to nothing. These fiedusingabi-directionalsearch. InatypicalKG,\n",
      "failedstepswillcausetheagenttoreceivenegative one entity node can be linked to a large number\n",
      "rewards. The agent will stay at the same state af- of neighbors with the same relation link. A sim-\n",
      "terthesefailedsteps. Sincetheagentisfollowing ple example is the relation personNationality−1,\n",
      "astochasticpolicy,theagentwillnotgetstuckby which denotes the inverse of personNationality.\n",
      "repeatingawrongstep. Toimprovethetrainingef- Following this link, the entity United States can\n",
      "ficiency,welimittheepisodelengthwithanupper reach numerous neighboring entities. If the for-\n",
      "Dataset #Ent. #R. #Triples #Tasks\n",
      "Algorithm 2: Bi-directional search for path\n",
      "FB15K-237 14,505 237 310,116 20\n",
      "verification\n",
      "NELL-995 75,492 200 154.213 12\n",
      "Givenareasoningpath\n",
      "1\n",
      "Table1:StatisticsoftheDatasets.#Ent.denotesthenumber\n",
      "p : r → r →... → r\n",
      "1 2 n ofuniqueentitiesand#R.denotesthenumberofrelations\n",
      "for(e,e )intestsetD do\n",
      "2 i j\n",
      "start←0;end←n\n",
      "3\n",
      "4 left ← ∅;right ← ∅ are subsets of larger datasets. The triples in\n",
      "5 whilestart<end do FB15K-237 (Toutanova et al., 2015) are sampled\n",
      "6 leftEx ← ∅;rightEx ← ∅ from FB15K (Bordes et al., 2013) with redun-\n",
      "7 iflen(left)<len(right)then dantrelationsremoved. Weperformthereasoning\n",
      "8 Extendpathontheleftside tasks on 20 relations which have enough reason-\n",
      "9 AddconnectednodestoleftEx ing paths. These tasks consists of relations from\n",
      "10 left ← leftEx different domains like Sports, People, Locations,\n",
      "Film, etc. Besides, we present a new NELL sub-\n",
      "else\n",
      "11\n",
      "set that is suitable for multi-hop reasoning from\n",
      "Extendpathontherightside\n",
      "12\n",
      "the 995th iteration of the NELL system. We first\n",
      "AddconnectednodestorightEx\n",
      "13\n",
      "removethetripleswithrelationgeneralizationsor\n",
      "right ← rightEx\n",
      "14\n",
      "haswikipediaurl. Thesetworelationsappearmore\n",
      "15 ifleft∩right (cid:54)= ∅then than2MtimesintheNELLdataset,buttheyhave\n",
      "16 returnTrue no reasoning values. After this step, we only se-\n",
      "else lectthetripleswithTop-200relations. Tofacilitate\n",
      "17\n",
      "returnFalse path finding, we also add the inverse triples. For\n",
      "18\n",
      "each triple (h,r,t), we append (t,r−1,h) to the\n",
      "datasets. With these inverse triples, the agent is\n",
      "abletostepbackwardintheKG.\n",
      "mula consists of such links, the number of inter- For each reasoning task r, we remove all the\n",
      "i\n",
      "mediate entities can exponentially increase as we tripleswithr orr−1fromtheKG.Theseremoved\n",
      "i i\n",
      "follow the reasoning formula. However, we ob- triples are split into train and test samples. For\n",
      "serve that for these formulas, if we verify the for- the link prediction task, each h in the test triples\n",
      "mulafromtheinversedirection. Thenumberofin- {(h,r,t)} is considered as one query. A set of\n",
      "termediate nodes can be tremendously decreased. candidatetargetentitiesarerankedusingdifferent\n",
      "Algorithm 2 shows a detailed description of the methods. For fact prediction, the true test triples\n",
      "proposedbi-directionalsearch. arerankedwithsomegeneratedfalsetriples.\n",
      "4 Experiments\n",
      "4.2 BaselinesandImplementationDetails\n",
      "To evaluate the reasoning formulas found by our Most KG reasoning methods are based on either\n",
      "RL agent, we explore two standard KG reason- path formulas or KG embeddings. we explore\n",
      "ing tasks: link prediction (predicting target en- methodsfrombothofthesetwoclassesinourex-\n",
      "tities) and fact prediction (predicting whether an periments. For path based methods, we compare\n",
      "unknown fact holds or not). We compare our our RL model with the PRA (Lao et al., 2011a)\n",
      "methodwithbothpath-basedmethodsandembed- algorithm,whichhasbeenusedinacoupleofrea-\n",
      "dingbasedmethods. Afterthat,wefurtheranalyze soning methods (Gardner et al., 2013; Neelakan-\n",
      "thereasoningpathsfoundbyourRLagent. These tan et al., 2015). PRA is a data-driven algorithm\n",
      "highly predictive paths validate the effectiveness usingrandomwalks(RW)tofindpathsandobtain\n",
      "oftherewardfunctions. Finally,weconductaex- path features. For embedding based methods, we\n",
      "perimenttoinvestigatetheeffectofthesupervised evaluate several state-of-the-art embeddings de-\n",
      "learningprocedure. signed for knowledge base completion, such as\n",
      "TransE(Bordesetal.,2013),TransH(Wangetal.,\n",
      "4.1 DatasetandSettings\n",
      "2014), TransR (Lin et al., 2015) and TransD (Ji\n",
      "Table 1 shows the statistics of the two datasets etal.,2015).\n",
      "we conduct our experiments on. Both of them The implementation of PRA is based on the\n",
      "FB15K-237 NELL-995\n",
      "Tasks PRA RL TransE TransR Tasks PRA RL TransE TransR\n",
      "teamSports 0.987 0.955 0.896 0.784 athletePlaysForTeam 0.547 0.750 0.627 0.673\n",
      "birthPlace 0.441 0.531 0.403 0.417 athletePlaysInLeague 0.841 0.960 0.773 0.912\n",
      "personNationality 0.846 0.823 0.641 0.720 athleteHomeStadium 0.859 0.890 0.718 0.722\n",
      "filmDirector 0.349 0.441 0.386 0.399 athletePlaysSport 0.474 0.957 0.876 0.963\n",
      "filmWrittenBy 0.601 0.457 0.563 0.605 teamPlaySports 0.791 0.738 0.761 0.814\n",
      "filmLanguage 0.663 0.670 0.642 0.641 orgHeadquaterCity 0.811 0.790 0.620 0.657\n",
      "tvLanguage 0.960 0.969 0.804 0.906 worksFor 0.681 0.711 0.677 0.692\n",
      "capitalOf 0.829 0.783 0.554 0.493 bornLocation 0.668 0.757 0.712 0.812\n",
      "organizationFounded 0.281 0.309 0.390 0.339 personLeadsOrg 0.700 0.795 0.751 0.772\n",
      "musicianOrigin 0.426 0.514 0.361 0.379 orgHiredPerson 0.599 0.742 0.719 0.737\n",
      "......\n",
      "Overall 0.541 0.572 0.532 0.540 0.675 0.796 0.737 0.789\n",
      "Table2:Linkpredictionresults(MAP)ontwodatasets.\n",
      "code released by (Lao et al., 2011a). We use the FactPredictionResults\n",
      "TopKnegativemodetogeneratenegativesamples\n",
      "Methods FB15K-237 NELL-995\n",
      "for both train and test samples. For each pos-\n",
      "itive samples, there are approximately 10 corre- RL 0.311 0.493\n",
      "spondingnegativesamples. Eachnegativesample TransE 0.277 0.383\n",
      "is generated by replacing the true target entity t TransH 0.309 0.389\n",
      "with a faked one t(cid:48) in each triple (h,r,t). These TransR 0.302 0.406\n",
      "positive and negative test pairs generated by PRA TransD 0.303 0.413\n",
      "make up the test set for all methods evaluated in\n",
      "thispaper. ForTransE,R,H,D,welearnaseparate Table3:Factpredictionresults(MAP)ontwodatasets.\n",
      "embedding matrix for each reasoning task using\n",
      "thepositivetrainingentitypairs. Alltheseembed- #ofReasoningPaths\n",
      "dingsaretrainedfor1,000epochs. 2\n",
      "Tasks PRA RL\n",
      "Our RL model make use of TransE to get the\n",
      "continuous representation of the entities and rela- worksFor 247 25\n",
      "tions. We use the same dimension as TransE, R teamPlaySports 113 27\n",
      "to embed the entities. Specifically, the state vec- teamPlaysInLeague 69 21\n",
      "tor we use has a dimension of 200, which is also athletehomestadium 37 11\n",
      "the input size of the policy network. To reason organizationHiredPerson 244 9\n",
      "using the path formulas, we adopt a similar lin-...\n",
      "ear regression approach as in PRA to re-rank the Average# 137.2 20.3\n",
      "paths. However,insteadofusingtherandomwalk\n",
      "Table4:NumberofreasoningpathsusedbyPRAandourRL\n",
      "probabilities as path features, which can be com-\n",
      "model. RLachievedbetterMAPwithamorecompactsetof\n",
      "putationallyexpensive,wesimplyusebinarypath learnedpaths.\n",
      "featuresobtainedbythebi-directionalsearch. We\n",
      "observethatwithonlyafewminedpathformulas,\n",
      "Since path-based methods generally work better\n",
      "our method can achieve better results than PRA’s\n",
      "than embedding methods for this task, we do not\n",
      "data-drivenapproach.\n",
      "include the other two embedding baselines in this\n",
      "4.3 Results table. Instead, we spare the room to show the de-\n",
      "tailedresultsoneachrelationreasoningtask.\n",
      "4.3.1 QuantitativeResults\n",
      "FortheoverallMAPshowninthelastrowofthe\n",
      "LinkPredictionThistaskistorankthetargeten-\n",
      "table,ourapproachsignificantlyoutperformsboth\n",
      "titiesgivenaqueryentity. Table2showsthemean\n",
      "thepath-basedmethodandembeddingmethodson\n",
      "average precision (MAP) results on two datasets.\n",
      "twodatasets,whichvalidatesthestrongreasoning\n",
      "abilityofourRLmodel. Formostrelations,since\n",
      "2The implementation we used can be found at https:\n",
      "//github.com/thunlp/Fast-TransX the embedding methods fail to use the path infor-\n",
      "120\n",
      "100\n",
      "80\n",
      "60\n",
      "40\n",
      "20\n",
      "0\n",
      "0 5 10 15 20 25\n",
      "distribution of reasoning paths\n",
      "shtap\n",
      "fo\n",
      "rebmun\n",
      "0.20\n",
      "NELL-995\n",
      "FB15K-237\n",
      "0.15\n",
      "0.10\n",
      "0.05\n",
      "0.00\n",
      "0 50 100 150 200\n",
      "training episodes\n",
      "Figure2:Thedistributionofpathslengthsontwodatasets\n",
      "mation in the KG, they generally perform worse\n",
      "thanourRLmodelorPRA.However, whenthere\n",
      "are not enough paths between entities, our model\n",
      "and PRA can give poor results. For example,\n",
      "fortherelationfilmWrittenBy,ourRLmodelonly\n",
      "finds4uniquereasoningpaths,<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  22101,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['FB15K-237', 'NELL-995']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  implementation we used can be found at https:\n",
      "//github.com/thunlp/Fast-TransX the embedding methods fail to use the path infor-\n",
      "120\n",
      "100\n",
      "80\n",
      "60\n",
      "40\n",
      "20\n",
      "0\n",
      "0 5 10 15 20 25\n",
      "distribution of reasoning paths\n",
      "shtap\n",
      "fo\n",
      "rebmun\n",
      "0.20\n",
      "NELL-995\n",
      "FB15K-237\n",
      "0.15\n",
      "0.10\n",
      "0.05\n",
      "0.00\n",
      "0 50 100 150 200\n",
      "training episodes\n",
      "Figure2:Thedistributionofpathslengthsontwodatasets\n",
      "mation in the KG, they generally perform worse\n",
      "thanourRLmodelorPRA.However, whenthere\n",
      "are not enough paths between entities, our model\n",
      "and PRA can give poor results. For example,\n",
      "fortherelationfilmWrittenBy,ourRLmodelonly\n",
      "finds4uniquereasoningpaths,whichmeansthere\n",
      "isactuallynotenoughreasoningevidenceexisting\n",
      "in the KG. Another observation is that we always\n",
      "get better performance on the NELL dataset. By\n",
      "analyzing the paths found from the KGs, we be-\n",
      "lievethepotentialreasonisthattheNELLdataset\n",
      "has more short paths than FB15K-237 and some\n",
      "ofthemaresimplysynonymsofthereasoningre-\n",
      "lations.\n",
      "Fact Prediction Instead of ranking the target en-\n",
      "tities, this task directly ranks all the positive and\n",
      "negative samples for a particular relation. The\n",
      "PRA is not included as a baseline here, since the\n",
      "PRA code only gives a target entity ranking for\n",
      "eachquerynodeinsteadofarankingofalltriples.\n",
      "Table 3 shows the overall results of all the meth-\n",
      "ods. OurRLmodelgetsevenbetterresultsonthis\n",
      "task. Wealso observethat theRLmodel beatsall\n",
      "theembeddingbaselinesonmostreasoningtasks.\n",
      "4.3.2 QualitativeAnalysisofReasoningPaths\n",
      "To analyze the properties of reasoning paths, we\n",
      "show a few reasoning paths found by the agent\n",
      "in Table 5. To illustrate the effect of the effi-\n",
      "ciency reward function, we show the path length\n",
      "distributions in Figure 2. To interpret these paths,\n",
      "take the personNationality relation for example,\n",
      "the first reasoning path indicates that if we know\n",
      "facts placeOfBirth(x,y) and locationContains(z,y)\n",
      "thenitishighlypossiblethatpersonxhasnation-\n",
      "ality z. These short but predictive paths indicate\n",
      "the effectiveness of the RL model. Another im-\n",
      "portant observation is that our model use much\n",
      "spets\n",
      "01\n",
      "nihtiw\n",
      "oitar\n",
      "sseccus\n",
      "Figure3: Thesuccessratio(succ )duringtraining. Task: 10\n",
      "athletePlaysForTeam.3\n",
      "fewer reasoning paths than PRA, which indicates\n",
      "that our model can actually extract the most reli-\n",
      "able reasoning evidence from KG. Table 4 shows\n",
      "somecomparisonsaboutthenumberofreasoning\n",
      "paths. We can see that, with the pre-defined re-\n",
      "wardfunctions,theRLagentiscapableofpicking\n",
      "the strong ones and filter out similar or irrelevant\n",
      "ones.\n",
      "4.3.3 EffectofSupervisedLearning\n",
      "AsmentionedinSection3.2,onemajorchallenge\n",
      "for applying RL to KG reasoning is the large ac-\n",
      "tion space. We address this issue by applying\n",
      "supervised learning before the reward retraining\n",
      "step. To show the effect of the supervised train-\n",
      "ing,weevaluatetheagent’ssuccessratioofreach-\n",
      "ingthetargetwithin10steps(succ )afterdiffer- 10\n",
      "ent number of training episodes. For each train-\n",
      "ing episode, one pair of entities (e,e ) source target\n",
      "in the train set is used to find paths. All the cor-\n",
      "rectpathslinkingtheentitieswillgeta+1global\n",
      "reward. Wethenpluginsometruepathsfortrain-\n",
      "ing. Thesucc iscalculatedonaheld-outtestset\n",
      "10\n",
      "that consists of 100 entity pairs. For the NELL-\n",
      "995 dataset, since we have 200 unique relations,\n",
      "the dimension of the action space will be 400 af-\n",
      "ter we add the backward actions. This means that\n",
      "randomwalkswillgetverylowsucc sincethere 10\n",
      "maybenearly40010invalidpaths. Figure3shows\n",
      "the succ during training. We see that even the 10\n",
      "agenthasnotseentheentitybefore,itcanactually\n",
      "pickthepromisingrelationtoextenditspath. This\n",
      "also validates the effectiveness of our state repre-\n",
      "sentations.\n",
      "3Theconfidencebandisgeneratedusing50differentruns.\n",
      "Relation ReasoningPath\n",
      "filmReleaseRegion\n",
      "filmCountry featureFilmLocation→locationContains−1\n",
      "actorFilm−1 →personNationality\n",
      "placeOfBirth→locationContains−1\n",
      "personNationality peoplePlaceLived→locationContains−1\n",
      "peopleMarriage→locationOfCeremony→locationContains−1\n",
      "tvCountryOfOrigin→countryOfficialLanguage\n",
      "tvProgramLanguage tvCountryOfOrigin→filmReleaseRegion−1 →filmLanguage\n",
      "tvCastActor→filmLanguage\n",
      "personBornInCity\n",
      "personBornInLocation graduatedUniversity→graduatedSchool−1 →personBornInCity\n",
      "personBornInCity→atLocation−1 →atLocation\n",
      "athleteHomeStadium→teamHomeStadium−1\n",
      "athletePlaysForTeam athletePlaysSport→teamPlaysSport−1\n",
      "athleteLedSportsTeam\n",
      "worksFor\n",
      "personLeadsOrganization organizationTerminatedPerson−1\n",
      "mutualProxyFor−1\n",
      "Table5: ExamplereasoningpathsfoundbyourRLmodel. ThefirstthreerelationscomefromtheFB15K-237dataset. The\n",
      "othersarefromNELL-995.Inversesofexistingrelationsaredenotedby−1.\n",
      "5 ConclusionandFutureWork Acknowledgments\n",
      "We gratefully acknowledge the support of\n",
      "NVIDIACorporationwiththedonationofoneTi-\n",
      "In this paper, we propose a reinforcement learn-\n",
      "tanXPascalGPUusedforthisresearch.\n",
      "ing framework to improve the performance of re-\n",
      "lation reasoning in KGs. Specifically, we train a\n",
      "RLagenttofindreasoningpathsintheknowledge\n",
      "References\n",
      "base. Unlikepreviouspathfindingmodelsthatare\n",
      "Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim\n",
      "based on random walks, the RL model allows us\n",
      "Sturge,andJamieTaylor.2008. Freebase: acollab-\n",
      "tocontrolthepropertiesofthefoundpaths. These\n",
      "oratively created graph database for structuring hu-\n",
      "effectivepathscanalsobeusedasanalternativeto man knowledge. In Proceedings of the 2008 ACM\n",
      "PRA in many path-based reasoning methods. For SIGMOD international conference on Management\n",
      "ofdata,pages1247–1250.ACM.\n",
      "two standard reasoning tasks, using the RL paths\n",
      "asreasoningformulas,ourapproachgenerallyout- Antoine Bordes, Nicolas Usunier, Alberto Garcia-\n",
      "performstwoclassesofbaselines. Duran, Jason Weston, and Oksana Yakhnenko.\n",
      "2013. Translating embeddings for modeling multi-\n",
      "For future studies, we plan to investigate relational data. In Advances in neural information\n",
      "the possibility of incorporating adversarial learn- processingsystems,pages2787–2795.\n",
      "ing (Goodfellow et al., 2014) to give better re- AndrewCarlson,JustinBetteridge,BryanKisiel,Burr\n",
      "wards than the human-defined reward functions Settles, Estevam R. Hruschka Jr., and Tom M.\n",
      "used in this work. Instead of designing rewards Mitchell. 2010a. Toward an architecture for never-\n",
      "endinglanguagelearning. InAAAI.\n",
      "according to path characteristics, a discriminative\n",
      "modelcanbetrainedtogiverewards. Also,toad- AndrewCarlson,JustinBetteridge,BryanKisiel,Burr\n",
      "dress the problematic scenario when the KG does Settles, Estevam R. Hruschka Jr., and Tom M.\n",
      "Mitchell. 2010b. Toward an architecture for never-\n",
      "nothaveenoughreasoningpaths,weareinterested\n",
      "ending language learning. In Proceedings of the\n",
      "in applying our RL framework to joint reasoning\n",
      "Twenty-FourthConferenceonArtificialIntelligence\n",
      "withKGtriplesandtextmentions. (AAAI2010).\n",
      "Rajarshi Das, Arvind Neelakantan, David Belanger, Ni Lao, Jun Zhu, Xinwang Liu, Yandong Liu, and\n",
      "andAndrewMcCallum.2017. Chainsofreasoning WilliamWCohen.2010. Efficientrelationallearn-\n",
      "overentities,relations,andtextusingrecurrentneu- ing with hidden variable detection. In NIPS, pages\n",
      "ralnetworks. EACL. 1234–1242.\n",
      "Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel, Chen Liang, Jonathan Berant, Quoc Le, Kenneth D\n",
      "and Tom M Mitchell. 2013. Improving learning Forbus, and Ni Lao. 2016. Neural symbolic\n",
      "andinferenceinalargeknowledge-baseusinglatent machines: Learning semantic parsers on free-\n",
      "syntacticcues. InEMNLP,pages833–838. base with weak supervision. arXiv preprint\n",
      "arXiv:1611.00020.\n",
      "MattGardner,ParthaPratimTalukdar,JayantKrishna-\n",
      "murthy,andTomMitchell.2014. Incorporatingvec- YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,and\n",
      "tor space similarity in random walk inference over Xuan Zhu. 2015. Learning entity and relation em-\n",
      "knowledgebases. beddingsforknowledgegraphcompletion. InAAAI,\n",
      "pages2181–2187.\n",
      "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\n",
      "BingXu,DavidWarde-Farley,SherjilOzair,Aaron Volodymyr Mnih, Koray Kavukcuoglu, David Sil-\n",
      "Courville,andYoshuaBengio.2014. Generativead- ver, Alex Graves, Ioannis Antonoglou, Daan Wier-\n",
      "versarial nets. In Advances in Neural Information stra, and Martin Riedmiller. 2013. Playing atari\n",
      "ProcessingSystems,pages2672–2680. with deep reinforcement learning. arXiv preprint\n",
      "arXiv:1312.5602.\n",
      "Kelvin Guu, John Miller, and Percy Liang. 2015.\n",
      "Traversing knowledge graphs in vector space. In\n",
      "Volodymyr Mnih, Koray Kavukcuoglu, David Silver,\n",
      "EMNLP.\n",
      "Andrei A Rusu, Joel Veness, Marc G Bellemare,\n",
      "Alex Graves, Martin Riedmiller, Andreas K Fidje-\n",
      "Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl,\n",
      "land, Georg Ostrovski, et al. 2015. Human-level\n",
      "Abdel-rahman Mohamed, Navdeep Jaitly, Andrew\n",
      "control through deep reinforcement learning. Na-\n",
      "Senior,VincentVanhoucke,PatrickNguyen,TaraN\n",
      "ture,518(7540):529–533.\n",
      "Sainath, et al. 2012. Deep neural networks for\n",
      "acousticmodelinginspeechrecognition:Theshared\n",
      "ArvindNeelakantan,BenjaminRoth,andAndrewMc-\n",
      "viewsoffourresearchgroups. IEEESignalProcess-\n",
      "Callum. 2015. Compositional vector space mod-\n",
      "ingMagazine,29(6):82–97.\n",
      "els for knowledge base completion. arXiv preprint\n",
      "arXiv:1504.06662.\n",
      "GuoliangJi,ShizhuHe,LihengXu,KangLiu,andJun\n",
      "Zhao. 2015. Knowledge graph embedding via dy-\n",
      "David Silver, Aja Huang, Chris J Maddison, Arthur\n",
      "namicmappingmatrix. InACL(1),pages687–696.\n",
      "Guez,LaurentSifre,GeorgeVanDenDriessche,Ju-\n",
      "lian Schrittwieser, Ioannis Antonoglou, Veda Pan-\n",
      "Justin Johnson, Bharath Hariharan, Laurens van der\n",
      "neershelvam, Marc Lanctot, et al. 2016. Mastering\n",
      "Maaten,JudyHoffman,LiFei-Fei,CLawrenceZit-\n",
      "the game of go with deep neural networks and tree\n",
      "nick, and Ross Girshick. 2017. Inferring and exe-\n",
      "cutingprogramsforvisualreasoning. arXivpreprint search. Nature,529(7587):484–489.\n",
      "arXiv:1705.03633.\n",
      "Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-\n",
      "Yoon Kim. 2014. Convolutional neural net- fungPoon,PallaviChoudhury,andMichaelGamon.\n",
      "works for sentence classification. arXiv preprint 2015. Representingtextforjointembeddingoftext\n",
      "arXiv:1408.5882. andknowledgebases. InEMNLP,volume15,pages\n",
      "1499–1509.Citeseer.\n",
      "Diederik Kingma and Jimmy Ba. 2014. Adam: A\n",
      "method for stochastic optimization. arXiv preprint William Yang Wang and William W Cohen. 2015.\n",
      "arXiv:1412.6980. Joint information extraction and reasoning: A scal-\n",
      "ablestatisticalrelationallearningapproach. InACL.\n",
      "AlexKrizhevsky, IlyaSutskever, andGeoffreyEHin-\n",
      "ton. 2012. Imagenet classification with deep con- Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\n",
      "volutional neural networks. In Advances in neural Chen.2014. Knowledgegraphembeddingbytrans-\n",
      "informationprocessingsystems,pages1097–1105. lating on hyperplanes. In AAAI, pages 1112–1119.\n",
      "Citeseer.\n",
      "Ni Lao, Tom Mitchell, and William W Cohen. 2011a.\n",
      "Randomwalkinferenceandlearninginalargescale Ronald J Williams. 1992. Simple statistical gradient-\n",
      "knowledge base. In Proceedings of the Conference following algorithms for connectionist reinforce-\n",
      "on Empirical Methods in Natural Language Pro- mentlearning. Machinelearning,8(3-4):229–256.\n",
      "cessing, pages 529–539. Association for Computa-\n",
      "tionalLinguistics. Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,\n",
      "Jun Zhao, et al. 2014. Relation classification via\n",
      "Ni Lao, Tom M. Mitchell, and William W. Cohen. convolutional deep neural network. In COLING,\n",
      "2011b. Random walk inference and learning in a pages2335–2344.\n",
      "largescaleknowledgebase. InEMNLP,pages529–\n",
      "539.ACL.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  22101,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['FB15K-237', 'NELL-995']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: DeepPath: A Reinforcement Learning Method for\n",
      "Knowledge Graph Reasoning\n",
      "WenhanXiong and ThienHoang and WilliamYangWang\n",
      "DepartmentofComputerScience\n",
      "UniversityofCalifornia,SantaBarbara\n",
      "SantaBarbara,CA93106USA\n",
      "{xwhan,william}@cs.ucsb.edu, thienhoang@umail.ucsb.edu\n",
      "Abstract of learning explicit inference formulas, given a\n",
      "large KG. For example, if the KG includes the\n",
      "Westudytheproblemoflearningtoreason\n",
      "beliefs such as Neymar plays for Barcelona, and\n",
      "in large scale knowledge graphs (KGs).\n",
      "Barcelona are in the La Liga league, then ma-\n",
      "More specifically, we describe a novel re-\n",
      "chines should be able to learn the following for-\n",
      "inforcementlearningframeworkforlearn-\n",
      "mula: playerPlaysForTeam(P,T) ∧ teamPlaysIn-\n",
      "ing multi-hop relational paths: we use a\n",
      "League(T,L) ⇒ playerPlaysInLeague(P,L). In the\n",
      "policy-based agent with continuous states\n",
      "testing time, by plugging in the learned formulas,\n",
      "based on knowledge graph embeddings,\n",
      "the system should be able to automatically infer\n",
      "which reasons in a KG vector space by\n",
      "the missing link between a pair of entities. This\n",
      "sampling the most promising relation to\n",
      "kind of reasoning machine will potentially serve\n",
      "extend its path. In contrast to prior work,\n",
      "as an essential components of complex QA sys-\n",
      "our approach includes a reward function\n",
      "tems.\n",
      "thattakestheaccuracy,diversity,andef-\n",
      "In recent years, the Path-Ranking Algorithm\n",
      "ficiency into consideration. Experimen-\n",
      "(PRA) (Lao et al., 2010, 2011a) emerges as a\n",
      "tally, we show that our proposed method\n",
      "promising method for learning inference paths in\n",
      "outperforms a path-ranking based algo-\n",
      "largeKGs. PRAusesarandom-walkwithrestarts\n",
      "rithm and knowledge graph embedding\n",
      "based inference mechanism to perform multiple\n",
      "methods on Freebase and Never-Ending\n",
      "bounded depth-first search processes to find rela-\n",
      "LanguageLearningdatasets.1\n",
      "tionalpaths. Coupledwithelastic-netbasedlearn-\n",
      "ing, PRA then picks more plausible paths using\n",
      "1 Introduction\n",
      "supervised learning. However, PRA operates in\n",
      "Deep neural networks for acoustic modeling in a fully discrete space, which makes it difficult to\n",
      "speech recognitionIn recent years, deep learn- evaluateandcomparesimilarentitiesandrelations\n",
      "ing techniques have obtained many state-of-the- inaKG.\n",
      "artresultsinvariousclassificationandrecognition In this work, we propose a novel approach\n",
      "problems (Krizhevsky et al., 2012; Hinton et al., for controllable multi-hop reasoning: we frame\n",
      "2012;Kim,2014). However,complexnaturallan- the path learning process as reinforcement learn-\n",
      "guage processing problems often require multi- ing (RL). In contrast to PRA, we use translation-\n",
      "ple inter-related decisions, and empowering deep based knowledge based embedding method (Bor-\n",
      "learningmodelswiththeabilityoflearningtorea- des et al., 2013) to encode the continuous state of\n",
      "son is still a challenging issue. To handle com- our RL agent, which reasons in the vector space\n",
      "plex queries where there are no obvious answers, environment of the knowledge graph. The agent\n",
      "intelligent machines must be able to reason with takes incremental steps by sampling a relation to\n",
      "existing resources, and learn to infer an unknown extend its path. To better guide the RL agent for\n",
      "answer. learning relational paths, we use policy gradient\n",
      "More specifically, we situate our study in the training (Mnih et al., 2015) with a novel reward\n",
      "context of multi-hop reasoning, which is the task function that jointly encourages accuracy, diver-\n",
      "sity,andefficiency. Empirically,weshowthatour\n",
      "1CodeandtheNELLdatasetareavailableathttps://\n",
      "github.com/xwhan/DeepPath. method outperforms PRA and embedding based\n",
      "8102\n",
      "luJ\n",
      "7\n",
      "]LC.sc[\n",
      "3v09660.7071:viXra\n",
      "methods on a Freebase and a Never-Ending Lan- therefore it does not scale. Note that many of the\n",
      "guage Learning (Carlson et al., 2010a) dataset. recentKGreasoningmethods(Neelakantanetal.,\n",
      "Ourcontributionsarethree-fold: 2015; Das et al., 2017) still rely on first learning\n",
      "the PRA paths, which only operates in a discrete\n",
      "• We are the first to consider reinforcement\n",
      "space. Comparing to PRA, our method reasons\n",
      "learning(RL)methodsforlearningrelational\n",
      "in a continuous space, and by incorporating vari-\n",
      "pathsinknowledgegraphs;\n",
      "ous criteria in the reward function, our reinforce-\n",
      "ment learning (RL) framework has better control\n",
      "• Our learning method uses a complex reward\n",
      "andmoreflexibilityoverthepath-findingprocess.\n",
      "function that considers accuracy, efficiency,\n",
      "Neural symbolic machine (Liang et al., 2016)\n",
      "and path diversity simultaneously, offering\n",
      "is a more recent work on KG reasoning, which\n",
      "bettercontrolandmoreflexibilityinthepath-\n",
      "also applies reinforcement learning but has a dif-\n",
      "findingprocess;\n",
      "ferent flavor from our work. NSM learns to com-\n",
      "• We show that our method can scale up to poseprogramsthatcanfindanswerstonaturallan-\n",
      "large scale knowledge graphs, outperform- guage questions, while our RL model tries to add\n",
      "ingPRAandKGembeddingmethodsintwo new facts to knowledge graph (KG) by reasoning\n",
      "tasks. on existing KG triples. In order to get answers,\n",
      "NSMlearnstogenerateasequenceofactionsthat\n",
      "In the next section, we outline related work in\n",
      "canbecombinedasaexecutableprogram. Theac-\n",
      "path-findingandembeddingmethodsinKGs. We\n",
      "tionspaceinNSMisasetofpredefinedtokens. In\n",
      "describe the proposed method in Section 3. We\n",
      "ourframework,thegoalistofindreasoningpaths,\n",
      "show experimental results in Section 4. Finally,\n",
      "thustheactionspaceisrelationspaceintheKG.A\n",
      "weconcludeinSection5.\n",
      "similar framework (Johnson et al., 2017) has also\n",
      "2 RelatedWork beenappliedtovisualreasoningtasks.\n",
      "ThePath-RankingAlgorithm(PRA)method(Lao 3 Methodology\n",
      "et al., 2011b) is a primary path-finding approach\n",
      "that uses random walk with restart strategies for Inthissection,wedescribeindetailourRL-based\n",
      "multi-hop reasoning. Gardner et al. (2013; 2014) framework for multi-hop relation reasoning. The\n",
      "proposeamodificationtoPRAthatcomputesfea- specific task of relation reasoning is to find re-\n",
      "ture similarity in the vector space. Wang and liable predictive paths between entity pairs. We\n",
      "Cohen (2015) introduce a recursive random walk formulate the path finding problem as a sequen-\n",
      "approach for integrating the background KG and tialdecisionmakingproblemwhichcanbesolved\n",
      "text—the method performs structure learning of with a RL agent. We first describe the environ-\n",
      "logic programs and information extraction from ment and the policy-based RL agent. By interact-\n",
      "text at the same time. A potential bottleneck for ingwiththeenvironmentdesignedaroundtheKG,\n",
      "randomwalkinferenceisthatsupernodesconnect- the agent learns to pick the promising reasoning\n",
      "ing to large amount of formulas will create huge paths. Thenwedescribethetrainingprocedureof\n",
      "fan-out areas that significantly slow down the in- ourRLmodel. Afterthat,wedescribeanefficient\n",
      "ferenceandaffecttheaccuracy. path-constrainedsearchalgorithmforrelationrea-\n",
      "Toutanovaetal.(2015)provideaconvolutional soningwiththepathsfoundbytheRLagent.\n",
      "neural network solution to multi-hop reasoning.\n",
      "3.1 ReinforcementLearningforRelation\n",
      "TheybuildaCNNmodelbasedonlexicalizedde-\n",
      "Reasoning\n",
      "pendencypaths,whichsuffersfromtheerrorprop-\n",
      "agationissueduetoparseerrors. Guuetal.(2015) The RL system consists of two parts (see Fig-\n",
      "usesKGembeddingstoanswerpathqueries. Zeng ure 1). The first part is the external environment\n",
      "et al. (2014) described a CNN model for rela- E which specifies the dynamics of the interaction\n",
      "tional extraction, but it does not explicitly model between the agent and the KG. This environment\n",
      "therelationalpaths. Neelakantanetal.(2015)pro- is modeled as a Markov decision process (MDP).\n",
      "posearecurrentneuralnetworksmodelformodel- A tuple < S,A,P,R > is defined to represent\n",
      "ingrelationalpathsinknowledgebasecompletion the MDP, where S is the continuous state space,\n",
      "(KBC),butittrainstoomanyseparatemodels,and A = {a,a,...,a } is the set of all available ac-\n",
      "1 2 n\n",
      "Query Node: Band of Brothers The KG Environment Policy Based Agent\n",
      "Reason Task: tvProgramLanguage\n",
      "English\n",
      "State\n",
      "Next State\n",
      "Caesars ReLU\n",
      "Entertain… personLanguages Actor\n",
      "Reward\n",
      "serviceLocation-1 Neal profession ReLU\n",
      "McDonough\n",
      "United Reason Step\n",
      "States Tom Hanks\n",
      "Softmax\n",
      "castActor\n",
      "countryOfOrigin awardWorkWinner 𝛑(a|s)\n",
      "Graham writtenBy Band of music Michael\n",
      "Yost Brothers Kamen\n",
      "tvProgramCreator... tvProgramGenre\n",
      "HBO Mini-Series\n",
      "Figure1:OverviewofourRLmodel.Left:TheKGenvironmentEmodeledbyaMDP.Thedottedarrows(partially)showthe\n",
      "existingrelationlinksintheKGandtheboldarrowsshowthereasoningpathsfoundbytheRLagent. −1denotestheinverse\n",
      "ofanrelation. Right: Thestructureofthepolicynetworkagent. Ateachstep,byinteractingwiththeenvironment,theagent\n",
      "learnstopickarelationlinktoextendthereasoningpaths.\n",
      "tions, P(S = s(cid:48)|S = s,A = a) is the transi- Beginningwiththesourceentitye,theagentuse\n",
      "t+1 t t s\n",
      "tion probability matrix, and R(s,a) is the reward the policy network to pick the most promising\n",
      "functionofevery(s,a)pairs. relation to extend its path at each step until it\n",
      "reaches the target entity e. To keep the output\n",
      "The second part of the system, the RL t\n",
      "dimension of the policy network consistent, the\n",
      "agent, is represented as a policy network\n",
      "action space is defined as all the relations in the\n",
      "π (s,a) = p(a|s;θ) which maps the state vec-\n",
      "θ\n",
      "KG.\n",
      "tor to a stochastic policy. The neural network\n",
      "parameters θ are updated using stochastic gra-\n",
      "States The entities and relations in a KG are\n",
      "dient descent. Compared to Deep Q Network\n",
      "naturally discrete atomic symbols. Since exist-\n",
      "(DQN) (Mnih et al., 2013), policy-based RL\n",
      "ing practical KGs like Freebase (Bollacker et al.,\n",
      "methods turn out to be more appropriate for our\n",
      "2008)andNELL(Carlsonetal.,2010b)oftenhave\n",
      "knowledge graph scenario. One reason is that\n",
      "huge amounts of triples. It is impossible to di-\n",
      "for the path finding problem in KG, the action\n",
      "rectly model all the symbolic atoms in states. To\n",
      "space can be very large due to complexity of the\n",
      "capture the semantic information of these sym-\n",
      "relationgraph. Thiscanleadtopoorconvergence\n",
      "bols,weusetranslation-basedembeddingssuchas\n",
      "properties for DQN. Besides, instead of learning\n",
      "TransE (Bordes et al., 2013) and TransH (Wang\n",
      "a greedy policy which is common in value-based\n",
      "etal.,2014)torepresenttheentitiesandrelations.\n",
      "methods like DQN, the policy network is able to\n",
      "These embeddings map all the symbols to a low-\n",
      "learn a stochastic policy which prevent the agent\n",
      "dimensionalvectorspace. Inourframework,each\n",
      "fromgettingstuckatanintermediatestate. Before\n",
      "statecapturestheagent’spositionintheKG.After\n",
      "we describe the structure of our policy network,\n",
      "takinganaction,theagentwillmovefromoneen-\n",
      "we first describe the components (actions, states,\n",
      "titytoanother. Thesetwoarelinkedbytheaction\n",
      "rewards)oftheRLenvironment.\n",
      "(relation)justtakenbytheagent. Thestatevector\n",
      "atsteptisgivenasfollows:\n",
      "Actions Given the entity pairs (e,e ) with\n",
      "s t\n",
      "s = (e,e −e )\n",
      "relation r, we want the agent to find the most t t target t\n",
      "informative paths linking these entity pairs. where e denotes the embeddings of the current\n",
      "t\n",
      "entitynodeande denotestheembeddingsof betweenthecurrentpathandtheexistingones:\n",
      "target\n",
      "the target entity. At the initial state, e = e.\n",
      "t source\n",
      "We do not incorporate the reasoning relation in\n",
      "|F|\n",
      "the state, because the embedding ofthe reasoning 1 (cid:88)\n",
      "r = − cos(p,p )\n",
      "relation remain constant during path finding, DIVERSITY |F| i\n",
      "i=1\n",
      "which is not helpful in training. However, we\n",
      "find out that by training the RL agent using a set\n",
      "of positive samples for one particular relation, where p =\n",
      "(cid:80)n\n",
      "i=1r i represents the path embed-\n",
      "the agent can successfully discover the relation dingfortherelationchainr 1 → r 2 →... → r n.\n",
      "semantics. Policy Network We use a fully-connected neu-\n",
      "ral network to parameterize the policy function\n",
      "RewardsThereareafewfactorsthatcontributeto π(s;θ) that maps the state vector s to a proba-\n",
      "thequalityofthepathsfoundbytheRLagent. To bility distribution over all possible actions. The\n",
      "encourage the agent to find predictive paths, our neuralnetworkconsistsoftwohiddenlayers,each\n",
      "rewardfunctionsincludethefollowingscoringcri- followed by a rectifier nonlinearity layer (ReLU).\n",
      "teria: The output layer is normalized using a softmax\n",
      "Global accuracy: For our environment settings, function(seeFigure1).\n",
      "the number of actions that can be taken by the\n",
      "agent can be very large. In other words, there are\n",
      "3.2 TrainingPipeline\n",
      "muchmoreincorrectsequentialdecisionsthanthe\n",
      "correct ones. The number of these incorrect de-\n",
      "In practice, one big challenge of KG reasoning is\n",
      "cision sequences can increase exponentially with\n",
      "that the relation set can be quite large. For a typ-\n",
      "the length of the path. In view of this challenge,\n",
      "ical KG, the RL agent is often faced with hun-\n",
      "the first reward function we add to the RL model\n",
      "dreds (thousands) of possible actions. In other\n",
      "isdefinedasfollows:\n",
      "words, the output layer of the policy network of-\n",
      "(cid:40)\n",
      "+1, ifthepathreachese ten has a large dimension. Due to the complexity\n",
      "target\n",
      "r =\n",
      "GLOBAL of the relation graph and the large action space,\n",
      "−1, otherwise\n",
      "if we directly train the RL model by trial and er-\n",
      "the agent is given an offline positive reward +1 if rors, which is typical for RL algorithms, the RL\n",
      "itreachesthetargetafterasequenceofactions. model will show very poor convergence proper-\n",
      "Path efficiency: For the relation reasoning task, ties. After a long-time training, the agents fails\n",
      "we observe that short paths tend to provide more to find any valuable path. To tackle this prob-\n",
      "reliable reasoning evidence than longer paths. lem,westartourtrainingwithasupervisedpolicy\n",
      "Shorter chains of relations can also improve the whichisinspiredbytheimitationlearningpipeline\n",
      "efficiency of the reasoning by limiting the length used by AlphaGo (Silver et al., 2016). In the Go\n",
      "oftheRL’sinteractionswiththeenvironment. The game, the player is facing nearly 250 possible le-\n",
      "efficiencyrewardisdefinedasfollows: galmovesateachstep. Directlytrainingtheagent\n",
      "to pick actions from the original action space can\n",
      "1\n",
      "r = beadifficulttask. AlphaGofirsttrainasupervised\n",
      "EFFICIENCY length(p)\n",
      "policy network using experts moves. In our case,\n",
      "thesupervisedpolicyistrainedwitharandomized\n",
      "where path p is defined as a sequence of relations\n",
      "breadth-firstsearch(BFS).\n",
      "r → r →... → r.\n",
      "1 2 n\n",
      "Pathdiversity: Wetraintheagenttofindpathsus- Supervised Policy Learning For each relation,\n",
      "ingpositivesamplesforeachrelation. Thesetrain- we use a subset of all the positive samples (en-\n",
      "ingsample(e,e )havesimilarstaterep- titypairs)tolearnthesupervisedpolicy. Foreach\n",
      "source target\n",
      "resentations in the vector space. The agent tends positive sample (e,e ), a two-side BFS\n",
      "source target\n",
      "to find paths with similar syntax and semantics. is conducted to find same correct paths between\n",
      "These paths often contains redundant information the entities. For each path p with a sequence of\n",
      "sincesomeofthemmaybecorrelated. Toencour- relations r → r →... → r, we update the pa-\n",
      "1 2 n\n",
      "agetheagenttofinddiversepaths,wedefineadi- rameters θ to maximize the expected cumulative\n",
      "versityrewardfunctionusingthecosinesimilarity reward using Monte-Carlo Policy Gradient (RE-\n",
      "INFORCE)(Williams,1992): Algorithm 1: Retraining Procedure with re-\n",
      "(cid:88) wardfunctions\n",
      "J(θ) = E ( R )\n",
      "a∼π(a|s;θ) st,at Restoreparametersθ fromsupervisedpolicy;\n",
      "1\n",
      "t\n",
      "forepisode←1toN do\n",
      "(cid:88)(cid:88) 2\n",
      "= π(a|s ;θ)R (1)\n",
      "t st,at\n",
      "3\n",
      "Initializestatevectors\n",
      "t\n",
      "← s\n",
      "0\n",
      "t a∈A Initializeepisodelengthsteps ← 0\n",
      "4\n",
      "where J(θ) is the expected total rewards for one 5\n",
      "whilenum steps < max lengthdo\n",
      "Randomlysampleactiona ∼ π(a|s )\n",
      "episode. For supervised learning, we give a re- 6 t\n",
      "ward of +1 for each step of a successful episode. 7\n",
      "ObserverewardR t,nextstates\n",
      "t+1\n",
      "// if the step fails\n",
      "By plugging in the paths found by the BFS, the\n",
      "ifR = −1then\n",
      "approximated gradient used to update the policy 8 t\n",
      "Save< s,a >toM\n",
      "networkisshownbelow: 9 t neg\n",
      "ifsuccessorsteps = max length\n",
      "10\n",
      "(cid:88)(cid:88)\n",
      "∇ J(θ) = π(a|s ;θ)∇ logπ(a|s ;θ) then\n",
      "θ t θ t\n",
      "t a∈A 11 break\n",
      "≈ ∇\n",
      "(cid:88)\n",
      "logπ(a = r |s ;θ) (2) 12\n",
      "Incrementnum steps\n",
      "θ t t\n",
      "t // penalize failed steps\n",
      "Updateθ using\n",
      "wherer belongstothepathp. 13\n",
      "t (cid:80)\n",
      "g ∝ ∇ logπ(a = r |s ;θ)(−1)\n",
      "However, the vanilla BFS is a biased search al- θ Mneg t t\n",
      "gorithm which prefers short paths. When plug- ifsuccessthen\n",
      "ging in these biased paths, it becomes difficult 14 R total ← λ 1r GLOBAL+λ 2r EFFICIENCY+\n",
      "for the agent to find longer paths which may po- λ 3r\n",
      "DIVERSITY\n",
      "tentially be useful. We want the paths to be 15 Updateθ using\n",
      "(cid:80)\n",
      "controlled only by the defined reward functions. g ∝ ∇ θ tlogπ(a = r t|s t;θ)R total\n",
      "To prevent the biased search, we adopt a sim-\n",
      "ple trick to add some random mechanisms to the\n",
      "BFS. Instead of directly searching the path be-\n",
      "boundmax length. Theepisodeendsiftheagent\n",
      "tween e and e, we randomly pick a in-\n",
      "source target failstoreachthetargetentitywithinmax length\n",
      "termediate node e and then conduct two BFS\n",
      "inter steps. After each episode, the policy network is\n",
      "between(e,e )and(e,e ). The\n",
      "source inter inter target updatedusingthefollowinggradient:\n",
      "concatenatedpathsareusedtotraintheagent. The\n",
      "supervised learning saves the agent great efforts (cid:88)\n",
      "∇ J(θ) = ∇ logπ(a = r |s ;θ)R (3)\n",
      "θ θ t t total\n",
      "learning from failed actions. With the learned ex-\n",
      "t\n",
      "perience, we then train the agent to find desirable\n",
      "paths. where R is the linear combination of the de-\n",
      "total\n",
      "Retraining with Rewards To find the reasoning fined reward functions. The detail of the retrain\n",
      "paths controlled by the reward functions, we use process is shown in Algorithm 1. In practice, θ is\n",
      "reward functions to retrain the supervised policy updated using the Adam Optimizer (Kingma and\n",
      "network. Foreachrelation,thereasoningwithone Ba,2014)withL 2 regularization.\n",
      "entitypairistreatedasoneepisode. Startingwith\n",
      "3.3 Bi-directionalPath-constrainedSearch\n",
      "the source node e, the agent picks a relation\n",
      "source\n",
      "accordingtothestochasticpolicyπ(a|s),whichis Given an entity pair, the reasoning paths learned\n",
      "a probability distribution over all relations, to ex- by the RL agent can be used as logical formulas\n",
      "tenditsreasoningpath. Thisrelationlinkmaylead to predict the relation link. Each formula is veri-\n",
      "to a new entity, or it may lead to nothing. These fiedusingabi-directionalsearch. InatypicalKG,\n",
      "failedstepswillcausetheagenttoreceivenegative one entity node can be linked to a large number\n",
      "rewards. The agent will stay at the same state af- of neighbors with the same relation link. A sim-\n",
      "terthesefailedsteps. Sincetheagentisfollowing ple example is the relation personNationality−1,\n",
      "astochasticpolicy,theagentwillnotgetstuckby which denotes the inverse of personNationality.\n",
      "repeatingawrongstep. Toimprovethetrainingef- Following this link, the entity United States can\n",
      "ficiency,welimittheepisodelengthwithanupper reach numerous neighboring entities. If the for-\n",
      "Dataset #Ent. #R. #Triples #Tasks\n",
      "Algorithm 2: Bi-directional search for path\n",
      "FB15K-237 14,505 237 310,116 20\n",
      "verification\n",
      "NELL-995 75,492 200 154.213 12\n",
      "Givenareasoningpath\n",
      "1\n",
      "Table1:StatisticsoftheDatasets.#Ent.denotesthenumber\n",
      "p : r → r →... → r\n",
      "1 2 n ofuniqueentitiesand#R.denotesthenumberofrelations\n",
      "for(e,e )intestsetD do\n",
      "2 i j\n",
      "start←0;end←n\n",
      "3\n",
      "4 left ← ∅;right ← ∅ are subsets of larger datasets. The triples in\n",
      "5 whilestart<end do FB15K-237 (Toutanova et al., 2015) are sampled\n",
      "6 leftEx ← ∅;rightEx ← ∅ from FB15K (Bordes et al., 2013) with redun-\n",
      "7 iflen(left)<len(right)then dantrelationsremoved. Weperformthereasoning\n",
      "8 Extendpathontheleftside tasks on 20 relations which have enough reason-\n",
      "9 AddconnectednodestoleftEx ing paths. These tasks consists of relations from\n",
      "10 left ← leftEx different domains like Sports, People, Locations,\n",
      "Film, etc. Besides, we present a new NELL sub-\n",
      "else\n",
      "11\n",
      "set that is suitable for multi-hop reasoning from\n",
      "Extendpathontherightside\n",
      "12\n",
      "the 995th iteration of the NELL system. We first\n",
      "AddconnectednodestorightEx\n",
      "13\n",
      "removethetripleswithrelationgeneralizationsor\n",
      "right ← rightEx\n",
      "14\n",
      "haswikipediaurl. Thesetworelationsappearmore\n",
      "15 ifleft∩right (cid:54)= ∅then than2MtimesintheNELLdataset,buttheyhave\n",
      "16 returnTrue no reasoning values. After this step, we only se-\n",
      "else lectthetripleswithTop-200relations. Tofacilitate\n",
      "17\n",
      "returnFalse path finding, we also add the inverse triples. For\n",
      "18\n",
      "each triple (h,r,t), we append (t,r−1,h) to the\n",
      "datasets. With these inverse triples, the agent is\n",
      "abletostepbackwardintheKG.\n",
      "mula consists of such links, the number of inter- For each reasoning task r, we remove all the\n",
      "i\n",
      "mediate entities can exponentially increase as we tripleswithr orr−1fromtheKG.Theseremoved\n",
      "i i\n",
      "follow the reasoning formula. However, we ob- triples are split into train and test samples. For\n",
      "serve that for these formulas, if we verify the for- the link prediction task, each h in the test triples\n",
      "mulafromtheinversedirection. Thenumberofin- {(h,r,t)} is considered as one query. A set of\n",
      "termediate nodes can be tremendously decreased. candidatetargetentitiesarerankedusingdifferent\n",
      "Algorithm 2 shows a detailed description of the methods. For fact prediction, the true test triples\n",
      "proposedbi-directionalsearch. arerankedwithsomegeneratedfalsetriples.\n",
      "4 Experiments\n",
      "4.2 BaselinesandImplementationDetails\n",
      "To evaluate the reasoning formulas found by our Most KG reasoning methods are based on either\n",
      "RL agent, we explore two standard KG reason- path formulas or KG embeddings. we explore\n",
      "ing tasks: link prediction (predicting target en- methodsfrombothofthesetwoclassesinourex-\n",
      "tities) and fact prediction (predicting whether an periments. For path based methods, we compare\n",
      "unknown fact holds or not). We compare our our RL model with the PRA (Lao et al., 2011a)\n",
      "methodwithbothpath-basedmethodsandembed- algorithm,whichhasbeenusedinacoupleofrea-\n",
      "dingbasedmethods. Afterthat,wefurtheranalyze soning methods (Gardner et al., 2013; Neelakan-\n",
      "thereasoningpathsfoundbyourRLagent. These tan et al., 2015). PRA is a data-driven algorithm\n",
      "highly predictive paths validate the effectiveness usingrandomwalks(RW)tofindpathsandobtain\n",
      "oftherewardfunctions. Finally,weconductaex- path features. For embedding based methods, we\n",
      "perimenttoinvestigatetheeffectofthesupervised evaluate several state-of-the-art embeddings de-\n",
      "learningprocedure. signed for knowledge base completion, such as\n",
      "TransE(Bordesetal.,2013),TransH(Wangetal.,\n",
      "4.1 DatasetandSettings\n",
      "2014), TransR (Lin et al., 2015) and TransD (Ji\n",
      "Table 1 shows the statistics of the two datasets etal.,2015).\n",
      "we conduct our experiments on. Both of them The implementation of PRA is based on the\n",
      "FB15K-237 NELL-995\n",
      "Tasks PRA RL TransE TransR Tasks PRA RL TransE TransR\n",
      "teamSports 0.987 0.955 0.896 0.784 athletePlaysForTeam 0.547 0.750 0.627 0.673\n",
      "birthPlace 0.441 0.531 0.403 0.417 athletePlaysInLeague 0.841 0.960 0.773 0.912\n",
      "personNationality 0.846 0.823 0.641 0.720 athleteHomeStadium 0.859 0.890 0.718 0.722\n",
      "filmDirector 0.349 0.441 0.386 0.399 athletePlaysSport 0.474 0.957 0.876 0.963\n",
      "filmWrittenBy 0.601 0.457 0.563 0.605 teamPlaySports 0.791 0.738 0.761 0.814\n",
      "filmLanguage 0.663 0.670 0.642 0.641 orgHeadquaterCity 0.811 0.790 0.620 0.657\n",
      "tvLanguage 0.960 0.969 0.804 0.906 worksFor 0.681 0.711 0.677 0.692\n",
      "capitalOf 0.829 0.783 0.554 0.493 bornLocation 0.668 0.757 0.712 0.812\n",
      "organizationFounded 0.281 0.309 0.390 0.339 personLeadsOrg 0.700 0.795 0.751 0.772\n",
      "musicianOrigin 0.426 0.514 0.361 0.379 orgHiredPerson 0.599 0.742 0.719 0.737\n",
      "......\n",
      "Overall 0.541 0.572 0.532 0.540 0.675 0.796 0.737 0.789\n",
      "Table2:Linkpredictionresults(MAP)ontwodatasets.\n",
      "code released by (Lao et al., 2011a). We use the FactPredictionResults\n",
      "TopKnegativemodetogeneratenegativesamples\n",
      "Methods FB15K-237 NELL-995\n",
      "for both train and test samples. For each pos-\n",
      "itive samples, there are approximately 10 corre- RL 0.311 0.493\n",
      "spondingnegativesamples. Eachnegativesample TransE 0.277 0.383\n",
      "is generated by replacing the true target entity t TransH 0.309 0.389\n",
      "with a faked one t(cid:48) in each triple (h,r,t). These TransR 0.302 0.406\n",
      "positive and negative test pairs generated by PRA TransD 0.303 0.413\n",
      "make up the test set for all methods evaluated in\n",
      "thispaper. ForTransE,R,H,D,welearnaseparate Table3:Factpredictionresults(MAP)ontwodatasets.\n",
      "embedding matrix for each reasoning task using\n",
      "thepositivetrainingentitypairs. Alltheseembed- #ofReasoningPaths\n",
      "dingsaretrainedfor1,000epochs. 2\n",
      "Tasks PRA RL\n",
      "Our RL model make use of TransE to get the\n",
      "continuous representation of the entities and rela- worksFor 247 25\n",
      "tions. We use the same dimension as TransE, R teamPlaySports 113 27\n",
      "to embed the entities. Specifically, the state vec- teamPlaysInLeague 69 21\n",
      "tor we use has a dimension of 200, which is also athletehomestadium 37 11\n",
      "the input size of the policy network. To reason organizationHiredPerson 244 9\n",
      "using the path formulas, we adopt a similar lin-...\n",
      "ear regression approach as in PRA to re-rank the Average# 137.2 20.3\n",
      "paths. However,insteadofusingtherandomwalk\n",
      "Table4:NumberofreasoningpathsusedbyPRAandourRL\n",
      "probabilities as path features, which can be com-\n",
      "model. RLachievedbetterMAPwithamorecompactsetof\n",
      "putationallyexpensive,wesimplyusebinarypath learnedpaths.\n",
      "featuresobtainedbythebi-directionalsearch. We\n",
      "observethatwithonlyafewminedpathformulas,\n",
      "Since path-based methods generally work better\n",
      "our method can achieve better results than PRA’s\n",
      "than embedding methods for this task, we do not\n",
      "data-drivenapproach.\n",
      "include the other two embedding baselines in this\n",
      "4.3 Results table. Instead, we spare the room to show the de-\n",
      "tailedresultsoneachrelationreasoningtask.\n",
      "4.3.1 QuantitativeResults\n",
      "FortheoverallMAPshowninthelastrowofthe\n",
      "LinkPredictionThistaskistorankthetargeten-\n",
      "table,ourapproachsignificantlyoutperformsboth\n",
      "titiesgivenaqueryentity. Table2showsthemean\n",
      "thepath-basedmethodandembeddingmethodson\n",
      "average precision (MAP) results on two datasets.\n",
      "twodatasets,whichvalidatesthestrongreasoning\n",
      "abilityofourRLmodel. Formostrelations,since\n",
      "2The implementation we used can be found at https:\n",
      "//github.com/thunlp/Fast-TransX the embedding methods fail to use the path infor-\n",
      "120\n",
      "100\n",
      "80\n",
      "60\n",
      "40\n",
      "20\n",
      "0\n",
      "0 5 10 15 20 25\n",
      "distribution of reasoning paths\n",
      "shtap\n",
      "fo\n",
      "rebmun\n",
      "0.20\n",
      "NELL-995\n",
      "FB15K-237\n",
      "0.15\n",
      "0.10\n",
      "0.05\n",
      "0.00\n",
      "0 50 100 150 200\n",
      "training episodes\n",
      "Figure2:Thedistributionofpathslengthsontwodatasets\n",
      "mation in the KG, they generally perform worse\n",
      "thanourRLmodelorPRA.However, whenthere\n",
      "are not enough paths between entities, our model\n",
      "and PRA can give poor results. For example,\n",
      "fortherelationfilmWrittenBy,ourRLmodelonly\n",
      "finds4uniquereasoningpaths,<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  20212,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['link prediction', 'fact prediction']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  implementation we used can be found at https:\n",
      "//github.com/thunlp/Fast-TransX the embedding methods fail to use the path infor-\n",
      "120\n",
      "100\n",
      "80\n",
      "60\n",
      "40\n",
      "20\n",
      "0\n",
      "0 5 10 15 20 25\n",
      "distribution of reasoning paths\n",
      "shtap\n",
      "fo\n",
      "rebmun\n",
      "0.20\n",
      "NELL-995\n",
      "FB15K-237\n",
      "0.15\n",
      "0.10\n",
      "0.05\n",
      "0.00\n",
      "0 50 100 150 200\n",
      "training episodes\n",
      "Figure2:Thedistributionofpathslengthsontwodatasets\n",
      "mation in the KG, they generally perform worse\n",
      "thanourRLmodelorPRA.However, whenthere\n",
      "are not enough paths between entities, our model\n",
      "and PRA can give poor results. For example,\n",
      "fortherelationfilmWrittenBy,ourRLmodelonly\n",
      "finds4uniquereasoningpaths,whichmeansthere\n",
      "isactuallynotenoughreasoningevidenceexisting\n",
      "in the KG. Another observation is that we always\n",
      "get better performance on the NELL dataset. By\n",
      "analyzing the paths found from the KGs, we be-\n",
      "lievethepotentialreasonisthattheNELLdataset\n",
      "has more short paths than FB15K-237 and some\n",
      "ofthemaresimplysynonymsofthereasoningre-\n",
      "lations.\n",
      "Fact Prediction Instead of ranking the target en-\n",
      "tities, this task directly ranks all the positive and\n",
      "negative samples for a particular relation. The\n",
      "PRA is not included as a baseline here, since the\n",
      "PRA code only gives a target entity ranking for\n",
      "eachquerynodeinsteadofarankingofalltriples.\n",
      "Table 3 shows the overall results of all the meth-\n",
      "ods. OurRLmodelgetsevenbetterresultsonthis\n",
      "task. Wealso observethat theRLmodel beatsall\n",
      "theembeddingbaselinesonmostreasoningtasks.\n",
      "4.3.2 QualitativeAnalysisofReasoningPaths\n",
      "To analyze the properties of reasoning paths, we\n",
      "show a few reasoning paths found by the agent\n",
      "in Table 5. To illustrate the effect of the effi-\n",
      "ciency reward function, we show the path length\n",
      "distributions in Figure 2. To interpret these paths,\n",
      "take the personNationality relation for example,\n",
      "the first reasoning path indicates that if we know\n",
      "facts placeOfBirth(x,y) and locationContains(z,y)\n",
      "thenitishighlypossiblethatpersonxhasnation-\n",
      "ality z. These short but predictive paths indicate\n",
      "the effectiveness of the RL model. Another im-\n",
      "portant observation is that our model use much\n",
      "spets\n",
      "01\n",
      "nihtiw\n",
      "oitar\n",
      "sseccus\n",
      "Figure3: Thesuccessratio(succ )duringtraining. Task: 10\n",
      "athletePlaysForTeam.3\n",
      "fewer reasoning paths than PRA, which indicates\n",
      "that our model can actually extract the most reli-\n",
      "able reasoning evidence from KG. Table 4 shows\n",
      "somecomparisonsaboutthenumberofreasoning\n",
      "paths. We can see that, with the pre-defined re-\n",
      "wardfunctions,theRLagentiscapableofpicking\n",
      "the strong ones and filter out similar or irrelevant\n",
      "ones.\n",
      "4.3.3 EffectofSupervisedLearning\n",
      "AsmentionedinSection3.2,onemajorchallenge\n",
      "for applying RL to KG reasoning is the large ac-\n",
      "tion space. We address this issue by applying\n",
      "supervised learning before the reward retraining\n",
      "step. To show the effect of the supervised train-\n",
      "ing,weevaluatetheagent’ssuccessratioofreach-\n",
      "ingthetargetwithin10steps(succ )afterdiffer- 10\n",
      "ent number of training episodes. For each train-\n",
      "ing episode, one pair of entities (e,e ) source target\n",
      "in the train set is used to find paths. All the cor-\n",
      "rectpathslinkingtheentitieswillgeta+1global\n",
      "reward. Wethenpluginsometruepathsfortrain-\n",
      "ing. Thesucc iscalculatedonaheld-outtestset\n",
      "10\n",
      "that consists of 100 entity pairs. For the NELL-\n",
      "995 dataset, since we have 200 unique relations,\n",
      "the dimension of the action space will be 400 af-\n",
      "ter we add the backward actions. This means that\n",
      "randomwalkswillgetverylowsucc sincethere 10\n",
      "maybenearly40010invalidpaths. Figure3shows\n",
      "the succ during training. We see that even the 10\n",
      "agenthasnotseentheentitybefore,itcanactually\n",
      "pickthepromisingrelationtoextenditspath. This\n",
      "also validates the effectiveness of our state repre-\n",
      "sentations.\n",
      "3Theconfidencebandisgeneratedusing50differentruns.\n",
      "Relation ReasoningPath\n",
      "filmReleaseRegion\n",
      "filmCountry featureFilmLocation→locationContains−1\n",
      "actorFilm−1 →personNationality\n",
      "placeOfBirth→locationContains−1\n",
      "personNationality peoplePlaceLived→locationContains−1\n",
      "peopleMarriage→locationOfCeremony→locationContains−1\n",
      "tvCountryOfOrigin→countryOfficialLanguage\n",
      "tvProgramLanguage tvCountryOfOrigin→filmReleaseRegion−1 →filmLanguage\n",
      "tvCastActor→filmLanguage\n",
      "personBornInCity\n",
      "personBornInLocation graduatedUniversity→graduatedSchool−1 →personBornInCity\n",
      "personBornInCity→atLocation−1 →atLocation\n",
      "athleteHomeStadium→teamHomeStadium−1\n",
      "athletePlaysForTeam athletePlaysSport→teamPlaysSport−1\n",
      "athleteLedSportsTeam\n",
      "worksFor\n",
      "personLeadsOrganization organizationTerminatedPerson−1\n",
      "mutualProxyFor−1\n",
      "Table5: ExamplereasoningpathsfoundbyourRLmodel. ThefirstthreerelationscomefromtheFB15K-237dataset. The\n",
      "othersarefromNELL-995.Inversesofexistingrelationsaredenotedby−1.\n",
      "5 ConclusionandFutureWork Acknowledgments\n",
      "We gratefully acknowledge the support of\n",
      "NVIDIACorporationwiththedonationofoneTi-\n",
      "In this paper, we propose a reinforcement learn-\n",
      "tanXPascalGPUusedforthisresearch.\n",
      "ing framework to improve the performance of re-\n",
      "lation reasoning in KGs. Specifically, we train a\n",
      "RLagenttofindreasoningpathsintheknowledge\n",
      "References\n",
      "base. Unlikepreviouspathfindingmodelsthatare\n",
      "Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim\n",
      "based on random walks, the RL model allows us\n",
      "Sturge,andJamieTaylor.2008. Freebase: acollab-\n",
      "tocontrolthepropertiesofthefoundpaths. These\n",
      "oratively created graph database for structuring hu-\n",
      "effectivepathscanalsobeusedasanalternativeto man knowledge. In Proceedings of the 2008 ACM\n",
      "PRA in many path-based reasoning methods. For SIGMOD international conference on Management\n",
      "ofdata,pages1247–1250.ACM.\n",
      "two standard reasoning tasks, using the RL paths\n",
      "asreasoningformulas,ourapproachgenerallyout- Antoine Bordes, Nicolas Usunier, Alberto Garcia-\n",
      "performstwoclassesofbaselines. Duran, Jason Weston, and Oksana Yakhnenko.\n",
      "2013. Translating embeddings for modeling multi-\n",
      "For future studies, we plan to investigate relational data. In Advances in neural information\n",
      "the possibility of incorporating adversarial learn- processingsystems,pages2787–2795.\n",
      "ing (Goodfellow et al., 2014) to give better re- AndrewCarlson,JustinBetteridge,BryanKisiel,Burr\n",
      "wards than the human-defined reward functions Settles, Estevam R. Hruschka Jr., and Tom M.\n",
      "used in this work. Instead of designing rewards Mitchell. 2010a. Toward an architecture for never-\n",
      "endinglanguagelearning. InAAAI.\n",
      "according to path characteristics, a discriminative\n",
      "modelcanbetrainedtogiverewards. Also,toad- AndrewCarlson,JustinBetteridge,BryanKisiel,Burr\n",
      "dress the problematic scenario when the KG does Settles, Estevam R. Hruschka Jr., and Tom M.\n",
      "Mitchell. 2010b. Toward an architecture for never-\n",
      "nothaveenoughreasoningpaths,weareinterested\n",
      "ending language learning. In Proceedings of the\n",
      "in applying our RL framework to joint reasoning\n",
      "Twenty-FourthConferenceonArtificialIntelligence\n",
      "withKGtriplesandtextmentions. (AAAI2010).\n",
      "Rajarshi Das, Arvind Neelakantan, David Belanger, Ni Lao, Jun Zhu, Xinwang Liu, Yandong Liu, and\n",
      "andAndrewMcCallum.2017. Chainsofreasoning WilliamWCohen.2010. Efficientrelationallearn-\n",
      "overentities,relations,andtextusingrecurrentneu- ing with hidden variable detection. In NIPS, pages\n",
      "ralnetworks. EACL. 1234–1242.\n",
      "Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel, Chen Liang, Jonathan Berant, Quoc Le, Kenneth D\n",
      "and Tom M Mitchell. 2013. Improving learning Forbus, and Ni Lao. 2016. Neural symbolic\n",
      "andinferenceinalargeknowledge-baseusinglatent machines: Learning semantic parsers on free-\n",
      "syntacticcues. InEMNLP,pages833–838. base with weak supervision. arXiv preprint\n",
      "arXiv:1611.00020.\n",
      "MattGardner,ParthaPratimTalukdar,JayantKrishna-\n",
      "murthy,andTomMitchell.2014. Incorporatingvec- YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,and\n",
      "tor space similarity in random walk inference over Xuan Zhu. 2015. Learning entity and relation em-\n",
      "knowledgebases. beddingsforknowledgegraphcompletion. InAAAI,\n",
      "pages2181–2187.\n",
      "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\n",
      "BingXu,DavidWarde-Farley,SherjilOzair,Aaron Volodymyr Mnih, Koray Kavukcuoglu, David Sil-\n",
      "Courville,andYoshuaBengio.2014. Generativead- ver, Alex Graves, Ioannis Antonoglou, Daan Wier-\n",
      "versarial nets. In Advances in Neural Information stra, and Martin Riedmiller. 2013. Playing atari\n",
      "ProcessingSystems,pages2672–2680. with deep reinforcement learning. arXiv preprint\n",
      "arXiv:1312.5602.\n",
      "Kelvin Guu, John Miller, and Percy Liang. 2015.\n",
      "Traversing knowledge graphs in vector space. In\n",
      "Volodymyr Mnih, Koray Kavukcuoglu, David Silver,\n",
      "EMNLP.\n",
      "Andrei A Rusu, Joel Veness, Marc G Bellemare,\n",
      "Alex Graves, Martin Riedmiller, Andreas K Fidje-\n",
      "Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl,\n",
      "land, Georg Ostrovski, et al. 2015. Human-level\n",
      "Abdel-rahman Mohamed, Navdeep Jaitly, Andrew\n",
      "control through deep reinforcement learning. Na-\n",
      "Senior,VincentVanhoucke,PatrickNguyen,TaraN\n",
      "ture,518(7540):529–533.\n",
      "Sainath, et al. 2012. Deep neural networks for\n",
      "acousticmodelinginspeechrecognition:Theshared\n",
      "ArvindNeelakantan,BenjaminRoth,andAndrewMc-\n",
      "viewsoffourresearchgroups. IEEESignalProcess-\n",
      "Callum. 2015. Compositional vector space mod-\n",
      "ingMagazine,29(6):82–97.\n",
      "els for knowledge base completion. arXiv preprint\n",
      "arXiv:1504.06662.\n",
      "GuoliangJi,ShizhuHe,LihengXu,KangLiu,andJun\n",
      "Zhao. 2015. Knowledge graph embedding via dy-\n",
      "David Silver, Aja Huang, Chris J Maddison, Arthur\n",
      "namicmappingmatrix. InACL(1),pages687–696.\n",
      "Guez,LaurentSifre,GeorgeVanDenDriessche,Ju-\n",
      "lian Schrittwieser, Ioannis Antonoglou, Veda Pan-\n",
      "Justin Johnson, Bharath Hariharan, Laurens van der\n",
      "neershelvam, Marc Lanctot, et al. 2016. Mastering\n",
      "Maaten,JudyHoffman,LiFei-Fei,CLawrenceZit-\n",
      "the game of go with deep neural networks and tree\n",
      "nick, and Ross Girshick. 2017. Inferring and exe-\n",
      "cutingprogramsforvisualreasoning. arXivpreprint search. Nature,529(7587):484–489.\n",
      "arXiv:1705.03633.\n",
      "Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-\n",
      "Yoon Kim. 2014. Convolutional neural net- fungPoon,PallaviChoudhury,andMichaelGamon.\n",
      "works for sentence classification. arXiv preprint 2015. Representingtextforjointembeddingoftext\n",
      "arXiv:1408.5882. andknowledgebases. InEMNLP,volume15,pages\n",
      "1499–1509.Citeseer.\n",
      "Diederik Kingma and Jimmy Ba. 2014. Adam: A\n",
      "method for stochastic optimization. arXiv preprint William Yang Wang and William W Cohen. 2015.\n",
      "arXiv:1412.6980. Joint information extraction and reasoning: A scal-\n",
      "ablestatisticalrelationallearningapproach. InACL.\n",
      "AlexKrizhevsky, IlyaSutskever, andGeoffreyEHin-\n",
      "ton. 2012. Imagenet classification with deep con- Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\n",
      "volutional neural networks. In Advances in neural Chen.2014. Knowledgegraphembeddingbytrans-\n",
      "informationprocessingsystems,pages1097–1105. lating on hyperplanes. In AAAI, pages 1112–1119.\n",
      "Citeseer.\n",
      "Ni Lao, Tom Mitchell, and William W Cohen. 2011a.\n",
      "Randomwalkinferenceandlearninginalargescale Ronald J Williams. 1992. Simple statistical gradient-\n",
      "knowledge base. In Proceedings of the Conference following algorithms for connectionist reinforce-\n",
      "on Empirical Methods in Natural Language Pro- mentlearning. Machinelearning,8(3-4):229–256.\n",
      "cessing, pages 529–539. Association for Computa-\n",
      "tionalLinguistics. Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,\n",
      "Jun Zhao, et al. 2014. Relation classification via\n",
      "Ni Lao, Tom M. Mitchell, and William W. Cohen. convolutional deep neural network. In COLING,\n",
      "2011b. Random walk inference and learning in a pages2335–2344.\n",
      "largescaleknowledgebase. InEMNLP,pages529–\n",
      "539.ACL.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  62965,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Relation Reasoning', 'Fact Prediction']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: DeepPath: A Reinforcement Learning Method for\n",
      "Knowledge Graph Reasoning\n",
      "WenhanXiong and ThienHoang and WilliamYangWang\n",
      "DepartmentofComputerScience\n",
      "UniversityofCalifornia,SantaBarbara\n",
      "SantaBarbara,CA93106USA\n",
      "{xwhan,william}@cs.ucsb.edu, thienhoang@umail.ucsb.edu\n",
      "Abstract of learning explicit inference formulas, given a\n",
      "large KG. For example, if the KG includes the\n",
      "Westudytheproblemoflearningtoreason\n",
      "beliefs such as Neymar plays for Barcelona, and\n",
      "in large scale knowledge graphs (KGs).\n",
      "Barcelona are in the La Liga league, then ma-\n",
      "More specifically, we describe a novel re-\n",
      "chines should be able to learn the following for-\n",
      "inforcementlearningframeworkforlearn-\n",
      "mula: playerPlaysForTeam(P,T) ∧ teamPlaysIn-\n",
      "ing multi-hop relational paths: we use a\n",
      "League(T,L) ⇒ playerPlaysInLeague(P,L). In the\n",
      "policy-based agent with continuous states\n",
      "testing time, by plugging in the learned formulas,\n",
      "based on knowledge graph embeddings,\n",
      "the system should be able to automatically infer\n",
      "which reasons in a KG vector space by\n",
      "the missing link between a pair of entities. This\n",
      "sampling the most promising relation to\n",
      "kind of reasoning machine will potentially serve\n",
      "extend its path. In contrast to prior work,\n",
      "as an essential components of complex QA sys-\n",
      "our approach includes a reward function\n",
      "tems.\n",
      "thattakestheaccuracy,diversity,andef-\n",
      "In recent years, the Path-Ranking Algorithm\n",
      "ficiency into consideration. Experimen-\n",
      "(PRA) (Lao et al., 2010, 2011a) emerges as a\n",
      "tally, we show that our proposed method\n",
      "promising method for learning inference paths in\n",
      "outperforms a path-ranking based algo-\n",
      "largeKGs. PRAusesarandom-walkwithrestarts\n",
      "rithm and knowledge graph embedding\n",
      "based inference mechanism to perform multiple\n",
      "methods on Freebase and Never-Ending\n",
      "bounded depth-first search processes to find rela-\n",
      "LanguageLearningdatasets.1\n",
      "tionalpaths. Coupledwithelastic-netbasedlearn-\n",
      "ing, PRA then picks more plausible paths using\n",
      "1 Introduction\n",
      "supervised learning. However, PRA operates in\n",
      "Deep neural networks for acoustic modeling in a fully discrete space, which makes it difficult to\n",
      "speech recognitionIn recent years, deep learn- evaluateandcomparesimilarentitiesandrelations\n",
      "ing techniques have obtained many state-of-the- inaKG.\n",
      "artresultsinvariousclassificationandrecognition In this work, we propose a novel approach\n",
      "problems (Krizhevsky et al., 2012; Hinton et al., for controllable multi-hop reasoning: we frame\n",
      "2012;Kim,2014). However,complexnaturallan- the path learning process as reinforcement learn-\n",
      "guage processing problems often require multi- ing (RL). In contrast to PRA, we use translation-\n",
      "ple inter-related decisions, and empowering deep based knowledge based embedding method (Bor-\n",
      "learningmodelswiththeabilityoflearningtorea- des et al., 2013) to encode the continuous state of\n",
      "son is still a challenging issue. To handle com- our RL agent, which reasons in the vector space\n",
      "plex queries where there are no obvious answers, environment of the knowledge graph. The agent\n",
      "intelligent machines must be able to reason with takes incremental steps by sampling a relation to\n",
      "existing resources, and learn to infer an unknown extend its path. To better guide the RL agent for\n",
      "answer. learning relational paths, we use policy gradient\n",
      "More specifically, we situate our study in the training (Mnih et al., 2015) with a novel reward\n",
      "context of multi-hop reasoning, which is the task function that jointly encourages accuracy, diver-\n",
      "sity,andefficiency. Empirically,weshowthatour\n",
      "1CodeandtheNELLdatasetareavailableathttps://\n",
      "github.com/xwhan/DeepPath. method outperforms PRA and embedding based\n",
      "8102\n",
      "luJ\n",
      "7\n",
      "]LC.sc[\n",
      "3v09660.7071:viXra\n",
      "methods on a Freebase and a Never-Ending Lan- therefore it does not scale. Note that many of the\n",
      "guage Learning (Carlson et al., 2010a) dataset. recentKGreasoningmethods(Neelakantanetal.,\n",
      "Ourcontributionsarethree-fold: 2015; Das et al., 2017) still rely on first learning\n",
      "the PRA paths, which only operates in a discrete\n",
      "• We are the first to consider reinforcement\n",
      "space. Comparing to PRA, our method reasons\n",
      "learning(RL)methodsforlearningrelational\n",
      "in a continuous space, and by incorporating vari-\n",
      "pathsinknowledgegraphs;\n",
      "ous criteria in the reward function, our reinforce-\n",
      "ment learning (RL) framework has better control\n",
      "• Our learning method uses a complex reward\n",
      "andmoreflexibilityoverthepath-findingprocess.\n",
      "function that considers accuracy, efficiency,\n",
      "Neural symbolic machine (Liang et al., 2016)\n",
      "and path diversity simultaneously, offering\n",
      "is a more recent work on KG reasoning, which\n",
      "bettercontrolandmoreflexibilityinthepath-\n",
      "also applies reinforcement learning but has a dif-\n",
      "findingprocess;\n",
      "ferent flavor from our work. NSM learns to com-\n",
      "• We show that our method can scale up to poseprogramsthatcanfindanswerstonaturallan-\n",
      "large scale knowledge graphs, outperform- guage questions, while our RL model tries to add\n",
      "ingPRAandKGembeddingmethodsintwo new facts to knowledge graph (KG) by reasoning\n",
      "tasks. on existing KG triples. In order to get answers,\n",
      "NSMlearnstogenerateasequenceofactionsthat\n",
      "In the next section, we outline related work in\n",
      "canbecombinedasaexecutableprogram. Theac-\n",
      "path-findingandembeddingmethodsinKGs. We\n",
      "tionspaceinNSMisasetofpredefinedtokens. In\n",
      "describe the proposed method in Section 3. We\n",
      "ourframework,thegoalistofindreasoningpaths,\n",
      "show experimental results in Section 4. Finally,\n",
      "thustheactionspaceisrelationspaceintheKG.A\n",
      "weconcludeinSection5.\n",
      "similar framework (Johnson et al., 2017) has also\n",
      "2 RelatedWork beenappliedtovisualreasoningtasks.\n",
      "ThePath-RankingAlgorithm(PRA)method(Lao 3 Methodology\n",
      "et al., 2011b) is a primary path-finding approach\n",
      "that uses random walk with restart strategies for Inthissection,wedescribeindetailourRL-based\n",
      "multi-hop reasoning. Gardner et al. (2013; 2014) framework for multi-hop relation reasoning. The\n",
      "proposeamodificationtoPRAthatcomputesfea- specific task of relation reasoning is to find re-\n",
      "ture similarity in the vector space. Wang and liable predictive paths between entity pairs. We\n",
      "Cohen (2015) introduce a recursive random walk formulate the path finding problem as a sequen-\n",
      "approach for integrating the background KG and tialdecisionmakingproblemwhichcanbesolved\n",
      "text—the method performs structure learning of with a RL agent. We first describe the environ-\n",
      "logic programs and information extraction from ment and the policy-based RL agent. By interact-\n",
      "text at the same time. A potential bottleneck for ingwiththeenvironmentdesignedaroundtheKG,\n",
      "randomwalkinferenceisthatsupernodesconnect- the agent learns to pick the promising reasoning\n",
      "ing to large amount of formulas will create huge paths. Thenwedescribethetrainingprocedureof\n",
      "fan-out areas that significantly slow down the in- ourRLmodel. Afterthat,wedescribeanefficient\n",
      "ferenceandaffecttheaccuracy. path-constrainedsearchalgorithmforrelationrea-\n",
      "Toutanovaetal.(2015)provideaconvolutional soningwiththepathsfoundbytheRLagent.\n",
      "neural network solution to multi-hop reasoning.\n",
      "3.1 ReinforcementLearningforRelation\n",
      "TheybuildaCNNmodelbasedonlexicalizedde-\n",
      "Reasoning\n",
      "pendencypaths,whichsuffersfromtheerrorprop-\n",
      "agationissueduetoparseerrors. Guuetal.(2015) The RL system consists of two parts (see Fig-\n",
      "usesKGembeddingstoanswerpathqueries. Zeng ure 1). The first part is the external environment\n",
      "et al. (2014) described a CNN model for rela- E which specifies the dynamics of the interaction\n",
      "tional extraction, but it does not explicitly model between the agent and the KG. This environment\n",
      "therelationalpaths. Neelakantanetal.(2015)pro- is modeled as a Markov decision process (MDP).\n",
      "posearecurrentneuralnetworksmodelformodel- A tuple < S,A,P,R > is defined to represent\n",
      "ingrelationalpathsinknowledgebasecompletion the MDP, where S is the continuous state space,\n",
      "(KBC),butittrainstoomanyseparatemodels,and A = {a,a,...,a } is the set of all available ac-\n",
      "1 2 n\n",
      "Query Node: Band of Brothers The KG Environment Policy Based Agent\n",
      "Reason Task: tvProgramLanguage\n",
      "English\n",
      "State\n",
      "Next State\n",
      "Caesars ReLU\n",
      "Entertain… personLanguages Actor\n",
      "Reward\n",
      "serviceLocation-1 Neal profession ReLU\n",
      "McDonough\n",
      "United Reason Step\n",
      "States Tom Hanks\n",
      "Softmax\n",
      "castActor\n",
      "countryOfOrigin awardWorkWinner 𝛑(a|s)\n",
      "Graham writtenBy Band of music Michael\n",
      "Yost Brothers Kamen\n",
      "tvProgramCreator... tvProgramGenre\n",
      "HBO Mini-Series\n",
      "Figure1:OverviewofourRLmodel.Left:TheKGenvironmentEmodeledbyaMDP.Thedottedarrows(partially)showthe\n",
      "existingrelationlinksintheKGandtheboldarrowsshowthereasoningpathsfoundbytheRLagent. −1denotestheinverse\n",
      "ofanrelation. Right: Thestructureofthepolicynetworkagent. Ateachstep,byinteractingwiththeenvironment,theagent\n",
      "learnstopickarelationlinktoextendthereasoningpaths.\n",
      "tions, P(S = s(cid:48)|S = s,A = a) is the transi- Beginningwiththesourceentitye,theagentuse\n",
      "t+1 t t s\n",
      "tion probability matrix, and R(s,a) is the reward the policy network to pick the most promising\n",
      "functionofevery(s,a)pairs. relation to extend its path at each step until it\n",
      "reaches the target entity e. To keep the output\n",
      "The second part of the system, the RL t\n",
      "dimension of the policy network consistent, the\n",
      "agent, is represented as a policy network\n",
      "action space is defined as all the relations in the\n",
      "π (s,a) = p(a|s;θ) which maps the state vec-\n",
      "θ\n",
      "KG.\n",
      "tor to a stochastic policy. The neural network\n",
      "parameters θ are updated using stochastic gra-\n",
      "States The entities and relations in a KG are\n",
      "dient descent. Compared to Deep Q Network\n",
      "naturally discrete atomic symbols. Since exist-\n",
      "(DQN) (Mnih et al., 2013), policy-based RL\n",
      "ing practical KGs like Freebase (Bollacker et al.,\n",
      "methods turn out to be more appropriate for our\n",
      "2008)andNELL(Carlsonetal.,2010b)oftenhave\n",
      "knowledge graph scenario. One reason is that\n",
      "huge amounts of triples. It is impossible to di-\n",
      "for the path finding problem in KG, the action\n",
      "rectly model all the symbolic atoms in states. To\n",
      "space can be very large due to complexity of the\n",
      "capture the semantic information of these sym-\n",
      "relationgraph. Thiscanleadtopoorconvergence\n",
      "bols,weusetranslation-basedembeddingssuchas\n",
      "properties for DQN. Besides, instead of learning\n",
      "TransE (Bordes et al., 2013) and TransH (Wang\n",
      "a greedy policy which is common in value-based\n",
      "etal.,2014)torepresenttheentitiesandrelations.\n",
      "methods like DQN, the policy network is able to\n",
      "These embeddings map all the symbols to a low-\n",
      "learn a stochastic policy which prevent the agent\n",
      "dimensionalvectorspace. Inourframework,each\n",
      "fromgettingstuckatanintermediatestate. Before\n",
      "statecapturestheagent’spositionintheKG.After\n",
      "we describe the structure of our policy network,\n",
      "takinganaction,theagentwillmovefromoneen-\n",
      "we first describe the components (actions, states,\n",
      "titytoanother. Thesetwoarelinkedbytheaction\n",
      "rewards)oftheRLenvironment.\n",
      "(relation)justtakenbytheagent. Thestatevector\n",
      "atsteptisgivenasfollows:\n",
      "Actions Given the entity pairs (e,e ) with\n",
      "s t\n",
      "s = (e,e −e )\n",
      "relation r, we want the agent to find the most t t target t\n",
      "informative paths linking these entity pairs. where e denotes the embeddings of the current\n",
      "t\n",
      "entitynodeande denotestheembeddingsof betweenthecurrentpathandtheexistingones:\n",
      "target\n",
      "the target entity. At the initial state, e = e.\n",
      "t source\n",
      "We do not incorporate the reasoning relation in\n",
      "|F|\n",
      "the state, because the embedding ofthe reasoning 1 (cid:88)\n",
      "r = − cos(p,p )\n",
      "relation remain constant during path finding, DIVERSITY |F| i\n",
      "i=1\n",
      "which is not helpful in training. However, we\n",
      "find out that by training the RL agent using a set\n",
      "of positive samples for one particular relation, where p =\n",
      "(cid:80)n\n",
      "i=1r i represents the path embed-\n",
      "the agent can successfully discover the relation dingfortherelationchainr 1 → r 2 →... → r n.\n",
      "semantics. Policy Network We use a fully-connected neu-\n",
      "ral network to parameterize the policy function\n",
      "RewardsThereareafewfactorsthatcontributeto π(s;θ) that maps the state vector s to a proba-\n",
      "thequalityofthepathsfoundbytheRLagent. To bility distribution over all possible actions. The\n",
      "encourage the agent to find predictive paths, our neuralnetworkconsistsoftwohiddenlayers,each\n",
      "rewardfunctionsincludethefollowingscoringcri- followed by a rectifier nonlinearity layer (ReLU).\n",
      "teria: The output layer is normalized using a softmax\n",
      "Global accuracy: For our environment settings, function(seeFigure1).\n",
      "the number of actions that can be taken by the\n",
      "agent can be very large. In other words, there are\n",
      "3.2 TrainingPipeline\n",
      "muchmoreincorrectsequentialdecisionsthanthe\n",
      "correct ones. The number of these incorrect de-\n",
      "In practice, one big challenge of KG reasoning is\n",
      "cision sequences can increase exponentially with\n",
      "that the relation set can be quite large. For a typ-\n",
      "the length of the path. In view of this challenge,\n",
      "ical KG, the RL agent is often faced with hun-\n",
      "the first reward function we add to the RL model\n",
      "dreds (thousands) of possible actions. In other\n",
      "isdefinedasfollows:\n",
      "words, the output layer of the policy network of-\n",
      "(cid:40)\n",
      "+1, ifthepathreachese ten has a large dimension. Due to the complexity\n",
      "target\n",
      "r =\n",
      "GLOBAL of the relation graph and the large action space,\n",
      "−1, otherwise\n",
      "if we directly train the RL model by trial and er-\n",
      "the agent is given an offline positive reward +1 if rors, which is typical for RL algorithms, the RL\n",
      "itreachesthetargetafterasequenceofactions. model will show very poor convergence proper-\n",
      "Path efficiency: For the relation reasoning task, ties. After a long-time training, the agents fails\n",
      "we observe that short paths tend to provide more to find any valuable path. To tackle this prob-\n",
      "reliable reasoning evidence than longer paths. lem,westartourtrainingwithasupervisedpolicy\n",
      "Shorter chains of relations can also improve the whichisinspiredbytheimitationlearningpipeline\n",
      "efficiency of the reasoning by limiting the length used by AlphaGo (Silver et al., 2016). In the Go\n",
      "oftheRL’sinteractionswiththeenvironment. The game, the player is facing nearly 250 possible le-\n",
      "efficiencyrewardisdefinedasfollows: galmovesateachstep. Directlytrainingtheagent\n",
      "to pick actions from the original action space can\n",
      "1\n",
      "r = beadifficulttask. AlphaGofirsttrainasupervised\n",
      "EFFICIENCY length(p)\n",
      "policy network using experts moves. In our case,\n",
      "thesupervisedpolicyistrainedwitharandomized\n",
      "where path p is defined as a sequence of relations\n",
      "breadth-firstsearch(BFS).\n",
      "r → r →... → r.\n",
      "1 2 n\n",
      "Pathdiversity: Wetraintheagenttofindpathsus- Supervised Policy Learning For each relation,\n",
      "ingpositivesamplesforeachrelation. Thesetrain- we use a subset of all the positive samples (en-\n",
      "ingsample(e,e )havesimilarstaterep- titypairs)tolearnthesupervisedpolicy. Foreach\n",
      "source target\n",
      "resentations in the vector space. The agent tends positive sample (e,e ), a two-side BFS\n",
      "source target\n",
      "to find paths with similar syntax and semantics. is conducted to find same correct paths between\n",
      "These paths often contains redundant information the entities. For each path p with a sequence of\n",
      "sincesomeofthemmaybecorrelated. Toencour- relations r → r →... → r, we update the pa-\n",
      "1 2 n\n",
      "agetheagenttofinddiversepaths,wedefineadi- rameters θ to maximize the expected cumulative\n",
      "versityrewardfunctionusingthecosinesimilarity reward using Monte-Carlo Policy Gradient (RE-\n",
      "INFORCE)(Williams,1992): Algorithm 1: Retraining Procedure with re-\n",
      "(cid:88) wardfunctions\n",
      "J(θ) = E ( R )\n",
      "a∼π(a|s;θ) st,at Restoreparametersθ fromsupervisedpolicy;\n",
      "1\n",
      "t\n",
      "forepisode←1toN do\n",
      "(cid:88)(cid:88) 2\n",
      "= π(a|s ;θ)R (1)\n",
      "t st,at\n",
      "3\n",
      "Initializestatevectors\n",
      "t\n",
      "← s\n",
      "0\n",
      "t a∈A Initializeepisodelengthsteps ← 0\n",
      "4\n",
      "where J(θ) is the expected total rewards for one 5\n",
      "whilenum steps < max lengthdo\n",
      "Randomlysampleactiona ∼ π(a|s )\n",
      "episode. For supervised learning, we give a re- 6 t\n",
      "ward of +1 for each step of a successful episode. 7\n",
      "ObserverewardR t,nextstates\n",
      "t+1\n",
      "// if the step fails\n",
      "By plugging in the paths found by the BFS, the\n",
      "ifR = −1then\n",
      "approximated gradient used to update the policy 8 t\n",
      "Save< s,a >toM\n",
      "networkisshownbelow: 9 t neg\n",
      "ifsuccessorsteps = max length\n",
      "10\n",
      "(cid:88)(cid:88)\n",
      "∇ J(θ) = π(a|s ;θ)∇ logπ(a|s ;θ) then\n",
      "θ t θ t\n",
      "t a∈A 11 break\n",
      "≈ ∇\n",
      "(cid:88)\n",
      "logπ(a = r |s ;θ) (2) 12\n",
      "Incrementnum steps\n",
      "θ t t\n",
      "t // penalize failed steps\n",
      "Updateθ using\n",
      "wherer belongstothepathp. 13\n",
      "t (cid:80)\n",
      "g ∝ ∇ logπ(a = r |s ;θ)(−1)\n",
      "However, the vanilla BFS is a biased search al- θ Mneg t t\n",
      "gorithm which prefers short paths. When plug- ifsuccessthen\n",
      "ging in these biased paths, it becomes difficult 14 R total ← λ 1r GLOBAL+λ 2r EFFICIENCY+\n",
      "for the agent to find longer paths which may po- λ 3r\n",
      "DIVERSITY\n",
      "tentially be useful. We want the paths to be 15 Updateθ using\n",
      "(cid:80)\n",
      "controlled only by the defined reward functions. g ∝ ∇ θ tlogπ(a = r t|s t;θ)R total\n",
      "To prevent the biased search, we adopt a sim-\n",
      "ple trick to add some random mechanisms to the\n",
      "BFS. Instead of directly searching the path be-\n",
      "boundmax length. Theepisodeendsiftheagent\n",
      "tween e and e, we randomly pick a in-\n",
      "source target failstoreachthetargetentitywithinmax length\n",
      "termediate node e and then conduct two BFS\n",
      "inter steps. After each episode, the policy network is\n",
      "between(e,e )and(e,e ). The\n",
      "source inter inter target updatedusingthefollowinggradient:\n",
      "concatenatedpathsareusedtotraintheagent. The\n",
      "supervised learning saves the agent great efforts (cid:88)\n",
      "∇ J(θ) = ∇ logπ(a = r |s ;θ)R (3)\n",
      "θ θ t t total\n",
      "learning from failed actions. With the learned ex-\n",
      "t\n",
      "perience, we then train the agent to find desirable\n",
      "paths. where R is the linear combination of the de-\n",
      "total\n",
      "Retraining with Rewards To find the reasoning fined reward functions. The detail of the retrain\n",
      "paths controlled by the reward functions, we use process is shown in Algorithm 1. In practice, θ is\n",
      "reward functions to retrain the supervised policy updated using the Adam Optimizer (Kingma and\n",
      "network. Foreachrelation,thereasoningwithone Ba,2014)withL 2 regularization.\n",
      "entitypairistreatedasoneepisode. Startingwith\n",
      "3.3 Bi-directionalPath-constrainedSearch\n",
      "the source node e, the agent picks a relation\n",
      "source\n",
      "accordingtothestochasticpolicyπ(a|s),whichis Given an entity pair, the reasoning paths learned\n",
      "a probability distribution over all relations, to ex- by the RL agent can be used as logical formulas\n",
      "tenditsreasoningpath. Thisrelationlinkmaylead to predict the relation link. Each formula is veri-\n",
      "to a new entity, or it may lead to nothing. These fiedusingabi-directionalsearch. InatypicalKG,\n",
      "failedstepswillcausetheagenttoreceivenegative one entity node can be linked to a large number\n",
      "rewards. The agent will stay at the same state af- of neighbors with the same relation link. A sim-\n",
      "terthesefailedsteps. Sincetheagentisfollowing ple example is the relation personNationality−1,\n",
      "astochasticpolicy,theagentwillnotgetstuckby which denotes the inverse of personNationality.\n",
      "repeatingawrongstep. Toimprovethetrainingef- Following this link, the entity United States can\n",
      "ficiency,welimittheepisodelengthwithanupper reach numerous neighboring entities. If the for-\n",
      "Dataset #Ent. #R. #Triples #Tasks\n",
      "Algorithm 2: Bi-directional search for path\n",
      "FB15K-237 14,505 237 310,116 20\n",
      "verification\n",
      "NELL-995 75,492 200 154.213 12\n",
      "Givenareasoningpath\n",
      "1\n",
      "Table1:StatisticsoftheDatasets.#Ent.denotesthenumber\n",
      "p : r → r →... → r\n",
      "1 2 n ofuniqueentitiesand#R.denotesthenumberofrelations\n",
      "for(e,e )intestsetD do\n",
      "2 i j\n",
      "start←0;end←n\n",
      "3\n",
      "4 left ← ∅;right ← ∅ are subsets of larger datasets. The triples in\n",
      "5 whilestart<end do FB15K-237 (Toutanova et al., 2015) are sampled\n",
      "6 leftEx ← ∅;rightEx ← ∅ from FB15K (Bordes et al., 2013) with redun-\n",
      "7 iflen(left)<len(right)then dantrelationsremoved. Weperformthereasoning\n",
      "8 Extendpathontheleftside tasks on 20 relations which have enough reason-\n",
      "9 AddconnectednodestoleftEx ing paths. These tasks consists of relations from\n",
      "10 left ← leftEx different domains like Sports, People, Locations,\n",
      "Film, etc. Besides, we present a new NELL sub-\n",
      "else\n",
      "11\n",
      "set that is suitable for multi-hop reasoning from\n",
      "Extendpathontherightside\n",
      "12\n",
      "the 995th iteration of the NELL system. We first\n",
      "AddconnectednodestorightEx\n",
      "13\n",
      "removethetripleswithrelationgeneralizationsor\n",
      "right ← rightEx\n",
      "14\n",
      "haswikipediaurl. Thesetworelationsappearmore\n",
      "15 ifleft∩right (cid:54)= ∅then than2MtimesintheNELLdataset,buttheyhave\n",
      "16 returnTrue no reasoning values. After this step, we only se-\n",
      "else lectthetripleswithTop-200relations. Tofacilitate\n",
      "17\n",
      "returnFalse path finding, we also add the inverse triples. For\n",
      "18\n",
      "each triple (h,r,t), we append (t,r−1,h) to the\n",
      "datasets. With these inverse triples, the agent is\n",
      "abletostepbackwardintheKG.\n",
      "mula consists of such links, the number of inter- For each reasoning task r, we remove all the\n",
      "i\n",
      "mediate entities can exponentially increase as we tripleswithr orr−1fromtheKG.Theseremoved\n",
      "i i\n",
      "follow the reasoning formula. However, we ob- triples are split into train and test samples. For\n",
      "serve that for these formulas, if we verify the for- the link prediction task, each h in the test triples\n",
      "mulafromtheinversedirection. Thenumberofin- {(h,r,t)} is considered as one query. A set of\n",
      "termediate nodes can be tremendously decreased. candidatetargetentitiesarerankedusingdifferent\n",
      "Algorithm 2 shows a detailed description of the methods. For fact prediction, the true test triples\n",
      "proposedbi-directionalsearch. arerankedwithsomegeneratedfalsetriples.\n",
      "4 Experiments\n",
      "4.2 BaselinesandImplementationDetails\n",
      "To evaluate the reasoning formulas found by our Most KG reasoning methods are based on either\n",
      "RL agent, we explore two standard KG reason- path formulas or KG embeddings. we explore\n",
      "ing tasks: link prediction (predicting target en- methodsfrombothofthesetwoclassesinourex-\n",
      "tities) and fact prediction (predicting whether an periments. For path based methods, we compare\n",
      "unknown fact holds or not). We compare our our RL model with the PRA (Lao et al., 2011a)\n",
      "methodwithbothpath-basedmethodsandembed- algorithm,whichhasbeenusedinacoupleofrea-\n",
      "dingbasedmethods. Afterthat,wefurtheranalyze soning methods (Gardner et al., 2013; Neelakan-\n",
      "thereasoningpathsfoundbyourRLagent. These tan et al., 2015). PRA is a data-driven algorithm\n",
      "highly predictive paths validate the effectiveness usingrandomwalks(RW)tofindpathsandobtain\n",
      "oftherewardfunctions. Finally,weconductaex- path features. For embedding based methods, we\n",
      "perimenttoinvestigatetheeffectofthesupervised evaluate several state-of-the-art embeddings de-\n",
      "learningprocedure. signed for knowledge base completion, such as\n",
      "TransE(Bordesetal.,2013),TransH(Wangetal.,\n",
      "4.1 DatasetandSettings\n",
      "2014), TransR (Lin et al., 2015) and TransD (Ji\n",
      "Table 1 shows the statistics of the two datasets etal.,2015).\n",
      "we conduct our experiments on. Both of them The implementation of PRA is based on the\n",
      "FB15K-237 NELL-995\n",
      "Tasks PRA RL TransE TransR Tasks PRA RL TransE TransR\n",
      "teamSports 0.987 0.955 0.896 0.784 athletePlaysForTeam 0.547 0.750 0.627 0.673\n",
      "birthPlace 0.441 0.531 0.403 0.417 athletePlaysInLeague 0.841 0.960 0.773 0.912\n",
      "personNationality 0.846 0.823 0.641 0.720 athleteHomeStadium 0.859 0.890 0.718 0.722\n",
      "filmDirector 0.349 0.441 0.386 0.399 athletePlaysSport 0.474 0.957 0.876 0.963\n",
      "filmWrittenBy 0.601 0.457 0.563 0.605 teamPlaySports 0.791 0.738 0.761 0.814\n",
      "filmLanguage 0.663 0.670 0.642 0.641 orgHeadquaterCity 0.811 0.790 0.620 0.657\n",
      "tvLanguage 0.960 0.969 0.804 0.906 worksFor 0.681 0.711 0.677 0.692\n",
      "capitalOf 0.829 0.783 0.554 0.493 bornLocation 0.668 0.757 0.712 0.812\n",
      "organizationFounded 0.281 0.309 0.390 0.339 personLeadsOrg 0.700 0.795 0.751 0.772\n",
      "musicianOrigin 0.426 0.514 0.361 0.379 orgHiredPerson 0.599 0.742 0.719 0.737\n",
      "......\n",
      "Overall 0.541 0.572 0.532 0.540 0.675 0.796 0.737 0.789\n",
      "Table2:Linkpredictionresults(MAP)ontwodatasets.\n",
      "code released by (Lao et al., 2011a). We use the FactPredictionResults\n",
      "TopKnegativemodetogeneratenegativesamples\n",
      "Methods FB15K-237 NELL-995\n",
      "for both train and test samples. For each pos-\n",
      "itive samples, there are approximately 10 corre- RL 0.311 0.493\n",
      "spondingnegativesamples. Eachnegativesample TransE 0.277 0.383\n",
      "is generated by replacing the true target entity t TransH 0.309 0.389\n",
      "with a faked one t(cid:48) in each triple (h,r,t). These TransR 0.302 0.406\n",
      "positive and negative test pairs generated by PRA TransD 0.303 0.413\n",
      "make up the test set for all methods evaluated in\n",
      "thispaper. ForTransE,R,H,D,welearnaseparate Table3:Factpredictionresults(MAP)ontwodatasets.\n",
      "embedding matrix for each reasoning task using\n",
      "thepositivetrainingentitypairs. Alltheseembed- #ofReasoningPaths\n",
      "dingsaretrainedfor1,000epochs. 2\n",
      "Tasks PRA RL\n",
      "Our RL model make use of TransE to get the\n",
      "continuous representation of the entities and rela- worksFor 247 25\n",
      "tions. We use the same dimension as TransE, R teamPlaySports 113 27\n",
      "to embed the entities. Specifically, the state vec- teamPlaysInLeague 69 21\n",
      "tor we use has a dimension of 200, which is also athletehomestadium 37 11\n",
      "the input size of the policy network. To reason organizationHiredPerson 244 9\n",
      "using the path formulas, we adopt a similar lin-...\n",
      "ear regression approach as in PRA to re-rank the Average# 137.2 20.3\n",
      "paths. However,insteadofusingtherandomwalk\n",
      "Table4:NumberofreasoningpathsusedbyPRAandourRL\n",
      "probabilities as path features, which can be com-\n",
      "model. RLachievedbetterMAPwithamorecompactsetof\n",
      "putationallyexpensive,wesimplyusebinarypath learnedpaths.\n",
      "featuresobtainedbythebi-directionalsearch. We\n",
      "observethatwithonlyafewminedpathformulas,\n",
      "Since path-based methods generally work better\n",
      "our method can achieve better results than PRA’s\n",
      "than embedding methods for this task, we do not\n",
      "data-drivenapproach.\n",
      "include the other two embedding baselines in this\n",
      "4.3 Results table. Instead, we spare the room to show the de-\n",
      "tailedresultsoneachrelationreasoningtask.\n",
      "4.3.1 QuantitativeResults\n",
      "FortheoverallMAPshowninthelastrowofthe\n",
      "LinkPredictionThistaskistorankthetargeten-\n",
      "table,ourapproachsignificantlyoutperformsboth\n",
      "titiesgivenaqueryentity. Table2showsthemean\n",
      "thepath-basedmethodandembeddingmethodson\n",
      "average precision (MAP) results on two datasets.\n",
      "twodatasets,whichvalidatesthestrongreasoning\n",
      "abilityofourRLmodel. Formostrelations,since\n",
      "2The implementation we used can be found at https:\n",
      "//github.com/thunlp/Fast-TransX the embedding methods fail to use the path infor-\n",
      "120\n",
      "100\n",
      "80\n",
      "60\n",
      "40\n",
      "20\n",
      "0\n",
      "0 5 10 15 20 25\n",
      "distribution of reasoning paths\n",
      "shtap\n",
      "fo\n",
      "rebmun\n",
      "0.20\n",
      "NELL-995\n",
      "FB15K-237\n",
      "0.15\n",
      "0.10\n",
      "0.05\n",
      "0.00\n",
      "0 50 100 150 200\n",
      "training episodes\n",
      "Figure2:Thedistributionofpathslengthsontwodatasets\n",
      "mation in the KG, they generally perform worse\n",
      "thanourRLmodelorPRA.However, whenthere\n",
      "are not enough paths between entities, our model\n",
      "and PRA can give poor results. For example,\n",
      "fortherelationfilmWrittenBy,ourRLmodelonly\n",
      "finds4uniquereasoningpaths,<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  29346,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Wenhan Xiong', 'Thien Hoang', 'William Yang Wang']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  implementation we used can be found at https:\n",
      "//github.com/thunlp/Fast-TransX the embedding methods fail to use the path infor-\n",
      "120\n",
      "100\n",
      "80\n",
      "60\n",
      "40\n",
      "20\n",
      "0\n",
      "0 5 10 15 20 25\n",
      "distribution of reasoning paths\n",
      "shtap\n",
      "fo\n",
      "rebmun\n",
      "0.20\n",
      "NELL-995\n",
      "FB15K-237\n",
      "0.15\n",
      "0.10\n",
      "0.05\n",
      "0.00\n",
      "0 50 100 150 200\n",
      "training episodes\n",
      "Figure2:Thedistributionofpathslengthsontwodatasets\n",
      "mation in the KG, they generally perform worse\n",
      "thanourRLmodelorPRA.However, whenthere\n",
      "are not enough paths between entities, our model\n",
      "and PRA can give poor results. For example,\n",
      "fortherelationfilmWrittenBy,ourRLmodelonly\n",
      "finds4uniquereasoningpaths,whichmeansthere\n",
      "isactuallynotenoughreasoningevidenceexisting\n",
      "in the KG. Another observation is that we always\n",
      "get better performance on the NELL dataset. By\n",
      "analyzing the paths found from the KGs, we be-\n",
      "lievethepotentialreasonisthattheNELLdataset\n",
      "has more short paths than FB15K-237 and some\n",
      "ofthemaresimplysynonymsofthereasoningre-\n",
      "lations.\n",
      "Fact Prediction Instead of ranking the target en-\n",
      "tities, this task directly ranks all the positive and\n",
      "negative samples for a particular relation. The\n",
      "PRA is not included as a baseline here, since the\n",
      "PRA code only gives a target entity ranking for\n",
      "eachquerynodeinsteadofarankingofalltriples.\n",
      "Table 3 shows the overall results of all the meth-\n",
      "ods. OurRLmodelgetsevenbetterresultsonthis\n",
      "task. Wealso observethat theRLmodel beatsall\n",
      "theembeddingbaselinesonmostreasoningtasks.\n",
      "4.3.2 QualitativeAnalysisofReasoningPaths\n",
      "To analyze the properties of reasoning paths, we\n",
      "show a few reasoning paths found by the agent\n",
      "in Table 5. To illustrate the effect of the effi-\n",
      "ciency reward function, we show the path length\n",
      "distributions in Figure 2. To interpret these paths,\n",
      "take the personNationality relation for example,\n",
      "the first reasoning path indicates that if we know\n",
      "facts placeOfBirth(x,y) and locationContains(z,y)\n",
      "thenitishighlypossiblethatpersonxhasnation-\n",
      "ality z. These short but predictive paths indicate\n",
      "the effectiveness of the RL model. Another im-\n",
      "portant observation is that our model use much\n",
      "spets\n",
      "01\n",
      "nihtiw\n",
      "oitar\n",
      "sseccus\n",
      "Figure3: Thesuccessratio(succ )duringtraining. Task: 10\n",
      "athletePlaysForTeam.3\n",
      "fewer reasoning paths than PRA, which indicates\n",
      "that our model can actually extract the most reli-\n",
      "able reasoning evidence from KG. Table 4 shows\n",
      "somecomparisonsaboutthenumberofreasoning\n",
      "paths. We can see that, with the pre-defined re-\n",
      "wardfunctions,theRLagentiscapableofpicking\n",
      "the strong ones and filter out similar or irrelevant\n",
      "ones.\n",
      "4.3.3 EffectofSupervisedLearning\n",
      "AsmentionedinSection3.2,onemajorchallenge\n",
      "for applying RL to KG reasoning is the large ac-\n",
      "tion space. We address this issue by applying\n",
      "supervised learning before the reward retraining\n",
      "step. To show the effect of the supervised train-\n",
      "ing,weevaluatetheagent’ssuccessratioofreach-\n",
      "ingthetargetwithin10steps(succ )afterdiffer- 10\n",
      "ent number of training episodes. For each train-\n",
      "ing episode, one pair of entities (e,e ) source target\n",
      "in the train set is used to find paths. All the cor-\n",
      "rectpathslinkingtheentitieswillgeta+1global\n",
      "reward. Wethenpluginsometruepathsfortrain-\n",
      "ing. Thesucc iscalculatedonaheld-outtestset\n",
      "10\n",
      "that consists of 100 entity pairs. For the NELL-\n",
      "995 dataset, since we have 200 unique relations,\n",
      "the dimension of the action space will be 400 af-\n",
      "ter we add the backward actions. This means that\n",
      "randomwalkswillgetverylowsucc sincethere 10\n",
      "maybenearly40010invalidpaths. Figure3shows\n",
      "the succ during training. We see that even the 10\n",
      "agenthasnotseentheentitybefore,itcanactually\n",
      "pickthepromisingrelationtoextenditspath. This\n",
      "also validates the effectiveness of our state repre-\n",
      "sentations.\n",
      "3Theconfidencebandisgeneratedusing50differentruns.\n",
      "Relation ReasoningPath\n",
      "filmReleaseRegion\n",
      "filmCountry featureFilmLocation→locationContains−1\n",
      "actorFilm−1 →personNationality\n",
      "placeOfBirth→locationContains−1\n",
      "personNationality peoplePlaceLived→locationContains−1\n",
      "peopleMarriage→locationOfCeremony→locationContains−1\n",
      "tvCountryOfOrigin→countryOfficialLanguage\n",
      "tvProgramLanguage tvCountryOfOrigin→filmReleaseRegion−1 →filmLanguage\n",
      "tvCastActor→filmLanguage\n",
      "personBornInCity\n",
      "personBornInLocation graduatedUniversity→graduatedSchool−1 →personBornInCity\n",
      "personBornInCity→atLocation−1 →atLocation\n",
      "athleteHomeStadium→teamHomeStadium−1\n",
      "athletePlaysForTeam athletePlaysSport→teamPlaysSport−1\n",
      "athleteLedSportsTeam\n",
      "worksFor\n",
      "personLeadsOrganization organizationTerminatedPerson−1\n",
      "mutualProxyFor−1\n",
      "Table5: ExamplereasoningpathsfoundbyourRLmodel. ThefirstthreerelationscomefromtheFB15K-237dataset. The\n",
      "othersarefromNELL-995.Inversesofexistingrelationsaredenotedby−1.\n",
      "5 ConclusionandFutureWork Acknowledgments\n",
      "We gratefully acknowledge the support of\n",
      "NVIDIACorporationwiththedonationofoneTi-\n",
      "In this paper, we propose a reinforcement learn-\n",
      "tanXPascalGPUusedforthisresearch.\n",
      "ing framework to improve the performance of re-\n",
      "lation reasoning in KGs. Specifically, we train a\n",
      "RLagenttofindreasoningpathsintheknowledge\n",
      "References\n",
      "base. Unlikepreviouspathfindingmodelsthatare\n",
      "Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim\n",
      "based on random walks, the RL model allows us\n",
      "Sturge,andJamieTaylor.2008. Freebase: acollab-\n",
      "tocontrolthepropertiesofthefoundpaths. These\n",
      "oratively created graph database for structuring hu-\n",
      "effectivepathscanalsobeusedasanalternativeto man knowledge. In Proceedings of the 2008 ACM\n",
      "PRA in many path-based reasoning methods. For SIGMOD international conference on Management\n",
      "ofdata,pages1247–1250.ACM.\n",
      "two standard reasoning tasks, using the RL paths\n",
      "asreasoningformulas,ourapproachgenerallyout- Antoine Bordes, Nicolas Usunier, Alberto Garcia-\n",
      "performstwoclassesofbaselines. Duran, Jason Weston, and Oksana Yakhnenko.\n",
      "2013. Translating embeddings for modeling multi-\n",
      "For future studies, we plan to investigate relational data. In Advances in neural information\n",
      "the possibility of incorporating adversarial learn- processingsystems,pages2787–2795.\n",
      "ing (Goodfellow et al., 2014) to give better re- AndrewCarlson,JustinBetteridge,BryanKisiel,Burr\n",
      "wards than the human-defined reward functions Settles, Estevam R. Hruschka Jr., and Tom M.\n",
      "used in this work. Instead of designing rewards Mitchell. 2010a. Toward an architecture for never-\n",
      "endinglanguagelearning. InAAAI.\n",
      "according to path characteristics, a discriminative\n",
      "modelcanbetrainedtogiverewards. Also,toad- AndrewCarlson,JustinBetteridge,BryanKisiel,Burr\n",
      "dress the problematic scenario when the KG does Settles, Estevam R. Hruschka Jr., and Tom M.\n",
      "Mitchell. 2010b. Toward an architecture for never-\n",
      "nothaveenoughreasoningpaths,weareinterested\n",
      "ending language learning. In Proceedings of the\n",
      "in applying our RL framework to joint reasoning\n",
      "Twenty-FourthConferenceonArtificialIntelligence\n",
      "withKGtriplesandtextmentions. (AAAI2010).\n",
      "Rajarshi Das, Arvind Neelakantan, David Belanger, Ni Lao, Jun Zhu, Xinwang Liu, Yandong Liu, and\n",
      "andAndrewMcCallum.2017. Chainsofreasoning WilliamWCohen.2010. Efficientrelationallearn-\n",
      "overentities,relations,andtextusingrecurrentneu- ing with hidden variable detection. In NIPS, pages\n",
      "ralnetworks. EACL. 1234–1242.\n",
      "Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel, Chen Liang, Jonathan Berant, Quoc Le, Kenneth D\n",
      "and Tom M Mitchell. 2013. Improving learning Forbus, and Ni Lao. 2016. Neural symbolic\n",
      "andinferenceinalargeknowledge-baseusinglatent machines: Learning semantic parsers on free-\n",
      "syntacticcues. InEMNLP,pages833–838. base with weak supervision. arXiv preprint\n",
      "arXiv:1611.00020.\n",
      "MattGardner,ParthaPratimTalukdar,JayantKrishna-\n",
      "murthy,andTomMitchell.2014. Incorporatingvec- YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,and\n",
      "tor space similarity in random walk inference over Xuan Zhu. 2015. Learning entity and relation em-\n",
      "knowledgebases. beddingsforknowledgegraphcompletion. InAAAI,\n",
      "pages2181–2187.\n",
      "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\n",
      "BingXu,DavidWarde-Farley,SherjilOzair,Aaron Volodymyr Mnih, Koray Kavukcuoglu, David Sil-\n",
      "Courville,andYoshuaBengio.2014. Generativead- ver, Alex Graves, Ioannis Antonoglou, Daan Wier-\n",
      "versarial nets. In Advances in Neural Information stra, and Martin Riedmiller. 2013. Playing atari\n",
      "ProcessingSystems,pages2672–2680. with deep reinforcement learning. arXiv preprint\n",
      "arXiv:1312.5602.\n",
      "Kelvin Guu, John Miller, and Percy Liang. 2015.\n",
      "Traversing knowledge graphs in vector space. In\n",
      "Volodymyr Mnih, Koray Kavukcuoglu, David Silver,\n",
      "EMNLP.\n",
      "Andrei A Rusu, Joel Veness, Marc G Bellemare,\n",
      "Alex Graves, Martin Riedmiller, Andreas K Fidje-\n",
      "Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl,\n",
      "land, Georg Ostrovski, et al. 2015. Human-level\n",
      "Abdel-rahman Mohamed, Navdeep Jaitly, Andrew\n",
      "control through deep reinforcement learning. Na-\n",
      "Senior,VincentVanhoucke,PatrickNguyen,TaraN\n",
      "ture,518(7540):529–533.\n",
      "Sainath, et al. 2012. Deep neural networks for\n",
      "acousticmodelinginspeechrecognition:Theshared\n",
      "ArvindNeelakantan,BenjaminRoth,andAndrewMc-\n",
      "viewsoffourresearchgroups. IEEESignalProcess-\n",
      "Callum. 2015. Compositional vector space mod-\n",
      "ingMagazine,29(6):82–97.\n",
      "els for knowledge base completion. arXiv preprint\n",
      "arXiv:1504.06662.\n",
      "GuoliangJi,ShizhuHe,LihengXu,KangLiu,andJun\n",
      "Zhao. 2015. Knowledge graph embedding via dy-\n",
      "David Silver, Aja Huang, Chris J Maddison, Arthur\n",
      "namicmappingmatrix. InACL(1),pages687–696.\n",
      "Guez,LaurentSifre,GeorgeVanDenDriessche,Ju-\n",
      "lian Schrittwieser, Ioannis Antonoglou, Veda Pan-\n",
      "Justin Johnson, Bharath Hariharan, Laurens van der\n",
      "neershelvam, Marc Lanctot, et al. 2016. Mastering\n",
      "Maaten,JudyHoffman,LiFei-Fei,CLawrenceZit-\n",
      "the game of go with deep neural networks and tree\n",
      "nick, and Ross Girshick. 2017. Inferring and exe-\n",
      "cutingprogramsforvisualreasoning. arXivpreprint search. Nature,529(7587):484–489.\n",
      "arXiv:1705.03633.\n",
      "Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-\n",
      "Yoon Kim. 2014. Convolutional neural net- fungPoon,PallaviChoudhury,andMichaelGamon.\n",
      "works for sentence classification. arXiv preprint 2015. Representingtextforjointembeddingoftext\n",
      "arXiv:1408.5882. andknowledgebases. InEMNLP,volume15,pages\n",
      "1499–1509.Citeseer.\n",
      "Diederik Kingma and Jimmy Ba. 2014. Adam: A\n",
      "method for stochastic optimization. arXiv preprint William Yang Wang and William W Cohen. 2015.\n",
      "arXiv:1412.6980. Joint information extraction and reasoning: A scal-\n",
      "ablestatisticalrelationallearningapproach. InACL.\n",
      "AlexKrizhevsky, IlyaSutskever, andGeoffreyEHin-\n",
      "ton. 2012. Imagenet classification with deep con- Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\n",
      "volutional neural networks. In Advances in neural Chen.2014. Knowledgegraphembeddingbytrans-\n",
      "informationprocessingsystems,pages1097–1105. lating on hyperplanes. In AAAI, pages 1112–1119.\n",
      "Citeseer.\n",
      "Ni Lao, Tom Mitchell, and William W Cohen. 2011a.\n",
      "Randomwalkinferenceandlearninginalargescale Ronald J Williams. 1992. Simple statistical gradient-\n",
      "knowledge base. In Proceedings of the Conference following algorithms for connectionist reinforce-\n",
      "on Empirical Methods in Natural Language Pro- mentlearning. Machinelearning,8(3-4):229–256.\n",
      "cessing, pages 529–539. Association for Computa-\n",
      "tionalLinguistics. Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,\n",
      "Jun Zhao, et al. 2014. Relation classification via\n",
      "Ni Lao, Tom M. Mitchell, and William W. Cohen. convolutional deep neural network. In COLING,\n",
      "2011b. Random walk inference and learning in a pages2335–2344.\n",
      "largescaleknowledgebase. InEMNLP,pages529–\n",
      "539.ACL.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   5355,   2761,  11583]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Kurt Bollacker', 'Colin Evans', 'Praveen Paritosh', 'Tim Sturge', 'Jamie Taylor', 'Antoine Bordes', 'Nicolas Usunier', 'Alberto Garcia-Duran', 'Jason Weston', 'Oksana Yakhnenko', 'Andrew Carlson', 'Justin Betteridge', 'Bryan Kisiel', 'Burr Settles', 'Estevam R. Hruschka Jr.', 'Tom M. Mitchell', 'Rajarshi Das', 'Arvind Neelakantan', 'David Belanger', 'Ni Lao', 'Jun Zhu', 'Xinwang Liu', 'Yandong Liu', 'William W. Cohen', 'Matt Gardner', 'Partha Pratim Talukdar', 'Bryan Kisiel', 'Chen Liang', 'Jonathan Berant', 'Quoc Le', 'Kenneth D. Forbus', 'Ni Lao', 'Yankai Lin', 'Zhiyuan Liu', 'Maosong Sun', 'Yang Liu', 'Xuan Zhu', 'Ian Goodfellow', 'Jean Pouget-Abadie', 'Mehdi Mirza', 'Bing Xu', 'David Warde-Farley', 'Sherjil Ozair', 'Aaron Courville', 'Yoshua Bengio', 'Volodymyr Mnih', 'Koray Kavukcuoglu', 'David Silver', 'Alex Graves', 'Martin Riedmiller', 'Andrei A. Rusu', 'Joel Veness', 'Marc G. Bellemare', 'Geoffrey Hinton', 'Li Deng', 'Dong Yu', 'George E. Dahl', 'Abdel-rahman Mohamed', 'Navdeep Jaitly', 'Andrew Senior', 'Vincent Vanhoucke', 'Patrick Nguyen', 'Tara N. Sainath', 'Guoliang Ji', 'Shizhu He', 'Liheng Xu', 'Kang Liu', 'Jun Zhao', 'David Silver', 'Aja Huang', 'Chris J. Maddison', 'Arthur Guez', 'Laurent Sifre', 'George Van Den Driessche', 'Julian Schrittwieser', 'Ioannis Antonoglou', 'Veda Panneershelvam', 'Marc Lanctot', 'Justin Johnson', 'Bharath Hariharan', 'Laurens van der Ma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: 7102\n",
      "ceD\n",
      "01\n",
      "]LC.sc[\n",
      "1v74530.2171:viXra\n",
      "Inducing Interpretability in Knowledge Graph Embeddings\n",
      "Chandrahas and Tathagata Sengupta and CibiPragadeesh and Partha PratimTalukdar\n",
      "chandrahas@iisc.ac.in\n",
      "Abstract over these vectors. Some of these methods like\n",
      "(Riedeletal., 2013), (Toutanovaetal., 2015) are\n",
      "We study the problem of inducing inter-\n",
      "capable of exploiting additional text data apart\n",
      "pretability in KG embeddings. Specifi-\n",
      "fromtheKG,resulting inbetterrepresentations.\n",
      "cally, we explore the Universal Schema\n",
      "Although these methods have shown good per-\n",
      "(Riedeletal.,2013)andproposeamethod\n",
      "formance in applications, they don’t address the\n",
      "toinduceinterpretability. Therehavebeen\n",
      "problem ofunderstanding semantics of individual\n",
      "many vector space models proposed for\n",
      "dimensions of the KGembedding. Arecent work\n",
      "theproblem,however,mostofthesemeth-\n",
      "(Xiaoetal.,2016)addressedtheproblemoflearn-\n",
      "ods don’t address the interpretability (se-\n",
      "ing semantic features for KGs. However, they\n",
      "mantics) of individual dimensions. In this\n",
      "don’tdirectlyusevectorspacemodeling.\n",
      "work,westudythisproblemandproposea\n",
      "In this work, we focus on incorporating in-\n",
      "methodforinducinginterpretabilityinKG\n",
      "terpretability in KG embeddings. Specifically,\n",
      "embeddings using entity co-occurrence\n",
      "we aim to learn interpretable embeddings for\n",
      "statistics. The proposed method signifi-\n",
      "KG entities by incorporating additional entity co-\n",
      "cantly improves the interpretability, while\n",
      "occurrence statistics from text data. This work is\n",
      "maintaining comparable performance in\n",
      "motivatedby(Lauetal.,2014)whopresented au-\n",
      "otherKGtasks.\n",
      "tomatedmethods forevaluating topics learned via\n",
      "topic modelling methods. We adapt these mea-\n",
      "1 Introduction\n",
      "sures for the vector space model and propose a\n",
      "Knowledge Graphs such as Freebase, WordNet method to directly maximize them while learning\n",
      "etc. havebecomeimportantresourcesforsupport- KGembedding. Tothebestofourknowledge,this\n",
      "ing many AI applications like web search, Q&A work presents the first regularization term which\n",
      "etc. They store a collection of facts in the form inducesinterpretability inKGembeddings.\n",
      "of a graph. The nodes in the graph represent\n",
      "2 RelatedWork\n",
      "real world entities such as Roger Federer, Tennis,\n",
      "United States etc while the edges represent rela- Several methods have been proposed for learning\n",
      "tionships betweenthem. KG embeddings. They differ on the modeling of\n",
      "These KGs have grown huge, but they are entities and relations, usage oftextdata andinter-\n",
      "still not complete (Toutanovaetal., 2015). Hence pretabilityofthelearnedembeddings. Wesumma-\n",
      "the task of inferring new facts becomes impor- rizesomeofthesemethodsinfollowingsections.\n",
      "tant. Many vector space models have been\n",
      "2.1 Vector-spacemodelsforKGEmbeddings\n",
      "proposed which can perform reasoning over\n",
      "KGsefficiently(Bordesetal.,2011),(Wangetal., A very effective and powerful set of models are\n",
      "2014), (Linetal., 2015), (Socheretal., 2013), based on translation vectors. These models rep-\n",
      "(Riedeletal., 2013), (Toutanovaetal., 2015) etc. resent entities as vectors in d-dimensional space,\n",
      "These methods learn representations for entities Rd and relations as translation vectors from head\n",
      "and relations as vectors in a vector space, cap- entity to tail entity, in either same or a pro-\n",
      "turing global information about the KG. The task jected space. TransE(Bordesetal., 2011) is one\n",
      "of KG inference is then defined as operations of the initial works, which was later improved\n",
      "by many works [(Wangetal., 2014), (Linetal., ties within the topic. This idea can also be used\n",
      "2015), (Xiaoetal., 2015b), (Xiaoetal., 2015a), in vector space models by treating dimensions of\n",
      "(Jietal.,2015),(Fanetal.,2014)]. Also,thereare the vector space as topics. With this assumption,\n",
      "methods which are able to incorporate text data wecanuseameasureofcoherence definedinfol-\n",
      "while learning KG embeddings. (Riedeletal., lowingsectionforevaluatinginterpretabilityofthe\n",
      "2013)is one such method, which assumes acom- embeddings.\n",
      "bined universal schema of relations from KG as\n",
      "well as text. (Toutanovaetal., 2015) further im-\n",
      "3.1.1 Coherence@k\n",
      "proves the performance by sharing parameters Coherence@k has been shown to have high\n",
      "amongsimilartextualrelations. correlation with human interpretability of\n",
      "topics learned via various topic modeling\n",
      "2.2 Interpretability ofEmbedding\n",
      "methods(Lauetal., 2014). Hence, we can expect\n",
      "While the vector space models perform well in interpretable embeddings bymaximizingit.\n",
      "many tasks, the semantics of learned representa- Coherence for top k entities along dimension l\n",
      "tionsarenotdirectlyclear. Thisproblemforword isdefinedasfollows:\n",
      "embeddings was addressed by (Murphyetal.,\n",
      "k i−1\n",
      "2012)wheretheyproposed asetofconstraints in-\n",
      "Coherence@k(l) = p (1)\n",
      "ducing interpretability. However, its adaptation XX ij\n",
      "i=2 j=1\n",
      "for KG embeddings hasn’t been addressed. A\n",
      "recent work (Xiaoetal., 2016) addressed a sim-\n",
      "where p is PMI score between entities e and e\n",
      "ij i j\n",
      "ilar problem, where they learn coherent seman-\n",
      "extracted from text data. Coherence@k for the\n",
      "tic features for entities and relations in KG. Our\n",
      "entity embedding matrix θ isdefinedasthe aver-\n",
      "e\n",
      "methoddiffersfromtheirsinthefollowingtwoas-\n",
      "ageoveralldimensions.\n",
      "pects. Firstly, weusevector space modeling lead-\n",
      "ing directly to KG embeddings while they need\n",
      "d\n",
      "1\n",
      "to infer KG embeddings from their probabilistic Coherence@k = Coherence@k(l) (2)\n",
      "dX\n",
      "model. Second, we incorporate additional infor-\n",
      "l=1\n",
      "mation about entities which helps in learning in-\n",
      "terpretable embeddings. 3.1.2 Inducingcoherencewhilelearning\n",
      "embeddings\n",
      "3 Proposed Method\n",
      "We want to learn an embedding matrix θ which\n",
      "e\n",
      "has high coherence (i.e. which maximizes\n",
      "We are interested in inducing interpretability in\n",
      "Coherence@k). Since θ changes during train-\n",
      "KG embeddings and regularization is one good e\n",
      "ing, thesetoftop k entities along each dimension\n",
      "waytodoit. Sowewanttolookatnovelregulariz-\n",
      "variesoveriterations. Hence,directlymaximizing\n",
      "ersinKGembeddings. Hence,weexploreamea-\n",
      "Coherence@k seemstobetricky.\n",
      "sure of coherence proposed in (Lauetal., 2014).\n",
      "An alternate approach could be to promote\n",
      "This measure allows automated evaluation of the\n",
      "higher values for entity pairs having high PMI\n",
      "quality of topics learned by topic modeling meth-\n",
      "score p. This will result in an embedding ma-\n",
      "ods by using additional Point-wise Mutual Infor- ij\n",
      "trix θ with a high value of Coherence@k since\n",
      "mation (PMI) for word pairs. It was also shown e\n",
      "highPMIentitypairsaremorelikelytobeamong\n",
      "tohavehigh correlation withhumanevaluation of\n",
      "topkentities.\n",
      "topics.\n",
      "This idea can be captured by following coher-\n",
      "Based on this measure of coherence, we pro-\n",
      "enceterm\n",
      "pose a regularization term. This term can be\n",
      "used with existing KG embedding methods (eg\n",
      "n i−1\n",
      "(Riedeletal., 2013)) for inducing interpretability. C(θ,P) = kv(e )⊺v(e )−p k2 (3)\n",
      "e XX i j ij\n",
      "Itisdescribed inthefollowingsections.\n",
      "i=2 j=1\n",
      "3.1 Coherence\n",
      "whereP isentity-pair PMImatrixandv(e)de-\n",
      "In topic models, coherence of a topic can be de- note vector for entity e. This term can be used in\n",
      "termined bysemantic relatedness among topenti- theobjective function definedinEquation6\n",
      "3.2 EntityModel(Model-E) for more details. For evaluating the learned em-\n",
      "beddings, we test them on different tasks. All\n",
      "We use the Entity Model proposed in\n",
      "thehyper-parametersaretunedusingperformance\n",
      "(Riedeletal., 2013) for learning KG embed-\n",
      "(MRR) on validation data. We use 100 dimen-\n",
      "dings. This model assumes a vector v(e) for\n",
      "sions after cross validating among 50, 100 and\n",
      "each entity and two vectors v (r) and v (r) for\n",
      "s o\n",
      "200 dimensions. For regularization, we use λ =\n",
      "each relation of the KG. The score for the triple r\n",
      "0.01 (from 10,1,0.1,0.01) and λ = 0.01 (from\n",
      "(e,r,e )isgivenby, c\n",
      "s o\n",
      "10,1,0.1,0.01) for L2 and coherence regulariza-\n",
      "tion respectively. Weuse multiple random initial-\n",
      "f(e,r,e ) = v(e )⊺v (r)+v(e )⊺v (r) (4) izationssampledfromaGaussiandistribution. For\n",
      "s o s s o o\n",
      "optimization,weusegradientdescentandstopop-\n",
      "Trainingthesevectorsrequiresincorrecttriples. timization when gradient becomes 0 upto 3 deci-\n",
      "So,weusetheclosedworldassumption. Foreach mal places. The final performance measures are\n",
      "triple t ∈ T, we create two negative triples t− reportedfortestdata.\n",
      "o\n",
      "andt− bycorrupting the object andsubject ofthe\n",
      "s\n",
      "4.3 Results\n",
      "triples respectively such that the corrupted triples\n",
      "don’t appear in training, test or validation data. In following sections, we compare the perfor-\n",
      "Thelossforatriplepairisdefinedasloss(t,t−)=\n",
      "mance of the proposed method with the baseline\n",
      "−log(σ(f(t)−f(t−))). Then,theaggregateloss method in different tasks. Please refer to Table 1\n",
      "function isdefinedas forresults.\n",
      "4.3.1 Interpretability\n",
      "1\n",
      "L(θ e,θ r,T)= |T| X(cid:0)loss(t,t− o)+loss(t,t− s ) (cid:1) For evaluating the interpretability, we use\n",
      "t∈T Coherence@k(Equation2),automatedandman-\n",
      "(5)\n",
      "ual word intrusion tests. In word intrusion test\n",
      "3.3 Objective (Changetal., 2009), top k(= 5) entities along a\n",
      "dimension are mixed with the bottom most en-\n",
      "Theoveralllossfunctioncanbewrittenasfollows:\n",
      "tity (the intruder) in that dimension and shuffled.\n",
      "Then multiple (3 in our case) human annotators\n",
      "L(θ,θ,T)+λ C(θ,P)+λ R(θ,θ ) (6)\n",
      "e r c e r e r\n",
      "are asked to find out the intruder. We use ma-\n",
      "jority voting to finalize one intruder. Amazon\n",
      "Where R(θ,θ ) = 1 kθ k2+kθ k2 is the\n",
      "e r 2 (cid:16) e r (cid:17) Mechanical Turk was used for crowdsourcing the\n",
      "L2 regularization term and λ and λ are hyper-\n",
      "c r task and we used 25 randomly selected dimen-\n",
      "parameters controlling thetrade-off amongdiffer-\n",
      "sionsforevaluation. Forautomatedwordintrusion\n",
      "enttermsintheobjectivefunction.\n",
      "(Lauetal.,2014),wecalculatefollowingscorefor\n",
      "allk+1entities\n",
      "4 Experiments andResults\n",
      "4.1 Datasets k+1\n",
      "AutoWI(e ) = p (7)\n",
      "We use the FB15k-237(Toutanova andChen, i X ij\n",
      "j=1,j6=i\n",
      "2015) dataset for experiments. It contains 14541\n",
      "entities and 237 relations. The triples are split wherep arethePMIscores. Theentityhaving\n",
      "ij\n",
      "into training, validation and test set having least score is identified as the intruder. We report\n",
      "272115, 17535 and 20466 triples respectively. thefractionofdimensionsforwhichwewereable\n",
      "For extracting entity co-occurrences, we use the toidentify theintruder correctly.\n",
      "textual relations used in (Toutanovaetal., 2015). Aswecan see inTable 1, the proposed method\n",
      "It contains around 3.7 millions textual triples, achieves better values for Coherence@5 as a\n",
      "whichweuseforcalculating PMIforentitypairs. direct consequence of the regularization term,\n",
      "thereby maximizing coherence between appropri-\n",
      "4.2 ExperimentalSetup\n",
      "ate entities. Performance on the word intrusion\n",
      "We use the method proposed in (Riedeletal., taskalsoimprovesdrasticallyastheintruderalong\n",
      "2013) as the baseline. Please refer to Section 3.2 each dimension is a lot easier to identify owing\n",
      "to the fact that the top entities for each dimension deemedincorrectbytheclosedworldassumption.\n",
      "grouptogethermoreconspicuously.\n",
      "4.3.3 TripleClassification\n",
      "Method LinkPrediction\n",
      "In this experiment, we test the model on classify-\n",
      "MRR MR Hits@10(%)\n",
      "Baseline 31.6±0.08 121.9±1.80 48.3±0.39 ing correct and incorrect triples. For finding in-\n",
      "Proposed 30.4±0.08 111.9±1.12 46.8±0.08\n",
      "correct triples, we corrupt the object entity with\n",
      "TripleClassification\n",
      "AUC(%) Accuracy(%) a randomly selected entity within the same cate-\n",
      "Baseline 72.9±0.16 63.2±0.50\n",
      "gory. For classification, we use validation data to\n",
      "Proposed 73.2±0.28 67.6±0.17\n",
      "Interpretability findthebestthresholdforeachrelationbytraining\n",
      "AutoWI@5(%) Coherence@5 ManualWI(%)\n",
      "an SVM classifier and later use this threshold for\n",
      "Baseline 6±4.14 −47.4±4.68 12\n",
      "Proposed 66±5.89 −12.5±4.48 84 classifying test triples. We report the mean accu-\n",
      "racyandmeanAUCoverallrelations.\n",
      "Table 1: Results on test data. The pro-\n",
      "We observe that the proposed method achieves\n",
      "posedmethodsignificantlyimprovesinterpretabil-\n",
      "slightlybetterperformancefortripleclassification\n",
      "itywhilemaintaining comparableperformance on\n",
      "improving the accuracy by 4.4. The PMI infor-\n",
      "KGtasks(Section4.3).\n",
      "mation adds more evidence to the correct triples\n",
      "which are related in text data, generating a better\n",
      "threshold that more accurately distinguishes cor-\n",
      "Top5\n",
      "Baseline rectandincorrecttriples.\n",
      "-Jurist,Pipeorgan,USA,LionsGateEntertainment,UK\n",
      "-Guitar,71stAcademyAwards,Jurist,Piano,Bassguitar\n",
      "-Actor,OfficialWebsite,Screenwriter,FilmProducer,USA 4.4 QualitativeAnalysisofResults\n",
      "-Jurist,USA,Marriage,Male,UK\n",
      "-Pipeorgan,OfficialWebsite,Actor,FilmProducer,Screenwriter Since our aim is to induce interpretability in rep-\n",
      "ProposedMethod resentations, in this section, we evaluate the em-\n",
      "-JurisDoctor,BusinessAdministration,Biology,Psychology,BS\n",
      "beddings learned by the baseline as well as the\n",
      "-BachelorofArts,PhD,Bachelor’sdegree,BS,MS\n",
      "-EuropeanUnion,Europe,Netherlands,Portugal,Government proposed method. For both methods, we select\n",
      "-UK,Hollywood,DVD,London,Europe\n",
      "some dimensions randomly and present top 5 en-\n",
      "-Hollywood,AcademyAwards,USA,DVD,LosAngeles\n",
      "titiesalong those dimensions. Theresults arepre-\n",
      "Table 2: Top 5 and bottom most entities for ran- sentedinTable2.\n",
      "domly selected dimensions. As we see, the pro-\n",
      "As we can see from the results, the proposed\n",
      "posed method produces more coherent entities\n",
      "method produces more coherent entities than the\n",
      "compared to the baseline. Incoherent entities are\n",
      "baselinemethod.\n",
      "markedinboldface. 2\n",
      "5 Conclusionand Future Works\n",
      "4.3.2 LinkPrediction\n",
      "In this work, we proposed a method for induc-\n",
      "In this experiment, we test the model’s ability to inginterpretability inKGembeddings using aco-\n",
      "predict the best object entity for a given subject herence regularization term. We evaluated the\n",
      "entity and relation. For each of the triples, we fix proposed and the baseline method on the inter-\n",
      "the subject and the relation and rank all entities pretability of the learned embeddings. We also\n",
      "(within samecategory astrue object entity) based evaluated the methods on different KG tasks and\n",
      "on their score according to Equation 4. We re- compared their performance. We found that the\n",
      "port Mean Rank (MR)and Mean Reciprocal rank proposed method achieves better interpretability\n",
      "(MRR)ofthe true object entity and Hits@10 (the while maintaining comparable performance on\n",
      "numberoftimestrueobjectentityisrankedintop KG tasks. As next steps, we plan to evaluate the\n",
      "10)aspercentage. generalizability of the method with more recent\n",
      "The objective of the coherence regularization KGembeddings.\n",
      "term being tangential to that of the original loss\n",
      "function, isnotexpected toaffect performance on\n",
      "thelinkpredictiontask. However,theresultsshow 2We have used abbreviations for BS (Bachelor of Sci-\n",
      "ence), MS (Master of Science), UK (United Kingdom) and\n",
      "a trivial drop of 1.2 in MRR as the coherence\n",
      "USA(UnitedStatesofAmerica).Theyappearasfullformin\n",
      "term gives credibility to triples that are otherwise thedata.\n",
      "References Han Xiao, Minlie Huang, Yu Hao, and Xiaoyan\n",
      "Zhu. 2015a. Transa: An adaptive approach\n",
      "Antoine Bordes, Jason Weston, Ronan Collobert, and\n",
      "for knowledge graph embedding. arXiv preprint\n",
      "Yoshua Bengio. 2011. Learningstructured embed-\n",
      "arXiv:1509.05490.\n",
      "dingsofknowledgebases. InConferenceonArtifi-\n",
      "cialIntelligence.EPFL-CONF-192344. Han Xiao, Minlie Huang, Yu Hao, and Xiaoyan\n",
      "Zhu. 2015b. Transg: A generative mixture model\n",
      "JonathanChang,JordanLBoyd-Graber,SeanGerrish,\n",
      "for knowledge graph embedding. arXiv preprint\n",
      "Chong Wang, and David M Blei. 2009. Reading\n",
      "arXiv:1509.05488.\n",
      "tea leaves: How humansinterprettopic models. In\n",
      "Nips.volume31,pages1–9. Han Xiao, Minlie Huang, and Xiaoyan Zhu. 2016.\n",
      "Knowledge semantic representation: A generative\n",
      "Miao Fan, Qiang Zhou, Emily Chang, and\n",
      "model for interpretable knowledge graph embed-\n",
      "Thomas Fang Zheng. 2014. Transition-based\n",
      "ding. arXivpreprintarXiv:1608.07685.\n",
      "knowledge graph embedding with relational\n",
      "mappingproperties. InPACLIC.pages328–337.\n",
      "GuoliangJi,ShizhuHe,LihengXu,KangLiu,andJun\n",
      "Zhao. 2015. Knowledge graph embedding via dy-\n",
      "namicmappingmatrix. InACL(1).pages687–696.\n",
      "Jey Han Lau, David Newman, and Timothy Baldwin.\n",
      "2014. Machine reading tea leaves: Automatically\n",
      "evaluatingtopiccoherenceandtopicmodelquality.\n",
      "InEACL.pages530–539.\n",
      "YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,and\n",
      "Xuan Zhu. 2015. Learning entity and relation em-\n",
      "beddingsforknowledgegraphcompletion. InAAAI.\n",
      "pages2181–2187.\n",
      "Brian Murphy, Partha Pratim Talukdar, and Tom\n",
      "Mitchell.2012. Learningeffectiveandinterpretable\n",
      "semantic modelsusing non-negativesparse embed-\n",
      "ding. In International Conference on Computa-\n",
      "tionalLinguistics(COLING 2012),Mumbai, India.\n",
      "http://aclweb.org/anthology/C/C12/C12-1118.pdf.\n",
      "SebastianRiedel, LiminYao,AndrewMcCallum,and\n",
      "BenjaminMMarlin.2013. Relationextractionwith\n",
      "matrixfactorizationanduniversalschemas. NAACL\n",
      "HLT2013pages74–84.\n",
      "RichardSocher,DanqiChen,ChristopherDManning,\n",
      "andAndrewNg. 2013. Reasoning with neuralten-\n",
      "sornetworksforknowledgebasecompletion. InAd-\n",
      "vances in Neural Information Processing Systems.\n",
      "pages926–934.\n",
      "Kristina Toutanovaand Danqi Chen. 2015. Observed\n",
      "versus latent features for knowledge base and text\n",
      "inference. In 3rd Workshop on Continuous Vec-\n",
      "torSpaceModelsandTheirCompositionality.ACL\n",
      "AssociationforComputationalLinguistics.\n",
      "Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-\n",
      "fungPoon,PallaviChoudhury,andMichaelGamon.\n",
      "2015. Representing Text for Joint Embedding of\n",
      "Text and KnowledgeBases. In Empirical Methods\n",
      "in Natural Language Processing (EMNLP). ACL\n",
      "AssociationforComputationalLinguistics.\n",
      "Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\n",
      "Chen.2014. Knowledgegraphembeddingbytrans-\n",
      "lating on hyperplanes. In AAAI. Citeseer, pages\n",
      "1112–1119.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  14590,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['FB15k-237']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: 7102\n",
      "ceD\n",
      "01\n",
      "]LC.sc[\n",
      "1v74530.2171:viXra\n",
      "Inducing Interpretability in Knowledge Graph Embeddings\n",
      "Chandrahas and Tathagata Sengupta and CibiPragadeesh and Partha PratimTalukdar\n",
      "chandrahas@iisc.ac.in\n",
      "Abstract over these vectors. Some of these methods like\n",
      "(Riedeletal., 2013), (Toutanovaetal., 2015) are\n",
      "We study the problem of inducing inter-\n",
      "capable of exploiting additional text data apart\n",
      "pretability in KG embeddings. Specifi-\n",
      "fromtheKG,resulting inbetterrepresentations.\n",
      "cally, we explore the Universal Schema\n",
      "Although these methods have shown good per-\n",
      "(Riedeletal.,2013)andproposeamethod\n",
      "formance in applications, they don’t address the\n",
      "toinduceinterpretability. Therehavebeen\n",
      "problem ofunderstanding semantics of individual\n",
      "many vector space models proposed for\n",
      "dimensions of the KGembedding. Arecent work\n",
      "theproblem,however,mostofthesemeth-\n",
      "(Xiaoetal.,2016)addressedtheproblemoflearn-\n",
      "ods don’t address the interpretability (se-\n",
      "ing semantic features for KGs. However, they\n",
      "mantics) of individual dimensions. In this\n",
      "don’tdirectlyusevectorspacemodeling.\n",
      "work,westudythisproblemandproposea\n",
      "In this work, we focus on incorporating in-\n",
      "methodforinducinginterpretabilityinKG\n",
      "terpretability in KG embeddings. Specifically,\n",
      "embeddings using entity co-occurrence\n",
      "we aim to learn interpretable embeddings for\n",
      "statistics. The proposed method signifi-\n",
      "KG entities by incorporating additional entity co-\n",
      "cantly improves the interpretability, while\n",
      "occurrence statistics from text data. This work is\n",
      "maintaining comparable performance in\n",
      "motivatedby(Lauetal.,2014)whopresented au-\n",
      "otherKGtasks.\n",
      "tomatedmethods forevaluating topics learned via\n",
      "topic modelling methods. We adapt these mea-\n",
      "1 Introduction\n",
      "sures for the vector space model and propose a\n",
      "Knowledge Graphs such as Freebase, WordNet method to directly maximize them while learning\n",
      "etc. havebecomeimportantresourcesforsupport- KGembedding. Tothebestofourknowledge,this\n",
      "ing many AI applications like web search, Q&A work presents the first regularization term which\n",
      "etc. They store a collection of facts in the form inducesinterpretability inKGembeddings.\n",
      "of a graph. The nodes in the graph represent\n",
      "2 RelatedWork\n",
      "real world entities such as Roger Federer, Tennis,\n",
      "United States etc while the edges represent rela- Several methods have been proposed for learning\n",
      "tionships betweenthem. KG embeddings. They differ on the modeling of\n",
      "These KGs have grown huge, but they are entities and relations, usage oftextdata andinter-\n",
      "still not complete (Toutanovaetal., 2015). Hence pretabilityofthelearnedembeddings. Wesumma-\n",
      "the task of inferring new facts becomes impor- rizesomeofthesemethodsinfollowingsections.\n",
      "tant. Many vector space models have been\n",
      "2.1 Vector-spacemodelsforKGEmbeddings\n",
      "proposed which can perform reasoning over\n",
      "KGsefficiently(Bordesetal.,2011),(Wangetal., A very effective and powerful set of models are\n",
      "2014), (Linetal., 2015), (Socheretal., 2013), based on translation vectors. These models rep-\n",
      "(Riedeletal., 2013), (Toutanovaetal., 2015) etc. resent entities as vectors in d-dimensional space,\n",
      "These methods learn representations for entities Rd and relations as translation vectors from head\n",
      "and relations as vectors in a vector space, cap- entity to tail entity, in either same or a pro-\n",
      "turing global information about the KG. The task jected space. TransE(Bordesetal., 2011) is one\n",
      "of KG inference is then defined as operations of the initial works, which was later improved\n",
      "by many works [(Wangetal., 2014), (Linetal., ties within the topic. This idea can also be used\n",
      "2015), (Xiaoetal., 2015b), (Xiaoetal., 2015a), in vector space models by treating dimensions of\n",
      "(Jietal.,2015),(Fanetal.,2014)]. Also,thereare the vector space as topics. With this assumption,\n",
      "methods which are able to incorporate text data wecanuseameasureofcoherence definedinfol-\n",
      "while learning KG embeddings. (Riedeletal., lowingsectionforevaluatinginterpretabilityofthe\n",
      "2013)is one such method, which assumes acom- embeddings.\n",
      "bined universal schema of relations from KG as\n",
      "well as text. (Toutanovaetal., 2015) further im-\n",
      "3.1.1 Coherence@k\n",
      "proves the performance by sharing parameters Coherence@k has been shown to have high\n",
      "amongsimilartextualrelations. correlation with human interpretability of\n",
      "topics learned via various topic modeling\n",
      "2.2 Interpretability ofEmbedding\n",
      "methods(Lauetal., 2014). Hence, we can expect\n",
      "While the vector space models perform well in interpretable embeddings bymaximizingit.\n",
      "many tasks, the semantics of learned representa- Coherence for top k entities along dimension l\n",
      "tionsarenotdirectlyclear. Thisproblemforword isdefinedasfollows:\n",
      "embeddings was addressed by (Murphyetal.,\n",
      "k i−1\n",
      "2012)wheretheyproposed asetofconstraints in-\n",
      "Coherence@k(l) = p (1)\n",
      "ducing interpretability. However, its adaptation XX ij\n",
      "i=2 j=1\n",
      "for KG embeddings hasn’t been addressed. A\n",
      "recent work (Xiaoetal., 2016) addressed a sim-\n",
      "where p is PMI score between entities e and e\n",
      "ij i j\n",
      "ilar problem, where they learn coherent seman-\n",
      "extracted from text data. Coherence@k for the\n",
      "tic features for entities and relations in KG. Our\n",
      "entity embedding matrix θ isdefinedasthe aver-\n",
      "e\n",
      "methoddiffersfromtheirsinthefollowingtwoas-\n",
      "ageoveralldimensions.\n",
      "pects. Firstly, weusevector space modeling lead-\n",
      "ing directly to KG embeddings while they need\n",
      "d\n",
      "1\n",
      "to infer KG embeddings from their probabilistic Coherence@k = Coherence@k(l) (2)\n",
      "dX\n",
      "model. Second, we incorporate additional infor-\n",
      "l=1\n",
      "mation about entities which helps in learning in-\n",
      "terpretable embeddings. 3.1.2 Inducingcoherencewhilelearning\n",
      "embeddings\n",
      "3 Proposed Method\n",
      "We want to learn an embedding matrix θ which\n",
      "e\n",
      "has high coherence (i.e. which maximizes\n",
      "We are interested in inducing interpretability in\n",
      "Coherence@k). Since θ changes during train-\n",
      "KG embeddings and regularization is one good e\n",
      "ing, thesetoftop k entities along each dimension\n",
      "waytodoit. Sowewanttolookatnovelregulariz-\n",
      "variesoveriterations. Hence,directlymaximizing\n",
      "ersinKGembeddings. Hence,weexploreamea-\n",
      "Coherence@k seemstobetricky.\n",
      "sure of coherence proposed in (Lauetal., 2014).\n",
      "An alternate approach could be to promote\n",
      "This measure allows automated evaluation of the\n",
      "higher values for entity pairs having high PMI\n",
      "quality of topics learned by topic modeling meth-\n",
      "score p. This will result in an embedding ma-\n",
      "ods by using additional Point-wise Mutual Infor- ij\n",
      "trix θ with a high value of Coherence@k since\n",
      "mation (PMI) for word pairs. It was also shown e\n",
      "highPMIentitypairsaremorelikelytobeamong\n",
      "tohavehigh correlation withhumanevaluation of\n",
      "topkentities.\n",
      "topics.\n",
      "This idea can be captured by following coher-\n",
      "Based on this measure of coherence, we pro-\n",
      "enceterm\n",
      "pose a regularization term. This term can be\n",
      "used with existing KG embedding methods (eg\n",
      "n i−1\n",
      "(Riedeletal., 2013)) for inducing interpretability. C(θ,P) = kv(e )⊺v(e )−p k2 (3)\n",
      "e XX i j ij\n",
      "Itisdescribed inthefollowingsections.\n",
      "i=2 j=1\n",
      "3.1 Coherence\n",
      "whereP isentity-pair PMImatrixandv(e)de-\n",
      "In topic models, coherence of a topic can be de- note vector for entity e. This term can be used in\n",
      "termined bysemantic relatedness among topenti- theobjective function definedinEquation6\n",
      "3.2 EntityModel(Model-E) for more details. For evaluating the learned em-\n",
      "beddings, we test them on different tasks. All\n",
      "We use the Entity Model proposed in\n",
      "thehyper-parametersaretunedusingperformance\n",
      "(Riedeletal., 2013) for learning KG embed-\n",
      "(MRR) on validation data. We use 100 dimen-\n",
      "dings. This model assumes a vector v(e) for\n",
      "sions after cross validating among 50, 100 and\n",
      "each entity and two vectors v (r) and v (r) for\n",
      "s o\n",
      "200 dimensions. For regularization, we use λ =\n",
      "each relation of the KG. The score for the triple r\n",
      "0.01 (from 10,1,0.1,0.01) and λ = 0.01 (from\n",
      "(e,r,e )isgivenby, c\n",
      "s o\n",
      "10,1,0.1,0.01) for L2 and coherence regulariza-\n",
      "tion respectively. Weuse multiple random initial-\n",
      "f(e,r,e ) = v(e )⊺v (r)+v(e )⊺v (r) (4) izationssampledfromaGaussiandistribution. For\n",
      "s o s s o o\n",
      "optimization,weusegradientdescentandstopop-\n",
      "Trainingthesevectorsrequiresincorrecttriples. timization when gradient becomes 0 upto 3 deci-\n",
      "So,weusetheclosedworldassumption. Foreach mal places. The final performance measures are\n",
      "triple t ∈ T, we create two negative triples t− reportedfortestdata.\n",
      "o\n",
      "andt− bycorrupting the object andsubject ofthe\n",
      "s\n",
      "4.3 Results\n",
      "triples respectively such that the corrupted triples\n",
      "don’t appear in training, test or validation data. In following sections, we compare the perfor-\n",
      "Thelossforatriplepairisdefinedasloss(t,t−)=\n",
      "mance of the proposed method with the baseline\n",
      "−log(σ(f(t)−f(t−))). Then,theaggregateloss method in different tasks. Please refer to Table 1\n",
      "function isdefinedas forresults.\n",
      "4.3.1 Interpretability\n",
      "1\n",
      "L(θ e,θ r,T)= |T| X(cid:0)loss(t,t− o)+loss(t,t− s ) (cid:1) For evaluating the interpretability, we use\n",
      "t∈T Coherence@k(Equation2),automatedandman-\n",
      "(5)\n",
      "ual word intrusion tests. In word intrusion test\n",
      "3.3 Objective (Changetal., 2009), top k(= 5) entities along a\n",
      "dimension are mixed with the bottom most en-\n",
      "Theoveralllossfunctioncanbewrittenasfollows:\n",
      "tity (the intruder) in that dimension and shuffled.\n",
      "Then multiple (3 in our case) human annotators\n",
      "L(θ,θ,T)+λ C(θ,P)+λ R(θ,θ ) (6)\n",
      "e r c e r e r\n",
      "are asked to find out the intruder. We use ma-\n",
      "jority voting to finalize one intruder. Amazon\n",
      "Where R(θ,θ ) = 1 kθ k2+kθ k2 is the\n",
      "e r 2 (cid:16) e r (cid:17) Mechanical Turk was used for crowdsourcing the\n",
      "L2 regularization term and λ and λ are hyper-\n",
      "c r task and we used 25 randomly selected dimen-\n",
      "parameters controlling thetrade-off amongdiffer-\n",
      "sionsforevaluation. Forautomatedwordintrusion\n",
      "enttermsintheobjectivefunction.\n",
      "(Lauetal.,2014),wecalculatefollowingscorefor\n",
      "allk+1entities\n",
      "4 Experiments andResults\n",
      "4.1 Datasets k+1\n",
      "AutoWI(e ) = p (7)\n",
      "We use the FB15k-237(Toutanova andChen, i X ij\n",
      "j=1,j6=i\n",
      "2015) dataset for experiments. It contains 14541\n",
      "entities and 237 relations. The triples are split wherep arethePMIscores. Theentityhaving\n",
      "ij\n",
      "into training, validation and test set having least score is identified as the intruder. We report\n",
      "272115, 17535 and 20466 triples respectively. thefractionofdimensionsforwhichwewereable\n",
      "For extracting entity co-occurrences, we use the toidentify theintruder correctly.\n",
      "textual relations used in (Toutanovaetal., 2015). Aswecan see inTable 1, the proposed method\n",
      "It contains around 3.7 millions textual triples, achieves better values for Coherence@5 as a\n",
      "whichweuseforcalculating PMIforentitypairs. direct consequence of the regularization term,\n",
      "thereby maximizing coherence between appropri-\n",
      "4.2 ExperimentalSetup\n",
      "ate entities. Performance on the word intrusion\n",
      "We use the method proposed in (Riedeletal., taskalsoimprovesdrasticallyastheintruderalong\n",
      "2013) as the baseline. Please refer to Section 3.2 each dimension is a lot easier to identify owing\n",
      "to the fact that the top entities for each dimension deemedincorrectbytheclosedworldassumption.\n",
      "grouptogethermoreconspicuously.\n",
      "4.3.3 TripleClassification\n",
      "Method LinkPrediction\n",
      "In this experiment, we test the model on classify-\n",
      "MRR MR Hits@10(%)\n",
      "Baseline 31.6±0.08 121.9±1.80 48.3±0.39 ing correct and incorrect triples. For finding in-\n",
      "Proposed 30.4±0.08 111.9±1.12 46.8±0.08\n",
      "correct triples, we corrupt the object entity with\n",
      "TripleClassification\n",
      "AUC(%) Accuracy(%) a randomly selected entity within the same cate-\n",
      "Baseline 72.9±0.16 63.2±0.50\n",
      "gory. For classification, we use validation data to\n",
      "Proposed 73.2±0.28 67.6±0.17\n",
      "Interpretability findthebestthresholdforeachrelationbytraining\n",
      "AutoWI@5(%) Coherence@5 ManualWI(%)\n",
      "an SVM classifier and later use this threshold for\n",
      "Baseline 6±4.14 −47.4±4.68 12\n",
      "Proposed 66±5.89 −12.5±4.48 84 classifying test triples. We report the mean accu-\n",
      "racyandmeanAUCoverallrelations.\n",
      "Table 1: Results on test data. The pro-\n",
      "We observe that the proposed method achieves\n",
      "posedmethodsignificantlyimprovesinterpretabil-\n",
      "slightlybetterperformancefortripleclassification\n",
      "itywhilemaintaining comparableperformance on\n",
      "improving the accuracy by 4.4. The PMI infor-\n",
      "KGtasks(Section4.3).\n",
      "mation adds more evidence to the correct triples\n",
      "which are related in text data, generating a better\n",
      "threshold that more accurately distinguishes cor-\n",
      "Top5\n",
      "Baseline rectandincorrecttriples.\n",
      "-Jurist,Pipeorgan,USA,LionsGateEntertainment,UK\n",
      "-Guitar,71stAcademyAwards,Jurist,Piano,Bassguitar\n",
      "-Actor,OfficialWebsite,Screenwriter,FilmProducer,USA 4.4 QualitativeAnalysisofResults\n",
      "-Jurist,USA,Marriage,Male,UK\n",
      "-Pipeorgan,OfficialWebsite,Actor,FilmProducer,Screenwriter Since our aim is to induce interpretability in rep-\n",
      "ProposedMethod resentations, in this section, we evaluate the em-\n",
      "-JurisDoctor,BusinessAdministration,Biology,Psychology,BS\n",
      "beddings learned by the baseline as well as the\n",
      "-BachelorofArts,PhD,Bachelor’sdegree,BS,MS\n",
      "-EuropeanUnion,Europe,Netherlands,Portugal,Government proposed method. For both methods, we select\n",
      "-UK,Hollywood,DVD,London,Europe\n",
      "some dimensions randomly and present top 5 en-\n",
      "-Hollywood,AcademyAwards,USA,DVD,LosAngeles\n",
      "titiesalong those dimensions. Theresults arepre-\n",
      "Table 2: Top 5 and bottom most entities for ran- sentedinTable2.\n",
      "domly selected dimensions. As we see, the pro-\n",
      "As we can see from the results, the proposed\n",
      "posed method produces more coherent entities\n",
      "method produces more coherent entities than the\n",
      "compared to the baseline. Incoherent entities are\n",
      "baselinemethod.\n",
      "markedinboldface. 2\n",
      "5 Conclusionand Future Works\n",
      "4.3.2 LinkPrediction\n",
      "In this work, we proposed a method for induc-\n",
      "In this experiment, we test the model’s ability to inginterpretability inKGembeddings using aco-\n",
      "predict the best object entity for a given subject herence regularization term. We evaluated the\n",
      "entity and relation. For each of the triples, we fix proposed and the baseline method on the inter-\n",
      "the subject and the relation and rank all entities pretability of the learned embeddings. We also\n",
      "(within samecategory astrue object entity) based evaluated the methods on different KG tasks and\n",
      "on their score according to Equation 4. We re- compared their performance. We found that the\n",
      "port Mean Rank (MR)and Mean Reciprocal rank proposed method achieves better interpretability\n",
      "(MRR)ofthe true object entity and Hits@10 (the while maintaining comparable performance on\n",
      "numberoftimestrueobjectentityisrankedintop KG tasks. As next steps, we plan to evaluate the\n",
      "10)aspercentage. generalizability of the method with more recent\n",
      "The objective of the coherence regularization KGembeddings.\n",
      "term being tangential to that of the original loss\n",
      "function, isnotexpected toaffect performance on\n",
      "thelinkpredictiontask. However,theresultsshow 2We have used abbreviations for BS (Bachelor of Sci-\n",
      "ence), MS (Master of Science), UK (United Kingdom) and\n",
      "a trivial drop of 1.2 in MRR as the coherence\n",
      "USA(UnitedStatesofAmerica).Theyappearasfullformin\n",
      "term gives credibility to triples that are otherwise thedata.\n",
      "References Han Xiao, Minlie Huang, Yu Hao, and Xiaoyan\n",
      "Zhu. 2015a. Transa: An adaptive approach\n",
      "Antoine Bordes, Jason Weston, Ronan Collobert, and\n",
      "for knowledge graph embedding. arXiv preprint\n",
      "Yoshua Bengio. 2011. Learningstructured embed-\n",
      "arXiv:1509.05490.\n",
      "dingsofknowledgebases. InConferenceonArtifi-\n",
      "cialIntelligence.EPFL-CONF-192344. Han Xiao, Minlie Huang, Yu Hao, and Xiaoyan\n",
      "Zhu. 2015b. Transg: A generative mixture model\n",
      "JonathanChang,JordanLBoyd-Graber,SeanGerrish,\n",
      "for knowledge graph embedding. arXiv preprint\n",
      "Chong Wang, and David M Blei. 2009. Reading\n",
      "arXiv:1509.05488.\n",
      "tea leaves: How humansinterprettopic models. In\n",
      "Nips.volume31,pages1–9. Han Xiao, Minlie Huang, and Xiaoyan Zhu. 2016.\n",
      "Knowledge semantic representation: A generative\n",
      "Miao Fan, Qiang Zhou, Emily Chang, and\n",
      "model for interpretable knowledge graph embed-\n",
      "Thomas Fang Zheng. 2014. Transition-based\n",
      "ding. arXivpreprintarXiv:1608.07685.\n",
      "knowledge graph embedding with relational\n",
      "mappingproperties. InPACLIC.pages328–337.\n",
      "GuoliangJi,ShizhuHe,LihengXu,KangLiu,andJun\n",
      "Zhao. 2015. Knowledge graph embedding via dy-\n",
      "namicmappingmatrix. InACL(1).pages687–696.\n",
      "Jey Han Lau, David Newman, and Timothy Baldwin.\n",
      "2014. Machine reading tea leaves: Automatically\n",
      "evaluatingtopiccoherenceandtopicmodelquality.\n",
      "InEACL.pages530–539.\n",
      "YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,and\n",
      "Xuan Zhu. 2015. Learning entity and relation em-\n",
      "beddingsforknowledgegraphcompletion. InAAAI.\n",
      "pages2181–2187.\n",
      "Brian Murphy, Partha Pratim Talukdar, and Tom\n",
      "Mitchell.2012. Learningeffectiveandinterpretable\n",
      "semantic modelsusing non-negativesparse embed-\n",
      "ding. In International Conference on Computa-\n",
      "tionalLinguistics(COLING 2012),Mumbai, India.\n",
      "http://aclweb.org/anthology/C/C12/C12-1118.pdf.\n",
      "SebastianRiedel, LiminYao,AndrewMcCallum,and\n",
      "BenjaminMMarlin.2013. Relationextractionwith\n",
      "matrixfactorizationanduniversalschemas. NAACL\n",
      "HLT2013pages74–84.\n",
      "RichardSocher,DanqiChen,ChristopherDManning,\n",
      "andAndrewNg. 2013. Reasoning with neuralten-\n",
      "sornetworksforknowledgebasecompletion. InAd-\n",
      "vances in Neural Information Processing Systems.\n",
      "pages926–934.\n",
      "Kristina Toutanovaand Danqi Chen. 2015. Observed\n",
      "versus latent features for knowledge base and text\n",
      "inference. In 3rd Workshop on Continuous Vec-\n",
      "torSpaceModelsandTheirCompositionality.ACL\n",
      "AssociationforComputationalLinguistics.\n",
      "Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-\n",
      "fungPoon,PallaviChoudhury,andMichaelGamon.\n",
      "2015. Representing Text for Joint Embedding of\n",
      "Text and KnowledgeBases. In Empirical Methods\n",
      "in Natural Language Processing (EMNLP). ACL\n",
      "AssociationforComputationalLinguistics.\n",
      "Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\n",
      "Chen.2014. Knowledgegraphembeddingbytrans-\n",
      "lating on hyperplanes. In AAAI. Citeseer, pages\n",
      "1112–1119.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  71647,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['KG inference', 'Link prediction', 'Triple classification', 'Inducing interpretability in KG embeddings']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: 7102\n",
      "ceD\n",
      "01\n",
      "]LC.sc[\n",
      "1v74530.2171:viXra\n",
      "Inducing Interpretability in Knowledge Graph Embeddings\n",
      "Chandrahas and Tathagata Sengupta and CibiPragadeesh and Partha PratimTalukdar\n",
      "chandrahas@iisc.ac.in\n",
      "Abstract over these vectors. Some of these methods like\n",
      "(Riedeletal., 2013), (Toutanovaetal., 2015) are\n",
      "We study the problem of inducing inter-\n",
      "capable of exploiting additional text data apart\n",
      "pretability in KG embeddings. Specifi-\n",
      "fromtheKG,resulting inbetterrepresentations.\n",
      "cally, we explore the Universal Schema\n",
      "Although these methods have shown good per-\n",
      "(Riedeletal.,2013)andproposeamethod\n",
      "formance in applications, they don’t address the\n",
      "toinduceinterpretability. Therehavebeen\n",
      "problem ofunderstanding semantics of individual\n",
      "many vector space models proposed for\n",
      "dimensions of the KGembedding. Arecent work\n",
      "theproblem,however,mostofthesemeth-\n",
      "(Xiaoetal.,2016)addressedtheproblemoflearn-\n",
      "ods don’t address the interpretability (se-\n",
      "ing semantic features for KGs. However, they\n",
      "mantics) of individual dimensions. In this\n",
      "don’tdirectlyusevectorspacemodeling.\n",
      "work,westudythisproblemandproposea\n",
      "In this work, we focus on incorporating in-\n",
      "methodforinducinginterpretabilityinKG\n",
      "terpretability in KG embeddings. Specifically,\n",
      "embeddings using entity co-occurrence\n",
      "we aim to learn interpretable embeddings for\n",
      "statistics. The proposed method signifi-\n",
      "KG entities by incorporating additional entity co-\n",
      "cantly improves the interpretability, while\n",
      "occurrence statistics from text data. This work is\n",
      "maintaining comparable performance in\n",
      "motivatedby(Lauetal.,2014)whopresented au-\n",
      "otherKGtasks.\n",
      "tomatedmethods forevaluating topics learned via\n",
      "topic modelling methods. We adapt these mea-\n",
      "1 Introduction\n",
      "sures for the vector space model and propose a\n",
      "Knowledge Graphs such as Freebase, WordNet method to directly maximize them while learning\n",
      "etc. havebecomeimportantresourcesforsupport- KGembedding. Tothebestofourknowledge,this\n",
      "ing many AI applications like web search, Q&A work presents the first regularization term which\n",
      "etc. They store a collection of facts in the form inducesinterpretability inKGembeddings.\n",
      "of a graph. The nodes in the graph represent\n",
      "2 RelatedWork\n",
      "real world entities such as Roger Federer, Tennis,\n",
      "United States etc while the edges represent rela- Several methods have been proposed for learning\n",
      "tionships betweenthem. KG embeddings. They differ on the modeling of\n",
      "These KGs have grown huge, but they are entities and relations, usage oftextdata andinter-\n",
      "still not complete (Toutanovaetal., 2015). Hence pretabilityofthelearnedembeddings. Wesumma-\n",
      "the task of inferring new facts becomes impor- rizesomeofthesemethodsinfollowingsections.\n",
      "tant. Many vector space models have been\n",
      "2.1 Vector-spacemodelsforKGEmbeddings\n",
      "proposed which can perform reasoning over\n",
      "KGsefficiently(Bordesetal.,2011),(Wangetal., A very effective and powerful set of models are\n",
      "2014), (Linetal., 2015), (Socheretal., 2013), based on translation vectors. These models rep-\n",
      "(Riedeletal., 2013), (Toutanovaetal., 2015) etc. resent entities as vectors in d-dimensional space,\n",
      "These methods learn representations for entities Rd and relations as translation vectors from head\n",
      "and relations as vectors in a vector space, cap- entity to tail entity, in either same or a pro-\n",
      "turing global information about the KG. The task jected space. TransE(Bordesetal., 2011) is one\n",
      "of KG inference is then defined as operations of the initial works, which was later improved\n",
      "by many works [(Wangetal., 2014), (Linetal., ties within the topic. This idea can also be used\n",
      "2015), (Xiaoetal., 2015b), (Xiaoetal., 2015a), in vector space models by treating dimensions of\n",
      "(Jietal.,2015),(Fanetal.,2014)]. Also,thereare the vector space as topics. With this assumption,\n",
      "methods which are able to incorporate text data wecanuseameasureofcoherence definedinfol-\n",
      "while learning KG embeddings. (Riedeletal., lowingsectionforevaluatinginterpretabilityofthe\n",
      "2013)is one such method, which assumes acom- embeddings.\n",
      "bined universal schema of relations from KG as\n",
      "well as text. (Toutanovaetal., 2015) further im-\n",
      "3.1.1 Coherence@k\n",
      "proves the performance by sharing parameters Coherence@k has been shown to have high\n",
      "amongsimilartextualrelations. correlation with human interpretability of\n",
      "topics learned via various topic modeling\n",
      "2.2 Interpretability ofEmbedding\n",
      "methods(Lauetal., 2014). Hence, we can expect\n",
      "While the vector space models perform well in interpretable embeddings bymaximizingit.\n",
      "many tasks, the semantics of learned representa- Coherence for top k entities along dimension l\n",
      "tionsarenotdirectlyclear. Thisproblemforword isdefinedasfollows:\n",
      "embeddings was addressed by (Murphyetal.,\n",
      "k i−1\n",
      "2012)wheretheyproposed asetofconstraints in-\n",
      "Coherence@k(l) = p (1)\n",
      "ducing interpretability. However, its adaptation XX ij\n",
      "i=2 j=1\n",
      "for KG embeddings hasn’t been addressed. A\n",
      "recent work (Xiaoetal., 2016) addressed a sim-\n",
      "where p is PMI score between entities e and e\n",
      "ij i j\n",
      "ilar problem, where they learn coherent seman-\n",
      "extracted from text data. Coherence@k for the\n",
      "tic features for entities and relations in KG. Our\n",
      "entity embedding matrix θ isdefinedasthe aver-\n",
      "e\n",
      "methoddiffersfromtheirsinthefollowingtwoas-\n",
      "ageoveralldimensions.\n",
      "pects. Firstly, weusevector space modeling lead-\n",
      "ing directly to KG embeddings while they need\n",
      "d\n",
      "1\n",
      "to infer KG embeddings from their probabilistic Coherence@k = Coherence@k(l) (2)\n",
      "dX\n",
      "model. Second, we incorporate additional infor-\n",
      "l=1\n",
      "mation about entities which helps in learning in-\n",
      "terpretable embeddings. 3.1.2 Inducingcoherencewhilelearning\n",
      "embeddings\n",
      "3 Proposed Method\n",
      "We want to learn an embedding matrix θ which\n",
      "e\n",
      "has high coherence (i.e. which maximizes\n",
      "We are interested in inducing interpretability in\n",
      "Coherence@k). Since θ changes during train-\n",
      "KG embeddings and regularization is one good e\n",
      "ing, thesetoftop k entities along each dimension\n",
      "waytodoit. Sowewanttolookatnovelregulariz-\n",
      "variesoveriterations. Hence,directlymaximizing\n",
      "ersinKGembeddings. Hence,weexploreamea-\n",
      "Coherence@k seemstobetricky.\n",
      "sure of coherence proposed in (Lauetal., 2014).\n",
      "An alternate approach could be to promote\n",
      "This measure allows automated evaluation of the\n",
      "higher values for entity pairs having high PMI\n",
      "quality of topics learned by topic modeling meth-\n",
      "score p. This will result in an embedding ma-\n",
      "ods by using additional Point-wise Mutual Infor- ij\n",
      "trix θ with a high value of Coherence@k since\n",
      "mation (PMI) for word pairs. It was also shown e\n",
      "highPMIentitypairsaremorelikelytobeamong\n",
      "tohavehigh correlation withhumanevaluation of\n",
      "topkentities.\n",
      "topics.\n",
      "This idea can be captured by following coher-\n",
      "Based on this measure of coherence, we pro-\n",
      "enceterm\n",
      "pose a regularization term. This term can be\n",
      "used with existing KG embedding methods (eg\n",
      "n i−1\n",
      "(Riedeletal., 2013)) for inducing interpretability. C(θ,P) = kv(e )⊺v(e )−p k2 (3)\n",
      "e XX i j ij\n",
      "Itisdescribed inthefollowingsections.\n",
      "i=2 j=1\n",
      "3.1 Coherence\n",
      "whereP isentity-pair PMImatrixandv(e)de-\n",
      "In topic models, coherence of a topic can be de- note vector for entity e. This term can be used in\n",
      "termined bysemantic relatedness among topenti- theobjective function definedinEquation6\n",
      "3.2 EntityModel(Model-E) for more details. For evaluating the learned em-\n",
      "beddings, we test them on different tasks. All\n",
      "We use the Entity Model proposed in\n",
      "thehyper-parametersaretunedusingperformance\n",
      "(Riedeletal., 2013) for learning KG embed-\n",
      "(MRR) on validation data. We use 100 dimen-\n",
      "dings. This model assumes a vector v(e) for\n",
      "sions after cross validating among 50, 100 and\n",
      "each entity and two vectors v (r) and v (r) for\n",
      "s o\n",
      "200 dimensions. For regularization, we use λ =\n",
      "each relation of the KG. The score for the triple r\n",
      "0.01 (from 10,1,0.1,0.01) and λ = 0.01 (from\n",
      "(e,r,e )isgivenby, c\n",
      "s o\n",
      "10,1,0.1,0.01) for L2 and coherence regulariza-\n",
      "tion respectively. Weuse multiple random initial-\n",
      "f(e,r,e ) = v(e )⊺v (r)+v(e )⊺v (r) (4) izationssampledfromaGaussiandistribution. For\n",
      "s o s s o o\n",
      "optimization,weusegradientdescentandstopop-\n",
      "Trainingthesevectorsrequiresincorrecttriples. timization when gradient becomes 0 upto 3 deci-\n",
      "So,weusetheclosedworldassumption. Foreach mal places. The final performance measures are\n",
      "triple t ∈ T, we create two negative triples t− reportedfortestdata.\n",
      "o\n",
      "andt− bycorrupting the object andsubject ofthe\n",
      "s\n",
      "4.3 Results\n",
      "triples respectively such that the corrupted triples\n",
      "don’t appear in training, test or validation data. In following sections, we compare the perfor-\n",
      "Thelossforatriplepairisdefinedasloss(t,t−)=\n",
      "mance of the proposed method with the baseline\n",
      "−log(σ(f(t)−f(t−))). Then,theaggregateloss method in different tasks. Please refer to Table 1\n",
      "function isdefinedas forresults.\n",
      "4.3.1 Interpretability\n",
      "1\n",
      "L(θ e,θ r,T)= |T| X(cid:0)loss(t,t− o)+loss(t,t− s ) (cid:1) For evaluating the interpretability, we use\n",
      "t∈T Coherence@k(Equation2),automatedandman-\n",
      "(5)\n",
      "ual word intrusion tests. In word intrusion test\n",
      "3.3 Objective (Changetal., 2009), top k(= 5) entities along a\n",
      "dimension are mixed with the bottom most en-\n",
      "Theoveralllossfunctioncanbewrittenasfollows:\n",
      "tity (the intruder) in that dimension and shuffled.\n",
      "Then multiple (3 in our case) human annotators\n",
      "L(θ,θ,T)+λ C(θ,P)+λ R(θ,θ ) (6)\n",
      "e r c e r e r\n",
      "are asked to find out the intruder. We use ma-\n",
      "jority voting to finalize one intruder. Amazon\n",
      "Where R(θ,θ ) = 1 kθ k2+kθ k2 is the\n",
      "e r 2 (cid:16) e r (cid:17) Mechanical Turk was used for crowdsourcing the\n",
      "L2 regularization term and λ and λ are hyper-\n",
      "c r task and we used 25 randomly selected dimen-\n",
      "parameters controlling thetrade-off amongdiffer-\n",
      "sionsforevaluation. Forautomatedwordintrusion\n",
      "enttermsintheobjectivefunction.\n",
      "(Lauetal.,2014),wecalculatefollowingscorefor\n",
      "allk+1entities\n",
      "4 Experiments andResults\n",
      "4.1 Datasets k+1\n",
      "AutoWI(e ) = p (7)\n",
      "We use the FB15k-237(Toutanova andChen, i X ij\n",
      "j=1,j6=i\n",
      "2015) dataset for experiments. It contains 14541\n",
      "entities and 237 relations. The triples are split wherep arethePMIscores. Theentityhaving\n",
      "ij\n",
      "into training, validation and test set having least score is identified as the intruder. We report\n",
      "272115, 17535 and 20466 triples respectively. thefractionofdimensionsforwhichwewereable\n",
      "For extracting entity co-occurrences, we use the toidentify theintruder correctly.\n",
      "textual relations used in (Toutanovaetal., 2015). Aswecan see inTable 1, the proposed method\n",
      "It contains around 3.7 millions textual triples, achieves better values for Coherence@5 as a\n",
      "whichweuseforcalculating PMIforentitypairs. direct consequence of the regularization term,\n",
      "thereby maximizing coherence between appropri-\n",
      "4.2 ExperimentalSetup\n",
      "ate entities. Performance on the word intrusion\n",
      "We use the method proposed in (Riedeletal., taskalsoimprovesdrasticallyastheintruderalong\n",
      "2013) as the baseline. Please refer to Section 3.2 each dimension is a lot easier to identify owing\n",
      "to the fact that the top entities for each dimension deemedincorrectbytheclosedworldassumption.\n",
      "grouptogethermoreconspicuously.\n",
      "4.3.3 TripleClassification\n",
      "Method LinkPrediction\n",
      "In this experiment, we test the model on classify-\n",
      "MRR MR Hits@10(%)\n",
      "Baseline 31.6±0.08 121.9±1.80 48.3±0.39 ing correct and incorrect triples. For finding in-\n",
      "Proposed 30.4±0.08 111.9±1.12 46.8±0.08\n",
      "correct triples, we corrupt the object entity with\n",
      "TripleClassification\n",
      "AUC(%) Accuracy(%) a randomly selected entity within the same cate-\n",
      "Baseline 72.9±0.16 63.2±0.50\n",
      "gory. For classification, we use validation data to\n",
      "Proposed 73.2±0.28 67.6±0.17\n",
      "Interpretability findthebestthresholdforeachrelationbytraining\n",
      "AutoWI@5(%) Coherence@5 ManualWI(%)\n",
      "an SVM classifier and later use this threshold for\n",
      "Baseline 6±4.14 −47.4±4.68 12\n",
      "Proposed 66±5.89 −12.5±4.48 84 classifying test triples. We report the mean accu-\n",
      "racyandmeanAUCoverallrelations.\n",
      "Table 1: Results on test data. The pro-\n",
      "We observe that the proposed method achieves\n",
      "posedmethodsignificantlyimprovesinterpretabil-\n",
      "slightlybetterperformancefortripleclassification\n",
      "itywhilemaintaining comparableperformance on\n",
      "improving the accuracy by 4.4. The PMI infor-\n",
      "KGtasks(Section4.3).\n",
      "mation adds more evidence to the correct triples\n",
      "which are related in text data, generating a better\n",
      "threshold that more accurately distinguishes cor-\n",
      "Top5\n",
      "Baseline rectandincorrecttriples.\n",
      "-Jurist,Pipeorgan,USA,LionsGateEntertainment,UK\n",
      "-Guitar,71stAcademyAwards,Jurist,Piano,Bassguitar\n",
      "-Actor,OfficialWebsite,Screenwriter,FilmProducer,USA 4.4 QualitativeAnalysisofResults\n",
      "-Jurist,USA,Marriage,Male,UK\n",
      "-Pipeorgan,OfficialWebsite,Actor,FilmProducer,Screenwriter Since our aim is to induce interpretability in rep-\n",
      "ProposedMethod resentations, in this section, we evaluate the em-\n",
      "-JurisDoctor,BusinessAdministration,Biology,Psychology,BS\n",
      "beddings learned by the baseline as well as the\n",
      "-BachelorofArts,PhD,Bachelor’sdegree,BS,MS\n",
      "-EuropeanUnion,Europe,Netherlands,Portugal,Government proposed method. For both methods, we select\n",
      "-UK,Hollywood,DVD,London,Europe\n",
      "some dimensions randomly and present top 5 en-\n",
      "-Hollywood,AcademyAwards,USA,DVD,LosAngeles\n",
      "titiesalong those dimensions. Theresults arepre-\n",
      "Table 2: Top 5 and bottom most entities for ran- sentedinTable2.\n",
      "domly selected dimensions. As we see, the pro-\n",
      "As we can see from the results, the proposed\n",
      "posed method produces more coherent entities\n",
      "method produces more coherent entities than the\n",
      "compared to the baseline. Incoherent entities are\n",
      "baselinemethod.\n",
      "markedinboldface. 2\n",
      "5 Conclusionand Future Works\n",
      "4.3.2 LinkPrediction\n",
      "In this work, we proposed a method for induc-\n",
      "In this experiment, we test the model’s ability to inginterpretability inKGembeddings using aco-\n",
      "predict the best object entity for a given subject herence regularization term. We evaluated the\n",
      "entity and relation. For each of the triples, we fix proposed and the baseline method on the inter-\n",
      "the subject and the relation and rank all entities pretability of the learned embeddings. We also\n",
      "(within samecategory astrue object entity) based evaluated the methods on different KG tasks and\n",
      "on their score according to Equation 4. We re- compared their performance. We found that the\n",
      "port Mean Rank (MR)and Mean Reciprocal rank proposed method achieves better interpretability\n",
      "(MRR)ofthe true object entity and Hits@10 (the while maintaining comparable performance on\n",
      "numberoftimestrueobjectentityisrankedintop KG tasks. As next steps, we plan to evaluate the\n",
      "10)aspercentage. generalizability of the method with more recent\n",
      "The objective of the coherence regularization KGembeddings.\n",
      "term being tangential to that of the original loss\n",
      "function, isnotexpected toaffect performance on\n",
      "thelinkpredictiontask. However,theresultsshow 2We have used abbreviations for BS (Bachelor of Sci-\n",
      "ence), MS (Master of Science), UK (United Kingdom) and\n",
      "a trivial drop of 1.2 in MRR as the coherence\n",
      "USA(UnitedStatesofAmerica).Theyappearasfullformin\n",
      "term gives credibility to triples that are otherwise thedata.\n",
      "References Han Xiao, Minlie Huang, Yu Hao, and Xiaoyan\n",
      "Zhu. 2015a. Transa: An adaptive approach\n",
      "Antoine Bordes, Jason Weston, Ronan Collobert, and\n",
      "for knowledge graph embedding. arXiv preprint\n",
      "Yoshua Bengio. 2011. Learningstructured embed-\n",
      "arXiv:1509.05490.\n",
      "dingsofknowledgebases. InConferenceonArtifi-\n",
      "cialIntelligence.EPFL-CONF-192344. Han Xiao, Minlie Huang, Yu Hao, and Xiaoyan\n",
      "Zhu. 2015b. Transg: A generative mixture model\n",
      "JonathanChang,JordanLBoyd-Graber,SeanGerrish,\n",
      "for knowledge graph embedding. arXiv preprint\n",
      "Chong Wang, and David M Blei. 2009. Reading\n",
      "arXiv:1509.05488.\n",
      "tea leaves: How humansinterprettopic models. In\n",
      "Nips.volume31,pages1–9. Han Xiao, Minlie Huang, and Xiaoyan Zhu. 2016.\n",
      "Knowledge semantic representation: A generative\n",
      "Miao Fan, Qiang Zhou, Emily Chang, and\n",
      "model for interpretable knowledge graph embed-\n",
      "Thomas Fang Zheng. 2014. Transition-based\n",
      "ding. arXivpreprintarXiv:1608.07685.\n",
      "knowledge graph embedding with relational\n",
      "mappingproperties. InPACLIC.pages328–337.\n",
      "GuoliangJi,ShizhuHe,LihengXu,KangLiu,andJun\n",
      "Zhao. 2015. Knowledge graph embedding via dy-\n",
      "namicmappingmatrix. InACL(1).pages687–696.\n",
      "Jey Han Lau, David Newman, and Timothy Baldwin.\n",
      "2014. Machine reading tea leaves: Automatically\n",
      "evaluatingtopiccoherenceandtopicmodelquality.\n",
      "InEACL.pages530–539.\n",
      "YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,and\n",
      "Xuan Zhu. 2015. Learning entity and relation em-\n",
      "beddingsforknowledgegraphcompletion. InAAAI.\n",
      "pages2181–2187.\n",
      "Brian Murphy, Partha Pratim Talukdar, and Tom\n",
      "Mitchell.2012. Learningeffectiveandinterpretable\n",
      "semantic modelsusing non-negativesparse embed-\n",
      "ding. In International Conference on Computa-\n",
      "tionalLinguistics(COLING 2012),Mumbai, India.\n",
      "http://aclweb.org/anthology/C/C12/C12-1118.pdf.\n",
      "SebastianRiedel, LiminYao,AndrewMcCallum,and\n",
      "BenjaminMMarlin.2013. Relationextractionwith\n",
      "matrixfactorizationanduniversalschemas. NAACL\n",
      "HLT2013pages74–84.\n",
      "RichardSocher,DanqiChen,ChristopherDManning,\n",
      "andAndrewNg. 2013. Reasoning with neuralten-\n",
      "sornetworksforknowledgebasecompletion. InAd-\n",
      "vances in Neural Information Processing Systems.\n",
      "pages926–934.\n",
      "Kristina Toutanovaand Danqi Chen. 2015. Observed\n",
      "versus latent features for knowledge base and text\n",
      "inference. In 3rd Workshop on Continuous Vec-\n",
      "torSpaceModelsandTheirCompositionality.ACL\n",
      "AssociationforComputationalLinguistics.\n",
      "Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-\n",
      "fungPoon,PallaviChoudhury,andMichaelGamon.\n",
      "2015. Representing Text for Joint Embedding of\n",
      "Text and KnowledgeBases. In Empirical Methods\n",
      "in Natural Language Processing (EMNLP). ACL\n",
      "AssociationforComputationalLinguistics.\n",
      "Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\n",
      "Chen.2014. Knowledgegraphembeddingbytrans-\n",
      "lating on hyperplanes. In AAAI. Citeseer, pages\n",
      "1112–1119.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  35223,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Chandrahas', 'Tathagata Sengupta', 'Cibi Pragadeesh', 'Partha Pratim Talukdar']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: 7102\n",
      "tcO\n",
      "03\n",
      "]LM.tats[\n",
      "1v18801.0171:viXra\n",
      "Fast Linear Model for Knowledge Graph Embeddings\n",
      "ArmandJoulin EdouardGrave\n",
      "FacebookAIResearch FacebookAIResearch\n",
      "ajoulin@fb.com egrave@fb.com\n",
      "PiotrBojanowski MaximilianNickel TomasMikolov\n",
      "FacebookAIResearch FacebookAIResearch FacebookAIResearch\n",
      "bojanowski@fb.com maxn@fb.com tmikolov@fb.com\n",
      "Abstract\n",
      "This paper shows that a simple baseline based on a Bag-of-Words (BoW) rep-\n",
      "resentation learns surprisingly good knowledge graph embeddings. By casting\n",
      "knowledgebase completion and question answering as supervised classification\n",
      "problems,weobservethatmodelingco-occurencesofentitiesandrelationsleads\n",
      "to state-of-the-art performance with a training time of a few minutes using the\n",
      "opensourcedlibraryfastText1.\n",
      "1 Introduction\n",
      "Learning representations for Knowledge bases (KBs) such as Freebase (Bollackeretal., 2008)\n",
      "has become a core problem in machine learning, with a large variety of applications from ques-\n",
      "tion answering (YaoandVanDurme, 2014) to image classification (Dengetal., 2014). Many\n",
      "approaches have been proposed to learn these representations, or embeddings, with either sin-\n",
      "gle relational (Hoffetal., 2002; Perozzietal., 2014) or multi-relational data (Nickeletal., 2011;\n",
      "Bordesetal.,2013).\n",
      "These approacheslearn graph embeddingsby modelingthe relation between the differententities\n",
      "in the graphs(Perozzietal., 2014; Nickeletal., 2016), Instead, we framethis problemin a multi-\n",
      "class multilabelclassification problem and model only the co-occurencesof entities and relations\n",
      "withalinearclassifierbasedonaBag-of-Words(BoW)representationandstandardcostfunctions.\n",
      "In practice, this approachworkssurprisinglywell on a variety of standarddatasets, obtainingper-\n",
      "formancecompetitivewith the state-of-the-artapproacheswhile using a standardtext library (i.e.,\n",
      "fastText)andrunninginafewminutes(Joulinetal.,2017).\n",
      "We focus our study on two standard approaches to learn representations for KBs: knowledge\n",
      "base completion and question answering. For KB completion, our conclusions extend those\n",
      "ofKadlecetal.(2017),thatsimplemodelslikeTransE(Bordesetal.,2013)workaswell,ifnotbet-\n",
      "terthanmoresophisticatedones, iftunedproperly. Kadlecetal. (2017)focusona bilinearmodel\n",
      "designed for KB completion, DistMul (Yangetal., 2014), that still takes a few hours to train on\n",
      "a high-endGPU. We show thatsimilar performancecan be achievedwith a linear classifier and a\n",
      "training time reduced to a few minutes. For question answering, we consider datasets where we\n",
      "have guarantees that the question answer pairs are covered by the graph in one hop to indirectly\n",
      "learngraphembeddings(Bordesetal., 2015;Milleretal.,2016). FollowingBordesetal.(2014a),\n",
      "wepredicttherelationbetweentheentitiesappearinginthequestionandanswerpairstolearnem-\n",
      "beddings of the graph edges. The embeddingsof the entities, or nodes, are indirectly learned by\n",
      "embeddingthe questions. Inthis setting, we achievecompetitiveperformanceaslongas we have\n",
      "accesstoacleanKBrelatedtothequestionansweringtask.\n",
      "1Codeavailableathttps://github.com/facebookresearch/fastText\n",
      "Method WN18 FB15k\n",
      "raw filtered raw filtered\n",
      "TransE(Bordesetal.,2013) 75.4 89.2 34.9 47.1\n",
      "Rescal(Nickeletal.,2012) - 92.8 - 58.7\n",
      "Fast-TransR(Linetal.,2015) 81.0 94.6 48.8 69.8\n",
      "HolE(Nickeletal.,2016) - 94.9 - 73.9\n",
      "TransE++(Nickeletal.,2016) - 94.3 - 74.9\n",
      "Fast-TransD(Linetal.,2015) 78.5 91.9 49.9 75.2\n",
      "ReverseModel(Dettmersetal.,2017) - 96.9 - 78.6\n",
      "HolE+Neg-LL(TrouillonandNickel,2017) - 94.7 - 82.5\n",
      "Complex(Trouillonetal.,2017) - 94.7 - 84.0\n",
      "R-GCN(Schlichtkrulletal.,2017) - 96.4 - 84.2\n",
      "ConvE(Dettmersetal.,2017) - 95.5 - 87.3\n",
      "DistMul(Kadlecetal.,2017) - 94.6 - 89.3\n",
      "EnsembleDistMul(Kadlecetal.,2017) - 95.0 - 90.4\n",
      "IRN(Shenetal.,2016) - 95.3 - 92.7\n",
      "fastText-train 80.6 94.9 52.3 86.5\n",
      "fastText-train+valid 83.2 97.6 53.4 89.9\n",
      "Table1:RawandfilteredHit@10onWN18andFB15k.Allthenumbersaretakenfromtheirpaper.\n",
      "Above, methods that should achieve better performancewith a finer hyper-parametergrid, below,\n",
      "methodsthatwereproperlytuned.Higherthebetter.\n",
      "2 Approach\n",
      "2.1 fastTextmodel\n",
      "Linearmodels(Joachims,1998)arepowerfulandefficientbaselinesfortextclassification.Inpartic-\n",
      "ular, the fastTextmodelproposedby Joulinetal. (2017) achievesstate-of-the-artperformance\n",
      "onmanydatasetsbycombiningseveralstandardtricks,suchaslowrankconstraints(Schutze,1992)\n",
      "andn-gramfeatures(WangandManning,2012). Thesameapproachcanbeappliedtoanyproblem\n",
      "wheretheinputisasetofdiscretetokens.Forexample,aKBiscomposedofentities(ornodes)and\n",
      "relations(oredges)thatcanberepresentedbyauniquediscretetoken.\n",
      "The model is composed of a matrix V which is used as a look-up table over the discrete tokens\n",
      "and a matrix W for the classifier. The representations of the discrete tokens are averaged into\n",
      "BoWrepresentation,whichisinturnfedtothelinearclassifier. Usingafunctionf tocomputethe\n",
      "probabilitydistributionovertheclasses,andN inputsetsfordiscretetoken(e.g.,sentences),leads\n",
      "tominimize:\n",
      "N\n",
      "1\n",
      "− Xy nlog(f(WVx n)),\n",
      "N\n",
      "n=1\n",
      "wherex isthenormalizedBoWofthen-thinputset,y thelabel.WhileBoWmodelsarememory\n",
      "n n\n",
      "inefficient,theirmemoryfootprintcanbesignificantlyreduced(Joulinetal.,2016a). Themodelis\n",
      "trainedasynchronouslyonmultipleCPUswithSGDandalinearlydecayinglearningrate.\n",
      "2.2 Lossfunctions\n",
      "We considertwolossfunctionsinourexperiments: thesoftmaxfunctionandaone-versus-allloss\n",
      "functionwithnegativesampling.\n",
      "Softmax. Given K classes, and a score s for each class k, the softmax function is defined as\n",
      "k\n",
      "f(s) = exp(s )/ K exp(s ).Thisfunctionrequiresthescoreofeveryclass,leadingtoacom-\n",
      "k k Pi=1 i\n",
      "plexityofO(Kh)wherehisthesizeoftheembeddings.Thisfunctionisoftenusedtocomputethe\n",
      "probabilitydistributionofafinitesetofdiscreteclasses.\n",
      "2\n",
      "one-versus-all loss. Computing the softmax function over a large number of classes is compu-\n",
      "tationally prohibitive. We replace it by an independent binary classifier per class, i.e., a set of\n",
      "one-versus-all losses. During training, for each positive example, we draw randomly k negative\n",
      "classes,andupdatethek+1classifiers. ThenumberkissignificantlysmallerthanK,reducingthe\n",
      "complexityfromO(Kh)to O(kh). Thislosshasbeenusedforwordembeddings(Mikolovetal.,\n",
      "2013;Bojanowskietal.,2017)aswellastoobjectclassification(Joulinetal.,2016b).\n",
      "2.3 Knowledgebasecompletion\n",
      "Aknowledgebaseisrepresentedasasetofsubject-relation-objecttriplets(e,r,p).Typically,theen-\n",
      "titypispredictedaccordingtothesubjecteandtherelationr. WiththenotationsofthefastText\n",
      "model described in Sec. 2.1, each entity e is associated with a vector v and each relation r with\n",
      "e\n",
      "a vector v of the same dimension h. The target entity p is also represented by a h dimensional\n",
      "r\n",
      "vectorw. Thescoringfunctions foratriplet(e,r,p)issimplythedotproductbetweentheBoW\n",
      "p p\n",
      "representationoftheinputpair(e,r)andthetarget:\n",
      "1\n",
      "s (e,r,p) = hv +v,w i. (1)\n",
      "p e r p\n",
      "2\n",
      "This scoring function does not define a relational model, it only captures co-occurence between\n",
      "entitiesandrelations. Additionally,itmakesnoassumptionaboutthedirectionoftherelation,i.e.,\n",
      "thesame relationembeddingis usedto predictbothendsofa triplet. To circumventthisproblem,\n",
      "weencodethedirectionintherelationembeddingbyassociatingarelationrwithtwoembeddings,\n",
      "onetopredictthesubjectandonetopredicttheobject.Whileourapproachsharesmanysimilarities\n",
      "withTransE(Bordesetal.,2013),itdiffersinseveralaspects: theyusearankingloss,theirscoring\n",
      "functionisanℓ2distance,andtheyhaveoneembeddingperentity.Similarly,ifthegoalistopredict\n",
      "1\n",
      "therelationbetweenapairofentities,ourscoringfunctions is: s (e,r,p) = hv +v,w i.As\n",
      "r r 2 e p r\n",
      "forentityprediction,wecircumventthesymmetrybetweensubjectandrelationbyassociatingeach\n",
      "entitywithtwoembeddings,oneiftheentityisthesubjectortheobjectofatriplet.\n",
      "2.4 Questionanswering\n",
      "Questionansweringproblemscanbeusedtolearngraphembeddingsifframedasedgeprediction\n",
      "problemsbetweenentitiesappearinginthequestionanswerpairs(Bordesetal.,2014a). Theques-\n",
      "tion is representedas a bag of words and the potentialrelationsare labels. An entity is indirectly\n",
      "representedbytheassociatedwordsinthequestion.\n",
      "Stringmatchingforentitylinking. ThequestionsandanswersarematchedtoentitiesintheKB\n",
      "withastringmatchingalgorithm(Bordesetal.,2014a),usingalook-uptablebetweenentitiesand\n",
      "their string representations. Everypair of questionand answer in the training set is thus matched\n",
      "toasetofpotentialpairsofentities. Severalentitiesareoftenmatchedtoaquestionandweusean\n",
      "ad-hoceuristictosortthem,i.e.,usingtheinverseoftheirfrequencyinthetrainingset,andthesize\n",
      "oftheirassociatedstringsincaseofties(toapproximatethefrequency).\n",
      "Relationpredictionforquestionanswering. Onceaquestion-answerpairisassociatedwithaset\n",
      "ofpairsofentities, candidaterelationsareextracted. FollowingBordesetal.(2014a),weconsider\n",
      "therelationsaslabelsandusefastTexttopredictthem. Attesttime,theanswertoaquestionis\n",
      "inferredbytakingthemostlikelyrelationandverifyifanyoftheentitiesmatchedtothequestion\n",
      "formsavalidpairintheKB.Ifnot,wemovetothenextmostlikelyrelationandreiteratetheprocess.\n",
      "3 Results\n",
      "3.1 Knowledgebasecompletion.\n",
      "Datasets. WeuseseveralstandardbenchmarksforKBcompletion:\n",
      "• TheWN18datasetisasubsetofWordNet,containing40,943entities,18relationtypes,and\n",
      "151,442triples. WordNetisa KBbuiltbygroupingsynonymwordsandprovideslexical\n",
      "relationshipsbetweenthem.\n",
      "3\n",
      "Method FB15k-237 Method SVO\n",
      "R-GCN(Schlichtkrulletal.,2017) 41.7 TransE(Garcia-Duranetal.,2015) 70.6\n",
      "DistMul(Yangetal.,2014) 41.9 SME(Bordesetal.,2014b) 77.0\n",
      "Complex(Trouillonetal.,2017) 41.9 LFM(Jenattonetal.,2012) 78.0\n",
      "ConvE(Dettmersetal.,2017) 45.8 TATEC(Garcia-Duranetal.,2015) 80.1\n",
      "fastText-train 44.8 fastText-train 79.8\n",
      "fastText-train+valid 45.8 fastText-train+valid 79.9\n",
      "Table 2: Filtered Hit@10 on FB15k-237. Table 3: Hit5% on SVO. The numbers are\n",
      "ThenumbersarefromDettmersetal.(2017). fromGarcia-Duranetal.(2015).\n",
      "• TheFB15kdatasetisasubsetofFreebase,containing14,951entities,1345relationtypes,\n",
      "and592,213triples. FreebaseisalargeKBcontaininggeneralfactsabouttheworld.\n",
      "• The FB15k-237 dataset that is a subset of FB15k with no reversible rela-\n",
      "tions (Toutanovaetal., 2015). It contains 237 relations and 14,541 entities, for a total\n",
      "of298,970triples.\n",
      "• The SVO dataset is a subset of subject-relation-object triplets extracted from Wikipedia\n",
      "articles,containing30,605entities,4,547relationtypesand1.3Mtriples.\n",
      "Experimental protocol. For WN18, FB15k and FB15k-237, the goal is to predictone end of a\n",
      "triple given the other end and the relation, e.g., the subject given the object and the relation. We\n",
      "report Hit@10, also known as Recall@10, on raw and filtered datasets. Raw means the standard\n",
      "recallmeasurewhilefilteredmeansthateveryrelationthatalreadyexistsintheKBarefirstremoved,\n",
      "eventhoseinthetestset. Thefilteredmeasureallowsadirectcomparisonofthetargetentitywith\n",
      "negativeones. OnSVO, thegoalis topredictthe relationgivena pairof entities. Themeasureis\n",
      "Hit@5%,i.e.,Hit@227for4,547relationtypes.\n",
      "Implementation details. For both WN18, FB15k and FB15k-237, we use a negative sampling\n",
      "approximationofthesoftmaxandselectthehyper-parametersbasedonthefilteredhits@10onthe\n",
      "validationset. OnWN18andFB15k,hegridofparametersusedis[10,25,50,100,150,200]forthe\n",
      "embeddingsize h, [100,150,200]forthe numberof epochsand[100,200,500]forthe numberof\n",
      "negativeexamples. SinceFB15k-237ismuchsmaller,we limitthenumberofepochsto [1,5,10].\n",
      "Theinitiallearningrateisfixedat0.2. OnWN18,thebestsetofhyper-parametersare100dimen-\n",
      "sions, 100 epochs and 500 negative samples. On FB15k, the selected hyper-parameters are 100\n",
      "dimensions,100epochsand100negativesamples.OnFB15k-237,thebestsetofhyper-parameters\n",
      "areahiddenof50,10epochsanda500negativesamples. ForSVO,thenumberofrelationstopre-\n",
      "dictisquitesmall, wethususeafullsoftmaxandselecthyper-parametersbasedonhit@5%. The\n",
      "gridofhyper-parametersis[10,25,50,100,150,200]fortheembeddingsizehand[1,2,3,4,5]for\n",
      "thenumberofepochs. Theinitiallearningrateisfixedat0.2. Foralltheseexperiments,wereport\n",
      "boththeperformanceonthemodeltrainonthetrain setandontheconcatenationofthe trainand\n",
      "validationset,runwiththesamehyper-parameters.\n",
      "Comparison. We compare our approach to several standard models in Table 1 on WN18 and\n",
      "FB15k. We report numbers from their original papers. Some of them are not using a fine grid\n",
      "of hyper-parameters, which partially explains the gap in performance. We separate these mod-\n",
      "els from more recent ones for fairer comparison. Despite its simplicity, our approach is compet-\n",
      "itive with dedicated pipelines both for raw and filtered measurements. This extends the findings\n",
      "of TrouillonandNickel (2017), i.e., the choice of loss function can have a significant impact on\n",
      "overallperformance. Table 2 extendsthis observation to a harder dataset, FB15k-237, where our\n",
      "BoWmodelcomparesfavorablywithexistingKBcompletionmodels.\n",
      "WealsoreportcomparisononrelationpredictiondatasetSVOinTable3. Ourapproachiscompet-\n",
      "itive with approachesusing bigram and high order information, like TATEC (Garcia-Duranetal.,\n",
      "2015). Note TATEC can be, theoretically, used for both relation and entity prediction, while our\n",
      "modelonlypredictsrelations.\n",
      "4\n",
      "Dataset WN18 FB15k SVO FB15k-237 SQ WikiMovies\n",
      "Time(sec.) 165 188 371 28 42 1\n",
      "Table4: TrainingtimeforfastTextusing20threadsonaIntelXeonCPUE5-2680v32.50GHz.\n",
      "Table4showtherunningtimeforafastTextimplementation.Itrunsinafewminutes,whichis\n",
      "comparablewithoptimizedpipelineslikeFast-TransDandFast-TransR(Linetal.,2015). Notethat\n",
      "similarrunningtimesshouldbeachievableforotherlinearmodelslikeTransE.\n",
      "3.2 Questionanswering.\n",
      "Datasets. Weconsidertwostandarddatasetswithasignificantamountofquestionanswerpairs.\n",
      "• SimpleQuestion consists of 108,442 question-answer pairs generated from Freebase. It\n",
      "comeswithasubsetofFreebasewith2Mtriplets.\n",
      "• WikiMovies consists of more than 100,000 questions about movies generated also from\n",
      "Freebase.ItcomeswithasubsetoftheKBassociatedwiththequestion-answerpairs.This\n",
      "datasetalsoprovideswithsettingswheredifferentpreprocessedversionsofWikipediaare\n",
      "consideredinsteadoftheKB.Thesesettingsarebeyondthescopeofthispaper.\n",
      "Implementation details. For both SimpleQuestion and MovieWiki, the number of relations are\n",
      "relativelysmall. We thususe a fullsoftmax. ForSimpleQuestion,the gridof hyper-parametersis\n",
      "[10,50,100,200]forthedimensionoftheembeddingsand[5,10,50,100]forthenumberofepochs.\n",
      "We use bigramsand an initial learningrate of 1. For MovieWiki, we fixed the embeddingsize to\n",
      "16sincethereareonly16relationsandthenumberofepochswasselectedonthevalidationsetin\n",
      "[1,5,10,50].Weuseaninitiallearningrateof.3.\n",
      "SimpleQuestion. Figure 5 compares this ap-\n",
      "Method SQ\n",
      "proachwiththestate-of-the-art.Welearnarela-\n",
      "tionclassifierwithfastTextin42sec. Using Randomguess(Bordesetal.,2015) 4.9\n",
      "a larger KB, i.e., FB5M, does not degrade the CFO(Daietal.,2016) 62.6\n",
      "performance,despitehavingmuchmoreirrele- MemNN(Bordesetal.,2015) 62.7\n",
      "vantentities. Ourapproachcomparesfavorably AMPCNN(Yinetal.,2016) 68.3\n",
      "well other with question answering systems. CharQA(GolubandHe,2016) 70.9\n",
      "Thissuggeststhatthelearnedembeddingscap- CFO+AP(Daietal.,2016) 75.7\n",
      "turesomeimportantinformationabouttheKB. AMPCNN+AP(Yinetal.,2016) 76.4\n",
      "Note,however,thattheperformanceisverysen-\n",
      "fastText-train 72.7\n",
      "sible to the quality of the entity linker and the\n",
      "fastText-train+valid 73.0\n",
      "ad-hocsortingofextractedsubjects. Typically,\n",
      "going from a random order to the one used in\n",
      "Table 5: Accuracy on the SimpleQuestions\n",
      "this paper gives a boost of up to 10% depend-\n",
      "dataset(Bordesetal.,2015).\n",
      "ingonthehyper-parameters.\n",
      "WikiMovies. Table 6 compares our models with several state-of-the-art pipelines. In the case\n",
      "wherethecleanKBisaccessible,ourmethodworksverywell.fastTextrunsin1sec.forrelation\n",
      "prediction. Notethatthisdatasetwasprimarilymadeforthecasewhereonlytextisavailable. This\n",
      "settinggoesbeyondthescopeofourmethod,whileamoregeneralapproachlikeKV-memNNstill\n",
      "worksreasonablywell(Milleretal.,2016).\n",
      "Method SE MemNN QASystem KV-MemNN fastText\n",
      "WikiMovies 54.4 78.5 93.5 93.9 95.9\n",
      "Table6:Testresult(%hits@1)ontheWikiMoviesdatasetwiththefullKB.Thenumbersaretaken\n",
      "formMilleretal.(2016). QASystemreferstoBordesetal.(2014a).\n",
      "5\n",
      "4 Conclusion\n",
      "In this paper, we show that linear models learn good embeddings from a KB by recasting graph\n",
      "related problems into supervised classification ones. The limitations of such approach are that it\n",
      "requires a clean KB and a task that uses direct information about local connectivity in the graph.\n",
      "Moreover,theobservationthatournon-relationalapproachprovidesstate-of-the-artperformanceon\n",
      "KBCbenchmarksraisesalsoimportantquestionsregardingtheevaluationoflink-predictionmodels\n",
      "andthedesignofbenchmarksforthistask.\n",
      "Acknowledgement. We thank Timothée Lacroix, Nicolas Usunier, Antoine Bordes and the rest\n",
      "ofFAIRfortheirprecioushelpandcomments. WealsowouldliketothankAdamFischandAlex\n",
      "MillerfortheirhelpregardingMovieWiki.\n",
      "References\n",
      "Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word\n",
      "vectorswithsubwordinformation.TransactionsoftheAssociationforComputationalLinguistics\n",
      "5:135–146.\n",
      "Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a\n",
      "collaborativelycreatedgraphdatabaseforstructuringhumanknowledge. InProceedingsofthe\n",
      "2008ACMSIGMODinternationalconferenceonManagementofdata.AcM,pages1247–1250.\n",
      "AntoineBordes,SumitChopra,andJasonWeston.2014a. Questionansweringwithsubgraphem-\n",
      "beddings. arXivpreprintarXiv:1406.3676.\n",
      "Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. 2014b. A semantic matching\n",
      "energyfunctionforlearningwithmulti-relationaldata. MachineLearning94(2):233–259.\n",
      "AntoineBordes,NicolasUsunier,SumitChopra,andJasonWeston.2015. Large-scalesimpleques-\n",
      "tionansweringwithmemorynetworks. arXivpreprintarXiv:1506.02075.\n",
      "Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.\n",
      "2013.Translatingembeddingsformodelingmulti-relationaldata. InAdvancesinneuralinforma-\n",
      "tionprocessingsystems.pages2787–2795.\n",
      "ZihangDai, LeiLi, andWei Xu.2016. Cfo: Conditionalfocusedneuralquestionansweringwith\n",
      "large-scaleknowledgebases. arXivpreprintarXiv:1606.01994.\n",
      "JiaDeng,NanDing,YangqingJia,AndreaFrome,KevinMurphy,SamyBengio,YuanLi,Hartmut\n",
      "Neven,andHartwigAdam.2014. Large-scaleobjectclassificationusinglabelrelationgraphs. In\n",
      "EuropeanConferenceonComputerVision.Springer,Cham,pages48–64.\n",
      "TimDettmers,PasqualeMinervini,PontusStenetorp,andSebastianRiedel.2017.Convolutional2d\n",
      "knowledgegraphembeddings. arXivpreprintarXiv:1707.01476.\n",
      "AlbertoGarcia-Duran,AntoineBordes, NicolasUsunier, andYvesGrandvalet.2015. Combining\n",
      "two and three-way embeddingsmodels for link prediction in knowledge bases. arXiv preprint\n",
      "arXiv:1506.00999.\n",
      "David Golub and Xiaodong He. 2016. Character-level question answering with attention. arXiv\n",
      "preprintarXiv:1604.00727.\n",
      "Peter D Hoff, Adrian E Raftery, and Mark S Handcock.2002. Latent space approachesto social\n",
      "networkanalysis. JournaloftheamericanStatisticalassociation97(460):1090–1098.\n",
      "RodolpheJenatton,NicolasLRoux,AntoineBordes,andGuillaumeRObozinski.2012. A latent\n",
      "factor model for highly multi-relational data. In Advances in Neural Information Processing\n",
      "Systems.pages3167–3175.\n",
      "ThorstenJoachims.1998. Text categorizationwith supportvectormachines: Learningwith many\n",
      "relevantfeatures. Springer.\n",
      "6\n",
      "Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hervé Jégou, and Tomas\n",
      "Mikolov. 2016a. Fasttext. zip: Compressing text classification models. arXiv preprint\n",
      "arXiv:1612.03651.\n",
      "Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2017. Bag of tricks for\n",
      "efficient text classification. In Proceedingsof the 15th Conference of the European Chapterof\n",
      "theAssociationforComputationalLinguistics: Volume2,ShortPapers.AssociationforCompu-\n",
      "tationalLinguistics,pages427–431.\n",
      "Armand Joulin, Laurens van der Maaten, Allan Jabri, and Nicolas Vasilache. 2016b. Learning\n",
      "visualfeaturesfromlargeweaklysuperviseddata. InEuropeanConferenceonComputerVision.\n",
      "SpringerInternationalPublishing,pages67–84.\n",
      "RudolfKadlec,OndrejBajgar,andJanKleindienst.2017. Knowledgebasecompletion: Baselines\n",
      "strikeback. arXivpreprintarXiv:1705.10744.\n",
      "YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,andXuanZhu.2015.Learningentityandrelation\n",
      "embeddingsforknowledgegraphcompletion. InAAAI.pages2181–2187.\n",
      "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word\n",
      "representationsinvectorspace. arXivpreprintarXiv:1301.3781.\n",
      "Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason\n",
      "Weston. 2016. Key-value memory networks for directly reading documents. arXiv preprint\n",
      "arXiv:1606.03126.\n",
      "Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. 2016. Holographic embeddings of\n",
      "knowledgegraphs. InThirtiethAAAIConferenceonArtificialIntelligence.\n",
      "MaximilianNickel,VolkerTresp,andHans-PeterKriegel.2011. Athree-waymodelforcollective\n",
      "learningonmulti-relationaldata. InProceedingsofthe28thinternationalconferenceonmachine\n",
      "learning(ICML-11).pages809–816.\n",
      "Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2012. Factorizing yago: Scalable ma-\n",
      "chine learning for linked data. In Proceedings of the 21st International Conference on World\n",
      "WideWeb.ACM,pages271–280.\n",
      "BryanPerozzi,RamiAl-Rfou,andStevenSkiena.2014. Deepwalk: Onlinelearningofsocialrep-\n",
      "resentations. InProceedingsofthe20thACMSIGKDDinternationalconferenceonKnowledge\n",
      "discoveryanddatamining.ACM,pages701–710.\n",
      "Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max\n",
      "Welling. 2017. Modeling relational data with graph convolutional networks. arXiv preprint\n",
      "arXiv:1703.06103.\n",
      "HinrichSchutze.1992. Dimensionsofmeaning. InSupercomputing.\n",
      "YelongShen, Po-SenHuang, Ming-WeiChang, andJianfengGao.2016. Implicitreasonet: Mod-\n",
      "elinglarge-scalestructuredrelationshipswithsharedmemory. arXivpreprintarXiv:1611.04642\n",
      ".\n",
      "KristinaToutanova,DanqiChen, andPatrickPantel.2015. Representingtextforjointembedding\n",
      "oftextandknowledgebases. InEMNLP.\n",
      "ThéoTrouillon,ChristopherR Dance, JohannesWelbl, Sebastian Riedel, Éric Gaussier, and Guil-\n",
      "laume Bouchard. 2017. Knowledge graph completion via complex tensor factorization. arXiv\n",
      "preprintarXiv:1702.06879.\n",
      "ThéoTrouillonandMaximilianNickel.2017. Complexandholographicembeddingsofknowledge\n",
      "graphs:acomparison. arXivpreprintarXiv:1707.01475.\n",
      "SidaWangandChristopherDManning.2012. Baselinesandbigrams:Simple,goodsentimentand\n",
      "topicclassification. InACL.\n",
      "7\n",
      "Bishan Yang, Wen-tau Yih, XiaodongHe, JianfengGao, and Li Deng. 2014. Embeddingentities\n",
      "andrelationsforlearningandinferenceinknowledgebases. arXivpreprintarXiv:1412.6575.\n",
      "XuchenYaoandBenjaminVanDurme.2014.Informationextractionoverstructureddata:Question\n",
      "answeringwithfreebase. InACL(1).pages956–966.\n",
      "Wenpeng Yin, Mo Yu, Bing Xiang, Bowen Zhou, and Hinrich Schütze. 2016. Simple question\n",
      "answeringbyattentiveconvolutionalneuralnetwork. arXivpreprintarXiv:1606.03391.\n",
      "8<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   7099,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['WN18', 'FB15k', 'FB15k-237', 'SVO', 'SimpleQuestion', 'WikiMovies', 'Freebase', 'WordNet']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: 7102\n",
      "tcO\n",
      "03\n",
      "]LM.tats[\n",
      "1v18801.0171:viXra\n",
      "Fast Linear Model for Knowledge Graph Embeddings\n",
      "ArmandJoulin EdouardGrave\n",
      "FacebookAIResearch FacebookAIResearch\n",
      "ajoulin@fb.com egrave@fb.com\n",
      "PiotrBojanowski MaximilianNickel TomasMikolov\n",
      "FacebookAIResearch FacebookAIResearch FacebookAIResearch\n",
      "bojanowski@fb.com maxn@fb.com tmikolov@fb.com\n",
      "Abstract\n",
      "This paper shows that a simple baseline based on a Bag-of-Words (BoW) rep-\n",
      "resentation learns surprisingly good knowledge graph embeddings. By casting\n",
      "knowledgebase completion and question answering as supervised classification\n",
      "problems,weobservethatmodelingco-occurencesofentitiesandrelationsleads\n",
      "to state-of-the-art performance with a training time of a few minutes using the\n",
      "opensourcedlibraryfastText1.\n",
      "1 Introduction\n",
      "Learning representations for Knowledge bases (KBs) such as Freebase (Bollackeretal., 2008)\n",
      "has become a core problem in machine learning, with a large variety of applications from ques-\n",
      "tion answering (YaoandVanDurme, 2014) to image classification (Dengetal., 2014). Many\n",
      "approaches have been proposed to learn these representations, or embeddings, with either sin-\n",
      "gle relational (Hoffetal., 2002; Perozzietal., 2014) or multi-relational data (Nickeletal., 2011;\n",
      "Bordesetal.,2013).\n",
      "These approacheslearn graph embeddingsby modelingthe relation between the differententities\n",
      "in the graphs(Perozzietal., 2014; Nickeletal., 2016), Instead, we framethis problemin a multi-\n",
      "class multilabelclassification problem and model only the co-occurencesof entities and relations\n",
      "withalinearclassifierbasedonaBag-of-Words(BoW)representationandstandardcostfunctions.\n",
      "In practice, this approachworkssurprisinglywell on a variety of standarddatasets, obtainingper-\n",
      "formancecompetitivewith the state-of-the-artapproacheswhile using a standardtext library (i.e.,\n",
      "fastText)andrunninginafewminutes(Joulinetal.,2017).\n",
      "We focus our study on two standard approaches to learn representations for KBs: knowledge\n",
      "base completion and question answering. For KB completion, our conclusions extend those\n",
      "ofKadlecetal.(2017),thatsimplemodelslikeTransE(Bordesetal.,2013)workaswell,ifnotbet-\n",
      "terthanmoresophisticatedones, iftunedproperly. Kadlecetal. (2017)focusona bilinearmodel\n",
      "designed for KB completion, DistMul (Yangetal., 2014), that still takes a few hours to train on\n",
      "a high-endGPU. We show thatsimilar performancecan be achievedwith a linear classifier and a\n",
      "training time reduced to a few minutes. For question answering, we consider datasets where we\n",
      "have guarantees that the question answer pairs are covered by the graph in one hop to indirectly\n",
      "learngraphembeddings(Bordesetal., 2015;Milleretal.,2016). FollowingBordesetal.(2014a),\n",
      "wepredicttherelationbetweentheentitiesappearinginthequestionandanswerpairstolearnem-\n",
      "beddings of the graph edges. The embeddingsof the entities, or nodes, are indirectly learned by\n",
      "embeddingthe questions. Inthis setting, we achievecompetitiveperformanceaslongas we have\n",
      "accesstoacleanKBrelatedtothequestionansweringtask.\n",
      "1Codeavailableathttps://github.com/facebookresearch/fastText\n",
      "Method WN18 FB15k\n",
      "raw filtered raw filtered\n",
      "TransE(Bordesetal.,2013) 75.4 89.2 34.9 47.1\n",
      "Rescal(Nickeletal.,2012) - 92.8 - 58.7\n",
      "Fast-TransR(Linetal.,2015) 81.0 94.6 48.8 69.8\n",
      "HolE(Nickeletal.,2016) - 94.9 - 73.9\n",
      "TransE++(Nickeletal.,2016) - 94.3 - 74.9\n",
      "Fast-TransD(Linetal.,2015) 78.5 91.9 49.9 75.2\n",
      "ReverseModel(Dettmersetal.,2017) - 96.9 - 78.6\n",
      "HolE+Neg-LL(TrouillonandNickel,2017) - 94.7 - 82.5\n",
      "Complex(Trouillonetal.,2017) - 94.7 - 84.0\n",
      "R-GCN(Schlichtkrulletal.,2017) - 96.4 - 84.2\n",
      "ConvE(Dettmersetal.,2017) - 95.5 - 87.3\n",
      "DistMul(Kadlecetal.,2017) - 94.6 - 89.3\n",
      "EnsembleDistMul(Kadlecetal.,2017) - 95.0 - 90.4\n",
      "IRN(Shenetal.,2016) - 95.3 - 92.7\n",
      "fastText-train 80.6 94.9 52.3 86.5\n",
      "fastText-train+valid 83.2 97.6 53.4 89.9\n",
      "Table1:RawandfilteredHit@10onWN18andFB15k.Allthenumbersaretakenfromtheirpaper.\n",
      "Above, methods that should achieve better performancewith a finer hyper-parametergrid, below,\n",
      "methodsthatwereproperlytuned.Higherthebetter.\n",
      "2 Approach\n",
      "2.1 fastTextmodel\n",
      "Linearmodels(Joachims,1998)arepowerfulandefficientbaselinesfortextclassification.Inpartic-\n",
      "ular, the fastTextmodelproposedby Joulinetal. (2017) achievesstate-of-the-artperformance\n",
      "onmanydatasetsbycombiningseveralstandardtricks,suchaslowrankconstraints(Schutze,1992)\n",
      "andn-gramfeatures(WangandManning,2012). Thesameapproachcanbeappliedtoanyproblem\n",
      "wheretheinputisasetofdiscretetokens.Forexample,aKBiscomposedofentities(ornodes)and\n",
      "relations(oredges)thatcanberepresentedbyauniquediscretetoken.\n",
      "The model is composed of a matrix V which is used as a look-up table over the discrete tokens\n",
      "and a matrix W for the classifier. The representations of the discrete tokens are averaged into\n",
      "BoWrepresentation,whichisinturnfedtothelinearclassifier. Usingafunctionf tocomputethe\n",
      "probabilitydistributionovertheclasses,andN inputsetsfordiscretetoken(e.g.,sentences),leads\n",
      "tominimize:\n",
      "N\n",
      "1\n",
      "− Xy nlog(f(WVx n)),\n",
      "N\n",
      "n=1\n",
      "wherex isthenormalizedBoWofthen-thinputset,y thelabel.WhileBoWmodelsarememory\n",
      "n n\n",
      "inefficient,theirmemoryfootprintcanbesignificantlyreduced(Joulinetal.,2016a). Themodelis\n",
      "trainedasynchronouslyonmultipleCPUswithSGDandalinearlydecayinglearningrate.\n",
      "2.2 Lossfunctions\n",
      "We considertwolossfunctionsinourexperiments: thesoftmaxfunctionandaone-versus-allloss\n",
      "functionwithnegativesampling.\n",
      "Softmax. Given K classes, and a score s for each class k, the softmax function is defined as\n",
      "k\n",
      "f(s) = exp(s )/ K exp(s ).Thisfunctionrequiresthescoreofeveryclass,leadingtoacom-\n",
      "k k Pi=1 i\n",
      "plexityofO(Kh)wherehisthesizeoftheembeddings.Thisfunctionisoftenusedtocomputethe\n",
      "probabilitydistributionofafinitesetofdiscreteclasses.\n",
      "2\n",
      "one-versus-all loss. Computing the softmax function over a large number of classes is compu-\n",
      "tationally prohibitive. We replace it by an independent binary classifier per class, i.e., a set of\n",
      "one-versus-all losses. During training, for each positive example, we draw randomly k negative\n",
      "classes,andupdatethek+1classifiers. ThenumberkissignificantlysmallerthanK,reducingthe\n",
      "complexityfromO(Kh)to O(kh). Thislosshasbeenusedforwordembeddings(Mikolovetal.,\n",
      "2013;Bojanowskietal.,2017)aswellastoobjectclassification(Joulinetal.,2016b).\n",
      "2.3 Knowledgebasecompletion\n",
      "Aknowledgebaseisrepresentedasasetofsubject-relation-objecttriplets(e,r,p).Typically,theen-\n",
      "titypispredictedaccordingtothesubjecteandtherelationr. WiththenotationsofthefastText\n",
      "model described in Sec. 2.1, each entity e is associated with a vector v and each relation r with\n",
      "e\n",
      "a vector v of the same dimension h. The target entity p is also represented by a h dimensional\n",
      "r\n",
      "vectorw. Thescoringfunctions foratriplet(e,r,p)issimplythedotproductbetweentheBoW\n",
      "p p\n",
      "representationoftheinputpair(e,r)andthetarget:\n",
      "1\n",
      "s (e,r,p) = hv +v,w i. (1)\n",
      "p e r p\n",
      "2\n",
      "This scoring function does not define a relational model, it only captures co-occurence between\n",
      "entitiesandrelations. Additionally,itmakesnoassumptionaboutthedirectionoftherelation,i.e.,\n",
      "thesame relationembeddingis usedto predictbothendsofa triplet. To circumventthisproblem,\n",
      "weencodethedirectionintherelationembeddingbyassociatingarelationrwithtwoembeddings,\n",
      "onetopredictthesubjectandonetopredicttheobject.Whileourapproachsharesmanysimilarities\n",
      "withTransE(Bordesetal.,2013),itdiffersinseveralaspects: theyusearankingloss,theirscoring\n",
      "functionisanℓ2distance,andtheyhaveoneembeddingperentity.Similarly,ifthegoalistopredict\n",
      "1\n",
      "therelationbetweenapairofentities,ourscoringfunctions is: s (e,r,p) = hv +v,w i.As\n",
      "r r 2 e p r\n",
      "forentityprediction,wecircumventthesymmetrybetweensubjectandrelationbyassociatingeach\n",
      "entitywithtwoembeddings,oneiftheentityisthesubjectortheobjectofatriplet.\n",
      "2.4 Questionanswering\n",
      "Questionansweringproblemscanbeusedtolearngraphembeddingsifframedasedgeprediction\n",
      "problemsbetweenentitiesappearinginthequestionanswerpairs(Bordesetal.,2014a). Theques-\n",
      "tion is representedas a bag of words and the potentialrelationsare labels. An entity is indirectly\n",
      "representedbytheassociatedwordsinthequestion.\n",
      "Stringmatchingforentitylinking. ThequestionsandanswersarematchedtoentitiesintheKB\n",
      "withastringmatchingalgorithm(Bordesetal.,2014a),usingalook-uptablebetweenentitiesand\n",
      "their string representations. Everypair of questionand answer in the training set is thus matched\n",
      "toasetofpotentialpairsofentities. Severalentitiesareoftenmatchedtoaquestionandweusean\n",
      "ad-hoceuristictosortthem,i.e.,usingtheinverseoftheirfrequencyinthetrainingset,andthesize\n",
      "oftheirassociatedstringsincaseofties(toapproximatethefrequency).\n",
      "Relationpredictionforquestionanswering. Onceaquestion-answerpairisassociatedwithaset\n",
      "ofpairsofentities, candidaterelationsareextracted. FollowingBordesetal.(2014a),weconsider\n",
      "therelationsaslabelsandusefastTexttopredictthem. Attesttime,theanswertoaquestionis\n",
      "inferredbytakingthemostlikelyrelationandverifyifanyoftheentitiesmatchedtothequestion\n",
      "formsavalidpairintheKB.Ifnot,wemovetothenextmostlikelyrelationandreiteratetheprocess.\n",
      "3 Results\n",
      "3.1 Knowledgebasecompletion.\n",
      "Datasets. WeuseseveralstandardbenchmarksforKBcompletion:\n",
      "• TheWN18datasetisasubsetofWordNet,containing40,943entities,18relationtypes,and\n",
      "151,442triples. WordNetisa KBbuiltbygroupingsynonymwordsandprovideslexical\n",
      "relationshipsbetweenthem.\n",
      "3\n",
      "Method FB15k-237 Method SVO\n",
      "R-GCN(Schlichtkrulletal.,2017) 41.7 TransE(Garcia-Duranetal.,2015) 70.6\n",
      "DistMul(Yangetal.,2014) 41.9 SME(Bordesetal.,2014b) 77.0\n",
      "Complex(Trouillonetal.,2017) 41.9 LFM(Jenattonetal.,2012) 78.0\n",
      "ConvE(Dettmersetal.,2017) 45.8 TATEC(Garcia-Duranetal.,2015) 80.1\n",
      "fastText-train 44.8 fastText-train 79.8\n",
      "fastText-train+valid 45.8 fastText-train+valid 79.9\n",
      "Table 2: Filtered Hit@10 on FB15k-237. Table 3: Hit5% on SVO. The numbers are\n",
      "ThenumbersarefromDettmersetal.(2017). fromGarcia-Duranetal.(2015).\n",
      "• TheFB15kdatasetisasubsetofFreebase,containing14,951entities,1345relationtypes,\n",
      "and592,213triples. FreebaseisalargeKBcontaininggeneralfactsabouttheworld.\n",
      "• The FB15k-237 dataset that is a subset of FB15k with no reversible rela-\n",
      "tions (Toutanovaetal., 2015). It contains 237 relations and 14,541 entities, for a total\n",
      "of298,970triples.\n",
      "• The SVO dataset is a subset of subject-relation-object triplets extracted from Wikipedia\n",
      "articles,containing30,605entities,4,547relationtypesand1.3Mtriples.\n",
      "Experimental protocol. For WN18, FB15k and FB15k-237, the goal is to predictone end of a\n",
      "triple given the other end and the relation, e.g., the subject given the object and the relation. We\n",
      "report Hit@10, also known as Recall@10, on raw and filtered datasets. Raw means the standard\n",
      "recallmeasurewhilefilteredmeansthateveryrelationthatalreadyexistsintheKBarefirstremoved,\n",
      "eventhoseinthetestset. Thefilteredmeasureallowsadirectcomparisonofthetargetentitywith\n",
      "negativeones. OnSVO, thegoalis topredictthe relationgivena pairof entities. Themeasureis\n",
      "Hit@5%,i.e.,Hit@227for4,547relationtypes.\n",
      "Implementation details. For both WN18, FB15k and FB15k-237, we use a negative sampling\n",
      "approximationofthesoftmaxandselectthehyper-parametersbasedonthefilteredhits@10onthe\n",
      "validationset. OnWN18andFB15k,hegridofparametersusedis[10,25,50,100,150,200]forthe\n",
      "embeddingsize h, [100,150,200]forthe numberof epochsand[100,200,500]forthe numberof\n",
      "negativeexamples. SinceFB15k-237ismuchsmaller,we limitthenumberofepochsto [1,5,10].\n",
      "Theinitiallearningrateisfixedat0.2. OnWN18,thebestsetofhyper-parametersare100dimen-\n",
      "sions, 100 epochs and 500 negative samples. On FB15k, the selected hyper-parameters are 100\n",
      "dimensions,100epochsand100negativesamples.OnFB15k-237,thebestsetofhyper-parameters\n",
      "areahiddenof50,10epochsanda500negativesamples. ForSVO,thenumberofrelationstopre-\n",
      "dictisquitesmall, wethususeafullsoftmaxandselecthyper-parametersbasedonhit@5%. The\n",
      "gridofhyper-parametersis[10,25,50,100,150,200]fortheembeddingsizehand[1,2,3,4,5]for\n",
      "thenumberofepochs. Theinitiallearningrateisfixedat0.2. Foralltheseexperiments,wereport\n",
      "boththeperformanceonthemodeltrainonthetrain setandontheconcatenationofthe trainand\n",
      "validationset,runwiththesamehyper-parameters.\n",
      "Comparison. We compare our approach to several standard models in Table 1 on WN18 and\n",
      "FB15k. We report numbers from their original papers. Some of them are not using a fine grid\n",
      "of hyper-parameters, which partially explains the gap in performance. We separate these mod-\n",
      "els from more recent ones for fairer comparison. Despite its simplicity, our approach is compet-\n",
      "itive with dedicated pipelines both for raw and filtered measurements. This extends the findings\n",
      "of TrouillonandNickel (2017), i.e., the choice of loss function can have a significant impact on\n",
      "overallperformance. Table 2 extendsthis observation to a harder dataset, FB15k-237, where our\n",
      "BoWmodelcomparesfavorablywithexistingKBcompletionmodels.\n",
      "WealsoreportcomparisononrelationpredictiondatasetSVOinTable3. Ourapproachiscompet-\n",
      "itive with approachesusing bigram and high order information, like TATEC (Garcia-Duranetal.,\n",
      "2015). Note TATEC can be, theoretically, used for both relation and entity prediction, while our\n",
      "modelonlypredictsrelations.\n",
      "4\n",
      "Dataset WN18 FB15k SVO FB15k-237 SQ WikiMovies\n",
      "Time(sec.) 165 188 371 28 42 1\n",
      "Table4: TrainingtimeforfastTextusing20threadsonaIntelXeonCPUE5-2680v32.50GHz.\n",
      "Table4showtherunningtimeforafastTextimplementation.Itrunsinafewminutes,whichis\n",
      "comparablewithoptimizedpipelineslikeFast-TransDandFast-TransR(Linetal.,2015). Notethat\n",
      "similarrunningtimesshouldbeachievableforotherlinearmodelslikeTransE.\n",
      "3.2 Questionanswering.\n",
      "Datasets. Weconsidertwostandarddatasetswithasignificantamountofquestionanswerpairs.\n",
      "• SimpleQuestion consists of 108,442 question-answer pairs generated from Freebase. It\n",
      "comeswithasubsetofFreebasewith2Mtriplets.\n",
      "• WikiMovies consists of more than 100,000 questions about movies generated also from\n",
      "Freebase.ItcomeswithasubsetoftheKBassociatedwiththequestion-answerpairs.This\n",
      "datasetalsoprovideswithsettingswheredifferentpreprocessedversionsofWikipediaare\n",
      "consideredinsteadoftheKB.Thesesettingsarebeyondthescopeofthispaper.\n",
      "Implementation details. For both SimpleQuestion and MovieWiki, the number of relations are\n",
      "relativelysmall. We thususe a fullsoftmax. ForSimpleQuestion,the gridof hyper-parametersis\n",
      "[10,50,100,200]forthedimensionoftheembeddingsand[5,10,50,100]forthenumberofepochs.\n",
      "We use bigramsand an initial learningrate of 1. For MovieWiki, we fixed the embeddingsize to\n",
      "16sincethereareonly16relationsandthenumberofepochswasselectedonthevalidationsetin\n",
      "[1,5,10,50].Weuseaninitiallearningrateof.3.\n",
      "SimpleQuestion. Figure 5 compares this ap-\n",
      "Method SQ\n",
      "proachwiththestate-of-the-art.Welearnarela-\n",
      "tionclassifierwithfastTextin42sec. Using Randomguess(Bordesetal.,2015) 4.9\n",
      "a larger KB, i.e., FB5M, does not degrade the CFO(Daietal.,2016) 62.6\n",
      "performance,despitehavingmuchmoreirrele- MemNN(Bordesetal.,2015) 62.7\n",
      "vantentities. Ourapproachcomparesfavorably AMPCNN(Yinetal.,2016) 68.3\n",
      "well other with question answering systems. CharQA(GolubandHe,2016) 70.9\n",
      "Thissuggeststhatthelearnedembeddingscap- CFO+AP(Daietal.,2016) 75.7\n",
      "turesomeimportantinformationabouttheKB. AMPCNN+AP(Yinetal.,2016) 76.4\n",
      "Note,however,thattheperformanceisverysen-\n",
      "fastText-train 72.7\n",
      "sible to the quality of the entity linker and the\n",
      "fastText-train+valid 73.0\n",
      "ad-hocsortingofextractedsubjects. Typically,\n",
      "going from a random order to the one used in\n",
      "Table 5: Accuracy on the SimpleQuestions\n",
      "this paper gives a boost of up to 10% depend-\n",
      "dataset(Bordesetal.,2015).\n",
      "ingonthehyper-parameters.\n",
      "WikiMovies. Table 6 compares our models with several state-of-the-art pipelines. In the case\n",
      "wherethecleanKBisaccessible,ourmethodworksverywell.fastTextrunsin1sec.forrelation\n",
      "prediction. Notethatthisdatasetwasprimarilymadeforthecasewhereonlytextisavailable. This\n",
      "settinggoesbeyondthescopeofourmethod,whileamoregeneralapproachlikeKV-memNNstill\n",
      "worksreasonablywell(Milleretal.,2016).\n",
      "Method SE MemNN QASystem KV-MemNN fastText\n",
      "WikiMovies 54.4 78.5 93.5 93.9 95.9\n",
      "Table6:Testresult(%hits@1)ontheWikiMoviesdatasetwiththefullKB.Thenumbersaretaken\n",
      "formMilleretal.(2016). QASystemreferstoBordesetal.(2014a).\n",
      "5\n",
      "4 Conclusion\n",
      "In this paper, we show that linear models learn good embeddings from a KB by recasting graph\n",
      "related problems into supervised classification ones. The limitations of such approach are that it\n",
      "requires a clean KB and a task that uses direct information about local connectivity in the graph.\n",
      "Moreover,theobservationthatournon-relationalapproachprovidesstate-of-the-artperformanceon\n",
      "KBCbenchmarksraisesalsoimportantquestionsregardingtheevaluationoflink-predictionmodels\n",
      "andthedesignofbenchmarksforthistask.\n",
      "Acknowledgement. We thank Timothée Lacroix, Nicolas Usunier, Antoine Bordes and the rest\n",
      "ofFAIRfortheirprecioushelpandcomments. WealsowouldliketothankAdamFischandAlex\n",
      "MillerfortheirhelpregardingMovieWiki.\n",
      "References\n",
      "Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word\n",
      "vectorswithsubwordinformation.TransactionsoftheAssociationforComputationalLinguistics\n",
      "5:135–146.\n",
      "Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a\n",
      "collaborativelycreatedgraphdatabaseforstructuringhumanknowledge. InProceedingsofthe\n",
      "2008ACMSIGMODinternationalconferenceonManagementofdata.AcM,pages1247–1250.\n",
      "AntoineBordes,SumitChopra,andJasonWeston.2014a. Questionansweringwithsubgraphem-\n",
      "beddings. arXivpreprintarXiv:1406.3676.\n",
      "Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. 2014b. A semantic matching\n",
      "energyfunctionforlearningwithmulti-relationaldata. MachineLearning94(2):233–259.\n",
      "AntoineBordes,NicolasUsunier,SumitChopra,andJasonWeston.2015. Large-scalesimpleques-\n",
      "tionansweringwithmemorynetworks. arXivpreprintarXiv:1506.02075.\n",
      "Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.\n",
      "2013.Translatingembeddingsformodelingmulti-relationaldata. InAdvancesinneuralinforma-\n",
      "tionprocessingsystems.pages2787–2795.\n",
      "ZihangDai, LeiLi, andWei Xu.2016. Cfo: Conditionalfocusedneuralquestionansweringwith\n",
      "large-scaleknowledgebases. arXivpreprintarXiv:1606.01994.\n",
      "JiaDeng,NanDing,YangqingJia,AndreaFrome,KevinMurphy,SamyBengio,YuanLi,Hartmut\n",
      "Neven,andHartwigAdam.2014. Large-scaleobjectclassificationusinglabelrelationgraphs. In\n",
      "EuropeanConferenceonComputerVision.Springer,Cham,pages48–64.\n",
      "TimDettmers,PasqualeMinervini,PontusStenetorp,andSebastianRiedel.2017.Convolutional2d\n",
      "knowledgegraphembeddings. arXivpreprintarXiv:1707.01476.\n",
      "AlbertoGarcia-Duran,AntoineBordes, NicolasUsunier, andYvesGrandvalet.2015. Combining\n",
      "two and three-way embeddingsmodels for link prediction in knowledge bases. arXiv preprint\n",
      "arXiv:1506.00999.\n",
      "David Golub and Xiaodong He. 2016. Character-level question answering with attention. arXiv\n",
      "preprintarXiv:1604.00727.\n",
      "Peter D Hoff, Adrian E Raftery, and Mark S Handcock.2002. Latent space approachesto social\n",
      "networkanalysis. JournaloftheamericanStatisticalassociation97(460):1090–1098.\n",
      "RodolpheJenatton,NicolasLRoux,AntoineBordes,andGuillaumeRObozinski.2012. A latent\n",
      "factor model for highly multi-relational data. In Advances in Neural Information Processing\n",
      "Systems.pages3167–3175.\n",
      "ThorstenJoachims.1998. Text categorizationwith supportvectormachines: Learningwith many\n",
      "relevantfeatures. Springer.\n",
      "6\n",
      "Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hervé Jégou, and Tomas\n",
      "Mikolov. 2016a. Fasttext. zip: Compressing text classification models. arXiv preprint\n",
      "arXiv:1612.03651.\n",
      "Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2017. Bag of tricks for\n",
      "efficient text classification. In Proceedingsof the 15th Conference of the European Chapterof\n",
      "theAssociationforComputationalLinguistics: Volume2,ShortPapers.AssociationforCompu-\n",
      "tationalLinguistics,pages427–431.\n",
      "Armand Joulin, Laurens van der Maaten, Allan Jabri, and Nicolas Vasilache. 2016b. Learning\n",
      "visualfeaturesfromlargeweaklysuperviseddata. InEuropeanConferenceonComputerVision.\n",
      "SpringerInternationalPublishing,pages67–84.\n",
      "RudolfKadlec,OndrejBajgar,andJanKleindienst.2017. Knowledgebasecompletion: Baselines\n",
      "strikeback. arXivpreprintarXiv:1705.10744.\n",
      "YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,andXuanZhu.2015.Learningentityandrelation\n",
      "embeddingsforknowledgegraphcompletion. InAAAI.pages2181–2187.\n",
      "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word\n",
      "representationsinvectorspace. arXivpreprintarXiv:1301.3781.\n",
      "Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason\n",
      "Weston. 2016. Key-value memory networks for directly reading documents. arXiv preprint\n",
      "arXiv:1606.03126.\n",
      "Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. 2016. Holographic embeddings of\n",
      "knowledgegraphs. InThirtiethAAAIConferenceonArtificialIntelligence.\n",
      "MaximilianNickel,VolkerTresp,andHans-PeterKriegel.2011. Athree-waymodelforcollective\n",
      "learningonmulti-relationaldata. InProceedingsofthe28thinternationalconferenceonmachine\n",
      "learning(ICML-11).pages809–816.\n",
      "Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2012. Factorizing yago: Scalable ma-\n",
      "chine learning for linked data. In Proceedings of the 21st International Conference on World\n",
      "WideWeb.ACM,pages271–280.\n",
      "BryanPerozzi,RamiAl-Rfou,andStevenSkiena.2014. Deepwalk: Onlinelearningofsocialrep-\n",
      "resentations. InProceedingsofthe20thACMSIGKDDinternationalconferenceonKnowledge\n",
      "discoveryanddatamining.ACM,pages701–710.\n",
      "Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max\n",
      "Welling. 2017. Modeling relational data with graph convolutional networks. arXiv preprint\n",
      "arXiv:1703.06103.\n",
      "HinrichSchutze.1992. Dimensionsofmeaning. InSupercomputing.\n",
      "YelongShen, Po-SenHuang, Ming-WeiChang, andJianfengGao.2016. Implicitreasonet: Mod-\n",
      "elinglarge-scalestructuredrelationshipswithsharedmemory. arXivpreprintarXiv:1611.04642\n",
      ".\n",
      "KristinaToutanova,DanqiChen, andPatrickPantel.2015. Representingtextforjointembedding\n",
      "oftextandknowledgebases. InEMNLP.\n",
      "ThéoTrouillon,ChristopherR Dance, JohannesWelbl, Sebastian Riedel, Éric Gaussier, and Guil-\n",
      "laume Bouchard. 2017. Knowledge graph completion via complex tensor factorization. arXiv\n",
      "preprintarXiv:1702.06879.\n",
      "ThéoTrouillonandMaximilianNickel.2017. Complexandholographicembeddingsofknowledge\n",
      "graphs:acomparison. arXivpreprintarXiv:1707.01475.\n",
      "SidaWangandChristopherDManning.2012. Baselinesandbigrams:Simple,goodsentimentand\n",
      "topicclassification. InACL.\n",
      "7\n",
      "Bishan Yang, Wen-tau Yih, XiaodongHe, JianfengGao, and Li Deng. 2014. Embeddingentities\n",
      "andrelationsforlearningandinferenceinknowledgebases. arXivpreprintarXiv:1412.6575.\n",
      "XuchenYaoandBenjaminVanDurme.2014.Informationextractionoverstructureddata:Question\n",
      "answeringwithfreebase. InACL(1).pages956–966.\n",
      "Wenpeng Yin, Mo Yu, Bing Xiang, Bowen Zhou, and Hinrich Schütze. 2016. Simple question\n",
      "answeringbyattentiveconvolutionalneuralnetwork. arXivpreprintarXiv:1606.03391.\n",
      "8<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  36864,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Knowledge base completion', 'Question answering']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: 7102\n",
      "tcO\n",
      "03\n",
      "]LM.tats[\n",
      "1v18801.0171:viXra\n",
      "Fast Linear Model for Knowledge Graph Embeddings\n",
      "ArmandJoulin EdouardGrave\n",
      "FacebookAIResearch FacebookAIResearch\n",
      "ajoulin@fb.com egrave@fb.com\n",
      "PiotrBojanowski MaximilianNickel TomasMikolov\n",
      "FacebookAIResearch FacebookAIResearch FacebookAIResearch\n",
      "bojanowski@fb.com maxn@fb.com tmikolov@fb.com\n",
      "Abstract\n",
      "This paper shows that a simple baseline based on a Bag-of-Words (BoW) rep-\n",
      "resentation learns surprisingly good knowledge graph embeddings. By casting\n",
      "knowledgebase completion and question answering as supervised classification\n",
      "problems,weobservethatmodelingco-occurencesofentitiesandrelationsleads\n",
      "to state-of-the-art performance with a training time of a few minutes using the\n",
      "opensourcedlibraryfastText1.\n",
      "1 Introduction\n",
      "Learning representations for Knowledge bases (KBs) such as Freebase (Bollackeretal., 2008)\n",
      "has become a core problem in machine learning, with a large variety of applications from ques-\n",
      "tion answering (YaoandVanDurme, 2014) to image classification (Dengetal., 2014). Many\n",
      "approaches have been proposed to learn these representations, or embeddings, with either sin-\n",
      "gle relational (Hoffetal., 2002; Perozzietal., 2014) or multi-relational data (Nickeletal., 2011;\n",
      "Bordesetal.,2013).\n",
      "These approacheslearn graph embeddingsby modelingthe relation between the differententities\n",
      "in the graphs(Perozzietal., 2014; Nickeletal., 2016), Instead, we framethis problemin a multi-\n",
      "class multilabelclassification problem and model only the co-occurencesof entities and relations\n",
      "withalinearclassifierbasedonaBag-of-Words(BoW)representationandstandardcostfunctions.\n",
      "In practice, this approachworkssurprisinglywell on a variety of standarddatasets, obtainingper-\n",
      "formancecompetitivewith the state-of-the-artapproacheswhile using a standardtext library (i.e.,\n",
      "fastText)andrunninginafewminutes(Joulinetal.,2017).\n",
      "We focus our study on two standard approaches to learn representations for KBs: knowledge\n",
      "base completion and question answering. For KB completion, our conclusions extend those\n",
      "ofKadlecetal.(2017),thatsimplemodelslikeTransE(Bordesetal.,2013)workaswell,ifnotbet-\n",
      "terthanmoresophisticatedones, iftunedproperly. Kadlecetal. (2017)focusona bilinearmodel\n",
      "designed for KB completion, DistMul (Yangetal., 2014), that still takes a few hours to train on\n",
      "a high-endGPU. We show thatsimilar performancecan be achievedwith a linear classifier and a\n",
      "training time reduced to a few minutes. For question answering, we consider datasets where we\n",
      "have guarantees that the question answer pairs are covered by the graph in one hop to indirectly\n",
      "learngraphembeddings(Bordesetal., 2015;Milleretal.,2016). FollowingBordesetal.(2014a),\n",
      "wepredicttherelationbetweentheentitiesappearinginthequestionandanswerpairstolearnem-\n",
      "beddings of the graph edges. The embeddingsof the entities, or nodes, are indirectly learned by\n",
      "embeddingthe questions. Inthis setting, we achievecompetitiveperformanceaslongas we have\n",
      "accesstoacleanKBrelatedtothequestionansweringtask.\n",
      "1Codeavailableathttps://github.com/facebookresearch/fastText\n",
      "Method WN18 FB15k\n",
      "raw filtered raw filtered\n",
      "TransE(Bordesetal.,2013) 75.4 89.2 34.9 47.1\n",
      "Rescal(Nickeletal.,2012) - 92.8 - 58.7\n",
      "Fast-TransR(Linetal.,2015) 81.0 94.6 48.8 69.8\n",
      "HolE(Nickeletal.,2016) - 94.9 - 73.9\n",
      "TransE++(Nickeletal.,2016) - 94.3 - 74.9\n",
      "Fast-TransD(Linetal.,2015) 78.5 91.9 49.9 75.2\n",
      "ReverseModel(Dettmersetal.,2017) - 96.9 - 78.6\n",
      "HolE+Neg-LL(TrouillonandNickel,2017) - 94.7 - 82.5\n",
      "Complex(Trouillonetal.,2017) - 94.7 - 84.0\n",
      "R-GCN(Schlichtkrulletal.,2017) - 96.4 - 84.2\n",
      "ConvE(Dettmersetal.,2017) - 95.5 - 87.3\n",
      "DistMul(Kadlecetal.,2017) - 94.6 - 89.3\n",
      "EnsembleDistMul(Kadlecetal.,2017) - 95.0 - 90.4\n",
      "IRN(Shenetal.,2016) - 95.3 - 92.7\n",
      "fastText-train 80.6 94.9 52.3 86.5\n",
      "fastText-train+valid 83.2 97.6 53.4 89.9\n",
      "Table1:RawandfilteredHit@10onWN18andFB15k.Allthenumbersaretakenfromtheirpaper.\n",
      "Above, methods that should achieve better performancewith a finer hyper-parametergrid, below,\n",
      "methodsthatwereproperlytuned.Higherthebetter.\n",
      "2 Approach\n",
      "2.1 fastTextmodel\n",
      "Linearmodels(Joachims,1998)arepowerfulandefficientbaselinesfortextclassification.Inpartic-\n",
      "ular, the fastTextmodelproposedby Joulinetal. (2017) achievesstate-of-the-artperformance\n",
      "onmanydatasetsbycombiningseveralstandardtricks,suchaslowrankconstraints(Schutze,1992)\n",
      "andn-gramfeatures(WangandManning,2012). Thesameapproachcanbeappliedtoanyproblem\n",
      "wheretheinputisasetofdiscretetokens.Forexample,aKBiscomposedofentities(ornodes)and\n",
      "relations(oredges)thatcanberepresentedbyauniquediscretetoken.\n",
      "The model is composed of a matrix V which is used as a look-up table over the discrete tokens\n",
      "and a matrix W for the classifier. The representations of the discrete tokens are averaged into\n",
      "BoWrepresentation,whichisinturnfedtothelinearclassifier. Usingafunctionf tocomputethe\n",
      "probabilitydistributionovertheclasses,andN inputsetsfordiscretetoken(e.g.,sentences),leads\n",
      "tominimize:\n",
      "N\n",
      "1\n",
      "− Xy nlog(f(WVx n)),\n",
      "N\n",
      "n=1\n",
      "wherex isthenormalizedBoWofthen-thinputset,y thelabel.WhileBoWmodelsarememory\n",
      "n n\n",
      "inefficient,theirmemoryfootprintcanbesignificantlyreduced(Joulinetal.,2016a). Themodelis\n",
      "trainedasynchronouslyonmultipleCPUswithSGDandalinearlydecayinglearningrate.\n",
      "2.2 Lossfunctions\n",
      "We considertwolossfunctionsinourexperiments: thesoftmaxfunctionandaone-versus-allloss\n",
      "functionwithnegativesampling.\n",
      "Softmax. Given K classes, and a score s for each class k, the softmax function is defined as\n",
      "k\n",
      "f(s) = exp(s )/ K exp(s ).Thisfunctionrequiresthescoreofeveryclass,leadingtoacom-\n",
      "k k Pi=1 i\n",
      "plexityofO(Kh)wherehisthesizeoftheembeddings.Thisfunctionisoftenusedtocomputethe\n",
      "probabilitydistributionofafinitesetofdiscreteclasses.\n",
      "2\n",
      "one-versus-all loss. Computing the softmax function over a large number of classes is compu-\n",
      "tationally prohibitive. We replace it by an independent binary classifier per class, i.e., a set of\n",
      "one-versus-all losses. During training, for each positive example, we draw randomly k negative\n",
      "classes,andupdatethek+1classifiers. ThenumberkissignificantlysmallerthanK,reducingthe\n",
      "complexityfromO(Kh)to O(kh). Thislosshasbeenusedforwordembeddings(Mikolovetal.,\n",
      "2013;Bojanowskietal.,2017)aswellastoobjectclassification(Joulinetal.,2016b).\n",
      "2.3 Knowledgebasecompletion\n",
      "Aknowledgebaseisrepresentedasasetofsubject-relation-objecttriplets(e,r,p).Typically,theen-\n",
      "titypispredictedaccordingtothesubjecteandtherelationr. WiththenotationsofthefastText\n",
      "model described in Sec. 2.1, each entity e is associated with a vector v and each relation r with\n",
      "e\n",
      "a vector v of the same dimension h. The target entity p is also represented by a h dimensional\n",
      "r\n",
      "vectorw. Thescoringfunctions foratriplet(e,r,p)issimplythedotproductbetweentheBoW\n",
      "p p\n",
      "representationoftheinputpair(e,r)andthetarget:\n",
      "1\n",
      "s (e,r,p) = hv +v,w i. (1)\n",
      "p e r p\n",
      "2\n",
      "This scoring function does not define a relational model, it only captures co-occurence between\n",
      "entitiesandrelations. Additionally,itmakesnoassumptionaboutthedirectionoftherelation,i.e.,\n",
      "thesame relationembeddingis usedto predictbothendsofa triplet. To circumventthisproblem,\n",
      "weencodethedirectionintherelationembeddingbyassociatingarelationrwithtwoembeddings,\n",
      "onetopredictthesubjectandonetopredicttheobject.Whileourapproachsharesmanysimilarities\n",
      "withTransE(Bordesetal.,2013),itdiffersinseveralaspects: theyusearankingloss,theirscoring\n",
      "functionisanℓ2distance,andtheyhaveoneembeddingperentity.Similarly,ifthegoalistopredict\n",
      "1\n",
      "therelationbetweenapairofentities,ourscoringfunctions is: s (e,r,p) = hv +v,w i.As\n",
      "r r 2 e p r\n",
      "forentityprediction,wecircumventthesymmetrybetweensubjectandrelationbyassociatingeach\n",
      "entitywithtwoembeddings,oneiftheentityisthesubjectortheobjectofatriplet.\n",
      "2.4 Questionanswering\n",
      "Questionansweringproblemscanbeusedtolearngraphembeddingsifframedasedgeprediction\n",
      "problemsbetweenentitiesappearinginthequestionanswerpairs(Bordesetal.,2014a). Theques-\n",
      "tion is representedas a bag of words and the potentialrelationsare labels. An entity is indirectly\n",
      "representedbytheassociatedwordsinthequestion.\n",
      "Stringmatchingforentitylinking. ThequestionsandanswersarematchedtoentitiesintheKB\n",
      "withastringmatchingalgorithm(Bordesetal.,2014a),usingalook-uptablebetweenentitiesand\n",
      "their string representations. Everypair of questionand answer in the training set is thus matched\n",
      "toasetofpotentialpairsofentities. Severalentitiesareoftenmatchedtoaquestionandweusean\n",
      "ad-hoceuristictosortthem,i.e.,usingtheinverseoftheirfrequencyinthetrainingset,andthesize\n",
      "oftheirassociatedstringsincaseofties(toapproximatethefrequency).\n",
      "Relationpredictionforquestionanswering. Onceaquestion-answerpairisassociatedwithaset\n",
      "ofpairsofentities, candidaterelationsareextracted. FollowingBordesetal.(2014a),weconsider\n",
      "therelationsaslabelsandusefastTexttopredictthem. Attesttime,theanswertoaquestionis\n",
      "inferredbytakingthemostlikelyrelationandverifyifanyoftheentitiesmatchedtothequestion\n",
      "formsavalidpairintheKB.Ifnot,wemovetothenextmostlikelyrelationandreiteratetheprocess.\n",
      "3 Results\n",
      "3.1 Knowledgebasecompletion.\n",
      "Datasets. WeuseseveralstandardbenchmarksforKBcompletion:\n",
      "• TheWN18datasetisasubsetofWordNet,containing40,943entities,18relationtypes,and\n",
      "151,442triples. WordNetisa KBbuiltbygroupingsynonymwordsandprovideslexical\n",
      "relationshipsbetweenthem.\n",
      "3\n",
      "Method FB15k-237 Method SVO\n",
      "R-GCN(Schlichtkrulletal.,2017) 41.7 TransE(Garcia-Duranetal.,2015) 70.6\n",
      "DistMul(Yangetal.,2014) 41.9 SME(Bordesetal.,2014b) 77.0\n",
      "Complex(Trouillonetal.,2017) 41.9 LFM(Jenattonetal.,2012) 78.0\n",
      "ConvE(Dettmersetal.,2017) 45.8 TATEC(Garcia-Duranetal.,2015) 80.1\n",
      "fastText-train 44.8 fastText-train 79.8\n",
      "fastText-train+valid 45.8 fastText-train+valid 79.9\n",
      "Table 2: Filtered Hit@10 on FB15k-237. Table 3: Hit5% on SVO. The numbers are\n",
      "ThenumbersarefromDettmersetal.(2017). fromGarcia-Duranetal.(2015).\n",
      "• TheFB15kdatasetisasubsetofFreebase,containing14,951entities,1345relationtypes,\n",
      "and592,213triples. FreebaseisalargeKBcontaininggeneralfactsabouttheworld.\n",
      "• The FB15k-237 dataset that is a subset of FB15k with no reversible rela-\n",
      "tions (Toutanovaetal., 2015). It contains 237 relations and 14,541 entities, for a total\n",
      "of298,970triples.\n",
      "• The SVO dataset is a subset of subject-relation-object triplets extracted from Wikipedia\n",
      "articles,containing30,605entities,4,547relationtypesand1.3Mtriples.\n",
      "Experimental protocol. For WN18, FB15k and FB15k-237, the goal is to predictone end of a\n",
      "triple given the other end and the relation, e.g., the subject given the object and the relation. We\n",
      "report Hit@10, also known as Recall@10, on raw and filtered datasets. Raw means the standard\n",
      "recallmeasurewhilefilteredmeansthateveryrelationthatalreadyexistsintheKBarefirstremoved,\n",
      "eventhoseinthetestset. Thefilteredmeasureallowsadirectcomparisonofthetargetentitywith\n",
      "negativeones. OnSVO, thegoalis topredictthe relationgivena pairof entities. Themeasureis\n",
      "Hit@5%,i.e.,Hit@227for4,547relationtypes.\n",
      "Implementation details. For both WN18, FB15k and FB15k-237, we use a negative sampling\n",
      "approximationofthesoftmaxandselectthehyper-parametersbasedonthefilteredhits@10onthe\n",
      "validationset. OnWN18andFB15k,hegridofparametersusedis[10,25,50,100,150,200]forthe\n",
      "embeddingsize h, [100,150,200]forthe numberof epochsand[100,200,500]forthe numberof\n",
      "negativeexamples. SinceFB15k-237ismuchsmaller,we limitthenumberofepochsto [1,5,10].\n",
      "Theinitiallearningrateisfixedat0.2. OnWN18,thebestsetofhyper-parametersare100dimen-\n",
      "sions, 100 epochs and 500 negative samples. On FB15k, the selected hyper-parameters are 100\n",
      "dimensions,100epochsand100negativesamples.OnFB15k-237,thebestsetofhyper-parameters\n",
      "areahiddenof50,10epochsanda500negativesamples. ForSVO,thenumberofrelationstopre-\n",
      "dictisquitesmall, wethususeafullsoftmaxandselecthyper-parametersbasedonhit@5%. The\n",
      "gridofhyper-parametersis[10,25,50,100,150,200]fortheembeddingsizehand[1,2,3,4,5]for\n",
      "thenumberofepochs. Theinitiallearningrateisfixedat0.2. Foralltheseexperiments,wereport\n",
      "boththeperformanceonthemodeltrainonthetrain setandontheconcatenationofthe trainand\n",
      "validationset,runwiththesamehyper-parameters.\n",
      "Comparison. We compare our approach to several standard models in Table 1 on WN18 and\n",
      "FB15k. We report numbers from their original papers. Some of them are not using a fine grid\n",
      "of hyper-parameters, which partially explains the gap in performance. We separate these mod-\n",
      "els from more recent ones for fairer comparison. Despite its simplicity, our approach is compet-\n",
      "itive with dedicated pipelines both for raw and filtered measurements. This extends the findings\n",
      "of TrouillonandNickel (2017), i.e., the choice of loss function can have a significant impact on\n",
      "overallperformance. Table 2 extendsthis observation to a harder dataset, FB15k-237, where our\n",
      "BoWmodelcomparesfavorablywithexistingKBcompletionmodels.\n",
      "WealsoreportcomparisononrelationpredictiondatasetSVOinTable3. Ourapproachiscompet-\n",
      "itive with approachesusing bigram and high order information, like TATEC (Garcia-Duranetal.,\n",
      "2015). Note TATEC can be, theoretically, used for both relation and entity prediction, while our\n",
      "modelonlypredictsrelations.\n",
      "4\n",
      "Dataset WN18 FB15k SVO FB15k-237 SQ WikiMovies\n",
      "Time(sec.) 165 188 371 28 42 1\n",
      "Table4: TrainingtimeforfastTextusing20threadsonaIntelXeonCPUE5-2680v32.50GHz.\n",
      "Table4showtherunningtimeforafastTextimplementation.Itrunsinafewminutes,whichis\n",
      "comparablewithoptimizedpipelineslikeFast-TransDandFast-TransR(Linetal.,2015). Notethat\n",
      "similarrunningtimesshouldbeachievableforotherlinearmodelslikeTransE.\n",
      "3.2 Questionanswering.\n",
      "Datasets. Weconsidertwostandarddatasetswithasignificantamountofquestionanswerpairs.\n",
      "• SimpleQuestion consists of 108,442 question-answer pairs generated from Freebase. It\n",
      "comeswithasubsetofFreebasewith2Mtriplets.\n",
      "• WikiMovies consists of more than 100,000 questions about movies generated also from\n",
      "Freebase.ItcomeswithasubsetoftheKBassociatedwiththequestion-answerpairs.This\n",
      "datasetalsoprovideswithsettingswheredifferentpreprocessedversionsofWikipediaare\n",
      "consideredinsteadoftheKB.Thesesettingsarebeyondthescopeofthispaper.\n",
      "Implementation details. For both SimpleQuestion and MovieWiki, the number of relations are\n",
      "relativelysmall. We thususe a fullsoftmax. ForSimpleQuestion,the gridof hyper-parametersis\n",
      "[10,50,100,200]forthedimensionoftheembeddingsand[5,10,50,100]forthenumberofepochs.\n",
      "We use bigramsand an initial learningrate of 1. For MovieWiki, we fixed the embeddingsize to\n",
      "16sincethereareonly16relationsandthenumberofepochswasselectedonthevalidationsetin\n",
      "[1,5,10,50].Weuseaninitiallearningrateof.3.\n",
      "SimpleQuestion. Figure 5 compares this ap-\n",
      "Method SQ\n",
      "proachwiththestate-of-the-art.Welearnarela-\n",
      "tionclassifierwithfastTextin42sec. Using Randomguess(Bordesetal.,2015) 4.9\n",
      "a larger KB, i.e., FB5M, does not degrade the CFO(Daietal.,2016) 62.6\n",
      "performance,despitehavingmuchmoreirrele- MemNN(Bordesetal.,2015) 62.7\n",
      "vantentities. Ourapproachcomparesfavorably AMPCNN(Yinetal.,2016) 68.3\n",
      "well other with question answering systems. CharQA(GolubandHe,2016) 70.9\n",
      "Thissuggeststhatthelearnedembeddingscap- CFO+AP(Daietal.,2016) 75.7\n",
      "turesomeimportantinformationabouttheKB. AMPCNN+AP(Yinetal.,2016) 76.4\n",
      "Note,however,thattheperformanceisverysen-\n",
      "fastText-train 72.7\n",
      "sible to the quality of the entity linker and the\n",
      "fastText-train+valid 73.0\n",
      "ad-hocsortingofextractedsubjects. Typically,\n",
      "going from a random order to the one used in\n",
      "Table 5: Accuracy on the SimpleQuestions\n",
      "this paper gives a boost of up to 10% depend-\n",
      "dataset(Bordesetal.,2015).\n",
      "ingonthehyper-parameters.\n",
      "WikiMovies. Table 6 compares our models with several state-of-the-art pipelines. In the case\n",
      "wherethecleanKBisaccessible,ourmethodworksverywell.fastTextrunsin1sec.forrelation\n",
      "prediction. Notethatthisdatasetwasprimarilymadeforthecasewhereonlytextisavailable. This\n",
      "settinggoesbeyondthescopeofourmethod,whileamoregeneralapproachlikeKV-memNNstill\n",
      "worksreasonablywell(Milleretal.,2016).\n",
      "Method SE MemNN QASystem KV-MemNN fastText\n",
      "WikiMovies 54.4 78.5 93.5 93.9 95.9\n",
      "Table6:Testresult(%hits@1)ontheWikiMoviesdatasetwiththefullKB.Thenumbersaretaken\n",
      "formMilleretal.(2016). QASystemreferstoBordesetal.(2014a).\n",
      "5\n",
      "4 Conclusion\n",
      "In this paper, we show that linear models learn good embeddings from a KB by recasting graph\n",
      "related problems into supervised classification ones. The limitations of such approach are that it\n",
      "requires a clean KB and a task that uses direct information about local connectivity in the graph.\n",
      "Moreover,theobservationthatournon-relationalapproachprovidesstate-of-the-artperformanceon\n",
      "KBCbenchmarksraisesalsoimportantquestionsregardingtheevaluationoflink-predictionmodels\n",
      "andthedesignofbenchmarksforthistask.\n",
      "Acknowledgement. We thank Timothée Lacroix, Nicolas Usunier, Antoine Bordes and the rest\n",
      "ofFAIRfortheirprecioushelpandcomments. WealsowouldliketothankAdamFischandAlex\n",
      "MillerfortheirhelpregardingMovieWiki.\n",
      "References\n",
      "Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word\n",
      "vectorswithsubwordinformation.TransactionsoftheAssociationforComputationalLinguistics\n",
      "5:135–146.\n",
      "Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a\n",
      "collaborativelycreatedgraphdatabaseforstructuringhumanknowledge. InProceedingsofthe\n",
      "2008ACMSIGMODinternationalconferenceonManagementofdata.AcM,pages1247–1250.\n",
      "AntoineBordes,SumitChopra,andJasonWeston.2014a. Questionansweringwithsubgraphem-\n",
      "beddings. arXivpreprintarXiv:1406.3676.\n",
      "Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. 2014b. A semantic matching\n",
      "energyfunctionforlearningwithmulti-relationaldata. MachineLearning94(2):233–259.\n",
      "AntoineBordes,NicolasUsunier,SumitChopra,andJasonWeston.2015. Large-scalesimpleques-\n",
      "tionansweringwithmemorynetworks. arXivpreprintarXiv:1506.02075.\n",
      "Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.\n",
      "2013.Translatingembeddingsformodelingmulti-relationaldata. InAdvancesinneuralinforma-\n",
      "tionprocessingsystems.pages2787–2795.\n",
      "ZihangDai, LeiLi, andWei Xu.2016. Cfo: Conditionalfocusedneuralquestionansweringwith\n",
      "large-scaleknowledgebases. arXivpreprintarXiv:1606.01994.\n",
      "JiaDeng,NanDing,YangqingJia,AndreaFrome,KevinMurphy,SamyBengio,YuanLi,Hartmut\n",
      "Neven,andHartwigAdam.2014. Large-scaleobjectclassificationusinglabelrelationgraphs. In\n",
      "EuropeanConferenceonComputerVision.Springer,Cham,pages48–64.\n",
      "TimDettmers,PasqualeMinervini,PontusStenetorp,andSebastianRiedel.2017.Convolutional2d\n",
      "knowledgegraphembeddings. arXivpreprintarXiv:1707.01476.\n",
      "AlbertoGarcia-Duran,AntoineBordes, NicolasUsunier, andYvesGrandvalet.2015. Combining\n",
      "two and three-way embeddingsmodels for link prediction in knowledge bases. arXiv preprint\n",
      "arXiv:1506.00999.\n",
      "David Golub and Xiaodong He. 2016. Character-level question answering with attention. arXiv\n",
      "preprintarXiv:1604.00727.\n",
      "Peter D Hoff, Adrian E Raftery, and Mark S Handcock.2002. Latent space approachesto social\n",
      "networkanalysis. JournaloftheamericanStatisticalassociation97(460):1090–1098.\n",
      "RodolpheJenatton,NicolasLRoux,AntoineBordes,andGuillaumeRObozinski.2012. A latent\n",
      "factor model for highly multi-relational data. In Advances in Neural Information Processing\n",
      "Systems.pages3167–3175.\n",
      "ThorstenJoachims.1998. Text categorizationwith supportvectormachines: Learningwith many\n",
      "relevantfeatures. Springer.\n",
      "6\n",
      "Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hervé Jégou, and Tomas\n",
      "Mikolov. 2016a. Fasttext. zip: Compressing text classification models. arXiv preprint\n",
      "arXiv:1612.03651.\n",
      "Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2017. Bag of tricks for\n",
      "efficient text classification. In Proceedingsof the 15th Conference of the European Chapterof\n",
      "theAssociationforComputationalLinguistics: Volume2,ShortPapers.AssociationforCompu-\n",
      "tationalLinguistics,pages427–431.\n",
      "Armand Joulin, Laurens van der Maaten, Allan Jabri, and Nicolas Vasilache. 2016b. Learning\n",
      "visualfeaturesfromlargeweaklysuperviseddata. InEuropeanConferenceonComputerVision.\n",
      "SpringerInternationalPublishing,pages67–84.\n",
      "RudolfKadlec,OndrejBajgar,andJanKleindienst.2017. Knowledgebasecompletion: Baselines\n",
      "strikeback. arXivpreprintarXiv:1705.10744.\n",
      "YankaiLin,ZhiyuanLiu,MaosongSun,YangLiu,andXuanZhu.2015.Learningentityandrelation\n",
      "embeddingsforknowledgegraphcompletion. InAAAI.pages2181–2187.\n",
      "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word\n",
      "representationsinvectorspace. arXivpreprintarXiv:1301.3781.\n",
      "Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason\n",
      "Weston. 2016. Key-value memory networks for directly reading documents. arXiv preprint\n",
      "arXiv:1606.03126.\n",
      "Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. 2016. Holographic embeddings of\n",
      "knowledgegraphs. InThirtiethAAAIConferenceonArtificialIntelligence.\n",
      "MaximilianNickel,VolkerTresp,andHans-PeterKriegel.2011. Athree-waymodelforcollective\n",
      "learningonmulti-relationaldata. InProceedingsofthe28thinternationalconferenceonmachine\n",
      "learning(ICML-11).pages809–816.\n",
      "Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2012. Factorizing yago: Scalable ma-\n",
      "chine learning for linked data. In Proceedings of the 21st International Conference on World\n",
      "WideWeb.ACM,pages271–280.\n",
      "BryanPerozzi,RamiAl-Rfou,andStevenSkiena.2014. Deepwalk: Onlinelearningofsocialrep-\n",
      "resentations. InProceedingsofthe20thACMSIGKDDinternationalconferenceonKnowledge\n",
      "discoveryanddatamining.ACM,pages701–710.\n",
      "Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max\n",
      "Welling. 2017. Modeling relational data with graph convolutional networks. arXiv preprint\n",
      "arXiv:1703.06103.\n",
      "HinrichSchutze.1992. Dimensionsofmeaning. InSupercomputing.\n",
      "YelongShen, Po-SenHuang, Ming-WeiChang, andJianfengGao.2016. Implicitreasonet: Mod-\n",
      "elinglarge-scalestructuredrelationshipswithsharedmemory. arXivpreprintarXiv:1611.04642\n",
      ".\n",
      "KristinaToutanova,DanqiChen, andPatrickPantel.2015. Representingtextforjointembedding\n",
      "oftextandknowledgebases. InEMNLP.\n",
      "ThéoTrouillon,ChristopherR Dance, JohannesWelbl, Sebastian Riedel, Éric Gaussier, and Guil-\n",
      "laume Bouchard. 2017. Knowledge graph completion via complex tensor factorization. arXiv\n",
      "preprintarXiv:1702.06879.\n",
      "ThéoTrouillonandMaximilianNickel.2017. Complexandholographicembeddingsofknowledge\n",
      "graphs:acomparison. arXivpreprintarXiv:1707.01475.\n",
      "SidaWangandChristopherDManning.2012. Baselinesandbigrams:Simple,goodsentimentand\n",
      "topicclassification. InACL.\n",
      "7\n",
      "Bishan Yang, Wen-tau Yih, XiaodongHe, JianfengGao, and Li Deng. 2014. Embeddingentities\n",
      "andrelationsforlearningandinferenceinknowledgebases. arXivpreprintarXiv:1412.6575.\n",
      "XuchenYaoandBenjaminVanDurme.2014.Informationextractionoverstructureddata:Question\n",
      "answeringwithfreebase. InACL(1).pages956–966.\n",
      "Wenpeng Yin, Mo Yu, Bing Xiang, Bowen Zhou, and Hinrich Schütze. 2016. Simple question\n",
      "answeringbyattentiveconvolutionalneuralnetwork. arXivpreprintarXiv:1606.03391.\n",
      "8<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,    916,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Armand Joulin', 'Edouard Grave', 'Piotr Bojanowski', 'Tomas Mikolov', 'Ajoulin@fb.com', 'Egrave@fb.com', 'Bojanowski@fb.com', 'Maxn@fb.com', 'Tmikolov@fb.com']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Complex and Holographic Embeddings of Knowledge Graphs:\n",
      "A Comparison\n",
      "ThéoTrouillon MaximilianNickel\n",
      "Univ. GrenobleAlpes∗ FacebookAIResearch\n",
      "theo.trouillon@gmail.com MassachusettsInstituteofTechnology,LCSL\n",
      "maxn@fb.com\n",
      "1 Introduction Forknowledgegraphswithalargenumberofpossible\n",
      "triplesweemploynegativesamplingasproposedby Embeddingsofknowledgegraphshavereceivedsig-\n",
      "Bordes et al. [2013]. The objective of knowledge\n",
      "nificantattentionduetotheirexcellentperformance\n",
      "graphcompletionisthentolearnascoringfunction\n",
      "fortaskslikelinkpredictionandentityresolution. In\n",
      "φ :R×E×E →R for any s,o ∈ E and p ∈ R\n",
      "this short paper, we are providing a comparison of pso\n",
      "whichpredictsthetruthvalueofpossibletriples. We\n",
      "twostate-of-the-artknowledgegraphembeddingsfor\n",
      "willwriteN = |E|andN = |R|.\n",
      "whichtheirequivalencehasrecentlybeenestablished, e r\n",
      "For notational convenience, we define the trilinear\n",
      "i.e.,COMPLEXandHOLE[Nickel,Rosasco,andPog-\n",
      "productofthreecomplexvectorsas:\n",
      "gio,2016;Trouillonetal.,2016;HayashiandShimbo,\n",
      "2017]. First,webrieflyreviewbothmodelsanddis-\n",
      "K\n",
      "cusshowtheirscoringfunctionsareequivalent. We (cid:104)a,b,c(cid:105) =(cid:213) a b c =a(cid:62)(b(cid:12)c)\n",
      "j j j\n",
      "thenanalyzethediscrepancyofresultsreportedinthe\n",
      "j=1\n",
      "originalarticles,andshowexperimentallythatthey\n",
      "arelikelyduetotheuseofdifferentlossfunctions. In where a,b,c ∈ CK, and (cid:12) denotes the Hadamard\n",
      "furtherexperiments,weevaluatetheabilityofboth product,i.e. theelement-wiseproductbetweentwo\n",
      "modelstoembedsymmetricandantisymmetricpat- vectorsofsamelength.\n",
      "terns. Finally,wediscussadvantagesanddisadvan- In the following, we will consider the discrete\n",
      "tagesofbothmodelsandunderwhichconditionsone Fouriertransform(DFT)ofpurelyrealvectorsonly:\n",
      "wouldbepreferabletotheother. F :RK →CK. For j ∈ {0,...,K−1}:\n",
      "2 EquivalenceofComplexand K−1\n",
      "HolographicEmbeddings\n",
      "F(x)\n",
      "j\n",
      "= (cid:213) x ke−2iπj Kk (1)\n",
      "k=0\n",
      "Inthissection,wewillbrieflyreviewHolographicand\n",
      "Complexembeddingsanddiscusstheequivalenceof whereF(x) j ∈Cisthe jthvalueintheresultingcom-\n",
      "theirscoringfunctions. plexvectorF(x)∈CK. Notethatthecomponentsin\n",
      "Let G = (E,R,T) be a knowledge graph, which Equation(1)areindexedfrom0toK−1.\n",
      "consistsofentitiesE,relationtypesR andobserved\n",
      "triples T ⊆ R × E × E. Furthermore, let D be a HolographicEmbeddings\n",
      "trainingset,whichassociateswitheachpossibletriple Theholographicembeddingsmodel(HOLE)[Nickel,\n",
      "inGitstruthvalues y ∈ {±1}. Thatis,forapossible Rosasco,andPoggio,2016]representsrelationsand\n",
      "triple(p,s,o)withs,o∈ E andp∈ R itholdsthat entities with real-valued embeddings E ∈ RNe×K,\n",
      "R ∈ RNr×K,andscoresatriple(p,s,o)withthedot\n",
      "(cid:40)\n",
      "+1, if(p,s,o)∈ T productbetweentheembeddingoftherelationpand\n",
      "y =\n",
      "pso −1, otherwise. the circular correlation(cid:63) : RK ×RK → RK of the\n",
      "embeddingsofentitiessando:\n",
      "∗WorkalsodonewhileatXeroxResearchCentreEu-\n",
      "rope. φh\n",
      "pso\n",
      "=r p(cid:62)(e s(cid:63)e o). (2)\n",
      "7102\n",
      "luJ\n",
      "32\n",
      "]GL.sc[\n",
      "2v57410.7071:viXra\n",
      "Thecircularcorrelationcanbewrittenwiththedis- into the frequency domain and back. However, to\n",
      "creteFouriertransform(DFT), ensurethatthetrilinearproductofthesecomplexem-\n",
      "beddingsisarealnumber,wewouldeitherneedto\n",
      "e s(cid:63)e\n",
      "o\n",
      "= F−1(F(e s)(cid:12)F(e o)) (3) enforcethesamesymmetryconstraintsonF(e i)and\n",
      "F(r ) that arise from the DFTs or—alternatively—\n",
      "j\n",
      "where F−1 : CK → CK is theinverseDFT.In this takeonlythereal-valuedpartofthetrilinearproduct.\n",
      "case,theembeddingvectorsarereale s,e o,r p ∈RK, WeshowinAppendixAthatthesearetwowaysof\n",
      "andsoistheresultoftheinverseDFT,sincethecir- performingthesameoperation,henceshowingthat\n",
      "cular correlation of real-valued vectors results in a the scoring functions of COMPLEX and HOLE are\n",
      "real-valuedvector. equivalent—uptoaconstantfactor.\n",
      "Furthermore,bothmodelshaveequalmemorycom-\n",
      "ComplexEmbeddings\n",
      "plexity,astheequivalentcomplexvectorsaretwiceas\n",
      "Thecomplexembeddingsmodel(COMPLEX)[Trouil- small(seeproofinAppendixA)butrequiretwiceas\n",
      "lonetal.,2016,2017]representsrelationsandenti- muchmemoryasreal-valuedonesofsamesize—fora\n",
      "ties with complex-valued embeddings E ∈ CNe×K, givenfloating-pointprecision. However,thecomplex\n",
      "R ∈ CNr×K,andscoresatriple(p,s,o)withthereal formulationofthescoringfunctionreducesthetime\n",
      "partofthetrilinearproductofthecorrespondingem- complexityfromO(Klog(K))(quasilinear)toO(K)\n",
      "beddings: (linear).\n",
      "φc\n",
      "pso\n",
      "=Re(cid:0) (cid:104)r p,e s,e o(cid:105)(cid:1) (4) 3 LossFunctions&PredictiveAbilities\n",
      "wheree,e,r ∈CK arecomplexvectors,ande is TheexperimentalresultsofHOLEandCOMPLEXas\n",
      "s o p o\n",
      "reportedbyNickel,Rosasco,andPoggio[2016]and\n",
      "thecomplexconjugateofthevectore.\n",
      "o\n",
      "Trouillonetal.[2016]agreedontheWN18dataset,\n",
      "butdivergedsignificantlyonFB15K[Bordesetal.,\n",
      "Equivalence\n",
      "2014]—althoughbothscoringfunctionareequivalent.\n",
      "TheequivalenceofHOLEandCOMPLEXhasrecently\n",
      "Sincethemaindifferenceintheexperimentalsettings\n",
      "beenshownbyHayashiandShimbo[2017]. Inthe\n",
      "wastheuseofdifferentlossfunctions—i.e.,margin\n",
      "following,webrieflydiscussthisequivalenceofboth\n",
      "lossversuslogisticloss—weanalyzeinthissection\n",
      "modelsandhowitcanbederived. Forcompleteness,\n",
      "whetherthediscrepancyofresultscanbeattributedto\n",
      "a full proof similar to that of Hayashi and Shimbo\n",
      "thisfact. Forthispurpose,weimplementedbothloss\n",
      "[2017]isincludedinAppendixA. functions for the complex representation φc within\n",
      "First,toderivetheconnectionbetweenHOLEand\n",
      "thesameframework,andcomparedtheresultsonthe\n",
      "COMPLEX,considerParseval’sTheorem:\n",
      "WN18andFB15Kdatasets.\n",
      "Theorem 1. Suppose x,y ∈ RK are real vectors. First,notethatinbothdatasets,onlypositivetrain-\n",
      "Then x(cid:62)y = 1F(x)(cid:62)F(y). ingtriplesareprovided. Negativeexamplesaregener-\n",
      "K\n",
      "atedbycorruptingthesubjectorobjectentityofeach\n",
      "UsingTheorem1aswellasEquations(2)and(3),we\n",
      "positive triple, as described in Bordes et al. [2013].\n",
      "canthenrewritethescoringfunctionofHOLEas:\n",
      "Intheoriginal HOLE publication[Nickel,Rosasco,\n",
      "φh(p,s,o)=r(cid:62)(e (cid:63)e ) (5) and Poggio, 2016], a pairwise margin loss is opti-\n",
      "p s o mizedovereachpositiveanditscorruptednegative\n",
      "=r p(cid:62)(F−1(F(e s)(cid:12)F(e o))) (p,s(cid:48),o(cid:48)):\n",
      "1\n",
      "= KF(r p)(cid:62)F(F−1(F(e s)(cid:12)F(e o))) L(D;Θ)=(cid:213) [γ+σ(φh ps(cid:48)o(cid:48))−σ(φh pso)]+ (7)\n",
      "1 ((p,s,o),y)∈D\n",
      "= F(r )(cid:62)(F(e )(cid:12)F(e ))\n",
      "p s o\n",
      "K\n",
      "where γ is the margin hyperparameter, and σ the\n",
      "1 (cid:68) (cid:69)\n",
      "= F(r p),F(e s),F(e o). (6) standardlogisticfunction. Theentityembeddingsare\n",
      "K\n",
      "also constrained to unit norm : ||e || ≤ 1, for all\n",
      "i 2\n",
      "Hence, for HOLE we could directly learn complex i ∈ E.\n",
      "embeddingse ≡ F(e ),r ≡ F(r ) ∈ Cd insteadof WhereasinTrouillonetal.[2016],thegenerated\n",
      "(cid:98)i i (cid:98)j j\n",
      "learningembeddingse,r ∈ Rd andmappingthem negativesaremergedintothetrainingsetD ateach\n",
      "i j\n",
      "batchsampling,andthelog-likelihoodisoptimized p∈ Raresymmetricwhentripleshavethesametruth\n",
      "withL2regularization: valuebypermutationofthesubjectandobjectenti-\n",
      "ties: y = y for all s,o ∈ E, whereas facts of\n",
      "(cid:213) pso pos\n",
      "L(D;Θ)= log(1+exp(−yφc ))+λ||Θ||2. (8) antisymmetricrelations phaveinversetruthvalues:\n",
      "pso 2\n",
      "((p,s,o),y)∈D y pso = −y pos. Toevaluatethisquestionexperimen-\n",
      "tally, we reproduced the joint learning of synthetic\n",
      "Optimizationisconductedwithstochasticgradient symmetricandantisymmetricrelationsdescribedin\n",
      "descent,AdaGrad[Duchi,Hazan,andSinger,2011], Trouillonetal.[2016]onbothscoringfunctions. We\n",
      "and early stopping, as described in Trouillon et al. used the log-likelihood loss as all negatives are ob-\n",
      "[2016].Asinglecorruptednegativetripleisgenerated served.\n",
      "for each positive training triple. The results are re- Wegeneratedrandomlya50×50symmetricma-\n",
      "portedforthebestvalidatedmodelsaftergrid-search trix,anda50×50antisymmetricmatrix. Jointly,they\n",
      "onthefollowingvalues: K ∈ {10,20,50,100,150, representa2×50×50tensor. Toensurethatalltest\n",
      "200},λ ∈ {0.1,0.03,0.01,0.003,0.001,0.0003,0.0}\n",
      "valuesarepredictable,theuppertriangularpartsofthe\n",
      "forthelog-likelihoodloss,andγ ∈ {0.1,0.2,0.3,0.4, matricesarealwayskeptinthetrainingset,andthe\n",
      "0.5,0.6,0.7,0.8,0.9,1.0} forthemax-marginloss. diagonalsareunobserved. Weconducted5-foldcross-\n",
      "Therawandfilteredmeanreciprocalranks(MRR), validationonthelower-triangularmatrices,usingthe\n",
      "aswellasthefilteredhitsat1,3and10arereported upper-triangularpartsplus3foldsfortraining, one\n",
      "inTable1. foldforvalidationandonefoldfortesting. Thereg-\n",
      "The margin loss results are consistent with the ularizationparameterλisvalidatedamongthesame\n",
      "HOLE ones originally reported in Nickel, Rosasco, valuesasinthepreviousexperiment.\n",
      "andPoggio[2016],whichconfirmstheequivalence Figure 1 shows the best cross-validated average\n",
      "ofthescoringfunctions,andsupportsthehypothesis precision(areaundertheprecision-recallcurve)for\n",
      "thatthelosswasresponsibleforthedifferenceinpre- thetwoscoringfunctionsforranksrangingupto50.\n",
      "viously reported results. The log-likelihood results Both models manage to perfectly model symmetry\n",
      "are also coherent, as one must note that the higher andantisymmetry. AstheComplExmodelhastwice\n",
      "scoresreportedonFB15KinTrouillonetal.[2016] has many parameters for a given rank, it reaches a\n",
      "areduetotheuseofmorethanonegeneratednega- perfectaverageprecisionwithatwicesmallerrank.\n",
      "tivesampleforeachpositivetrainingtriple. Here,we Thisconfirmsthattherepresentationofthescoring\n",
      "generatedasinglenegativesampleforeachpositive functiondoesnotaffectthelearningabilitiesofthe\n",
      "oneinordertokeepthecomparisonfairbetweenthe modelsinpractice.\n",
      "two losses. The max-margin loss achieves a better\n",
      "raw MRR (rankings without removing the training 5 Discussion\n",
      "samples) on both datasets, but much worse filtered\n",
      "Wehavedemonstratedthatthescoringfunctionsof\n",
      "metrics on FB15K, suggesting that this loss can be\n",
      "morepronetooverfitting.\n",
      "the HOLE and COMPLEX models are directly pro-\n",
      "portional. Thishenceextendstheexistenceproperty\n",
      "oftheCOMPLEXmodeloverallknowledgegraphs\n",
      "4 ScoringFunction&Symmetry\n",
      "[Trouillonetal.,2017]totheHOLEmodel. Wealso\n",
      "TheresultsinSection3suggestthatthechoiceofscor- showed experimentally that the difference between\n",
      "ingfunction,i.e.,COMPLEXorHOLE,doesnotaffect thereportedresultsofthetwomodelswasduetothe\n",
      "the predictive abilities of the model. An additional useofdifferentlossfunctions,andspecificallythatthe\n",
      "importantquestioniswhetheroneofthemodels—in log-likelihoodlosscanproducealargeimprovement\n",
      "practice—isbettersuitedformodelingcertaintypes ofpredictiveperformancesoverthemoreoftenused\n",
      "of relations. In particular, for symmetric relations, marginloss. WehavealsoshownthatComplexand\n",
      "HOLEneedstolearnembeddingsforwhichtheimag- Holographicembeddingscanbetrainedequallywell\n",
      "inarypartaftertheDFTisclosetozero. COMPLEX, onsymmetricandantisymmetricpatterns. Allthese\n",
      "ontheotherhand,canlearnsuchrepresentationseas- thingsbeingequal,aninterestingquestionisthenin\n",
      "ilyasitoperatesdirectlyinthecomplexdomain. The which settings one of the two models is preferable.\n",
      "questionwhetherthisdifferenceinmodelstranslates Complexembeddingshaveanadvantageintermsof\n",
      "todifferencesinpracticeaffectsthelearningofboth timecomplexityastheyscalelinearlywiththeembed-\n",
      "symmetric and antisymmetric relations. Relations ding dimension, whereas Holographic embeddings\n",
      "WN18 FB15K\n",
      "MRR Hitsat MRR Hitsat\n",
      "Loss Filtered Raw 1 3 10 Filtered Raw 1 3 10\n",
      "Margin 0.938 0.605 0.932 0.942 0.949 0.541 0.298 0.411 0.627 0.757\n",
      "Neg-LL 0.941 0.587 0.936 0.945 0.947 0.639 0.250 0.523 0.725 0.825\n",
      "Table1: Filteredandrawmeanreciprocalrank(MRR),Hits@Nmetricsarefiltered,forthepairwisemax-margin\n",
      "lossandthenegativelog-likelihoodonWN18andFB15Kdatasets.\n",
      "Figure1: Averageprecision(AP)foreachfactorizationrankrangingfrom1to50fortheHolEandComplEx\n",
      "scoringfunctions,withlog-likelihoodloss. Learningisperformedjointlyonthesymmetricrelationandonthe\n",
      "antisymmetricrelation. Top-left: APoverthesymmetricrelationonly. Top-right: APovertheantisymmetric\n",
      "relationonly. Bottom: OverallAP.\n",
      "scale quasilinearly. An advantage of Holographic Riedeletal.,2013]. Aninterestingdirectionoffuture\n",
      "embeddingshoweveristhattheembeddingsremain workisthereforeamoredetailedstudyoflossfunc-\n",
      "strictlyintherealdomain,whichmakesiteasierfor tionsforknowledgegraphembeddings—especially\n",
      "themtobeusedinotherreal-valuedmachinelearn- inlightofthehighlyskewedlabeldistributionandthe\n",
      "ing models. In contrast, Complex embeddings can open-worldassumptionwhicharecharacteristicfor\n",
      "noteasilybetransformedtoreal-valuedvectorsand knowledgegraphsbutunusualforstandardmachine\n",
      "used without loss of information—i.e. the specific learningsettings.\n",
      "waytherealandimaginarypartsinteractinalgebraic\n",
      "operations. Complex-valuedmodelsinwhichCom- Acknowledgments\n",
      "plexembeddingscanbedirectlyinputareemerging\n",
      "This work was supported in part by the Associa-\n",
      "inmachinelearning[Trabelsietal.,2017;Danihelka\n",
      "tionNationaledelaRechercheetdelaTechnologie\n",
      "et al., 2016], but this path is yet to be explored for\n",
      "throughtheCIFREgrant2014/0121.\n",
      "otherrelationallearningproblems. Hence,ifthetask\n",
      "of interest is link prediction, Complex embeddings\n",
      "A ProofofEquivalence\n",
      "offeranimprovedruntimecomplexityintheorderof\n",
      "O(logK).Iftheembeddingsshouldbeusedinfurther Inthissection,weprovidethefullprooffortheequiv-\n",
      "machinelearningmodels,e.g.forentityclassification, alenceofbothmodels. Notethatasimilarproofhas\n",
      "Holographicembeddingsprovidebettercompatibility recentlybeenderivedbyHayashiandShimbo[2017].\n",
      "withexistingreal-valuedmethods. We start from Equation (5) and show that there\n",
      "alwaysexistscorrespondingreal-valuedholographic\n",
      "Furthermore,whilethechoiceofthelossisoflittle embeddingsandcomplexembeddingssuchthatthe\n",
      "consequenceontheWN18dataset,ourexperiments scoring functions of HOLE and COMPLEX are di-\n",
      "showed that the log-likelihood loss performed sig- rectlyproportional,i.e. theyaremathematicallyequal\n",
      "nificantlybetteronFB15K.Whilemuchresearchat- up to a constant multiplier a ∈ R: φh = aφc.\n",
      "pso pso\n",
      "tention has been given to scoring functions in link Thekeyideaisinshowingthatthesymmetrystruc-\n",
      "prediction,littlehasbeensaidaboutthelosses,and ture of vectors resulting from Fourier transform of\n",
      "the max-margin loss has been used in most of the real-valuedvectorsissuchthat,thetrilinearproduct\n",
      "existingwork[Bordesetal.,2013;Yangetal.,2015; betweenthesestructuredvectorsisactuallyequalto\n",
      "keepingtherealpartofthetrilinearproductoftheir vector x ∈RK:\n",
      "firsthalf.\n",
      "(cid:40)\n",
      "[s(x) x(cid:48) t(x) x(cid:48) ], ifK iseven,\n",
      "F(x)= ← (12)\n",
      "First, we derive a property of the DFT on real [s(x) x(cid:48) x(cid:48) ], ifK isodd.\n",
      "←\n",
      "vectors x, showing that the resulting complex vec-\n",
      "t jo ∈r {F 1(,x.)..h,Kas −a 1p }a :rtially symmetric structure, for where x(cid:48),x ←(cid:48) ∈ C(cid:100)K 2(cid:101)−1, with x(cid:48) =\n",
      "[F(x),...,F(x) ], and x(cid:48) is x(cid:48) in reversed\n",
      "1 (cid:100)K(cid:101)−1 ←\n",
      "F(x)\n",
      "(K−j)\n",
      "=K (cid:213)−1\n",
      "x ke−2iπ(K−j) Kk\n",
      "ord Wer e: cx a←(cid:48) nt= he[ nF d( ex ri) v(cid:100)2 eK\n",
      "2\n",
      "E(cid:101) q− u1, a. ti. o. n,F (6( )x f) o1 r] r.\n",
      ",e,e ∈RK,\n",
      "p s o\n",
      "k=0\n",
      "firstwithK beingodd:\n",
      "K−1\n",
      "= (cid:213) x ke−2iπke2iπj Kk 1 (cid:68) (cid:69)\n",
      "k=0\n",
      "φh\n",
      "pso\n",
      "=\n",
      "K\n",
      "F(r p),F(e s),F(e o)\n",
      "1 (cid:68) (cid:69)\n",
      "andgiventhatk isaninteger: e−2iπk =1, = [s(r ) r(cid:48) r(cid:48) ],[s(e ) e(cid:48) e(cid:48) ],[s(e ) e(cid:48) e(cid:48) ]\n",
      "K p p p← s s s← o o o←\n",
      "=K (cid:213)−1\n",
      "x ke2iπj Kk\n",
      "=K (cid:213)−1\n",
      "x ke−2iπj Kk\n",
      "= K1 (cid:10) [s(r p) r p(cid:48) r(cid:48) p],[s(e s) e s(cid:48) e s(cid:48)],[s(e o) e o(cid:48) e o(cid:48)](cid:11)\n",
      "k=0 k=0 = 1 (cid:16) s(r )s(e )s(e )+(cid:10) r(cid:48),e(cid:48),e(cid:48)(cid:11)+(cid:10) r(cid:48),e(cid:48),e(cid:48)(cid:11)(cid:17)\n",
      "K p s o p s o p s o\n",
      "andsince x k ∈R, = 1 (cid:16) s(r )s(e )s(e )+(cid:10) r(cid:48),e(cid:48),e(cid:48)(cid:11)+(cid:10) r(cid:48),e(cid:48),e(cid:48)(cid:11)(cid:17)\n",
      "K p s o p s o p s o\n",
      "K−1\n",
      "= (cid:213)\n",
      "k=0\n",
      "x ke−2iπj Kk = F(x) j. (9) = K1 (cid:16) s(r p)s(e s)s(e o)+2Re(cid:16)(cid:10) r p(cid:48),e s(cid:48),e o(cid:48)(cid:11)(cid:17)(cid:17)\n",
      "2\n",
      "(cid:18)(cid:28)(cid:20)(cid:113) (cid:21) (cid:20)(cid:113) (cid:21) (cid:20)(cid:113) (cid:21)(cid:29)(cid:19)\n",
      "= Re 3 1s(r ) r(cid:48), 3 1s(e ) e(cid:48), 3 1s(e ) e(cid:48)\n",
      "K 2 p p 2 s s 2 o o\n",
      "Two special cases arise, the first one is F(x),\n",
      "whichisnotconcernedbytheabovesymmetryprop0\n",
      "- =\n",
      "2 Re(cid:16)(cid:10) r(cid:48)(cid:48),e(cid:48)(cid:48),e(cid:48)(cid:48)(cid:11)(cid:17)\n",
      "K p s o\n",
      "erty:\n",
      "2\n",
      "= φc (13)\n",
      "K−1 K pso\n",
      "F(x)\n",
      "0\n",
      "= (cid:213) x ke−2iπ0 Kk\n",
      "k=0 wherer p(cid:48)(cid:48),e s(cid:48)(cid:48),e o(cid:48)(cid:48) ∈ C(cid:100)K 2(cid:101). Thederivationissimilar\n",
      "K−1 when K is even, with double prime vectors being\n",
      "= (cid:213) x k =: s(x)∈R. x(cid:48)(cid:48) =[(cid:113) 3 1s(x) (cid:113) 3 1t(x) x(cid:48)]∈CK 2+1.\n",
      "k=0 2 2\n",
      "As mentioned in Section 2, the complex vectors\n",
      "(10)\n",
      "r r(cid:48)(cid:48),e s(cid:48)(cid:48),e o(cid:48)(cid:48) ∈ C(cid:100)K 2(cid:101) equivalent to the real vectors\n",
      "r,e,e ∈ RK are twice smaller, but take twice as\n",
      "AndthesecondoneisF(x) whenK iseven: p s o\n",
      "K\n",
      "2 muchmemoryasreal-valuedonesofsamesizeata\n",
      "given floating-point precision. Both models hence\n",
      "F(x) = F(x) = F(x)\n",
      "(K−K) K K havetheexactsamememorycomplexity.\n",
      "2 2 2\n",
      "K−1 K−1\n",
      "= (cid:213) x ke−2iπK 2Kk = (cid:213) x ke−iπk\n",
      "References\n",
      "k=0 k=0\n",
      "K (cid:213)2−1 Bordes,A.;Usunier,N.;Garcia-Duran,A.;Weston,\n",
      "= x 2k −x 2k+1 =:t(x)∈R. J.;andYakhnenko,O. 2013. Translatingembed-\n",
      "k=0 dings for modeling multi-relational data. In Ad-\n",
      "(11) vancesinNeuralInformationProcessingSystems,\n",
      "2787–2795.\n",
      "FromEquations(9)to(11),wewritethegeneral Bordes, A.; Glorot, X.; Weston, J.; and Bengio, Y.\n",
      "form of the Fourier transform F(x) ∈ CK of a real 2014. A semantic matching energy function for\n",
      "learningwithmulti-relationaldata. MachineLearn-\n",
      "ing94(2):233–259.\n",
      "Danihelka,I.;Wayne,G.;Uria,B.;Kalchbrenner,N.;\n",
      "andGraves,A. 2016. Associativelongshort-term\n",
      "memory. arXivpreprintarXiv:1602.03032.\n",
      "Duchi, J.; Hazan, E.; and Singer, Y. 2011. Adap-\n",
      "tivesubgradientmethodsforonlinelearningand\n",
      "stochasticoptimization. JournalofMachineLearn-\n",
      "ingResearch12:2121–2159.\n",
      "Hayashi,K.,andShimbo,M. 2017. Ontheequiva-\n",
      "lenceofholographicandcomplexembeddingsfor\n",
      "linkprediction. arXivpreprintarXiv:1702.05563.\n",
      "Nickel, M.; Rosasco, L.; and Poggio, T. A. 2016.\n",
      "Holographicembeddingsofknowledgegraphs. In\n",
      "AAAIConferenceonArtificialIntelligence,1955–\n",
      "1961.\n",
      "Riedel,S.;Yao,L.;McCallum,A.;andMarlin,B.M.\n",
      "2013. Relation extraction with matrix factoriza-\n",
      "tionanduniversalschemas. InHumanLanguage\n",
      "Technologies: ConferenceoftheNorthAmerican\n",
      "ChapteroftheAssociationofComputationalLin-\n",
      "guistics,74–84.\n",
      "Trabelsi, C.; Bilaniuk, O.; Serdyuk, D.; Subrama-\n",
      "nian,S.;Santos,J.F.;Mehri,S.;Rostamzadeh,N.;\n",
      "Bengio, Y.; and Pal, C. J. 2017. Deep complex\n",
      "networks. arXivpreprintarXiv:1705.09792.\n",
      "Trouillon,T.;Welbl,J.;Riedel,S.;Gaussier,E.;and\n",
      "Bouchard, G. 2016. Complex embeddings for\n",
      "simplelinkprediction. InInternationalConference\n",
      "onMachineLearning,volume48,2071–2080.\n",
      "Trouillon, T.; Dance, C. R.; Welbl, J.; Riedel, S.;\n",
      "Gaussier,É.;andBouchard,G. 2017. Knowledge\n",
      "graphcompletionviacomplextensorfactorization.\n",
      "arXivpreprintarXiv:1702.06879,toappearinthe\n",
      "JournalofMachineLearningResearch.\n",
      "Yang,B.;Yih,W.-T.;He,X.;Gao,J.;andDeng,L.\n",
      "2015.Embeddingentitiesandrelationsforlearning\n",
      "andinferenceinknowledgebases. InInternational\n",
      "ConferenceonLearningRepresentations.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,     42,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['WN18', 'FB15K']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Complex and Holographic Embeddings of Knowledge Graphs:\n",
      "A Comparison\n",
      "ThéoTrouillon MaximilianNickel\n",
      "Univ. GrenobleAlpes∗ FacebookAIResearch\n",
      "theo.trouillon@gmail.com MassachusettsInstituteofTechnology,LCSL\n",
      "maxn@fb.com\n",
      "1 Introduction Forknowledgegraphswithalargenumberofpossible\n",
      "triplesweemploynegativesamplingasproposedby Embeddingsofknowledgegraphshavereceivedsig-\n",
      "Bordes et al. [2013]. The objective of knowledge\n",
      "nificantattentionduetotheirexcellentperformance\n",
      "graphcompletionisthentolearnascoringfunction\n",
      "fortaskslikelinkpredictionandentityresolution. In\n",
      "φ :R×E×E →R for any s,o ∈ E and p ∈ R\n",
      "this short paper, we are providing a comparison of pso\n",
      "whichpredictsthetruthvalueofpossibletriples. We\n",
      "twostate-of-the-artknowledgegraphembeddingsfor\n",
      "willwriteN = |E|andN = |R|.\n",
      "whichtheirequivalencehasrecentlybeenestablished, e r\n",
      "For notational convenience, we define the trilinear\n",
      "i.e.,COMPLEXandHOLE[Nickel,Rosasco,andPog-\n",
      "productofthreecomplexvectorsas:\n",
      "gio,2016;Trouillonetal.,2016;HayashiandShimbo,\n",
      "2017]. First,webrieflyreviewbothmodelsanddis-\n",
      "K\n",
      "cusshowtheirscoringfunctionsareequivalent. We (cid:104)a,b,c(cid:105) =(cid:213) a b c =a(cid:62)(b(cid:12)c)\n",
      "j j j\n",
      "thenanalyzethediscrepancyofresultsreportedinthe\n",
      "j=1\n",
      "originalarticles,andshowexperimentallythatthey\n",
      "arelikelyduetotheuseofdifferentlossfunctions. In where a,b,c ∈ CK, and (cid:12) denotes the Hadamard\n",
      "furtherexperiments,weevaluatetheabilityofboth product,i.e. theelement-wiseproductbetweentwo\n",
      "modelstoembedsymmetricandantisymmetricpat- vectorsofsamelength.\n",
      "terns. Finally,wediscussadvantagesanddisadvan- In the following, we will consider the discrete\n",
      "tagesofbothmodelsandunderwhichconditionsone Fouriertransform(DFT)ofpurelyrealvectorsonly:\n",
      "wouldbepreferabletotheother. F :RK →CK. For j ∈ {0,...,K−1}:\n",
      "2 EquivalenceofComplexand K−1\n",
      "HolographicEmbeddings\n",
      "F(x)\n",
      "j\n",
      "= (cid:213) x ke−2iπj Kk (1)\n",
      "k=0\n",
      "Inthissection,wewillbrieflyreviewHolographicand\n",
      "Complexembeddingsanddiscusstheequivalenceof whereF(x) j ∈Cisthe jthvalueintheresultingcom-\n",
      "theirscoringfunctions. plexvectorF(x)∈CK. Notethatthecomponentsin\n",
      "Let G = (E,R,T) be a knowledge graph, which Equation(1)areindexedfrom0toK−1.\n",
      "consistsofentitiesE,relationtypesR andobserved\n",
      "triples T ⊆ R × E × E. Furthermore, let D be a HolographicEmbeddings\n",
      "trainingset,whichassociateswitheachpossibletriple Theholographicembeddingsmodel(HOLE)[Nickel,\n",
      "inGitstruthvalues y ∈ {±1}. Thatis,forapossible Rosasco,andPoggio,2016]representsrelationsand\n",
      "triple(p,s,o)withs,o∈ E andp∈ R itholdsthat entities with real-valued embeddings E ∈ RNe×K,\n",
      "R ∈ RNr×K,andscoresatriple(p,s,o)withthedot\n",
      "(cid:40)\n",
      "+1, if(p,s,o)∈ T productbetweentheembeddingoftherelationpand\n",
      "y =\n",
      "pso −1, otherwise. the circular correlation(cid:63) : RK ×RK → RK of the\n",
      "embeddingsofentitiessando:\n",
      "∗WorkalsodonewhileatXeroxResearchCentreEu-\n",
      "rope. φh\n",
      "pso\n",
      "=r p(cid:62)(e s(cid:63)e o). (2)\n",
      "7102\n",
      "luJ\n",
      "32\n",
      "]GL.sc[\n",
      "2v57410.7071:viXra\n",
      "Thecircularcorrelationcanbewrittenwiththedis- into the frequency domain and back. However, to\n",
      "creteFouriertransform(DFT), ensurethatthetrilinearproductofthesecomplexem-\n",
      "beddingsisarealnumber,wewouldeitherneedto\n",
      "e s(cid:63)e\n",
      "o\n",
      "= F−1(F(e s)(cid:12)F(e o)) (3) enforcethesamesymmetryconstraintsonF(e i)and\n",
      "F(r ) that arise from the DFTs or—alternatively—\n",
      "j\n",
      "where F−1 : CK → CK is theinverseDFT.In this takeonlythereal-valuedpartofthetrilinearproduct.\n",
      "case,theembeddingvectorsarereale s,e o,r p ∈RK, WeshowinAppendixAthatthesearetwowaysof\n",
      "andsoistheresultoftheinverseDFT,sincethecir- performingthesameoperation,henceshowingthat\n",
      "cular correlation of real-valued vectors results in a the scoring functions of COMPLEX and HOLE are\n",
      "real-valuedvector. equivalent—uptoaconstantfactor.\n",
      "Furthermore,bothmodelshaveequalmemorycom-\n",
      "ComplexEmbeddings\n",
      "plexity,astheequivalentcomplexvectorsaretwiceas\n",
      "Thecomplexembeddingsmodel(COMPLEX)[Trouil- small(seeproofinAppendixA)butrequiretwiceas\n",
      "lonetal.,2016,2017]representsrelationsandenti- muchmemoryasreal-valuedonesofsamesize—fora\n",
      "ties with complex-valued embeddings E ∈ CNe×K, givenfloating-pointprecision. However,thecomplex\n",
      "R ∈ CNr×K,andscoresatriple(p,s,o)withthereal formulationofthescoringfunctionreducesthetime\n",
      "partofthetrilinearproductofthecorrespondingem- complexityfromO(Klog(K))(quasilinear)toO(K)\n",
      "beddings: (linear).\n",
      "φc\n",
      "pso\n",
      "=Re(cid:0) (cid:104)r p,e s,e o(cid:105)(cid:1) (4) 3 LossFunctions&PredictiveAbilities\n",
      "wheree,e,r ∈CK arecomplexvectors,ande is TheexperimentalresultsofHOLEandCOMPLEXas\n",
      "s o p o\n",
      "reportedbyNickel,Rosasco,andPoggio[2016]and\n",
      "thecomplexconjugateofthevectore.\n",
      "o\n",
      "Trouillonetal.[2016]agreedontheWN18dataset,\n",
      "butdivergedsignificantlyonFB15K[Bordesetal.,\n",
      "Equivalence\n",
      "2014]—althoughbothscoringfunctionareequivalent.\n",
      "TheequivalenceofHOLEandCOMPLEXhasrecently\n",
      "Sincethemaindifferenceintheexperimentalsettings\n",
      "beenshownbyHayashiandShimbo[2017]. Inthe\n",
      "wastheuseofdifferentlossfunctions—i.e.,margin\n",
      "following,webrieflydiscussthisequivalenceofboth\n",
      "lossversuslogisticloss—weanalyzeinthissection\n",
      "modelsandhowitcanbederived. Forcompleteness,\n",
      "whetherthediscrepancyofresultscanbeattributedto\n",
      "a full proof similar to that of Hayashi and Shimbo\n",
      "thisfact. Forthispurpose,weimplementedbothloss\n",
      "[2017]isincludedinAppendixA. functions for the complex representation φc within\n",
      "First,toderivetheconnectionbetweenHOLEand\n",
      "thesameframework,andcomparedtheresultsonthe\n",
      "COMPLEX,considerParseval’sTheorem:\n",
      "WN18andFB15Kdatasets.\n",
      "Theorem 1. Suppose x,y ∈ RK are real vectors. First,notethatinbothdatasets,onlypositivetrain-\n",
      "Then x(cid:62)y = 1F(x)(cid:62)F(y). ingtriplesareprovided. Negativeexamplesaregener-\n",
      "K\n",
      "atedbycorruptingthesubjectorobjectentityofeach\n",
      "UsingTheorem1aswellasEquations(2)and(3),we\n",
      "positive triple, as described in Bordes et al. [2013].\n",
      "canthenrewritethescoringfunctionofHOLEas:\n",
      "Intheoriginal HOLE publication[Nickel,Rosasco,\n",
      "φh(p,s,o)=r(cid:62)(e (cid:63)e ) (5) and Poggio, 2016], a pairwise margin loss is opti-\n",
      "p s o mizedovereachpositiveanditscorruptednegative\n",
      "=r p(cid:62)(F−1(F(e s)(cid:12)F(e o))) (p,s(cid:48),o(cid:48)):\n",
      "1\n",
      "= KF(r p)(cid:62)F(F−1(F(e s)(cid:12)F(e o))) L(D;Θ)=(cid:213) [γ+σ(φh ps(cid:48)o(cid:48))−σ(φh pso)]+ (7)\n",
      "1 ((p,s,o),y)∈D\n",
      "= F(r )(cid:62)(F(e )(cid:12)F(e ))\n",
      "p s o\n",
      "K\n",
      "where γ is the margin hyperparameter, and σ the\n",
      "1 (cid:68) (cid:69)\n",
      "= F(r p),F(e s),F(e o). (6) standardlogisticfunction. Theentityembeddingsare\n",
      "K\n",
      "also constrained to unit norm : ||e || ≤ 1, for all\n",
      "i 2\n",
      "Hence, for HOLE we could directly learn complex i ∈ E.\n",
      "embeddingse ≡ F(e ),r ≡ F(r ) ∈ Cd insteadof WhereasinTrouillonetal.[2016],thegenerated\n",
      "(cid:98)i i (cid:98)j j\n",
      "learningembeddingse,r ∈ Rd andmappingthem negativesaremergedintothetrainingsetD ateach\n",
      "i j\n",
      "batchsampling,andthelog-likelihoodisoptimized p∈ Raresymmetricwhentripleshavethesametruth\n",
      "withL2regularization: valuebypermutationofthesubjectandobjectenti-\n",
      "ties: y = y for all s,o ∈ E, whereas facts of\n",
      "(cid:213) pso pos\n",
      "L(D;Θ)= log(1+exp(−yφc ))+λ||Θ||2. (8) antisymmetricrelations phaveinversetruthvalues:\n",
      "pso 2\n",
      "((p,s,o),y)∈D y pso = −y pos. Toevaluatethisquestionexperimen-\n",
      "tally, we reproduced the joint learning of synthetic\n",
      "Optimizationisconductedwithstochasticgradient symmetricandantisymmetricrelationsdescribedin\n",
      "descent,AdaGrad[Duchi,Hazan,andSinger,2011], Trouillonetal.[2016]onbothscoringfunctions. We\n",
      "and early stopping, as described in Trouillon et al. used the log-likelihood loss as all negatives are ob-\n",
      "[2016].Asinglecorruptednegativetripleisgenerated served.\n",
      "for each positive training triple. The results are re- Wegeneratedrandomlya50×50symmetricma-\n",
      "portedforthebestvalidatedmodelsaftergrid-search trix,anda50×50antisymmetricmatrix. Jointly,they\n",
      "onthefollowingvalues: K ∈ {10,20,50,100,150, representa2×50×50tensor. Toensurethatalltest\n",
      "200},λ ∈ {0.1,0.03,0.01,0.003,0.001,0.0003,0.0}\n",
      "valuesarepredictable,theuppertriangularpartsofthe\n",
      "forthelog-likelihoodloss,andγ ∈ {0.1,0.2,0.3,0.4, matricesarealwayskeptinthetrainingset,andthe\n",
      "0.5,0.6,0.7,0.8,0.9,1.0} forthemax-marginloss. diagonalsareunobserved. Weconducted5-foldcross-\n",
      "Therawandfilteredmeanreciprocalranks(MRR), validationonthelower-triangularmatrices,usingthe\n",
      "aswellasthefilteredhitsat1,3and10arereported upper-triangularpartsplus3foldsfortraining, one\n",
      "inTable1. foldforvalidationandonefoldfortesting. Thereg-\n",
      "The margin loss results are consistent with the ularizationparameterλisvalidatedamongthesame\n",
      "HOLE ones originally reported in Nickel, Rosasco, valuesasinthepreviousexperiment.\n",
      "andPoggio[2016],whichconfirmstheequivalence Figure 1 shows the best cross-validated average\n",
      "ofthescoringfunctions,andsupportsthehypothesis precision(areaundertheprecision-recallcurve)for\n",
      "thatthelosswasresponsibleforthedifferenceinpre- thetwoscoringfunctionsforranksrangingupto50.\n",
      "viously reported results. The log-likelihood results Both models manage to perfectly model symmetry\n",
      "are also coherent, as one must note that the higher andantisymmetry. AstheComplExmodelhastwice\n",
      "scoresreportedonFB15KinTrouillonetal.[2016] has many parameters for a given rank, it reaches a\n",
      "areduetotheuseofmorethanonegeneratednega- perfectaverageprecisionwithatwicesmallerrank.\n",
      "tivesampleforeachpositivetrainingtriple. Here,we Thisconfirmsthattherepresentationofthescoring\n",
      "generatedasinglenegativesampleforeachpositive functiondoesnotaffectthelearningabilitiesofthe\n",
      "oneinordertokeepthecomparisonfairbetweenthe modelsinpractice.\n",
      "two losses. The max-margin loss achieves a better\n",
      "raw MRR (rankings without removing the training 5 Discussion\n",
      "samples) on both datasets, but much worse filtered\n",
      "Wehavedemonstratedthatthescoringfunctionsof\n",
      "metrics on FB15K, suggesting that this loss can be\n",
      "morepronetooverfitting.\n",
      "the HOLE and COMPLEX models are directly pro-\n",
      "portional. Thishenceextendstheexistenceproperty\n",
      "oftheCOMPLEXmodeloverallknowledgegraphs\n",
      "4 ScoringFunction&Symmetry\n",
      "[Trouillonetal.,2017]totheHOLEmodel. Wealso\n",
      "TheresultsinSection3suggestthatthechoiceofscor- showed experimentally that the difference between\n",
      "ingfunction,i.e.,COMPLEXorHOLE,doesnotaffect thereportedresultsofthetwomodelswasduetothe\n",
      "the predictive abilities of the model. An additional useofdifferentlossfunctions,andspecificallythatthe\n",
      "importantquestioniswhetheroneofthemodels—in log-likelihoodlosscanproducealargeimprovement\n",
      "practice—isbettersuitedformodelingcertaintypes ofpredictiveperformancesoverthemoreoftenused\n",
      "of relations. In particular, for symmetric relations, marginloss. WehavealsoshownthatComplexand\n",
      "HOLEneedstolearnembeddingsforwhichtheimag- Holographicembeddingscanbetrainedequallywell\n",
      "inarypartaftertheDFTisclosetozero. COMPLEX, onsymmetricandantisymmetricpatterns. Allthese\n",
      "ontheotherhand,canlearnsuchrepresentationseas- thingsbeingequal,aninterestingquestionisthenin\n",
      "ilyasitoperatesdirectlyinthecomplexdomain. The which settings one of the two models is preferable.\n",
      "questionwhetherthisdifferenceinmodelstranslates Complexembeddingshaveanadvantageintermsof\n",
      "todifferencesinpracticeaffectsthelearningofboth timecomplexityastheyscalelinearlywiththeembed-\n",
      "symmetric and antisymmetric relations. Relations ding dimension, whereas Holographic embeddings\n",
      "WN18 FB15K\n",
      "MRR Hitsat MRR Hitsat\n",
      "Loss Filtered Raw 1 3 10 Filtered Raw 1 3 10\n",
      "Margin 0.938 0.605 0.932 0.942 0.949 0.541 0.298 0.411 0.627 0.757\n",
      "Neg-LL 0.941 0.587 0.936 0.945 0.947 0.639 0.250 0.523 0.725 0.825\n",
      "Table1: Filteredandrawmeanreciprocalrank(MRR),Hits@Nmetricsarefiltered,forthepairwisemax-margin\n",
      "lossandthenegativelog-likelihoodonWN18andFB15Kdatasets.\n",
      "Figure1: Averageprecision(AP)foreachfactorizationrankrangingfrom1to50fortheHolEandComplEx\n",
      "scoringfunctions,withlog-likelihoodloss. Learningisperformedjointlyonthesymmetricrelationandonthe\n",
      "antisymmetricrelation. Top-left: APoverthesymmetricrelationonly. Top-right: APovertheantisymmetric\n",
      "relationonly. Bottom: OverallAP.\n",
      "scale quasilinearly. An advantage of Holographic Riedeletal.,2013]. Aninterestingdirectionoffuture\n",
      "embeddingshoweveristhattheembeddingsremain workisthereforeamoredetailedstudyoflossfunc-\n",
      "strictlyintherealdomain,whichmakesiteasierfor tionsforknowledgegraphembeddings—especially\n",
      "themtobeusedinotherreal-valuedmachinelearn- inlightofthehighlyskewedlabeldistributionandthe\n",
      "ing models. In contrast, Complex embeddings can open-worldassumptionwhicharecharacteristicfor\n",
      "noteasilybetransformedtoreal-valuedvectorsand knowledgegraphsbutunusualforstandardmachine\n",
      "used without loss of information—i.e. the specific learningsettings.\n",
      "waytherealandimaginarypartsinteractinalgebraic\n",
      "operations. Complex-valuedmodelsinwhichCom- Acknowledgments\n",
      "plexembeddingscanbedirectlyinputareemerging\n",
      "This work was supported in part by the Associa-\n",
      "inmachinelearning[Trabelsietal.,2017;Danihelka\n",
      "tionNationaledelaRechercheetdelaTechnologie\n",
      "et al., 2016], but this path is yet to be explored for\n",
      "throughtheCIFREgrant2014/0121.\n",
      "otherrelationallearningproblems. Hence,ifthetask\n",
      "of interest is link prediction, Complex embeddings\n",
      "A ProofofEquivalence\n",
      "offeranimprovedruntimecomplexityintheorderof\n",
      "O(logK).Iftheembeddingsshouldbeusedinfurther Inthissection,weprovidethefullprooffortheequiv-\n",
      "machinelearningmodels,e.g.forentityclassification, alenceofbothmodels. Notethatasimilarproofhas\n",
      "Holographicembeddingsprovidebettercompatibility recentlybeenderivedbyHayashiandShimbo[2017].\n",
      "withexistingreal-valuedmethods. We start from Equation (5) and show that there\n",
      "alwaysexistscorrespondingreal-valuedholographic\n",
      "Furthermore,whilethechoiceofthelossisoflittle embeddingsandcomplexembeddingssuchthatthe\n",
      "consequenceontheWN18dataset,ourexperiments scoring functions of HOLE and COMPLEX are di-\n",
      "showed that the log-likelihood loss performed sig- rectlyproportional,i.e. theyaremathematicallyequal\n",
      "nificantlybetteronFB15K.Whilemuchresearchat- up to a constant multiplier a ∈ R: φh = aφc.\n",
      "pso pso\n",
      "tention has been given to scoring functions in link Thekeyideaisinshowingthatthesymmetrystruc-\n",
      "prediction,littlehasbeensaidaboutthelosses,and ture of vectors resulting from Fourier transform of\n",
      "the max-margin loss has been used in most of the real-valuedvectorsissuchthat,thetrilinearproduct\n",
      "existingwork[Bordesetal.,2013;Yangetal.,2015; betweenthesestructuredvectorsisactuallyequalto\n",
      "keepingtherealpartofthetrilinearproductoftheir vector x ∈RK:\n",
      "firsthalf.\n",
      "(cid:40)\n",
      "[s(x) x(cid:48) t(x) x(cid:48) ], ifK iseven,\n",
      "F(x)= ← (12)\n",
      "First, we derive a property of the DFT on real [s(x) x(cid:48) x(cid:48) ], ifK isodd.\n",
      "←\n",
      "vectors x, showing that the resulting complex vec-\n",
      "t jo ∈r {F 1(,x.)..h,Kas −a 1p }a :rtially symmetric structure, for where x(cid:48),x ←(cid:48) ∈ C(cid:100)K 2(cid:101)−1, with x(cid:48) =\n",
      "[F(x),...,F(x) ], and x(cid:48) is x(cid:48) in reversed\n",
      "1 (cid:100)K(cid:101)−1 ←\n",
      "F(x)\n",
      "(K−j)\n",
      "=K (cid:213)−1\n",
      "x ke−2iπ(K−j) Kk\n",
      "ord Wer e: cx a←(cid:48) nt= he[ nF d( ex ri) v(cid:100)2 eK\n",
      "2\n",
      "E(cid:101) q− u1, a. ti. o. n,F (6( )x f) o1 r] r.\n",
      ",e,e ∈RK,\n",
      "p s o\n",
      "k=0\n",
      "firstwithK beingodd:\n",
      "K−1\n",
      "= (cid:213) x ke−2iπke2iπj Kk 1 (cid:68) (cid:69)\n",
      "k=0\n",
      "φh\n",
      "pso\n",
      "=\n",
      "K\n",
      "F(r p),F(e s),F(e o)\n",
      "1 (cid:68) (cid:69)\n",
      "andgiventhatk isaninteger: e−2iπk =1, = [s(r ) r(cid:48) r(cid:48) ],[s(e ) e(cid:48) e(cid:48) ],[s(e ) e(cid:48) e(cid:48) ]\n",
      "K p p p← s s s← o o o←\n",
      "=K (cid:213)−1\n",
      "x ke2iπj Kk\n",
      "=K (cid:213)−1\n",
      "x ke−2iπj Kk\n",
      "= K1 (cid:10) [s(r p) r p(cid:48) r(cid:48) p],[s(e s) e s(cid:48) e s(cid:48)],[s(e o) e o(cid:48) e o(cid:48)](cid:11)\n",
      "k=0 k=0 = 1 (cid:16) s(r )s(e )s(e )+(cid:10) r(cid:48),e(cid:48),e(cid:48)(cid:11)+(cid:10) r(cid:48),e(cid:48),e(cid:48)(cid:11)(cid:17)\n",
      "K p s o p s o p s o\n",
      "andsince x k ∈R, = 1 (cid:16) s(r )s(e )s(e )+(cid:10) r(cid:48),e(cid:48),e(cid:48)(cid:11)+(cid:10) r(cid:48),e(cid:48),e(cid:48)(cid:11)(cid:17)\n",
      "K p s o p s o p s o\n",
      "K−1\n",
      "= (cid:213)\n",
      "k=0\n",
      "x ke−2iπj Kk = F(x) j. (9) = K1 (cid:16) s(r p)s(e s)s(e o)+2Re(cid:16)(cid:10) r p(cid:48),e s(cid:48),e o(cid:48)(cid:11)(cid:17)(cid:17)\n",
      "2\n",
      "(cid:18)(cid:28)(cid:20)(cid:113) (cid:21) (cid:20)(cid:113) (cid:21) (cid:20)(cid:113) (cid:21)(cid:29)(cid:19)\n",
      "= Re 3 1s(r ) r(cid:48), 3 1s(e ) e(cid:48), 3 1s(e ) e(cid:48)\n",
      "K 2 p p 2 s s 2 o o\n",
      "Two special cases arise, the first one is F(x),\n",
      "whichisnotconcernedbytheabovesymmetryprop0\n",
      "- =\n",
      "2 Re(cid:16)(cid:10) r(cid:48)(cid:48),e(cid:48)(cid:48),e(cid:48)(cid:48)(cid:11)(cid:17)\n",
      "K p s o\n",
      "erty:\n",
      "2\n",
      "= φc (13)\n",
      "K−1 K pso\n",
      "F(x)\n",
      "0\n",
      "= (cid:213) x ke−2iπ0 Kk\n",
      "k=0 wherer p(cid:48)(cid:48),e s(cid:48)(cid:48),e o(cid:48)(cid:48) ∈ C(cid:100)K 2(cid:101). Thederivationissimilar\n",
      "K−1 when K is even, with double prime vectors being\n",
      "= (cid:213) x k =: s(x)∈R. x(cid:48)(cid:48) =[(cid:113) 3 1s(x) (cid:113) 3 1t(x) x(cid:48)]∈CK 2+1.\n",
      "k=0 2 2\n",
      "As mentioned in Section 2, the complex vectors\n",
      "(10)\n",
      "r r(cid:48)(cid:48),e s(cid:48)(cid:48),e o(cid:48)(cid:48) ∈ C(cid:100)K 2(cid:101) equivalent to the real vectors\n",
      "r,e,e ∈ RK are twice smaller, but take twice as\n",
      "AndthesecondoneisF(x) whenK iseven: p s o\n",
      "K\n",
      "2 muchmemoryasreal-valuedonesofsamesizeata\n",
      "given floating-point precision. Both models hence\n",
      "F(x) = F(x) = F(x)\n",
      "(K−K) K K havetheexactsamememorycomplexity.\n",
      "2 2 2\n",
      "K−1 K−1\n",
      "= (cid:213) x ke−2iπK 2Kk = (cid:213) x ke−iπk\n",
      "References\n",
      "k=0 k=0\n",
      "K (cid:213)2−1 Bordes,A.;Usunier,N.;Garcia-Duran,A.;Weston,\n",
      "= x 2k −x 2k+1 =:t(x)∈R. J.;andYakhnenko,O. 2013. Translatingembed-\n",
      "k=0 dings for modeling multi-relational data. In Ad-\n",
      "(11) vancesinNeuralInformationProcessingSystems,\n",
      "2787–2795.\n",
      "FromEquations(9)to(11),wewritethegeneral Bordes, A.; Glorot, X.; Weston, J.; and Bengio, Y.\n",
      "form of the Fourier transform F(x) ∈ CK of a real 2014. A semantic matching energy function for\n",
      "learningwithmulti-relationaldata. MachineLearn-\n",
      "ing94(2):233–259.\n",
      "Danihelka,I.;Wayne,G.;Uria,B.;Kalchbrenner,N.;\n",
      "andGraves,A. 2016. Associativelongshort-term\n",
      "memory. arXivpreprintarXiv:1602.03032.\n",
      "Duchi, J.; Hazan, E.; and Singer, Y. 2011. Adap-\n",
      "tivesubgradientmethodsforonlinelearningand\n",
      "stochasticoptimization. JournalofMachineLearn-\n",
      "ingResearch12:2121–2159.\n",
      "Hayashi,K.,andShimbo,M. 2017. Ontheequiva-\n",
      "lenceofholographicandcomplexembeddingsfor\n",
      "linkprediction. arXivpreprintarXiv:1702.05563.\n",
      "Nickel, M.; Rosasco, L.; and Poggio, T. A. 2016.\n",
      "Holographicembeddingsofknowledgegraphs. In\n",
      "AAAIConferenceonArtificialIntelligence,1955–\n",
      "1961.\n",
      "Riedel,S.;Yao,L.;McCallum,A.;andMarlin,B.M.\n",
      "2013. Relation extraction with matrix factoriza-\n",
      "tionanduniversalschemas. InHumanLanguage\n",
      "Technologies: ConferenceoftheNorthAmerican\n",
      "ChapteroftheAssociationofComputationalLin-\n",
      "guistics,74–84.\n",
      "Trabelsi, C.; Bilaniuk, O.; Serdyuk, D.; Subrama-\n",
      "nian,S.;Santos,J.F.;Mehri,S.;Rostamzadeh,N.;\n",
      "Bengio, Y.; and Pal, C. J. 2017. Deep complex\n",
      "networks. arXivpreprintarXiv:1705.09792.\n",
      "Trouillon,T.;Welbl,J.;Riedel,S.;Gaussier,E.;and\n",
      "Bouchard, G. 2016. Complex embeddings for\n",
      "simplelinkprediction. InInternationalConference\n",
      "onMachineLearning,volume48,2071–2080.\n",
      "Trouillon, T.; Dance, C. R.; Welbl, J.; Riedel, S.;\n",
      "Gaussier,É.;andBouchard,G. 2017. Knowledge\n",
      "graphcompletionviacomplextensorfactorization.\n",
      "arXivpreprintarXiv:1702.06879,toappearinthe\n",
      "JournalofMachineLearningResearch.\n",
      "Yang,B.;Yih,W.-T.;He,X.;Gao,J.;andDeng,L.\n",
      "2015.Embeddingentitiesandrelationsforlearning\n",
      "andinferenceinknowledgebases. InInternational\n",
      "ConferenceonLearningRepresentations.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  11175,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Link prediction', 'Entity resolution']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Complex and Holographic Embeddings of Knowledge Graphs:\n",
      "A Comparison\n",
      "ThéoTrouillon MaximilianNickel\n",
      "Univ. GrenobleAlpes∗ FacebookAIResearch\n",
      "theo.trouillon@gmail.com MassachusettsInstituteofTechnology,LCSL\n",
      "maxn@fb.com\n",
      "1 Introduction Forknowledgegraphswithalargenumberofpossible\n",
      "triplesweemploynegativesamplingasproposedby Embeddingsofknowledgegraphshavereceivedsig-\n",
      "Bordes et al. [2013]. The objective of knowledge\n",
      "nificantattentionduetotheirexcellentperformance\n",
      "graphcompletionisthentolearnascoringfunction\n",
      "fortaskslikelinkpredictionandentityresolution. In\n",
      "φ :R×E×E →R for any s,o ∈ E and p ∈ R\n",
      "this short paper, we are providing a comparison of pso\n",
      "whichpredictsthetruthvalueofpossibletriples. We\n",
      "twostate-of-the-artknowledgegraphembeddingsfor\n",
      "willwriteN = |E|andN = |R|.\n",
      "whichtheirequivalencehasrecentlybeenestablished, e r\n",
      "For notational convenience, we define the trilinear\n",
      "i.e.,COMPLEXandHOLE[Nickel,Rosasco,andPog-\n",
      "productofthreecomplexvectorsas:\n",
      "gio,2016;Trouillonetal.,2016;HayashiandShimbo,\n",
      "2017]. First,webrieflyreviewbothmodelsanddis-\n",
      "K\n",
      "cusshowtheirscoringfunctionsareequivalent. We (cid:104)a,b,c(cid:105) =(cid:213) a b c =a(cid:62)(b(cid:12)c)\n",
      "j j j\n",
      "thenanalyzethediscrepancyofresultsreportedinthe\n",
      "j=1\n",
      "originalarticles,andshowexperimentallythatthey\n",
      "arelikelyduetotheuseofdifferentlossfunctions. In where a,b,c ∈ CK, and (cid:12) denotes the Hadamard\n",
      "furtherexperiments,weevaluatetheabilityofboth product,i.e. theelement-wiseproductbetweentwo\n",
      "modelstoembedsymmetricandantisymmetricpat- vectorsofsamelength.\n",
      "terns. Finally,wediscussadvantagesanddisadvan- In the following, we will consider the discrete\n",
      "tagesofbothmodelsandunderwhichconditionsone Fouriertransform(DFT)ofpurelyrealvectorsonly:\n",
      "wouldbepreferabletotheother. F :RK →CK. For j ∈ {0,...,K−1}:\n",
      "2 EquivalenceofComplexand K−1\n",
      "HolographicEmbeddings\n",
      "F(x)\n",
      "j\n",
      "= (cid:213) x ke−2iπj Kk (1)\n",
      "k=0\n",
      "Inthissection,wewillbrieflyreviewHolographicand\n",
      "Complexembeddingsanddiscusstheequivalenceof whereF(x) j ∈Cisthe jthvalueintheresultingcom-\n",
      "theirscoringfunctions. plexvectorF(x)∈CK. Notethatthecomponentsin\n",
      "Let G = (E,R,T) be a knowledge graph, which Equation(1)areindexedfrom0toK−1.\n",
      "consistsofentitiesE,relationtypesR andobserved\n",
      "triples T ⊆ R × E × E. Furthermore, let D be a HolographicEmbeddings\n",
      "trainingset,whichassociateswitheachpossibletriple Theholographicembeddingsmodel(HOLE)[Nickel,\n",
      "inGitstruthvalues y ∈ {±1}. Thatis,forapossible Rosasco,andPoggio,2016]representsrelationsand\n",
      "triple(p,s,o)withs,o∈ E andp∈ R itholdsthat entities with real-valued embeddings E ∈ RNe×K,\n",
      "R ∈ RNr×K,andscoresatriple(p,s,o)withthedot\n",
      "(cid:40)\n",
      "+1, if(p,s,o)∈ T productbetweentheembeddingoftherelationpand\n",
      "y =\n",
      "pso −1, otherwise. the circular correlation(cid:63) : RK ×RK → RK of the\n",
      "embeddingsofentitiessando:\n",
      "∗WorkalsodonewhileatXeroxResearchCentreEu-\n",
      "rope. φh\n",
      "pso\n",
      "=r p(cid:62)(e s(cid:63)e o). (2)\n",
      "7102\n",
      "luJ\n",
      "32\n",
      "]GL.sc[\n",
      "2v57410.7071:viXra\n",
      "Thecircularcorrelationcanbewrittenwiththedis- into the frequency domain and back. However, to\n",
      "creteFouriertransform(DFT), ensurethatthetrilinearproductofthesecomplexem-\n",
      "beddingsisarealnumber,wewouldeitherneedto\n",
      "e s(cid:63)e\n",
      "o\n",
      "= F−1(F(e s)(cid:12)F(e o)) (3) enforcethesamesymmetryconstraintsonF(e i)and\n",
      "F(r ) that arise from the DFTs or—alternatively—\n",
      "j\n",
      "where F−1 : CK → CK is theinverseDFT.In this takeonlythereal-valuedpartofthetrilinearproduct.\n",
      "case,theembeddingvectorsarereale s,e o,r p ∈RK, WeshowinAppendixAthatthesearetwowaysof\n",
      "andsoistheresultoftheinverseDFT,sincethecir- performingthesameoperation,henceshowingthat\n",
      "cular correlation of real-valued vectors results in a the scoring functions of COMPLEX and HOLE are\n",
      "real-valuedvector. equivalent—uptoaconstantfactor.\n",
      "Furthermore,bothmodelshaveequalmemorycom-\n",
      "ComplexEmbeddings\n",
      "plexity,astheequivalentcomplexvectorsaretwiceas\n",
      "Thecomplexembeddingsmodel(COMPLEX)[Trouil- small(seeproofinAppendixA)butrequiretwiceas\n",
      "lonetal.,2016,2017]representsrelationsandenti- muchmemoryasreal-valuedonesofsamesize—fora\n",
      "ties with complex-valued embeddings E ∈ CNe×K, givenfloating-pointprecision. However,thecomplex\n",
      "R ∈ CNr×K,andscoresatriple(p,s,o)withthereal formulationofthescoringfunctionreducesthetime\n",
      "partofthetrilinearproductofthecorrespondingem- complexityfromO(Klog(K))(quasilinear)toO(K)\n",
      "beddings: (linear).\n",
      "φc\n",
      "pso\n",
      "=Re(cid:0) (cid:104)r p,e s,e o(cid:105)(cid:1) (4) 3 LossFunctions&PredictiveAbilities\n",
      "wheree,e,r ∈CK arecomplexvectors,ande is TheexperimentalresultsofHOLEandCOMPLEXas\n",
      "s o p o\n",
      "reportedbyNickel,Rosasco,andPoggio[2016]and\n",
      "thecomplexconjugateofthevectore.\n",
      "o\n",
      "Trouillonetal.[2016]agreedontheWN18dataset,\n",
      "butdivergedsignificantlyonFB15K[Bordesetal.,\n",
      "Equivalence\n",
      "2014]—althoughbothscoringfunctionareequivalent.\n",
      "TheequivalenceofHOLEandCOMPLEXhasrecently\n",
      "Sincethemaindifferenceintheexperimentalsettings\n",
      "beenshownbyHayashiandShimbo[2017]. Inthe\n",
      "wastheuseofdifferentlossfunctions—i.e.,margin\n",
      "following,webrieflydiscussthisequivalenceofboth\n",
      "lossversuslogisticloss—weanalyzeinthissection\n",
      "modelsandhowitcanbederived. Forcompleteness,\n",
      "whetherthediscrepancyofresultscanbeattributedto\n",
      "a full proof similar to that of Hayashi and Shimbo\n",
      "thisfact. Forthispurpose,weimplementedbothloss\n",
      "[2017]isincludedinAppendixA. functions for the complex representation φc within\n",
      "First,toderivetheconnectionbetweenHOLEand\n",
      "thesameframework,andcomparedtheresultsonthe\n",
      "COMPLEX,considerParseval’sTheorem:\n",
      "WN18andFB15Kdatasets.\n",
      "Theorem 1. Suppose x,y ∈ RK are real vectors. First,notethatinbothdatasets,onlypositivetrain-\n",
      "Then x(cid:62)y = 1F(x)(cid:62)F(y). ingtriplesareprovided. Negativeexamplesaregener-\n",
      "K\n",
      "atedbycorruptingthesubjectorobjectentityofeach\n",
      "UsingTheorem1aswellasEquations(2)and(3),we\n",
      "positive triple, as described in Bordes et al. [2013].\n",
      "canthenrewritethescoringfunctionofHOLEas:\n",
      "Intheoriginal HOLE publication[Nickel,Rosasco,\n",
      "φh(p,s,o)=r(cid:62)(e (cid:63)e ) (5) and Poggio, 2016], a pairwise margin loss is opti-\n",
      "p s o mizedovereachpositiveanditscorruptednegative\n",
      "=r p(cid:62)(F−1(F(e s)(cid:12)F(e o))) (p,s(cid:48),o(cid:48)):\n",
      "1\n",
      "= KF(r p)(cid:62)F(F−1(F(e s)(cid:12)F(e o))) L(D;Θ)=(cid:213) [γ+σ(φh ps(cid:48)o(cid:48))−σ(φh pso)]+ (7)\n",
      "1 ((p,s,o),y)∈D\n",
      "= F(r )(cid:62)(F(e )(cid:12)F(e ))\n",
      "p s o\n",
      "K\n",
      "where γ is the margin hyperparameter, and σ the\n",
      "1 (cid:68) (cid:69)\n",
      "= F(r p),F(e s),F(e o). (6) standardlogisticfunction. Theentityembeddingsare\n",
      "K\n",
      "also constrained to unit norm : ||e || ≤ 1, for all\n",
      "i 2\n",
      "Hence, for HOLE we could directly learn complex i ∈ E.\n",
      "embeddingse ≡ F(e ),r ≡ F(r ) ∈ Cd insteadof WhereasinTrouillonetal.[2016],thegenerated\n",
      "(cid:98)i i (cid:98)j j\n",
      "learningembeddingse,r ∈ Rd andmappingthem negativesaremergedintothetrainingsetD ateach\n",
      "i j\n",
      "batchsampling,andthelog-likelihoodisoptimized p∈ Raresymmetricwhentripleshavethesametruth\n",
      "withL2regularization: valuebypermutationofthesubjectandobjectenti-\n",
      "ties: y = y for all s,o ∈ E, whereas facts of\n",
      "(cid:213) pso pos\n",
      "L(D;Θ)= log(1+exp(−yφc ))+λ||Θ||2. (8) antisymmetricrelations phaveinversetruthvalues:\n",
      "pso 2\n",
      "((p,s,o),y)∈D y pso = −y pos. Toevaluatethisquestionexperimen-\n",
      "tally, we reproduced the joint learning of synthetic\n",
      "Optimizationisconductedwithstochasticgradient symmetricandantisymmetricrelationsdescribedin\n",
      "descent,AdaGrad[Duchi,Hazan,andSinger,2011], Trouillonetal.[2016]onbothscoringfunctions. We\n",
      "and early stopping, as described in Trouillon et al. used the log-likelihood loss as all negatives are ob-\n",
      "[2016].Asinglecorruptednegativetripleisgenerated served.\n",
      "for each positive training triple. The results are re- Wegeneratedrandomlya50×50symmetricma-\n",
      "portedforthebestvalidatedmodelsaftergrid-search trix,anda50×50antisymmetricmatrix. Jointly,they\n",
      "onthefollowingvalues: K ∈ {10,20,50,100,150, representa2×50×50tensor. Toensurethatalltest\n",
      "200},λ ∈ {0.1,0.03,0.01,0.003,0.001,0.0003,0.0}\n",
      "valuesarepredictable,theuppertriangularpartsofthe\n",
      "forthelog-likelihoodloss,andγ ∈ {0.1,0.2,0.3,0.4, matricesarealwayskeptinthetrainingset,andthe\n",
      "0.5,0.6,0.7,0.8,0.9,1.0} forthemax-marginloss. diagonalsareunobserved. Weconducted5-foldcross-\n",
      "Therawandfilteredmeanreciprocalranks(MRR), validationonthelower-triangularmatrices,usingthe\n",
      "aswellasthefilteredhitsat1,3and10arereported upper-triangularpartsplus3foldsfortraining, one\n",
      "inTable1. foldforvalidationandonefoldfortesting. Thereg-\n",
      "The margin loss results are consistent with the ularizationparameterλisvalidatedamongthesame\n",
      "HOLE ones originally reported in Nickel, Rosasco, valuesasinthepreviousexperiment.\n",
      "andPoggio[2016],whichconfirmstheequivalence Figure 1 shows the best cross-validated average\n",
      "ofthescoringfunctions,andsupportsthehypothesis precision(areaundertheprecision-recallcurve)for\n",
      "thatthelosswasresponsibleforthedifferenceinpre- thetwoscoringfunctionsforranksrangingupto50.\n",
      "viously reported results. The log-likelihood results Both models manage to perfectly model symmetry\n",
      "are also coherent, as one must note that the higher andantisymmetry. AstheComplExmodelhastwice\n",
      "scoresreportedonFB15KinTrouillonetal.[2016] has many parameters for a given rank, it reaches a\n",
      "areduetotheuseofmorethanonegeneratednega- perfectaverageprecisionwithatwicesmallerrank.\n",
      "tivesampleforeachpositivetrainingtriple. Here,we Thisconfirmsthattherepresentationofthescoring\n",
      "generatedasinglenegativesampleforeachpositive functiondoesnotaffectthelearningabilitiesofthe\n",
      "oneinordertokeepthecomparisonfairbetweenthe modelsinpractice.\n",
      "two losses. The max-margin loss achieves a better\n",
      "raw MRR (rankings without removing the training 5 Discussion\n",
      "samples) on both datasets, but much worse filtered\n",
      "Wehavedemonstratedthatthescoringfunctionsof\n",
      "metrics on FB15K, suggesting that this loss can be\n",
      "morepronetooverfitting.\n",
      "the HOLE and COMPLEX models are directly pro-\n",
      "portional. Thishenceextendstheexistenceproperty\n",
      "oftheCOMPLEXmodeloverallknowledgegraphs\n",
      "4 ScoringFunction&Symmetry\n",
      "[Trouillonetal.,2017]totheHOLEmodel. Wealso\n",
      "TheresultsinSection3suggestthatthechoiceofscor- showed experimentally that the difference between\n",
      "ingfunction,i.e.,COMPLEXorHOLE,doesnotaffect thereportedresultsofthetwomodelswasduetothe\n",
      "the predictive abilities of the model. An additional useofdifferentlossfunctions,andspecificallythatthe\n",
      "importantquestioniswhetheroneofthemodels—in log-likelihoodlosscanproducealargeimprovement\n",
      "practice—isbettersuitedformodelingcertaintypes ofpredictiveperformancesoverthemoreoftenused\n",
      "of relations. In particular, for symmetric relations, marginloss. WehavealsoshownthatComplexand\n",
      "HOLEneedstolearnembeddingsforwhichtheimag- Holographicembeddingscanbetrainedequallywell\n",
      "inarypartaftertheDFTisclosetozero. COMPLEX, onsymmetricandantisymmetricpatterns. Allthese\n",
      "ontheotherhand,canlearnsuchrepresentationseas- thingsbeingequal,aninterestingquestionisthenin\n",
      "ilyasitoperatesdirectlyinthecomplexdomain. The which settings one of the two models is preferable.\n",
      "questionwhetherthisdifferenceinmodelstranslates Complexembeddingshaveanadvantageintermsof\n",
      "todifferencesinpracticeaffectsthelearningofboth timecomplexityastheyscalelinearlywiththeembed-\n",
      "symmetric and antisymmetric relations. Relations ding dimension, whereas Holographic embeddings\n",
      "WN18 FB15K\n",
      "MRR Hitsat MRR Hitsat\n",
      "Loss Filtered Raw 1 3 10 Filtered Raw 1 3 10\n",
      "Margin 0.938 0.605 0.932 0.942 0.949 0.541 0.298 0.411 0.627 0.757\n",
      "Neg-LL 0.941 0.587 0.936 0.945 0.947 0.639 0.250 0.523 0.725 0.825\n",
      "Table1: Filteredandrawmeanreciprocalrank(MRR),Hits@Nmetricsarefiltered,forthepairwisemax-margin\n",
      "lossandthenegativelog-likelihoodonWN18andFB15Kdatasets.\n",
      "Figure1: Averageprecision(AP)foreachfactorizationrankrangingfrom1to50fortheHolEandComplEx\n",
      "scoringfunctions,withlog-likelihoodloss. Learningisperformedjointlyonthesymmetricrelationandonthe\n",
      "antisymmetricrelation. Top-left: APoverthesymmetricrelationonly. Top-right: APovertheantisymmetric\n",
      "relationonly. Bottom: OverallAP.\n",
      "scale quasilinearly. An advantage of Holographic Riedeletal.,2013]. Aninterestingdirectionoffuture\n",
      "embeddingshoweveristhattheembeddingsremain workisthereforeamoredetailedstudyoflossfunc-\n",
      "strictlyintherealdomain,whichmakesiteasierfor tionsforknowledgegraphembeddings—especially\n",
      "themtobeusedinotherreal-valuedmachinelearn- inlightofthehighlyskewedlabeldistributionandthe\n",
      "ing models. In contrast, Complex embeddings can open-worldassumptionwhicharecharacteristicfor\n",
      "noteasilybetransformedtoreal-valuedvectorsand knowledgegraphsbutunusualforstandardmachine\n",
      "used without loss of information—i.e. the specific learningsettings.\n",
      "waytherealandimaginarypartsinteractinalgebraic\n",
      "operations. Complex-valuedmodelsinwhichCom- Acknowledgments\n",
      "plexembeddingscanbedirectlyinputareemerging\n",
      "This work was supported in part by the Associa-\n",
      "inmachinelearning[Trabelsietal.,2017;Danihelka\n",
      "tionNationaledelaRechercheetdelaTechnologie\n",
      "et al., 2016], but this path is yet to be explored for\n",
      "throughtheCIFREgrant2014/0121.\n",
      "otherrelationallearningproblems. Hence,ifthetask\n",
      "of interest is link prediction, Complex embeddings\n",
      "A ProofofEquivalence\n",
      "offeranimprovedruntimecomplexityintheorderof\n",
      "O(logK).Iftheembeddingsshouldbeusedinfurther Inthissection,weprovidethefullprooffortheequiv-\n",
      "machinelearningmodels,e.g.forentityclassification, alenceofbothmodels. Notethatasimilarproofhas\n",
      "Holographicembeddingsprovidebettercompatibility recentlybeenderivedbyHayashiandShimbo[2017].\n",
      "withexistingreal-valuedmethods. We start from Equation (5) and show that there\n",
      "alwaysexistscorrespondingreal-valuedholographic\n",
      "Furthermore,whilethechoiceofthelossisoflittle embeddingsandcomplexembeddingssuchthatthe\n",
      "consequenceontheWN18dataset,ourexperiments scoring functions of HOLE and COMPLEX are di-\n",
      "showed that the log-likelihood loss performed sig- rectlyproportional,i.e. theyaremathematicallyequal\n",
      "nificantlybetteronFB15K.Whilemuchresearchat- up to a constant multiplier a ∈ R: φh = aφc.\n",
      "pso pso\n",
      "tention has been given to scoring functions in link Thekeyideaisinshowingthatthesymmetrystruc-\n",
      "prediction,littlehasbeensaidaboutthelosses,and ture of vectors resulting from Fourier transform of\n",
      "the max-margin loss has been used in most of the real-valuedvectorsissuchthat,thetrilinearproduct\n",
      "existingwork[Bordesetal.,2013;Yangetal.,2015; betweenthesestructuredvectorsisactuallyequalto\n",
      "keepingtherealpartofthetrilinearproductoftheir vector x ∈RK:\n",
      "firsthalf.\n",
      "(cid:40)\n",
      "[s(x) x(cid:48) t(x) x(cid:48) ], ifK iseven,\n",
      "F(x)= ← (12)\n",
      "First, we derive a property of the DFT on real [s(x) x(cid:48) x(cid:48) ], ifK isodd.\n",
      "←\n",
      "vectors x, showing that the resulting complex vec-\n",
      "t jo ∈r {F 1(,x.)..h,Kas −a 1p }a :rtially symmetric structure, for where x(cid:48),x ←(cid:48) ∈ C(cid:100)K 2(cid:101)−1, with x(cid:48) =\n",
      "[F(x),...,F(x) ], and x(cid:48) is x(cid:48) in reversed\n",
      "1 (cid:100)K(cid:101)−1 ←\n",
      "F(x)\n",
      "(K−j)\n",
      "=K (cid:213)−1\n",
      "x ke−2iπ(K−j) Kk\n",
      "ord Wer e: cx a←(cid:48) nt= he[ nF d( ex ri) v(cid:100)2 eK\n",
      "2\n",
      "E(cid:101) q− u1, a. ti. o. n,F (6( )x f) o1 r] r.\n",
      ",e,e ∈RK,\n",
      "p s o\n",
      "k=0\n",
      "firstwithK beingodd:\n",
      "K−1\n",
      "= (cid:213) x ke−2iπke2iπj Kk 1 (cid:68) (cid:69)\n",
      "k=0\n",
      "φh\n",
      "pso\n",
      "=\n",
      "K\n",
      "F(r p),F(e s),F(e o)\n",
      "1 (cid:68) (cid:69)\n",
      "andgiventhatk isaninteger: e−2iπk =1, = [s(r ) r(cid:48) r(cid:48) ],[s(e ) e(cid:48) e(cid:48) ],[s(e ) e(cid:48) e(cid:48) ]\n",
      "K p p p← s s s← o o o←\n",
      "=K (cid:213)−1\n",
      "x ke2iπj Kk\n",
      "=K (cid:213)−1\n",
      "x ke−2iπj Kk\n",
      "= K1 (cid:10) [s(r p) r p(cid:48) r(cid:48) p],[s(e s) e s(cid:48) e s(cid:48)],[s(e o) e o(cid:48) e o(cid:48)](cid:11)\n",
      "k=0 k=0 = 1 (cid:16) s(r )s(e )s(e )+(cid:10) r(cid:48),e(cid:48),e(cid:48)(cid:11)+(cid:10) r(cid:48),e(cid:48),e(cid:48)(cid:11)(cid:17)\n",
      "K p s o p s o p s o\n",
      "andsince x k ∈R, = 1 (cid:16) s(r )s(e )s(e )+(cid:10) r(cid:48),e(cid:48),e(cid:48)(cid:11)+(cid:10) r(cid:48),e(cid:48),e(cid:48)(cid:11)(cid:17)\n",
      "K p s o p s o p s o\n",
      "K−1\n",
      "= (cid:213)\n",
      "k=0\n",
      "x ke−2iπj Kk = F(x) j. (9) = K1 (cid:16) s(r p)s(e s)s(e o)+2Re(cid:16)(cid:10) r p(cid:48),e s(cid:48),e o(cid:48)(cid:11)(cid:17)(cid:17)\n",
      "2\n",
      "(cid:18)(cid:28)(cid:20)(cid:113) (cid:21) (cid:20)(cid:113) (cid:21) (cid:20)(cid:113) (cid:21)(cid:29)(cid:19)\n",
      "= Re 3 1s(r ) r(cid:48), 3 1s(e ) e(cid:48), 3 1s(e ) e(cid:48)\n",
      "K 2 p p 2 s s 2 o o\n",
      "Two special cases arise, the first one is F(x),\n",
      "whichisnotconcernedbytheabovesymmetryprop0\n",
      "- =\n",
      "2 Re(cid:16)(cid:10) r(cid:48)(cid:48),e(cid:48)(cid:48),e(cid:48)(cid:48)(cid:11)(cid:17)\n",
      "K p s o\n",
      "erty:\n",
      "2\n",
      "= φc (13)\n",
      "K−1 K pso\n",
      "F(x)\n",
      "0\n",
      "= (cid:213) x ke−2iπ0 Kk\n",
      "k=0 wherer p(cid:48)(cid:48),e s(cid:48)(cid:48),e o(cid:48)(cid:48) ∈ C(cid:100)K 2(cid:101). Thederivationissimilar\n",
      "K−1 when K is even, with double prime vectors being\n",
      "= (cid:213) x k =: s(x)∈R. x(cid:48)(cid:48) =[(cid:113) 3 1s(x) (cid:113) 3 1t(x) x(cid:48)]∈CK 2+1.\n",
      "k=0 2 2\n",
      "As mentioned in Section 2, the complex vectors\n",
      "(10)\n",
      "r r(cid:48)(cid:48),e s(cid:48)(cid:48),e o(cid:48)(cid:48) ∈ C(cid:100)K 2(cid:101) equivalent to the real vectors\n",
      "r,e,e ∈ RK are twice smaller, but take twice as\n",
      "AndthesecondoneisF(x) whenK iseven: p s o\n",
      "K\n",
      "2 muchmemoryasreal-valuedonesofsamesizeata\n",
      "given floating-point precision. Both models hence\n",
      "F(x) = F(x) = F(x)\n",
      "(K−K) K K havetheexactsamememorycomplexity.\n",
      "2 2 2\n",
      "K−1 K−1\n",
      "= (cid:213) x ke−2iπK 2Kk = (cid:213) x ke−iπk\n",
      "References\n",
      "k=0 k=0\n",
      "K (cid:213)2−1 Bordes,A.;Usunier,N.;Garcia-Duran,A.;Weston,\n",
      "= x 2k −x 2k+1 =:t(x)∈R. J.;andYakhnenko,O. 2013. Translatingembed-\n",
      "k=0 dings for modeling multi-relational data. In Ad-\n",
      "(11) vancesinNeuralInformationProcessingSystems,\n",
      "2787–2795.\n",
      "FromEquations(9)to(11),wewritethegeneral Bordes, A.; Glorot, X.; Weston, J.; and Bengio, Y.\n",
      "form of the Fourier transform F(x) ∈ CK of a real 2014. A semantic matching energy function for\n",
      "learningwithmulti-relationaldata. MachineLearn-\n",
      "ing94(2):233–259.\n",
      "Danihelka,I.;Wayne,G.;Uria,B.;Kalchbrenner,N.;\n",
      "andGraves,A. 2016. Associativelongshort-term\n",
      "memory. arXivpreprintarXiv:1602.03032.\n",
      "Duchi, J.; Hazan, E.; and Singer, Y. 2011. Adap-\n",
      "tivesubgradientmethodsforonlinelearningand\n",
      "stochasticoptimization. JournalofMachineLearn-\n",
      "ingResearch12:2121–2159.\n",
      "Hayashi,K.,andShimbo,M. 2017. Ontheequiva-\n",
      "lenceofholographicandcomplexembeddingsfor\n",
      "linkprediction. arXivpreprintarXiv:1702.05563.\n",
      "Nickel, M.; Rosasco, L.; and Poggio, T. A. 2016.\n",
      "Holographicembeddingsofknowledgegraphs. In\n",
      "AAAIConferenceonArtificialIntelligence,1955–\n",
      "1961.\n",
      "Riedel,S.;Yao,L.;McCallum,A.;andMarlin,B.M.\n",
      "2013. Relation extraction with matrix factoriza-\n",
      "tionanduniversalschemas. InHumanLanguage\n",
      "Technologies: ConferenceoftheNorthAmerican\n",
      "ChapteroftheAssociationofComputationalLin-\n",
      "guistics,74–84.\n",
      "Trabelsi, C.; Bilaniuk, O.; Serdyuk, D.; Subrama-\n",
      "nian,S.;Santos,J.F.;Mehri,S.;Rostamzadeh,N.;\n",
      "Bengio, Y.; and Pal, C. J. 2017. Deep complex\n",
      "networks. arXivpreprintarXiv:1705.09792.\n",
      "Trouillon,T.;Welbl,J.;Riedel,S.;Gaussier,E.;and\n",
      "Bouchard, G. 2016. Complex embeddings for\n",
      "simplelinkprediction. InInternationalConference\n",
      "onMachineLearning,volume48,2071–2080.\n",
      "Trouillon, T.; Dance, C. R.; Welbl, J.; Riedel, S.;\n",
      "Gaussier,É.;andBouchard,G. 2017. Knowledge\n",
      "graphcompletionviacomplextensorfactorization.\n",
      "arXivpreprintarXiv:1702.06879,toappearinthe\n",
      "JournalofMachineLearningResearch.\n",
      "Yang,B.;Yih,W.-T.;He,X.;Gao,J.;andDeng,L.\n",
      "2015.Embeddingentitiesandrelationsforlearning\n",
      "andinferenceinknowledgebases. InInternational\n",
      "ConferenceonLearningRepresentations.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  67942,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Théo Trouillon', 'Maximilian Nickel']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Multilingual Knowledge Graph Embeddings for\n",
      "Cross-lingual Knowledge Alignment\n",
      "MuhaoChen1,YingtaoTian2,MohanYang1,CarloZaniolo1\n",
      "{muhaochen,yang,zaniolo}@cs.ucla.edu;yittian@cs.stonybrook.edu\n",
      "DepartmentofComputerScience,UCLA1\n",
      "DepartmentofComputerScience,StonyBrookUniversity2\n",
      "Abstract and the cross-lingual knowledge that matches the monolin-\n",
      "gualknowledgeamongvarioushumanlanguages.\n",
      "Many recent works have demonstrated the bene-\n",
      "The coverage issue of monolingual knowledge has been\n",
      "fits of knowledge graph embeddings in complet-\n",
      "widelyaddressed,andparsing-basedtechniquesforcomplet-\n",
      "ing monolingual knowledge graphs. Inasmuch as\n",
      "ing monolingual knowledge bases have been well studied\n",
      "related knowledge bases are built in several dif-\n",
      "in the past [Culotta and Sorensen, 2004; Zhou et al., 2005;\n",
      "ferent languages, achieving cross-lingual knowl-\n",
      "Sun et al., 2011]. More recently, much attention has been\n",
      "edge alignment will help people in constructing\n",
      "paid to embedding-based techniques, which provide simple\n",
      "a coherent knowledge base, and assist machines\n",
      "methods to encode entities in low-dimensional embedding\n",
      "in dealing with different expressions of entity re-\n",
      "spacesandcapturerelationsasmeansoftranslationsamong\n",
      "lationships across diverse human languages. Un-\n",
      "entity vectors. Given a triple (h,r,t) where r is the rela-\n",
      "fortunately, achieving this highly desirable cross-\n",
      "tion between entities h and t, then h and t are represented\n",
      "lingual alignment by human labor is very costly\n",
      "astwok-dimensionalvectorshandt, respectively. Afunc-\n",
      "and error-prone. Thus, we propose MTransE, a\n",
      "tion f (h,t) is used to measure the plausibility of (h,r,t),\n",
      "translation-based model for multilingual knowl- r\n",
      "whichalsoimpliesthetransformationrthatcharacterizesr.\n",
      "edge graph embeddings, to provide a simple and\n",
      "Exemplarily, the translation-based model TransE [Bordes et\n",
      "automated solution. By encoding entities and re-\n",
      "al., 2013] uses the loss function f (h,t) = (cid:107)h+r−t(cid:107) 1,\n",
      "lationsofeachlanguageinaseparatedembedding r\n",
      "where r is characterized as a translation vector learnt from\n",
      "space, MTransE provides transitions for each em-\n",
      "thelatentconnectivitypatternsintheknowledgegraph. This\n",
      "bedding vector to its cross-lingual counterparts in\n",
      "modelprovidesaflexiblewayofpredictingamissingitemin\n",
      "other spaces, while preserving the functionalities\n",
      "atriple,orverifyingthevalidityofageneratedtriple. Other\n",
      "ofmonolingualembeddings. Wedeploythreedif-\n",
      "works like TransH [Wang et al., 2014] and TransR [Lin et\n",
      "ferent techniques to represent cross-lingual transi-\n",
      "al., 2015], introduce different loss functions that represent\n",
      "tions, namely axis calibration, translation vectors,\n",
      "the relational translation in other forms, and have achieved\n",
      "andlineartransformations,andderivefivevariants\n",
      "promisingresultsincompletingtheknowledgegraphs.\n",
      "for MTransE using different loss functions. Our\n",
      "While embedding-based techniques can help improve the\n",
      "models can be trained on partially aligned graphs,\n",
      "completenessofmonolingualknowledge,theproblemofap-\n",
      "wherejustasmallportionoftriplesarealignedwith\n",
      "plyingthese techniqueson cross-lingual knowledgeremains\n",
      "their cross-lingual counterparts. The experiments\n",
      "largelyunexplored. Suchknowledge,includinginter-lingual\n",
      "on cross-lingual entity matching and triple-wise\n",
      "links (ILLs) that match the same entities, and triple-wise\n",
      "alignmentverificationshowpromisingresults,with\n",
      "alignment (TWA) that represents the same relations, is very\n",
      "somevariantsconsistentlyoutperformingotherson\n",
      "helpfulinsynchronizingdifferentlanguage-specificversions\n",
      "differenttasks. WealsoexplorehowMTransEpre-\n",
      "of a knowledge base that evolve independently, as needed\n",
      "serves the key properties of its monolingual coun-\n",
      "to further improve applications built on knowledge bases,\n",
      "terpartTransE.\n",
      "such as Q&A systems, semantic Web, and Web search. In\n",
      "spiteofitsimportance,thiscross-lingualknowledgeremains\n",
      "1 Introduction largelyintact. Infact,inthemostsuccessfulknowledgebase\n",
      "MultilingualknowledgebasessuchasWikipedia[Wikipedia, Wikipedia,wefindthatILLscoverlessthan15%entityalign-\n",
      "2017], WordNet [Bond and Foster, 2013], and Concept- ment.\n",
      "Net[SpeerandHavasi,2013]arebecomingessentialsources Leveragingknowledgegraphembeddingstocross-lingual\n",
      "of knowledge for people and AI-related applications. These knowledge no doubt provides a generic way to help extract\n",
      "knowledgebasesaremodeledasknowledgegraphsthatstore and apply such knowledge. However, it is a non-trivial task\n",
      "two aspects of knowledge: the monolingual knowledge that\n",
      "includesentitiesandrelationsrecordedintheformoftriples, 1Hereafter,(cid:107)·(cid:107)meansl orl normunlessexplicitlyspecified.\n",
      "1 2\n",
      "7102\n",
      "yaM\n",
      "71\n",
      "]IA.sc[\n",
      "3v45930.1161:viXra\n",
      "tofindatractabletechniquetocapturethecross-lingualtran- TransD [Ji et al., 2015], and other forms [Jia et al., 2016;\n",
      "sitions2. Such transitions are more difficult to capture than Nguyen et al., 2016]. All these variants of TransE special-\n",
      "relationaltranslationsforseveralreasons: (i)across-lingual ize entity embeddings for different relations, therefore im-\n",
      "transition has a far larger domain than any monolingual re- proving knowledge graph completion on multi-mapping re-\n",
      "lational translation; (ii) it applies on both entities and rela- lationsatthecostofincreasedmodelcomplexity. Meanwhile\n",
      "tions, which have incoherent vocabularies among different translation-based models cooperate well with other models.\n",
      "languages;(iii)theknownalignmentfortrainingsuchtransi- Forexample,variantsofTransEarecombinedwithwordem-\n",
      "tionsusuallyaccountsforasmallpercentageofaknowledge beddingstohelprelationextractionfromtext[Westonetal.,\n",
      "base. Moreover, thecharacterizationofmonolingualknowl- 2013;Zhongetal.,2015].\n",
      "edge graph structures has to be well-preserved to ensure the\n",
      "correctrepresentationoftheknowledgetobealigned.\n",
      "To address the above issues, we propose a multilingual Inadditiontothese,therearenon-translation-basedmeth-\n",
      "knowledgegraphembeddingmodelMTransE,thatlearnsthe ods. Some of those including UM [Bordes et al., 2011], SE\n",
      "multilingualknowledgegraphstructureusingacombination [Bordes et al., 2012], Bilinear [Jenatton et al., 2012], and\n",
      "of two component models, namely knowledge model and HolE [Nickel et al., 2016], do not explicitly represent re-\n",
      "alignmentmodel. Theknowledgemodelencodesentitiesand lation embeddings. Others including neural-based models\n",
      "relationsinalanguage-specificversionofknowledgegraph.\n",
      "SLM[CollobertandWeston,2008]andNTN[Socheretal.,\n",
      "Weexplorethemethodthatorganizeseachlanguage-specific 2013], and random-walk-based model TADW [Yang et al.,\n",
      "version in a separated embedding space, in which MTransE\n",
      "2015a],areexpressiveandadaptableforbothstructuredand\n",
      "adopts TransE as the knowledge model. On top of that, the text corpora, but are too complex to be incorporated into an\n",
      "alignmentmodellearnscross-lingualtransitionsforbothen- architecturesupportingmultilingualknowledge.\n",
      "titiesandrelationsacrossdifferentembeddingspaces,where\n",
      "the following three representations of cross-lingual align-\n",
      "MultilingualWordEmbeddings. Severalapproacheslearn\n",
      "ment are considered: distance-based axis calibration, trans-\n",
      "multilingualwordembeddingsonparalleltextcorpora.Some\n",
      "lation vectors, and linear transformations. Thus, we obtain\n",
      "of those can be extended to multilingual knowledge graphs,\n",
      "five variants of MTransE based on different loss functions,\n",
      "such as LM [Mikolov et al., 2013] and CCA [Faruqui and\n",
      "and identify the best variant by comparing them on cross-\n",
      "Dyer, 2014] which induce offline transitions among pre-\n",
      "lingualalignmenttasksusingtwopartiallyalignedtrilingual\n",
      "trained monolingual embeddings in forms of linear trans-\n",
      "graphs constructed from Wikipedia triples. We also show\n",
      "formations and canonical component analysis respectively.\n",
      "that MTransE performs as well as its monolingual counter-\n",
      "Theseapproachesdonotadjusttheinconsistentvectorspaces\n",
      "partTransEonmonolingualtasks.\n",
      "via calibration or jointly training with the alignment model,\n",
      "Therestofthepaperisorganizedasfollows. Wefirstdis-\n",
      "thus fail to perform well on knowledge graphs as the par-\n",
      "cusstherelatedwork,andthenintroduceourapproachinthe\n",
      "allelism exists only in small portions. A better approach\n",
      "section that follows. After that we present the experimental\n",
      "OT[Xingetal.,2015]jointlylearnsregularizedembeddings\n",
      "results,andconcludethepaperinthelastsection.\n",
      "and orthogonal transformations, which is however found to\n",
      "2 RelatedWork beovercomplicatedduetotheinconsistencyofmonolingual\n",
      "vectorspacesandthelargediversityofrelationsamongenti-\n",
      "While,atthebestofourknowledge,thereisnopreviouswork ties.\n",
      "on learning multilingual knowledge graph embeddings, we\n",
      "will describe next three lines of work which are closely re-\n",
      "latedtothistopic. KnowledgeBasesAlignment. Someprojectsproducecross-\n",
      "Knowledge Graph Embeddings. Recently, significant ad- lingualalignmentinknowledgebasesatthecostofextensive\n",
      "vancement has been made in using the translation-based humaninvolvementanddesigninghand-craftedfeaturesded-\n",
      "method to train monolingual knowledge graph embeddings. icated to specific applications. Wikidata [Vrandecˇic´, 2012]\n",
      "To characterize a triple (h,r,t), models of this family fol- and DBpedia [Lehmann et al., 2015] rely on crowdsourc-\n",
      "low a common assumption h +r ≈ t, where h and t ing to create ILLs and relation alignment. YAGO [Mahdis-\n",
      "r r r r\n",
      "are either the original vectors of h and t, or the transformed oltanietal.,2015]minesassociationrulesonknownmatches,\n",
      "vectors under a certain transformation w.r.t. relation r. The which combines many confident scores and requires exten-\n",
      "forerunner TransE [Bordes et al., 2013] sets h and t as sively fine tuning. Many other works require sources that\n",
      "r r\n",
      "the original h and t, and achieves promising results in han- are external to the graphs, from well-established schemata\n",
      "dling1-to-1relations. LaterworksimproveTransEonmulti- or ontologies [Nguyen et al., 2011; Suchanek et al., 2011;\n",
      "mapping relations by introducing relation-specific transfor- Rinseretal.,2013]toentitydescriptions[Yangetal.,2015b],\n",
      "mations on entities to obtain different h and t, including which being unavailable to many knowledge bases such as\n",
      "r r\n",
      "projectionsonrelation-specifichyperplanesinTransH[Wang YAGO,WordNet,andConceptNet[SpeerandHavasi,2013].\n",
      "et al., 2014], linear transformations to heterogeneous rela- Such approaches also involve complicated model depen-\n",
      "tionspacesinTransR[Linetal.,2015],dynamicmatricesin dencies that are not tractable and reusable. By contrast,\n",
      "embedding-based methods are simple and general, require\n",
      "2Weusethewordtransitionheretodifferentiatefromtherela- littlehumaninvolvement,andgeneratetask-independentfea-\n",
      "tionaltranslationsamongentitiesintranslation-basedmethods. turesthatcancontributetootherNLPtasks.\n",
      "3 MultilingualKnowledgeGraph tionisgivenasbelow:\n",
      "Embeddings (cid:88)\n",
      "S = S (T,T(cid:48))\n",
      "A a\n",
      "Weherebybeginourmodelingwiththeformalizationofmul-\n",
      "(T,T(cid:48))∈δ(Li,Lj)\n",
      "tilingualknowledgegraphs.\n",
      "Thereof, the alignment score S (T,T(cid:48)) iterates through all\n",
      "a\n",
      "3.1 MultilingualKnowledgeGraphs pairs of aligned triples. Three different techniques to score\n",
      "InaknowledgebaseKB,weuseLtodenotethesetoflan- thealignmentareconsidered:distance-basedaxiscalibration,\n",
      "guages, and L2 to denote the 2-combination of L (i.e., the translationvectors,andlineartransformations. Eachofthem\n",
      "set of unordered language pairs). For a language L ∈ L, is based on a different assumption, and constitutes different\n",
      "G denotesthelanguage-specificknowledgegraphofL,and formsofS aalongside.\n",
      "L\n",
      "E and R respectively denote the corresponding vocabu- Distance-based Axis Calibration. This type of alignment\n",
      "L L\n",
      "laries of entity expression and relation expression. T = models penalize the alignment based on the distances of\n",
      "(h,r,t) denotes a triple in G such that h,t ∈ E and cross-lingualcounterparts. Eitherofthefollowingtwoscor-\n",
      "L L\n",
      "r ∈ R. Boldfaced h, r, t respectively represent the em- ingscanbeadoptedtothemodel.\n",
      "L\n",
      "bedding vectors of head h, relation r, and tail t. For a lan- S =(cid:107)h−h(cid:48)(cid:107)+(cid:107)t−t(cid:48)(cid:107)\n",
      "guage pair (L,L ) ∈ L2, δ(L,L ) denotes the alignment a1\n",
      "1 2 1 2\n",
      "setwhichcontainsthepairsoftriplesthathavealreadybeen S regulatesthatcorrectlyalignedmultilingualexpressions\n",
      "a1\n",
      "aligned between L and L. For example, across the lan- ofthesameentitytendtohavecloseembeddingvectors.Thus\n",
      "1 2\n",
      "(cid:0)\n",
      "guages English and French, we may have (State of Cal- byminimizingthelossfunctionthatinvolvesS onknown\n",
      "a1\n",
      "ifornia, capital city, Sacramento),(E´tat de Californie, cap- pairs of aligned triples, the alignment model adjusts axes of\n",
      "(cid:1) embeddingspacestowardsthegoalofcoincidingthevectors\n",
      "itale, Sacramento) ∈ δ(English,French). The alignment\n",
      "ofthesameentityindifferentlanguages.\n",
      "set commonly exists in a small portion in a multilingual\n",
      "knowledgebase[Vrandecˇic´,2012;Mahdisoltanietal.,2015; S =(cid:107)h−h(cid:48)(cid:107)+(cid:107)r−r(cid:48)(cid:107)+(cid:107)t−t(cid:48)(cid:107)\n",
      "Lehmannetal.,2015],andisonepartofknowledgewewant\n",
      "a2\n",
      "toextend. S a2 overlays the penalty of relation alignment to S a1 to ex-\n",
      "Our model consists of two components that learn on the plicitlyconvergecoordinatesofthesamerelation.\n",
      "twofacetsofKB: theknowledgemodelthatencodestheen- The alignment models based on axis calibration assume\n",
      "tities and relations from each language-specific graph struc- analogous spatial emergence of items in each language.\n",
      "ture, and the alignment model that learns the cross-lingual Therefore, itrealizesthecross-lingualtransitionbycarrying\n",
      "transitions from the existing alignment. We define a model forwardthevectorofagivenentityorrelationfromthespace\n",
      "for each language pair from L2 that has a non-empty align- oftheoriginallanguagetothatoftheotherlanguage.\n",
      "mentset.Thus,foraKB withmorethantwolanguages,aset TranslationVectors. Thismodelencodescross-lingualtran-\n",
      "ofmodelscomposesthesolution. Inthefollowing,weusea sitions into vectors. It consolidates alignment into graph\n",
      "language pair (L, L ) ∈ L2 as an example to describe how structures and characterizes cross-lingual transitions as reg-\n",
      "i j\n",
      "wedefineeachcomponentofamodel. ularrelationaltranslations. HenceS a3 asbelowisderived.\n",
      "3.2 KnowledgeModel S a3 =(cid:13) (cid:13)h+v ie j −h(cid:48)(cid:13) (cid:13)+(cid:13) (cid:13)r+v ir j −r(cid:48)(cid:13) (cid:13)+(cid:13) (cid:13)t+v ie j −t(cid:48)(cid:13) (cid:13)\n",
      "ForeachlanguageL∈L,adedicatedk-dimensionalembed- Thereof ve and vr are respectively deployed as the entity-\n",
      "ij ij\n",
      "dingspaceRk isassignedforvectorsofE andR, where dedicatedandrelation-dedicatedtranslationvectorsbetween\n",
      "L L L\n",
      "Risthefieldofrealnumbers. Weadoptthebasictranslation- L andL,suchthatwehavee+ve ≈e(cid:48)forembeddingvec-\n",
      "i j ij\n",
      "based method of TransE for each involved language, which tors e, e(cid:48) of the same entity e expressed in both languages,\n",
      "benefits the cross-lingual tasks by representing embeddings and r + vr ≈ r(cid:48) for those of the same relation. We de-\n",
      "ij\n",
      "uniformlyindifferentcontextsofrelations. Thereforeitsloss ploytwotranslationvectorsinsteadofone,becausethereare\n",
      "functionisgivenasbelow: farmoredistinctentitiesthanrelations,andusingonevector\n",
      "(cid:88) (cid:88) easilyleadstoimbalancedsignalsfromrelations.\n",
      "S K = (cid:107)h+r−t(cid:107) Such a model obtains a cross-lingual transition of an em-\n",
      "L∈{Li,Lj}(h,r,t)∈GL beddingvectorbyaddingthecorrespondingtranslationvec-\n",
      "tor. Moreover, it is easy to see that ve = −ve and vr =\n",
      "Itmeasurestheplausibilityofallgiventriples. Byminimiz- −vr hold. Therefore, as we obtain ti hj e translj ai tion vecij tors\n",
      "ingthelossfunction,theknowledgemodelpreservesmono- ji\n",
      "fromL toL,wecanalwaysusethesamevectorstotrans-\n",
      "lingual relations among entities, while also acts as a regu- i j\n",
      "lateintheoppositedirection.\n",
      "larizer for the alignment model. Meanwhile, the knowledge\n",
      "Linear Transformations. The last category of alignment\n",
      "modelpartitionstheknowledgebaseintodisjointsubsetsthat\n",
      "models deduce linear transformations between embedding\n",
      "canbetrainedinparallel.\n",
      "spaces. S as below learns a k ×k square matrix Me as\n",
      "a4 ij\n",
      "3.3 AlignmentModel alineartransformationonentityvectorsfromL itoL j,given\n",
      "kasthedimensionalityoftheembeddingspaces.\n",
      "Theobjectiveofthealignmentmodelistoconstructthetran-\n",
      "sitionsbetweenthevectorspacesofL iandL j. Itslossfunc- S a4 =(cid:13) (cid:13)Me ijh−h(cid:48)(cid:13) (cid:13)+(cid:13) (cid:13)Me ijt−t(cid:48)(cid:13) (cid:13)\n",
      "Table1: Summaryofmodelvariants. Table2: StatisticsoftheWK3ldatasets.\n",
      "Var ModelComplexity Cross-lingualTransition SearchComplexity Dataset #Entriples #Frtriples #Detriples #Alignedtriples\n",
      "Var1 O(nekl+nrkl) τ τi ij j( (e r) )= =e\n",
      "r\n",
      "OO (( nn rek k)\n",
      ")\n",
      "WK3l-15k 203,502 170,605 145,616 EE nn -- DFr e: :1 36 7,,4 17 70\n",
      "0\n",
      "Var2 O(nekl+nrkl) τ τi ij j( (e r) )= =e\n",
      "r\n",
      "OO (( nn rek k)\n",
      ")\n",
      "WK3l-120k 1,376,011 767,750 391,108 E En n- -F Dr e:1 :62 94,, 44 13 33\n",
      "Var3 O(nekl+ +n kr lk 2)l τ τi ij j( (e r) )= =e r++ vv irie jj OO (( nn rek k) ) Table3: Numberofentityinter-linguallinks(ILLs).\n",
      "Var4 O(ne +kl 0+.5kn 2r lk 2l ) τ τi ij j( (e r) )= =M Me i e ij je r OO (( nn rek k2 2+ +n ne rk k) ) WD Kat 3a l-S 1e 5t k E 3,n 7- 3F 3r F 3,r 8-E 15n E 1n,8- 4D 0e D 1,e 6- 1E 0n\n",
      "Var5 O(nekl ++ kn 2r l2k )l τ τi ij j( (e r) )= =M Me i r ij je r OO (( nn rek k2 2+ +n ne rk k) ) WK3l-120k 42,413 41,513 7,567 5,921\n",
      "Notation:eandrarerespectivelythevectorsofanentityeandarelationr,kis We enforce the constraint that the l 2 norm of any entity\n",
      "thedimensionoftheembeddingspaces,listhecardinalityofL,neandnrare embeddingvectoris1,thusregularizeembeddingvectorsto\n",
      "respectivelythenumberofentitiesandthenumberofrelations,wherene(cid:29)nr.\n",
      "a unit spherical surface. This constraint is employed in the\n",
      "S additionallybringsinasecondlineartransformationMr\n",
      "a5 ij literature[Bordesetal.,2013;Bordesetal.,2014;Jenattonet\n",
      "forrelationvectors,whichisofthesameshapeasMe. The\n",
      "ij al.,2012]andhastwoimportanteffects: (i)ithelpsavoidthe\n",
      "useofadifferentmatrixisagainduetodifferentredundancy\n",
      "case where the training process trivially minimizes the loss\n",
      "ofentitiesandrelations.\n",
      "functionbyshrinkingthenormofembeddingvectors,and(ii)\n",
      "S a5 =(cid:13) (cid:13)Me ijh−h(cid:48)(cid:13) (cid:13)+(cid:13) (cid:13)Mr ijr−r(cid:48)(cid:13) (cid:13)+(cid:13) (cid:13)Me ijt−t(cid:48)(cid:13) (cid:13) eit ti am l.p,l 2ie 0s 15th ]e foin rv Ve art ribi al nit dy Vo af rth.elineartransformations[Xing\n",
      "4 5\n",
      "Unlike axis calibration, linear-transformation-based align- We initialize vectors by drawing from a uniform distribu-\n",
      "ment model treats cross-lingual transitions as the topologi- tionontheunitsphericalsurface,andinitializematricesusing\n",
      "caltransformationofembeddingspaceswithoutassumingthe randomorthogonalinitialization[Saxeetal.,2014].Negative\n",
      "similarityofspatialemergence. samplingisnotemployedintraining,whichwefinddoesnot\n",
      "The cross-lingual transition of a vector is obtained by ap- noticeablyaffecttheresults.\n",
      "plyingthecorrespondinglineartransformation. Itisnotewor-\n",
      "thy that, regularization of embedding vectors in the training 4 Experiments\n",
      "process(whichwillbeintroducedsoonafter)ensuresthein-\n",
      "vertibility of the linear transformations such that Me −1 = In this section, we evaluate the proposed methods on two\n",
      "ij cross-lingualtasks: cross-lingualentitymatching,andtriple-\n",
      "Me andMr −1 = Mr. Thusthetransitionintherevertdi-\n",
      "ji ij ji wisealignmentverification. Wealsoconductexperimentson\n",
      "rectionisalwaysenabledeventhoughthemodelonlylearns twomonolingualtasks.Besides,acasestudywithknowledge\n",
      "thetransformationsofonedirection. alignmentexamplesisincludedintheAppendixof[Chenet\n",
      "3.4 VariantsofMTransE al.,2017].\n",
      "Data Sets. Experimental results on the trilingual data sets\n",
      "Combiningtheabovetwocomponentmodels,MTransEmin-\n",
      "WK3l are reported in this section. WK3l contains English\n",
      "imizesthefollowinglossfunctionJ = S +αS,whereα\n",
      "K A\n",
      "(En), French (Fr), and German (De) knowledge graphs un-\n",
      "isahyperparameterthatweightsS andS.\n",
      "K A der DBpedia’s dbo:Person domain, where a part of triples\n",
      "Aswehavegivenoutfivevariantsofthealignmentmodel,\n",
      "are aligned by verifying the ILLs on entities, and multi-\n",
      "each of which correspondingly defines its specific way of\n",
      "lingual labels of the DBpedia ontology on some relations.\n",
      "computing cross-lingual transitions of embedding vectors.\n",
      "The number of entities in each language is adjusted to ob-\n",
      "We denote Var as the variant of MTransE that adopts the\n",
      "k\n",
      "tain two data sets. For each of the three languages thereof,\n",
      "k-th alignment model which employs S. In practice, the\n",
      "ak\n",
      "WK3l-15kmatchesthenumberofnodes(about15,000)with\n",
      "searchingofacross-lingualcounterpartforasourceisalways\n",
      "FB15k—thelargestmonolingualgraphusedbymanyrecent\n",
      "done by querying the nearest neighbor from the result point\n",
      "works [Zhong et al., 2015; Lin et al., 2015; Ji et al., 2015;\n",
      "of the cross-lingual transition. We denote function τ that\n",
      "ij Jia et al., 2016], and the number of nodes in WK3l-120k is\n",
      "mapsacross-lingualtransitionofavectorfromL toL,or\n",
      "i j\n",
      "several times larger. For both data sets, German graphs are\n",
      "simply τ in a bilingual context. As stated, the solution in a\n",
      "sparserthanEnglishandFrenchgraphs.Wealsocollectextra\n",
      "multi-lingualscenarioconsistsofasetofmodelsofthesame\n",
      "variant defined on every language pair in L2. Table 1 sum- entity ILLs for the evaluation of cross-lingual entity match-\n",
      "ing,whosequantityisshowninTable3. Meanwhile,wede-\n",
      "marizesthemodelcomplexity,thedefinitionofcross-lingual\n",
      "riveanothertrilingualdatasetCN3lfromConceptNet[Speer\n",
      "transitions, and the complexity of searching a cross-lingual\n",
      "and Havasi, 2013]. Additional results on CN3l that lead to\n",
      "counterpartforeachvariant.\n",
      "similar evaluation conclusions are reported in the Appendix\n",
      "3.5 Training\n",
      "of[Chenetal.,2017].\n",
      "We optimize the loss function using on-line stochastic gra-\n",
      "4.1 Cross-lingualEntityMatching\n",
      "dient descent [Wilson and Martinez, 2003]. At each step,\n",
      "we update the parameter θ by setting θ ← θ − λ∇ J, The objective of this task is to match the same entities from\n",
      "θ\n",
      "where λ is the learning rate. Instead of directly updating differentlanguagesinKB. Duetothelargecandidatespace,\n",
      "J, our implementation optimizes S and αS alternately. this task emphasizes more on ranking a set of candidates\n",
      "K A\n",
      "In detail, at each epoch we optimize θ ← θ −λ∇ S and rather than acquiring the best answer. We perform this task\n",
      "θ K\n",
      "θ ←θ−λ∇ αS inseparatedgroupsofsteps. onbothdatasetstocomparefivevariantsofMTransE.\n",
      "θ A\n",
      "Table4: Cross-lingualentitymatchingresult.\n",
      "DataSet WK3l-15k WK3l-120k\n",
      "AlignedLanguages En-Fr Fr-En En-De De-En En-Fr Fr-En En-De De-En\n",
      "Metric Hits@10 Mean Hits@10 Mean Hits@10 Mean Hits@10 Mean Hits@10 Hits@10 Hits@10 Hits@10\n",
      "LM 12.31 3621.17 10.42 3660.98 22.17 5891.13 15.21 6114.08 11.74 14.26 24.52 13.58\n",
      "CCA 20.78 3094.25 19.44 3017.90 26.46 5550.89 22.30 5855.61 19.47 12.85 25.54 20.39\n",
      "OT 44.97 508.39 40.92 461.18 44.47 155.47 49.24 145.47 38.91 37.19 38.85 34.21\n",
      "Var1 51.05 470.29 46.64 436.47 48.67 146.13 50.60 167.02 38.58 36.52 42.06 47.79\n",
      "Var2 45.25 570.72 41.74 565.38 46.27 168.33 49.00 211.94 31.88 30.84 41.22 40.39\n",
      "Var3 38.64 587.46 36.44 464.64 50.82 125.15 52.16 151.84 38.26 36.45 50.48 52.24\n",
      "Var4 59.24 190.26 57.48 199.64 66.25 74.62 68.53 42.31 48.66 47.43 57.56 63.49\n",
      "Var5 59.52 191.36 57.07 204.45 60.25 99.48 66.03 54.69 45.65 47.48 64.22 67.85\n",
      "Figure1: Precision-recallcurvesforcross-lingualentitymatchingonWK3l-15k.\n",
      "ToshowthesuperiorityofMTransE,weadaptLM,CCA, indicatethattheinterferencecausedbylearninganadditional\n",
      "and OT (which are introduced in Section 2) to their knowl- relation-dedicatedtransformationinVar isnegligibletothe\n",
      "5\n",
      "edgegraphequivalences. entity-dedicatedtransformation.Correspondingly,webelieve\n",
      "thatthereasonforVar tobeoutperformedbyVar andVar\n",
      "Evaluation Protocol. Each MTransE variant is trained on 3 4 5\n",
      "is that it fails to differentiate well the over-frequent cross-\n",
      "a complete data set. LM and CCA are implemented by in-\n",
      "lingualalignmentfromregularrelations. Therefore,thechar-\n",
      "ducing the corresponding transformations across separately\n",
      "acterizationforcross-lingualalignmentisnegativelyaffected\n",
      "trainedknowledgemodelsonmonolingualgraphs,whileus-\n",
      "bythelearningprocessformonolingualrelationsinavisible\n",
      "ingthealignmentsetsasanchors. TrainingOTisquitesim-\n",
      "degree. Axis calibration appears to be unstable on this task.\n",
      "ilar to MTransE, we add the process of orthogonalization to\n",
      "Wehypothesizethatthissimpletechniqueisaffectedbytwo\n",
      "the training of the alignment model, since the regularization\n",
      "factors: coherence between language-specific versions, and\n",
      "of vectors has already been enforced. The entity ILLs are\n",
      "density of the graphs. Var is always outperformed by Var\n",
      "used as ground truth for test. We take these unidirectional 2 1\n",
      "due to the negative effect of the calibration based on rela-\n",
      "linksbetweenEnglish-FrenchandEnglish-German,i.e.,four\n",
      "tions. Webelievethisisbecausemulti-mappingrelationsare\n",
      "directions in total. For each ILL (e,e(cid:48)), we perform a kNN\n",
      "notsowell-capturedbyTransEasexplainedin[Wangetal.,\n",
      "searchfromthecross-lingualtransitionpointofe(i.e.,τ(e))\n",
      "2014], therefore disturb the calibration of the entire embed-\n",
      "and record the rank of e(cid:48). Following the convention [Xing\n",
      "ding spaces. Although Var still outperforms Var on entity\n",
      "etal.,2015;Jiaetal.,2016],weaggregatetwometricsover 1 3\n",
      "matchingbetweenEnglishandFrenchgraphsinWK3l-15k<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,     75,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['WK3l-15k', 'WK3l-120k', 'CN3l']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  by Var\n",
      "used as ground truth for test. We take these unidirectional 2 1\n",
      "due to the negative effect of the calibration based on rela-\n",
      "linksbetweenEnglish-FrenchandEnglish-German,i.e.,four\n",
      "tions. Webelievethisisbecausemulti-mappingrelationsare\n",
      "directions in total. For each ILL (e,e(cid:48)), we perform a kNN\n",
      "notsowell-capturedbyTransEasexplainedin[Wangetal.,\n",
      "searchfromthecross-lingualtransitionpointofe(i.e.,τ(e))\n",
      "2014], therefore disturb the calibration of the entire embed-\n",
      "and record the rank of e(cid:48). Following the convention [Xing\n",
      "ding spaces. Although Var still outperforms Var on entity\n",
      "etal.,2015;Jiaetal.,2016],weaggregatetwometricsover 1 3\n",
      "matchingbetweenEnglishandFrenchgraphsinWK3l-15k,\n",
      "all test cases, i.e., the proportion of ranks no larger than 10\n",
      "coherencesomewhatdropsalongsidewhenscalinguptothe\n",
      "Hits@10(inpercentage),andthemeanrankMean. Wepre-\n",
      "larger data set so as to hinder the calibration. The German\n",
      "fer higher Hits@10 and lower Mean that indicate a better\n",
      "graphsaresparse,thusshouldhavesetabarrierforprecisely\n",
      "outcome.\n",
      "constructing embedding vectors and hindered calibration on\n",
      "For training, we select the learning rate λ among {0.001,\n",
      "theotherside.ThereforeVar stillperformscloselytoVar in\n",
      "1 3\n",
      "0.01, 0.1}, α among {1, 2.5, 5, 7.5}, l or l norm in loss\n",
      "1 2 the English-German task on WK3l-15k and English-French\n",
      "functions, and dimensionality k among {50, 75, 100, 125}.\n",
      "task on WK3l-120k, but is outperformed by Var in the last\n",
      "3\n",
      "The best configuration on WK3l-15k is λ = 0.01, α = 5,\n",
      "setting. In general, the variants that use linear transforma-\n",
      "k = 75, l normforVar, Var, LM,andCCA,l normfor\n",
      "1 1 2 2 tions are the most desired. This conclusion is supported by\n",
      "othervariantsandOT.WhilethebestconfigurationonWK3l-\n",
      "their promising outcomeon this task, and it isalso reflected\n",
      "120k is λ = 0.01, α = 5, k = 100, and l norm for all\n",
      "2 intheprecision-recallcurvesshowninFigure1.\n",
      "models. Thetrainingonbothdatasetstakes400epochs.\n",
      "4.2 Triple-wiseAlignmentVerification\n",
      "Results. WereportHits@10andMean forWK3l-15k, and\n",
      "Hits@10 for WK3l-120k, on the four involved directions Thistaskistoverifywhetheragivenpairofalignedtriplesare\n",
      "of cross-lingual matching in Table 4. As expected, with- truly cross-lingual counterparts. It produces a classifier that\n",
      "out jointly adapting the monolingual vector spaces with the helpswithverifyingcandidatesoftriplematching[Nguyenet\n",
      "knowledgealignment,LMandCCAarelargelyoutperformed al.,2011;Rinseretal.,2013].\n",
      "by the rest. While the orthogonality constraint being too Evaluation Protocol. We create positive cases by isolating\n",
      "strong to be enforced in these cases, OT performs at most 20%ofthealignmentset. Similarto[Socheretal.,2013],we\n",
      "closelytothesimplestcasesofMTransE.ForMTransE,Var randomly corrupt positive cases to generate negative cases.\n",
      "4\n",
      "and Var outperform the other three variants under all set- Indetail,givenapairofcorrectlyalignedtriples(T,T(cid:48)),itis\n",
      "5\n",
      "tings. Thefairlycloseresultsobtainedbythesetwovariants corruptedby(i)randomlyreplacingoneofthesixelementsin\n",
      "Table5: AccuracyofTWAverification(%). Table6: Resultsoftailprediction Table7: Resultsofrelationprediction\n",
      "DataSet WK3l-15k WK3l-120k (Hits@10). (Hits@10).\n",
      "Languages En&Fr En&De En&Fr En&De DataSet WK3l-15k WK3l-120k DataSet WK3l-15k WK3l-120k\n",
      "LM 52.23 63.61 59.98 59.98 Language En Fr En Fr Language En Fr En Fr\n",
      "CCA 52.28 66.49 65.89 61.01 TransE 42.19 25.06 36.78 25.38 TransE 61.79 62.55 60.06 65.29\n",
      "V V V V VO a a a a aT r r r r r1 2 3 4 5 9 9 9 9 9 93 3 0 0 4 4......2 2 2 3 5 90 5 4 8 8 0 9 98 9 8 8 5 47 1 6 4...... 0 99 2 5 2 3 57 4 9 4 8 9 8 8 9 98 1 9 7 3 2......6 2 3 9 4 65 7 6 9 8 3 9 98 9 8 8 3 35 1 6 7...... 0 62 3 2 0 6 64 5 9 4 V V V V Va a a a ar r r r r1 2 3 4 5 4 4 4 4 40 0 0 1 1.....3 8 9 0 77 0 7 3 9 2 2 2 2 23 4 2 5 5.....4 7 2 4 75 7 6 6 7 3 3 3 3 39 6 5 9 8.....0 0 9 6 39 2 9 4 5 2 2 1 2 25 1 9 5 4.....5 1 6 5 62 3 9 9 8 V V VV Va a aa ar r rr r1 3 42 5 6 5 5 6 60 4 8 3 4.....1 3 3 7 78 3 2 4 9 6 6 5 6 60 2 9 4 3.....7 9 4 7 73 8 4 7 1 6 6 6 6 61 1 0 0 0.....7 1 1 2 75 1 4 6 7 6 4 66 61 8 65 7.....4 0 84 67 6 66 4\n",
      "thetwotripleswithanotherelementfromthesamelanguage, ofMTransE,butisstillleftbehindbyVar andVar.\n",
      "4 5\n",
      "or(ii)randomlysubstitutingeitherT orT(cid:48)withanothertriple\n",
      "4.3 MonolingualTasks\n",
      "fromthesamelanguage. Cases(i)and(ii)respectivelycon-\n",
      "tribute negative cases that are as many as 100% and 50% of The above experiments have shown the strong capability\n",
      "positivecases.Weuse10-foldcross-validationonthesecases of MTransE in handling cross-lingual tasks. Now we re-\n",
      "totrainandevaluatetheclassifier. port the results on comparing MTransE with its monolin-\n",
      "We use a simple threshold-based classifier similar to the gual counterpart TransE on two monolingual tasks intro-\n",
      "widely-usedonesfortripleclassification[Socheretal.,2013; duced in the literature [Bordes et al., 2013; Bordes et al.,\n",
      "Wang et al., 2014; Lin et al., 2015]. For a given pair of 2014], namely tail prediction (predicting t given h and r)\n",
      "aligned triples (T,T(cid:48)) = (cid:0) (h,r,t),(h(cid:48),r(cid:48),t(cid:48))(cid:1), the dissim- and relation prediction (predicting r given h and t), us-\n",
      "ilarity function is defined as f (T,T(cid:48)) = (cid:107)τ(h)−h(cid:48)(cid:107) + ing the English and French versions of our data sets. Like\n",
      "(cid:107)τ(r)−r(cid:48)(cid:107) + (cid:107)τ(t)−t(cid:48)(cid:107). Td he classifier finds a thre2 sh- previous works [Bordes et al., 2013; Wang et al., 2014;\n",
      "oldσ sucht2 hatf < σ impl2 iespositive,otherwisenegative. Jia et al., 2016], for each language version, 10% triples are\n",
      "d\n",
      "The value of σ is determined by maximizing the accuracy selectedasthetestset,andtheremainingbecomesthetrain-\n",
      "foreachfoldonthetrainingset. Suchasimpleclassification ingset. EachMTransEvariantistraineduponbothlanguage\n",
      "ruleadequatelyreliesonhowpreciselyeachmodelrepresents versions of the training set for the knowledge model, while\n",
      "cross-lingualtransitionsforbothentitiesandrelations. the intersection between the alignment set and the training\n",
      "setisusedforthealignmentmodels. TransEistrainedonei-\n",
      "Wecarryforwardthecorrespondingconfigurationfromthe\n",
      "ther language version of the training set. Again, we use the\n",
      "lastexperiment,justtoshowtheperformanceofeachvariant\n",
      "configurationfromthepreviousexperiment.\n",
      "undercontrolledvariables.\n",
      "Results. Table 5 shows the mean accuracy, with a standard Results. The results for Hits@10 are reported in Tables 6\n",
      "and 7. They imply that MTransE preserves well the char-\n",
      "deviation below 0.009 in cross-validation for all settings.\n",
      "acterization of monolingual knowledge. For each setting,\n",
      "Thus, the results are statistically sufficient to reflect the per-\n",
      "Var, Var, andVar performatleastaswellasTransE,and\n",
      "formance of classifiers. Note that the results appear to be 1 4 5\n",
      "some even outperforms TransE under certain settings. This\n",
      "better than those of the previous task since this is a binary\n",
      "signifies that the alignment model does not interfere much\n",
      "classificationproblem. Intuitively,thelinear-transformation-\n",
      "withtheknowledgemodelincharacterizingmonolingualre-\n",
      "basedMTransEperformsteadilyandtaketheleadonallset-\n",
      "lations,butmighthaveactuallystrengtheneditsincecoherent\n",
      "tings. WealsoobservethatVar,thoughlearnsanadditional\n",
      "5\n",
      "portions of knowledge are unified by the alignment model.\n",
      "relation-dedicatedtransformation,stillperformsconsiderably\n",
      "Since such coherence is currently not measured, this ques-\n",
      "close to Var (the difference is at most 0.85%). The simple\n",
      "4\n",
      "tionisleftasafuturework. Theotherquestionthatdeserves\n",
      "Var is the runner-up, and is between 1.65% and 3.79% to\n",
      "1\n",
      "further attention is, how other knowledge models involving\n",
      "the optimal solutions. However the relation-dedicated cali-\n",
      "brationinVar causesanotablesetback(4.12%∼8.44%from relation-specific entity transformations [Wang et al., 2014;\n",
      "2\n",
      "Linetal.,2015;Jietal.,2015;Jiaetal.,2016;Nguyenetal.,\n",
      "the optimal). The performance of Var falls behind slightly\n",
      "3\n",
      "more than Var (4.52%∼10.79% from the optimal) due to\n",
      "2016]mayinfluencemonolingualandcross-lingualtasks.\n",
      "2\n",
      "thefailureindistinguishingcross-lingualalignmentfromreg-\n",
      "5 ConclusionandFutureWork\n",
      "ular relations. Meanwhile, we single out the accuracy on\n",
      "the portion of negative cases where only the relation is cor- Atthebestofourknowledge,thispaperisthefirstworkthat\n",
      "rupted for English-French in WK3l-15k. The five variants generalizesknowledgegraphembeddingstothemultilingual\n",
      "receive 97.73%, 93.78%, 82.34%, 98.57%, and 98.54%, re- scenario. OurmodelMTransEcharacterizesmonolingualre-\n",
      "spectively. The close accuracy of Var and Var indicates lationsandcomparesthreedifferenttechniquestolearncross-\n",
      "4 5\n",
      "that the only transformation learnt from entities in Var is lingualalignmentforentitiesandrelations. Extensiveexperi-\n",
      "4\n",
      "enoughtosubstitutetherelation-dedicatedtransformationin mentsonthetasksofcross-lingualentitymatchingandtriple\n",
      "Var fordiscriminatingrelationalignment,whilelearningthe alignment verification show that the linear-transformation-\n",
      "5\n",
      "additional transformation in Var does not notably interfere technique is the best among the three. Moreover, MTransE\n",
      "5\n",
      "theoriginalone. However, itappliesdifferentlytoaxiscali- preservesthekeypropertiesofmonolingualknowledgegraph\n",
      "brationsinceVar doesnotimprovebutactuallyimpairsthe embeddingsonmonolingualtasks.\n",
      "2\n",
      "cross-lingualtransitionsforrelations.Forthesamereasonsas The results here are very encouraging, but we also point\n",
      "above,LMandCCAdonotmatchwithMTransEinthisex- outopportunitiesforfurtherworkandimprovements. Inpar-\n",
      "perimentaswell,whileOTperformscloselytosomevariants ticular, we should explore how to substitute the simple loss\n",
      "functionoftheknowledgemodelusedinMTransEwithmore [Linetal.,2015] Yankai Lin, Zhiyuan Liu, Maosong Sun,\n",
      "advanced ones involving relation-specific entity transforma- Yang Liu, and Xuan Zhu. Learning entity and relation\n",
      "tions. More sophisticated tasks of cross-lingual triple com- embeddings for knowledge graph completion. In AAAI,\n",
      "pletion may also be conducted. Combining MTransE with 2015.\n",
      "multilingualwordembeddings[Xingetal.,2015]isanother\n",
      "[Mahdisoltanietal.,2015] Farzaneh Mahdisoltani, Joanna\n",
      "meaningfuldirectionsinceitwillprovideausefultooltoex-\n",
      "Biega,FabianSuchanek,etal. Yago3: Aknowledgebase\n",
      "tractnewrelationsfrommultilingualtextcorpora.\n",
      "frommultilingualWikipedias. InCIDR,2015.\n",
      "References [Mikolovetal.,2013] TomasMikolov,QuocVLe,andIlya\n",
      "Sutskever. Exploiting similarities among languages for\n",
      "[BondandFoster,2013] Francis Bond and Ryan Foster. machinetranslation. arXiv,2013.\n",
      "Linking and extending an open multilingual Wordnet. In\n",
      "ACL,pages1352–1362,2013. [Nguyenetal.,2011] Thanh Nguyen, Viviane Moreira,\n",
      "HuongNguyen,HoaNguyen,andJulianaFreire.Multilin-\n",
      "[Bordesetal.,2011] Antoine Bordes, Jason Weston, Ronan\n",
      "gualschemamatchingforWikipediainfoboxes. PVLDB,\n",
      "Collobert, and Yoshua Bengio. Learning structured em-\n",
      "5(2):133–144,2011.\n",
      "beddingsofknowledgebases. InAAAI,2011.\n",
      "[Nguyenetal.,2016] DatQuocNguyen,KairitSirts,Lizhen\n",
      "[Bordesetal.,2012] Antoine Bordes, Xavier Glorot, Jason\n",
      "Qu, and Mark Johnson. Stranse: a novel embedding\n",
      "Weston,andYoshuaBengio. Jointlearningofwordsand\n",
      "modelofentitiesandrelationshipsinknowledgebases. In\n",
      "meaningrepresentationsforopen-textsemanticparsing.In\n",
      "NAACLHLT,pages460–466,2016.\n",
      "AISTATS,pages127–135,2012.\n",
      "[Nickeletal.,2016] Maximilian Nickel, Lorenzo Rosasco,\n",
      "[Bordesetal.,2013] Antoine Bordes, Nicolas Usunier,\n",
      "TomasoPoggio,etal. Holographicembeddingsofknowl-\n",
      "Alberto Garcia-Duran, Jason Weston, and Oksana\n",
      "edgegraphs. InAAAI,2016.\n",
      "Yakhnenko. Translating embeddings for modeling\n",
      "multi-relationaldata. InNIPS,pages2787–2795,2013. [Rinseretal.,2013] Daniel Rinser, Dustin Lange, Felix\n",
      "Naumann, and Gerhard Weikum. Cross-lingual entity\n",
      "[Bordesetal.,2014] Antoine Bordes, Jason Weston, and\n",
      "matching and infobox alignment in Wikipedia. Informa-\n",
      "Nicolas Usunier. Open question answering with weakly\n",
      "tionSystems,38(6):887–907,2013.\n",
      "supervised embedding models. In ECML-PKDD, pages\n",
      "165–180,2014. [Saxeetal.,2014] Andrew M Saxe, James L McClelland,\n",
      "and Surya Ganguli. Exact solutions to the nonlinear dy-\n",
      "[Chenetal.,2017] Muhao Chen, Yingtao Tian, Mohan\n",
      "namicsoflearningindeeplinearneuralnetworks. ICLR,\n",
      "Yang,andCarloZaniolo. Multi-lingualknowledgegraph\n",
      "2014.\n",
      "embeddingsforcross-lingualknowledgealignment. arXiv\n",
      "preprintarXiv:1611.03954,2017. [Socheretal.,2013] Richard Socher, Danqi Chen, Christo-\n",
      "pherDManning,andAndrewNg. Reasoningwithneural\n",
      "[CollobertandWeston,2008] Ronan Collobert and Jason\n",
      "tensornetworksforknowledgebasecompletion. InNIPS,\n",
      "Weston. A unified architecture for natural language pro-\n",
      "pages926–934,2013.\n",
      "cessing: Deepneuralnetworkswithmultitasklearning. In\n",
      "ICML,pages160–167,2008. [SpeerandHavasi,2013] Robert Speer and Catherine\n",
      "[CulottaandSorensen,2004] Aron Culotta and Jeffrey Havasi. Conceptnet 5: A large semantic network for\n",
      "relational knowledge. The People’s Web Meets NLP,\n",
      "Sorensen. Dependencytreekernelsforrelationextraction.\n",
      "pages161–176,2013.\n",
      "InACL,page423,2004.\n",
      "[FaruquiandDyer,2014] Manaal Faruqui and Chris Dyer. [Suchaneketal.,2011] Fabian M Suchanek, Serge Abite-\n",
      "Improving vector space word representations using mul- boul, Pierre Senellart, and Tom Mitchell. Paris: Prob-\n",
      "tilingualcorrelation. EACL,2014. abilistic alignment of relations, instances, and schema.\n",
      "PVLDB,5(3):157–168,2011.\n",
      "[Jenattonetal.,2012] Rodolphe Jenatton, Nicolas L Roux,\n",
      "AntoineBordes,andGuillaumeRObozinski.Alatentfac- [Sunetal.,2011] Ang Sun, Ralph Grishman, and Satoshi\n",
      "tormodelforhighlymulti-relationaldata. InNIPS,2012. Sekine. Semi-supervised relation extraction with large-\n",
      "scalewordclustering. InACL,pages521–529,2011.\n",
      "[Jietal.,2015] Guoliang Ji, Shizhu He, Liheng Xu, Kang\n",
      "Liu, andJunZhao. Knowledgegraphembeddingviady- [Vrandecˇic´,2012] DennyVrandecˇic´. Wikidata: Anewplat-\n",
      "namicmappingmatrix. InACL,pages687–696,2015. formforcollaborativedatacollection. InWWW,2012.\n",
      "[Jiaetal.,2016] Yantao Jia, Yuanzhuo Wang, Hailun Lin, [Wangetal.,2014] Zhen Wang, Jianwen Zhang, Jianlin\n",
      "Xiaolong Jin, and Xueqi Cheng. Locally adaptive trans- Feng, andZhengChen. Knowledgegraphembeddingby\n",
      "lationforknowledgegraphembedding. InAAAI,2016. translatingonhyperplanes. InAAAI,2014.\n",
      "[Lehmannetal.,2015] Jens Lehmann, Robert Isele, Max [Westonetal.,2013] Jason Weston, Antoine Bordes, Ok-\n",
      "Jakob,AnjaJentzsch,etal. Dbpedia–alarge-scale,multi- sana Yakhnenko, and Nicolas Usunier. Connecting lan-\n",
      "lingualknowledgebaseextractedfromWikipedia.Seman- guage and knowledge bases with embedding models for\n",
      "ticWeb,6(2):167–195,2015. relationextraction. InEMNLP,pages1366–1371,2013.\n",
      "[Wikipedia,2017] Wikipedia, 2017. https://www.\n",
      "wikipedia.org/.\n",
      "[WilsonandMartinez,2003] D Randall Wilson and Tony R\n",
      "Martinez. The general inefficiency of batch training for\n",
      "gradientdescentlearning. NeuralNetworks,16(10),2003.\n",
      "[Xingetal.,2015] Chao Xing, Dong Wang, Chao Liu, and\n",
      "Yiye Lin. Normalized word embedding and orthogonal\n",
      "transformforbilingualwordtranslation. InNAACLHLT,\n",
      "pages1006–1011,2015.\n",
      "[Yangetal.,2015a] Cheng Yang, Zhiyuan Liu, Deli Zhao,\n",
      "Maosong Sun, and Edward Chang. Network representa-\n",
      "tionlearningwithrichtextinformation. InIJCAI,2015.\n",
      "[Yangetal.,2015b] Yang Yang, Yizhou Sun, Jie Tang,\n",
      "Bo Ma, and Juanzi Li. Entity matching across heteroge-\n",
      "neoussources. InKDD,pages1395–1404,2015.\n",
      "[Zhongetal.,2015] Huaping Zhong, Jianwen Zhang, Zhen\n",
      "Wang, Hai Wan, and Zheng Chen. Aligning knowledge\n",
      "and text embeddings by entity descriptions. In EMNLP,\n",
      "pages267–272,2015.\n",
      "[Zhouetal.,2005] Guodong Zhou, Jian Su, Jie Zhang, and\n",
      "Min Zhang. Exploring various knowledge in relation ex-\n",
      "traction. InACL,pages427–434,2005.\n",
      "6 Appendix Table11: StatisticsoftheCN3ldataset.\n",
      "Typeoftriples Entriples Frtriples Detriples Alignedtriples\n",
      "6.1 ExamplesofKnowledgeAlignment En-Fr:3,668\n",
      "Numberoftriples 47,696 18,624 25,560\n",
      "En-De:8,588\n",
      "We have already shown the effectiveness of MTransE\n",
      "TypeofILLs En-Fr Fr-En En-De De-En\n",
      "in aligning cross-lingual knowledge, especially the linear- NumberofILLs 2,154 2,146 3,485 3,813\n",
      "transformation-based variants Var and Var. Now we dis-\n",
      "4 5\n",
      "cussseveralexamplestorevealinsightsonhowourmethods basic queries are already useful for aided cross-lingual aug-\n",
      "maybeusedincross-lingualknowledgeaugmentation. mentationofknowledge. However,developingajointmodel\n",
      "tosupportcomplexqueriesonmultilingualknowledgegraphs\n",
      "Table8: Examplesofcross-lingualentitymatching. basedonMTransEgeneratedfeaturesappearstobeapromis-\n",
      "ing future work to support Q&A on multilingual knowledge\n",
      "Entity Target Candidates(inascendingorderofrankbyEuclideandistance)\n",
      "Barack French BarackObama,GeorgeBush,JimmyCarter,GeorgeKalkoa bases.\n",
      "Obama German BarackObama,BillClinton,Georgeh.w.Bush,HamidKarzai Figure2showsthePCAprojectionofthesamesixEnglish\n",
      "French Paris,Amsterdam,a`Paris,Manchester,DeSmet\n",
      "Paris\n",
      "German Paris,Languedoc,Constantine,Saint-maurice,Nancy entities in their original English space and in French space\n",
      "French SanFrancisco,LosAngeles,SantaMonica,Californie\n",
      "California German Kalifornien,LosAngeles,PalmSprings,SantaMonica aftertransformation. WecanobservethatthevectorsofEn-\n",
      "French post-punk,rockalternatif,smoothjazz,souljazz glish entities show certain structures, where the U.S. cities\n",
      "rockmusic\n",
      "German rockmusik,soul,deathmetal,dance-pop\n",
      "aregroupedtogetherandothercountries’citiesarewellsep-\n",
      "Table9: Examplesofcross-lingualrelationmatching. arated. AftertransformationintoFrenchspace,theseEnglish\n",
      "entities not only keep their original spatial emergence, but\n",
      "Relation Target Candidates(inascendingorderofrankbyEuclideandistance)\n",
      "French capitale,territoire,paysaccre`ditant,lieudeveneration alsoareclosetotheircorrespondingentitiesinFrench. This\n",
      "capital\n",
      "German hauptstadt,hauptort,gru¨ndungsort,city\n",
      "illustrates the transformation preserves mono-lingual struc-\n",
      "French nationalie´,paysdenaissance,domicile,re´sidence\n",
      "nationality\n",
      "German nationalita¨t,nation,letzterstart,sterbeort ture and also it is able to capture cross-lingual information.\n",
      "French langue,re´alisations,lieudeces,nationalite`\n",
      "language We believe this example illustrates the good performance\n",
      "German sprache,originalsprache,lang,land\n",
      "French surnom,descendant,texte,nomdering wehavedemonstratedincross-lingualtasksincludingcross-\n",
      "nickname\n",
      "German spitzname,originaltitel,names,alternativnamen\n",
      "lingual entity matching and triple-wise alignment verifica-\n",
      "Table10: Examplesofcross-lingualtriplecompletion. tion.\n",
      "Query Target Candidates(inascendingorderofrank)\n",
      "6.2 AdditionalExperimentalResults\n",
      "musiqueinde`pendante,musiquealternative,\n",
      "(AdamLambert, French\n",
      "ode,glamrock\n",
      "genre,?t) German popmusik,dance-pop,nowave,soul We derive another data set CN3l from the MIT ConceptNet\n",
      "(Ronaldinho, French milieuoffensif,attaquant,quarterback,late`ralgauche toevaluateMTransE,whosestatisticsareshowninTable11.\n",
      "position,?t) German stu¨rmer,linkerflu¨gel,angriffsspieler,rechterflgel\n",
      "French capitale,plusgrandeville,chef-lieu,garnison ThoughbeingasmallerdatasetthanWK3l-15k,knowledge\n",
      "(Italy,?r,Rome)\n",
      "German hauptstadt,hauptort,verwaltungssitz,stadion graphs in CN3l are highly sparse. Thereof, each language\n",
      "ministre-pre`sident,pre`de`cesseur,premierministre,\n",
      "(BarackObama,?r, French pre`sidentduconseil version of CN3l contains around 7,500 nodes and less than\n",
      "GeorgeBush)\n",
      "German vorga¨nger,vorga¨ngerin,besetzung,lied 41 types of relations. The alignment sets are created based\n",
      "BrantBjork,ChrisGarneau,DavidDraiman,\n",
      "(?h,instrument, French IanMackaye on the relation TranslationOf of the ConceptNet. In this\n",
      "guitar)\n",
      "German PhilManzanera,StylesP.,TinaCharles,LukeBryan section we report the results of the two cross-lingual tasks\n",
      "We start with the search of cross-lingual counterparts of on the CN3l data set for MTransE as well as all baselines.\n",
      "entities and relations. We choose an entity (or relation) in Basically,theseresultsleadtosimilarconclusionsaswehave\n",
      "English and then show the nearest candidates in French and onWK3l.\n",
      "German,respectively.Thesecandidatesarelistedbydecreas- EvaluationProtocol. Themetricsandevaluationprocedures\n",
      "ingvaluesoftheEuclideandistancebetweentheirvectorsin arethesameasthoseonWK3l. Weselectλamong{0.001,\n",
      "thetargetlanguagespaceandtheresultpointofcross-lingual 0.01, 0.05}, α among {1, 2.5, 5, 7.5}, l 1 or l 2 norm in loss\n",
      "transition. Several examples are shown in Table 8 and Ta- functions,anddimensionalitykamong{25,50,75}.Optimal\n",
      "ble9. Inalltablesofthissubsection, wemarktheexactan- parameters are configured as λ = 0.001, α = 2.5, k = 50,\n",
      "swersasboldfaced,andtheconceptuallycloseonesasitalic. andl\n",
      "1\n",
      "normforallmodels. Toperformtheevaluationunder\n",
      "For example, in Table 8, besides boldfacing the exactly cor- controlledvariables,weagainuseoneconfigurationoneach\n",
      "rectanswersforBarackObamaandParis,weconsiderthose modelrespectivelyinthetwoexperiments. Sincethedataset\n",
      "whohavealsobeenU.S.presidentsasconceptuallycloseto issmaller,trainingislimitedto200epochs.\n",
      "BarackObama, andEuropeancitiesotherthanParisascon- Results of Cross-lingual Entity Matching. The results are\n",
      "ceptually close to Paris. Also, in Table 9, those French and reported in Table 12. For the baselines, LM and CCA are\n",
      "Germanrelationsthathavethemeaningofsettlementsofsig- againleftfarbehindforbeingdisjointedlytrained. OT,how-\n",
      "nificanceareconsideredasconceptuallyclosetocapital. ever,takesthepositionaheadofVar. Thisislikelybecause\n",
      "1\n",
      "We then move on to the more complicated cross-lingual the knowledge graphs in CN3l are highly sparse, therefore\n",
      "triplecompletiontask. Weconstructqueriesbyreplacingone fewer interference of monolingual relations among entities\n",
      "elementinanEnglishtriplewithaquestionmark,forwhich makestheorthogonalityconstrainteasiertofulfill. Evenso,\n",
      "weseekforanswersinanotherlanguage. Ourmethodsneed in all settings, OT is still largely outperformed by Var and\n",
      "4\n",
      "to transfer the remaining elements to the space of the target Var, which receives amazingly good results, thus steadily\n",
      "5\n",
      "language and pick the best answer for the missing element. being the optimal solutions. Interestingly, Var now ranks\n",
      "3\n",
      "Table10showssomequeryanswers. Itisnoteworthythatthe rightbehindthelinear-transformation-basedvariantsinmost\n",
      "Figure2: VisualizationoftheresultofVar forthesamesixEnglishentitiesintheiroriginalspace(left)andinFrenchspace\n",
      "4\n",
      "afterbeingtransformed(right). Englishentitiesarerenderedinblue,andthecorrespondingFrenchentitiesareinlightruby.\n",
      "Table12: Cross-lingualentitymatching(CN3l). Table13: Accuracyoftriple-wise\n",
      "alignmentverification(%).\n",
      "Languages En-Fr Fr-En En-De De-En\n",
      "Metric Hits@10 Mean Hits@10 Mean Hits@10 Mean Hits@10 Mean Languages En&Fr En&De\n",
      "LM 25.45 1302.83 20.16 1884.70 30.12 954.71 18.04 1487.90 LM 60.53 51.55\n",
      "CCA 27.96 1204.91 26.40 1740.83 28.76 1176.09 25.30 1834.21 CCA 81.57 79.54\n",
      "OT 68.43 42.30 67.06 33.86 72.34 74.98 69.47 44.38 OT 93.01 87.59\n",
      "V V V V Va a a a ar r r r r2 3 4 51 6 4 7 8 81 4 3 6 6.....3 0 7 8 27 6 3 3 1 25 2 1 125 9 6 66....1. 3 6 966 4 4 93 6 5 7 8 89 7 7 0 0.....2 1 0 6 17 5 2 2 9 3 9<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,     75,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['WK3l-15k', 'WK3l-120k', 'CN3l']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: 09 25.30 1834.21 CCA 81.57 79.54\n",
      "OT 68.43 42.30 67.06 33.86 72.34 74.98 69.47 44.38 OT 93.01 87.59\n",
      "V V V V Va a a a ar r r r r2 3 4 51 6 4 7 8 81 4 3 6 6.....3 0 7 8 27 6 3 3 1 25 2 1 125 9 6 66....1. 3 6 966 4 4 93 6 5 7 8 89 7 7 0 0.....2 1 0 6 17 5 2 2 9 3 9 1 7 73 5 4..... 8 36 1 8 6 40 3 2 6 4 7 8 83 9 0 8 9.....0 0 5 8 16 7 5 9 9 27 51 7 84 09.... 1 25. 89 6 74 37 6 4 7 9 93 9 0 5 5.....5 1 9 6 56 5 6 7 3 27 41 1 19 74.... 4 67. 95 7 39 98 V V V V Va a a a ar r r r r1 2 3 4 5 9 8 8 9 93 7 8 7 7.....9 3 9 4 12 0 5 6 8 9 8 8 9 91 2 4 6 5.....8 7 8 6 49 0 0 3 2\n",
      "settings. This is quite reasonable because the cross-lingual\n",
      "transitions, which are regarded as a type of relation by Var\n",
      "3\n",
      "in the graphs, are now way less frequent in CN3l than they\n",
      "areinthemuchdenserandmoreheterogeneousWK3l. Thus,\n",
      "thisexplainswhyitperformsbetterthanVar. Forthesame\n",
      "1\n",
      "reasonaswestatedinSection4.1,Var isplacedatlastofthe\n",
      "2\n",
      "fiveMTransEvariantsinmatchingcross-lingualentities.\n",
      "ResultsofTriple-wiseAlignmentVerification. Theresults\n",
      "shown in Table 13 reflect the same conclusions of the ex-\n",
      "perimentperformedonWK3lthat,thelinear-transformation-\n",
      "based variants takes the lead ahead of the rest MTransE\n",
      "variants and the baselines. While Var, despite being the\n",
      "1\n",
      "simplest, takes the second place with a satisfying accuracy\n",
      "in triple-wise alignment verification as well. The relation-\n",
      "dedicatedcalibrationstillcausesinterferenceintheoptimiza-\n",
      "tionprocessofVar,thereforeleadstoa4%∼9%dropofac-\n",
      "2\n",
      "curacy from Var. Var performs slightly better than Var.\n",
      "1 3 3\n",
      "On triple-wise alignment verification on CN3l, we receive\n",
      "exactly the same placement for evaluating the five MTransE\n",
      "variants. Meanwhile, for the baselines, OT is slightly worse\n",
      "than Var, CCA also receives acceptable accuracy which is\n",
      "1\n",
      "howeverworsethanallMTransEvariants,whiletheaccuracy\n",
      "ofLMisjustslightlybetterthanrandomguessing.\n",
      "Aboveall,theresultsinCN3lindicatesthatMTransEalso\n",
      "workspromisinglyonverysparsemultilingualgraphs,while\n",
      "thelinear-transformation-basedvariantsarethebestrepresen-\n",
      "tationtechniques.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "            220,   2545,    220,    914,     13,    966,    220,  10750,     19,\n",
      "             13,   1691,    356,   5158,    220,   5932,     13,   3226,    220,\n",
      "           4643,     13,   4370,    198,   1831,    220,   2614,     13,   3391,\n",
      "            220,   2983,     13,    966,    220,   3080,     13,   2705,    220,\n",
      "           1644,     13,   4218,    220,   5332,     13,   1958,    220,   5728,\n",
      "             13,   3264,    220,   3076,     13,   2618,    220,   2096,     13,\n",
      "           1987,   8775,    220,   6365,     13,   1721,    220,   4044,     13,\n",
      "           2946,    198,     53,    650,    650,    650,  27713,    264,    264,\n",
      "            264,    802,    436,    436,    436,    436,     17,    220,     18,\n",
      "            220,     19,    220,   3971,    220,     21,    220,     19,    220,\n",
      "             22,    220,     23,    220,   5932,    220,     19,    220,     18,\n",
      "            220,     21,    220,     21,  18575,     18,    220,     15,    220,\n",
      "             22,    220,     23,    220,   1544,    220,     21,    220,     18,\n",
      "            220,     18,    220,     16,    220,    914,    220,     17,    220,\n",
      "             16,    220,   6549,    220,     24,    220,     21,    220,   2287,\n",
      "           1975,     16,     13,    220,     18,    220,     21,    220,  25285,\n",
      "            220,     19,    220,     19,    220,   6365,    220,     21,    220,\n",
      "             20,    220,     22,    220,     23,    220,   4578,    220,     22,\n",
      "            220,     22,    220,     15,    220,     15,  18575,     17,    220,\n",
      "             16,    220,     15,    220,     21,    220,   1114,    220,     20,\n",
      "            220,     17,    220,     17,    220,     24,    220,     18,    220,\n",
      "             24,    220,     16,    220,     22,    220,   5958,    220,     20,\n",
      "            220,     19,  18575,    220,     23,    220,   1927,    220,     16,\n",
      "            220,     23,    220,     21,    220,   1272,    220,     18,    220,\n",
      "             17,    220,     21,    220,     19,    220,     22,    220,     23,\n",
      "            220,   6069,    220,     24,    220,     15,    220,     23,    220,\n",
      "             24,  18575,     15,    220,     15,    220,     20,    220,     23,\n",
      "            220,    845,    220,     22,    220,     20,    220,     24,    220,\n",
      "             24,    220,   1544,    220,   3971,    220,     22,    220,   5833,\n",
      "            220,   2545,   1975,    220,     16,    220,    914,     13,    220,\n",
      "           4578,    220,     21,    220,   5728,    220,   1806,    220,     21,\n",
      "            220,     19,    220,     22,    220,     24,    220,   6365,    220,\n",
      "             24,    220,     15,    220,     20,    220,     20,  18575,     20,\n",
      "            220,     16,    220,     24,    220,     21,    220,   3487,    220,\n",
      "             20,    220,     21,    220,     22,    220,     18,    220,   1544,\n",
      "            220,   3174,    220,     16,    220,    777,    220,   5728,   1975,\n",
      "            220,     19,    220,   3080,     13,    220,   2721,    220,     22,\n",
      "            220,   2137,    220,   3264,    650,    650,    650,    650,  27713,\n",
      "            264,    264,    264,    802,    436,    436,    436,    436,     16,\n",
      "            220,     17,    220,     18,    220,     19,    220,     20,    220,\n",
      "             24,    220,     23,    220,     23,    220,     24,    220,   6365,\n",
      "            220,     22,    220,     23,    220,     22,    220,     22,  18575,\n",
      "             24,    220,     18,    220,     24,    220,     19,    220,    717,\n",
      "            220,     15,    220,     20,    220,     21,    220,     23,    220,\n",
      "             24,    220,     23,    220,     23,    220,     24,    220,   5925,\n",
      "            220,     17,    220,     19,    220,     21,    220,     20,  18575,\n",
      "             23,    220,     22,    220,     23,    220,     21,    220,   2491,\n",
      "            220,     15,    220,     15,    220,     18,    220,     17,    198,\n",
      "           6648,     13,   1115,    374,   5115,  13579,   1606,    279,   5425,\n",
      "             12,   2785,    940,    198,   1485,   6055,     11,    902,    527,\n",
      "          27458,    439,    264,    955,    315,  12976,    555,   8909,    198,\n",
      "             18,    198,    258,    279,  40099,     11,    527,   1457,   1648,\n",
      "           2753,  21420,    304,  25914,     18,     75,   1109,    814,    198,\n",
      "            548,  48121,    336,   1412,  53321,    261,    438,   6518,     71,\n",
      "           1430,  49122,  69416,     18,     75,     13,  14636,    345,    576,\n",
      "          30992,   1771,  35734,    275,    716,  10008,  58234,  54895,   4050,\n",
      "             13,   1789,   6509,    373,    198,     16,    198,  20489,    300,\n",
      "          11285,    660,    258,   9817,     19,     13,     16,     11,   4050,\n",
      "            374,  37469,    266,   4354,   1073,   1820,    198,     17,    198,\n",
      "          53770,     44,   3246,     36,  55711,    258,  90143,  29942,     12,\n",
      "           2785,    940,  10720,    627,   2122,    708,     69,  83926,  45539,\n",
      "           7178,  63439,     13,    666,  13213, 122761,    198,  70463,    304,\n",
      "           6771,    220,   1032,   8881,    279,   1890,  31342,    315,    279,\n",
      "            506,   7058,  14666,    716,  10365,    263,  69416,     18,     75,\n",
      "           9210,     11,    339,   4939,    277,  39160,   1659,   7058,  31039,\n",
      "          27103,   5097,    279,   3063,   8469,    315,    279,   2800,    386,\n",
      "           3246,     36,    198,  55711,    323,    279,   3122,  11243,     13,\n",
      "           6104,   8909,     11,   8994,   1694,    279,    198,     16,    198,\n",
      "          23796,    267,     11,   5097,    279,   2132,   2035,    449,    264,\n",
      "          37154,  13708,    198,    258,  24657,  45539,  17632,  23751,    439,\n",
      "           1664,     13,    578,  12976,   7058,   9988,  10297,   5531,  18856,\n",
      "          44466,    936,   4881,   2295,   2251,    258,   1820,  19680,  17528,\n",
      "           7058,  28491,   4734,   1073,   4050,     11,  19041,   1348,  27152,\n",
      "            267,  20103,     19,      4,  22447,    120,     24,      4,   6861,\n",
      "           1073,    582,   7058,     17,    198,  22222,    505,   8909,     13,\n",
      "           8909,  27772,  10284,   2731,   1109,   8909,    627,     16,    220,\n",
      "             18,    220,     18,    198,   1966,  24657,  45539,  17632,  23751,\n",
      "            389,  25914,     18,     75,     11,    584,   5371,    198,    327,\n",
      "          33839,    279,   1890,  22165,    369,  38663,    279,   4330,    386,\n",
      "           3246,     36,    198,  55711,     13,  26982,     11,    369,    279,\n",
      "           3122,  11243,     11,   8775,    374,  10284,  11201,    198,  54895,\n",
      "           8909,     11,    356,   5158,   1101,  21879,  22281,  13708,    902,\n",
      "            374,    198,     16,    198,  98936,  50810,    751,  10118,    543,\n",
      "             44,   3246,     36,  55711,     11,   3556,   1820,  33829,    198,\n",
      "           1073,  11237,    285,   4345,   3306,  73048,  58234,  54895,  11719,\n",
      "          52851,    287,    627,  59907,    543,     11,    700,    288,    495,\n",
      "          16319,  29768,     18,     75,    485,   8630,    267,   9379,     44,\n",
      "           3246,     36,  19171,    198,   1816,   2203,    442,   3876,    398,\n",
      "            263,    424,   1065,   6534,  26961,  50923,  87286,     11,   3556,\n",
      "            198,    339,   4939,    277,  39160,   1659,   6108,  55711,    548,\n",
      "           1820,  16241,  10200,    417,    268,   7058,     83,    367,  26522,\n",
      "           8467,     13, 128009, 128006,    882, 128007,    271,   7184,     11,\n",
      "           2728,    420,   3488,     25,   3639,    527,    279,    836,    315,\n",
      "          30525,   1511,    304,    279,   5684,   4710,    220,  21335,   1203,\n",
      "            279,   4320,   1193,    304,    264,  13325,   1160,   3645,     11,\n",
      "            369,   3187,     25,   2570,     32,   1882,     33,   7352,   1442,\n",
      "            499,   1541,    956,   1440,    279,   4320,     11,   1120,    471,\n",
      "            459,   4384,   1160,     13, 128009, 128006,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "            220,   2545,    220,    914,     13,    966,    220,  10750,     19,\n",
      "             13,   1691,    356,   5158,    220,   5932,     13,   3226,    220,\n",
      "           4643,     13,   4370,    198,   1831,    220,   2614,     13,   3391,\n",
      "            220,   2983,     13,    966,    220,   3080,     13,   2705,    220,\n",
      "           1644,     13,   4218,    220,   5332,     13,   1958,    220,   5728,\n",
      "             13,   3264,    220,   3076,     13,   2618,    220,   2096,     13,\n",
      "           1987,   8775,    220,   6365,     13,   1721,    220,   4044,     13,\n",
      "           2946,    198,     53,    650,    650,    650,  27713,    264,    264,\n",
      "            264,    802,    436,    436,    436,    436,     17,    220,     18,\n",
      "            220,     19,    220,   3971,    220,     21,    220,     19,    220,\n",
      "             22,    220,     23,    220,   5932,    220,     19,    220,     18,\n",
      "            220,     21,    220,     21,  18575,     18,    220,     15,    220,\n",
      "             22,    220,     23,    220,   1544,    220,     21,    220,     18,\n",
      "            220,     18,    220,     16,    220,    914,    220,     17,    220,\n",
      "             16,    220,   6549,    220,     24,    220,     21,    220,   2287,\n",
      "           1975,     16,     13,    220,     18,    220,     21,    220,  25285,\n",
      "            220,     19,    220,     19,    220,   6365,    220,     21,    220,\n",
      "             20,    220,     22,    220,     23,    220,   4578,    220,     22,\n",
      "            220,     22,    220,     15,    220,     15,  18575,     17,    220,\n",
      "             16,    220,     15,    220,     21,    220,   1114,    220,     20,\n",
      "            220,     17,    220,     17,    220,     24,    220,     18,    220,\n",
      "             24,    220,     16,    220,     22,    220,   5958,    220,     20,\n",
      "            220,     19,  18575,    220,     23,    220,   1927,    220,     16,\n",
      "            220,     23,    220,     21,    220,   1272,    220,     18,    220,\n",
      "             17,    220,     21,    220,     19,    220,     22,    220,     23,\n",
      "            220,   6069,    220,     24,    220,     15,    220,     23,    220,\n",
      "             24,  18575,     15,    220,     15,    220,     20,    220,     23,\n",
      "            220,    845,    220,     22,    220,     20,    220,     24,    220,\n",
      "             24,    220,   1544,    220,   3971,    220,     22,    220,   5833,\n",
      "            220,   2545,   1975,    220,     16,    220,    914,     13,    220,\n",
      "           4578,    220,     21,    220,   5728,    220,   1806,    220,     21,\n",
      "            220,     19,    220,     22,    220,     24,    220,   6365,    220,\n",
      "             24,    220,     15,    220,     20,    220,     20,  18575,     20,\n",
      "            220,     16,    220,     24,    220,     21,    220,   3487,    220,\n",
      "             20,    220,     21,    220,     22,    220,     18,    220,   1544,\n",
      "            220,   3174,    220,     16,    220,    777,    220,   5728,   1975,\n",
      "            220,     19,    220,   3080,     13,    220,   2721,    220,     22,\n",
      "            220,   2137,    220,   3264,    650,    650,    650,    650,  27713,\n",
      "            264,    264,    264,    802,    436,    436,    436,    436,     16,\n",
      "            220,     17,    220,     18,    220,     19,    220,     20,    220,\n",
      "             24,    220,     23,    220,     23,    220,     24,    220,   6365,\n",
      "            220,     22,    220,     23,    220,     22,    220,     22,  18575,\n",
      "             24,    220,     18,    220,     24,    220,     19,    220,    717,\n",
      "            220,     15,    220,     20,    220,     21,    220,     23,    220,\n",
      "             24,    220,     23,    220,     23,    220,     24,    220,   5925,\n",
      "            220,     17,    220,     19,    220,     21,    220,     20,  18575,\n",
      "             23,    220,     22,    220,     23,    220,     21,    220,   2491,\n",
      "            220,     15,    220,     15,    220,     18,    220,     17,    198,\n",
      "           6648,     13,   1115,    374,   5115,  13579,   1606,    279,   5425,\n",
      "             12,   2785,    940,    198,   1485,   6055,     11,    902,    527,\n",
      "          27458,    439,    264,    955,    315,  12976,    555,   8909,    198,\n",
      "             18,    198,    258,    279,  40099,     11,    527,   1457,   1648,\n",
      "           2753,  21420,    304,  25914,     18,     75,   1109,    814,    198,\n",
      "            548,  48121,    336,   1412,  53321,    261,    438,   6518,     71,\n",
      "           1430,  49122,  69416,     18,     75,     13,  14636,    345,    576,\n",
      "          30992,   1771,  35734,    275,    716,  10008,  58234,  54895,   4050,\n",
      "             13,   1789,   6509,    373,    198,     16,    198,  20489,    300,\n",
      "          11285,    660,    258,   9817,     19,     13,     16,     11,   4050,\n",
      "            374,  37469,    266,   4354,   1073,   1820,    198,     17,    198,\n",
      "          53770,     44,   3246,     36,  55711,    258,  90143,  29942,     12,\n",
      "           2785,    940,  10720,    627,   2122,    708,     69,  83926,  45539,\n",
      "           7178,  63439,     13,    666,  13213, 122761,    198,  70463,    304,\n",
      "           6771,    220,   1032,   8881,    279,   1890,  31342,    315,    279,\n",
      "            506,   7058,  14666,    716,  10365,    263,  69416,     18,     75,\n",
      "           9210,     11,    339,   4939,    277,  39160,   1659,   7058,  31039,\n",
      "          27103,   5097,    279,   3063,   8469,    315,    279,   2800,    386,\n",
      "           3246,     36,    198,  55711,    323,    279,   3122,  11243,     13,\n",
      "           6104,   8909,     11,   8994,   1694,    279,    198,     16,    198,\n",
      "          23796,    267,     11,   5097,    279,   2132,   2035,    449,    264,\n",
      "          37154,  13708,    198,    258,  24657,  45539,  17632,  23751,    439,\n",
      "           1664,     13,    578,  12976,   7058,   9988,  10297,   5531,  18856,\n",
      "          44466,    936,   4881,   2295,   2251,    258,   1820,  19680,  17528,\n",
      "           7058,  28491,   4734,   1073,   4050,     11,  19041,   1348,  27152,\n",
      "            267,  20103,     19,      4,  22447,    120,     24,      4,   6861,\n",
      "           1073,    582,   7058,     17,    198,  22222,    505,   8909,     13,\n",
      "           8909,  27772,  10284,   2731,   1109,   8909,    627,     16,    220,\n",
      "             18,    220,     18,    198,   1966,  24657,  45539,  17632,  23751,\n",
      "            389,  25914,     18,     75,     11,    584,   5371,    198,    327,\n",
      "          33839,    279,   1890,  22165,    369,  38663,    279,   4330,    386,\n",
      "           3246,     36,    198,  55711,     13,  26982,     11,    369,    279,\n",
      "           3122,  11243,     11,   8775,    374,  10284,  11201,    198,  54895,\n",
      "           8909,     11,    356,   5158,   1101,  21879,  22281,  13708,    902,\n",
      "            374,    198,     16,    198,  98936,  50810,    751,  10118,    543,\n",
      "             44,   3246,     36,  55711,     11,   3556,   1820,  33829,    198,\n",
      "           1073,  11237,    285,   4345,   3306,  73048,  58234,  54895,  11719,\n",
      "          52851,    287,    627,  59907,    543,     11,    700,    288,    495,\n",
      "          16319,  29768,     18,     75,    485,   8630,    267,   9379,     44,\n",
      "           3246,     36,  19171,    198,   1816,   2203,    442,   3876,    398,\n",
      "            263,    424,   1065,   6534,  26961,  50923,  87286,     11,   3556,\n",
      "            198,    339,   4939,    277,  39160,   1659,   6108,  55711,    548,\n",
      "           1820,  16241,  10200,    417,    268,   7058,     83,    367,  26522,\n",
      "           8467,     13, 128009, 128006,    882, 128007,    271,   7184,     11,\n",
      "           2728,    420,   3488,     25,   3639,    527,    279,    836,    315,\n",
      "          30525,   1511,    304,    279,   5684,   4710,    220,  21335,   1203,\n",
      "            279,   4320,   1193,    304,    264,  13325,   1160,   3645,     11,\n",
      "            369,   3187,     25,   2570,     32,   1882,     33,   7352,   1442,\n",
      "            499,   1541,    956,   1440,    279,   4320,     11,   1120,    471,\n",
      "            459,   4384,   1160,     13, 128009, 128006,  78191, 128007,    271,\n",
      "            681,  69416,     18,     75,    518,    364,  29768,     18,     75,\n",
      "            663, 128009]], device='cuda:0')\n",
      "Decoded output:\n",
      " ['WK3l', 'CN3l']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Multilingual Knowledge Graph Embeddings for\n",
      "Cross-lingual Knowledge Alignment\n",
      "MuhaoChen1,YingtaoTian2,MohanYang1,CarloZaniolo1\n",
      "{muhaochen,yang,zaniolo}@cs.ucla.edu;yittian@cs.stonybrook.edu\n",
      "DepartmentofComputerScience,UCLA1\n",
      "DepartmentofComputerScience,StonyBrookUniversity2\n",
      "Abstract and the cross-lingual knowledge that matches the monolin-\n",
      "gualknowledgeamongvarioushumanlanguages.\n",
      "Many recent works have demonstrated the bene-\n",
      "The coverage issue of monolingual knowledge has been\n",
      "fits of knowledge graph embeddings in complet-\n",
      "widelyaddressed,andparsing-basedtechniquesforcomplet-\n",
      "ing monolingual knowledge graphs. Inasmuch as\n",
      "ing monolingual knowledge bases have been well studied\n",
      "related knowledge bases are built in several dif-\n",
      "in the past [Culotta and Sorensen, 2004; Zhou et al., 2005;\n",
      "ferent languages, achieving cross-lingual knowl-\n",
      "Sun et al., 2011]. More recently, much attention has been\n",
      "edge alignment will help people in constructing\n",
      "paid to embedding-based techniques, which provide simple\n",
      "a coherent knowledge base, and assist machines\n",
      "methods to encode entities in low-dimensional embedding\n",
      "in dealing with different expressions of entity re-\n",
      "spacesandcapturerelationsasmeansoftranslationsamong\n",
      "lationships across diverse human languages. Un-\n",
      "entity vectors. Given a triple (h,r,t) where r is the rela-\n",
      "fortunately, achieving this highly desirable cross-\n",
      "tion between entities h and t, then h and t are represented\n",
      "lingual alignment by human labor is very costly\n",
      "astwok-dimensionalvectorshandt, respectively. Afunc-\n",
      "and error-prone. Thus, we propose MTransE, a\n",
      "tion f (h,t) is used to measure the plausibility of (h,r,t),\n",
      "translation-based model for multilingual knowl- r\n",
      "whichalsoimpliesthetransformationrthatcharacterizesr.\n",
      "edge graph embeddings, to provide a simple and\n",
      "Exemplarily, the translation-based model TransE [Bordes et\n",
      "automated solution. By encoding entities and re-\n",
      "al., 2013] uses the loss function f (h,t) = (cid:107)h+r−t(cid:107) 1,\n",
      "lationsofeachlanguageinaseparatedembedding r\n",
      "where r is characterized as a translation vector learnt from\n",
      "space, MTransE provides transitions for each em-\n",
      "thelatentconnectivitypatternsintheknowledgegraph. This\n",
      "bedding vector to its cross-lingual counterparts in\n",
      "modelprovidesaflexiblewayofpredictingamissingitemin\n",
      "other spaces, while preserving the functionalities\n",
      "atriple,orverifyingthevalidityofageneratedtriple. Other\n",
      "ofmonolingualembeddings. Wedeploythreedif-\n",
      "works like TransH [Wang et al., 2014] and TransR [Lin et\n",
      "ferent techniques to represent cross-lingual transi-\n",
      "al., 2015], introduce different loss functions that represent\n",
      "tions, namely axis calibration, translation vectors,\n",
      "the relational translation in other forms, and have achieved\n",
      "andlineartransformations,andderivefivevariants\n",
      "promisingresultsincompletingtheknowledgegraphs.\n",
      "for MTransE using different loss functions. Our\n",
      "While embedding-based techniques can help improve the\n",
      "models can be trained on partially aligned graphs,\n",
      "completenessofmonolingualknowledge,theproblemofap-\n",
      "wherejustasmallportionoftriplesarealignedwith\n",
      "plyingthese techniqueson cross-lingual knowledgeremains\n",
      "their cross-lingual counterparts. The experiments\n",
      "largelyunexplored. Suchknowledge,includinginter-lingual\n",
      "on cross-lingual entity matching and triple-wise\n",
      "links (ILLs) that match the same entities, and triple-wise\n",
      "alignmentverificationshowpromisingresults,with\n",
      "alignment (TWA) that represents the same relations, is very\n",
      "somevariantsconsistentlyoutperformingotherson\n",
      "helpfulinsynchronizingdifferentlanguage-specificversions\n",
      "differenttasks. WealsoexplorehowMTransEpre-\n",
      "of a knowledge base that evolve independently, as needed\n",
      "serves the key properties of its monolingual coun-\n",
      "to further improve applications built on knowledge bases,\n",
      "terpartTransE.\n",
      "such as Q&A systems, semantic Web, and Web search. In\n",
      "spiteofitsimportance,thiscross-lingualknowledgeremains\n",
      "1 Introduction largelyintact. Infact,inthemostsuccessfulknowledgebase\n",
      "MultilingualknowledgebasessuchasWikipedia[Wikipedia, Wikipedia,wefindthatILLscoverlessthan15%entityalign-\n",
      "2017], WordNet [Bond and Foster, 2013], and Concept- ment.\n",
      "Net[SpeerandHavasi,2013]arebecomingessentialsources Leveragingknowledgegraphembeddingstocross-lingual\n",
      "of knowledge for people and AI-related applications. These knowledge no doubt provides a generic way to help extract\n",
      "knowledgebasesaremodeledasknowledgegraphsthatstore and apply such knowledge. However, it is a non-trivial task\n",
      "two aspects of knowledge: the monolingual knowledge that\n",
      "includesentitiesandrelationsrecordedintheformoftriples, 1Hereafter,(cid:107)·(cid:107)meansl orl normunlessexplicitlyspecified.\n",
      "1 2\n",
      "7102\n",
      "yaM\n",
      "71\n",
      "]IA.sc[\n",
      "3v45930.1161:viXra\n",
      "tofindatractabletechniquetocapturethecross-lingualtran- TransD [Ji et al., 2015], and other forms [Jia et al., 2016;\n",
      "sitions2. Such transitions are more difficult to capture than Nguyen et al., 2016]. All these variants of TransE special-\n",
      "relationaltranslationsforseveralreasons: (i)across-lingual ize entity embeddings for different relations, therefore im-\n",
      "transition has a far larger domain than any monolingual re- proving knowledge graph completion on multi-mapping re-\n",
      "lational translation; (ii) it applies on both entities and rela- lationsatthecostofincreasedmodelcomplexity. Meanwhile\n",
      "tions, which have incoherent vocabularies among different translation-based models cooperate well with other models.\n",
      "languages;(iii)theknownalignmentfortrainingsuchtransi- Forexample,variantsofTransEarecombinedwithwordem-\n",
      "tionsusuallyaccountsforasmallpercentageofaknowledge beddingstohelprelationextractionfromtext[Westonetal.,\n",
      "base. Moreover, thecharacterizationofmonolingualknowl- 2013;Zhongetal.,2015].\n",
      "edge graph structures has to be well-preserved to ensure the\n",
      "correctrepresentationoftheknowledgetobealigned.\n",
      "To address the above issues, we propose a multilingual Inadditiontothese,therearenon-translation-basedmeth-\n",
      "knowledgegraphembeddingmodelMTransE,thatlearnsthe ods. Some of those including UM [Bordes et al., 2011], SE\n",
      "multilingualknowledgegraphstructureusingacombination [Bordes et al., 2012], Bilinear [Jenatton et al., 2012], and\n",
      "of two component models, namely knowledge model and HolE [Nickel et al., 2016], do not explicitly represent re-\n",
      "alignmentmodel. Theknowledgemodelencodesentitiesand lation embeddings. Others including neural-based models\n",
      "relationsinalanguage-specificversionofknowledgegraph.\n",
      "SLM[CollobertandWeston,2008]andNTN[Socheretal.,\n",
      "Weexplorethemethodthatorganizeseachlanguage-specific 2013], and random-walk-based model TADW [Yang et al.,\n",
      "version in a separated embedding space, in which MTransE\n",
      "2015a],areexpressiveandadaptableforbothstructuredand\n",
      "adopts TransE as the knowledge model. On top of that, the text corpora, but are too complex to be incorporated into an\n",
      "alignmentmodellearnscross-lingualtransitionsforbothen- architecturesupportingmultilingualknowledge.\n",
      "titiesandrelationsacrossdifferentembeddingspaces,where\n",
      "the following three representations of cross-lingual align-\n",
      "MultilingualWordEmbeddings. Severalapproacheslearn\n",
      "ment are considered: distance-based axis calibration, trans-\n",
      "multilingualwordembeddingsonparalleltextcorpora.Some\n",
      "lation vectors, and linear transformations. Thus, we obtain\n",
      "of those can be extended to multilingual knowledge graphs,\n",
      "five variants of MTransE based on different loss functions,\n",
      "such as LM [Mikolov et al., 2013] and CCA [Faruqui and\n",
      "and identify the best variant by comparing them on cross-\n",
      "Dyer, 2014] which induce offline transitions among pre-\n",
      "lingualalignmenttasksusingtwopartiallyalignedtrilingual\n",
      "trained monolingual embeddings in forms of linear trans-\n",
      "graphs constructed from Wikipedia triples. We also show\n",
      "formations and canonical component analysis respectively.\n",
      "that MTransE performs as well as its monolingual counter-\n",
      "Theseapproachesdonotadjusttheinconsistentvectorspaces\n",
      "partTransEonmonolingualtasks.\n",
      "via calibration or jointly training with the alignment model,\n",
      "Therestofthepaperisorganizedasfollows. Wefirstdis-\n",
      "thus fail to perform well on knowledge graphs as the par-\n",
      "cusstherelatedwork,andthenintroduceourapproachinthe\n",
      "allelism exists only in small portions. A better approach\n",
      "section that follows. After that we present the experimental\n",
      "OT[Xingetal.,2015]jointlylearnsregularizedembeddings\n",
      "results,andconcludethepaperinthelastsection.\n",
      "and orthogonal transformations, which is however found to\n",
      "2 RelatedWork beovercomplicatedduetotheinconsistencyofmonolingual\n",
      "vectorspacesandthelargediversityofrelationsamongenti-\n",
      "While,atthebestofourknowledge,thereisnopreviouswork ties.\n",
      "on learning multilingual knowledge graph embeddings, we\n",
      "will describe next three lines of work which are closely re-\n",
      "latedtothistopic. KnowledgeBasesAlignment. Someprojectsproducecross-\n",
      "Knowledge Graph Embeddings. Recently, significant ad- lingualalignmentinknowledgebasesatthecostofextensive\n",
      "vancement has been made in using the translation-based humaninvolvementanddesigninghand-craftedfeaturesded-\n",
      "method to train monolingual knowledge graph embeddings. icated to specific applications. Wikidata [Vrandecˇic´, 2012]\n",
      "To characterize a triple (h,r,t), models of this family fol- and DBpedia [Lehmann et al., 2015] rely on crowdsourc-\n",
      "low a common assumption h +r ≈ t, where h and t ing to create ILLs and relation alignment. YAGO [Mahdis-\n",
      "r r r r\n",
      "are either the original vectors of h and t, or the transformed oltanietal.,2015]minesassociationrulesonknownmatches,\n",
      "vectors under a certain transformation w.r.t. relation r. The which combines many confident scores and requires exten-\n",
      "forerunner TransE [Bordes et al., 2013] sets h and t as sively fine tuning. Many other works require sources that\n",
      "r r\n",
      "the original h and t, and achieves promising results in han- are external to the graphs, from well-established schemata\n",
      "dling1-to-1relations. LaterworksimproveTransEonmulti- or ontologies [Nguyen et al., 2011; Suchanek et al., 2011;\n",
      "mapping relations by introducing relation-specific transfor- Rinseretal.,2013]toentitydescriptions[Yangetal.,2015b],\n",
      "mations on entities to obtain different h and t, including which being unavailable to many knowledge bases such as\n",
      "r r\n",
      "projectionsonrelation-specifichyperplanesinTransH[Wang YAGO,WordNet,andConceptNet[SpeerandHavasi,2013].\n",
      "et al., 2014], linear transformations to heterogeneous rela- Such approaches also involve complicated model depen-\n",
      "tionspacesinTransR[Linetal.,2015],dynamicmatricesin dencies that are not tractable and reusable. By contrast,\n",
      "embedding-based methods are simple and general, require\n",
      "2Weusethewordtransitionheretodifferentiatefromtherela- littlehumaninvolvement,andgeneratetask-independentfea-\n",
      "tionaltranslationsamongentitiesintranslation-basedmethods. turesthatcancontributetootherNLPtasks.\n",
      "3 MultilingualKnowledgeGraph tionisgivenasbelow:\n",
      "Embeddings (cid:88)\n",
      "S = S (T,T(cid:48))\n",
      "A a\n",
      "Weherebybeginourmodelingwiththeformalizationofmul-\n",
      "(T,T(cid:48))∈δ(Li,Lj)\n",
      "tilingualknowledgegraphs.\n",
      "Thereof, the alignment score S (T,T(cid:48)) iterates through all\n",
      "a\n",
      "3.1 MultilingualKnowledgeGraphs pairs of aligned triples. Three different techniques to score\n",
      "InaknowledgebaseKB,weuseLtodenotethesetoflan- thealignmentareconsidered:distance-basedaxiscalibration,\n",
      "guages, and L2 to denote the 2-combination of L (i.e., the translationvectors,andlineartransformations. Eachofthem\n",
      "set of unordered language pairs). For a language L ∈ L, is based on a different assumption, and constitutes different\n",
      "G denotesthelanguage-specificknowledgegraphofL,and formsofS aalongside.\n",
      "L\n",
      "E and R respectively denote the corresponding vocabu- Distance-based Axis Calibration. This type of alignment\n",
      "L L\n",
      "laries of entity expression and relation expression. T = models penalize the alignment based on the distances of\n",
      "(h,r,t) denotes a triple in G such that h,t ∈ E and cross-lingualcounterparts. Eitherofthefollowingtwoscor-\n",
      "L L\n",
      "r ∈ R. Boldfaced h, r, t respectively represent the em- ingscanbeadoptedtothemodel.\n",
      "L\n",
      "bedding vectors of head h, relation r, and tail t. For a lan- S =(cid:107)h−h(cid:48)(cid:107)+(cid:107)t−t(cid:48)(cid:107)\n",
      "guage pair (L,L ) ∈ L2, δ(L,L ) denotes the alignment a1\n",
      "1 2 1 2\n",
      "setwhichcontainsthepairsoftriplesthathavealreadybeen S regulatesthatcorrectlyalignedmultilingualexpressions\n",
      "a1\n",
      "aligned between L and L. For example, across the lan- ofthesameentitytendtohavecloseembeddingvectors.Thus\n",
      "1 2\n",
      "(cid:0)\n",
      "guages English and French, we may have (State of Cal- byminimizingthelossfunctionthatinvolvesS onknown\n",
      "a1\n",
      "ifornia, capital city, Sacramento),(E´tat de Californie, cap- pairs of aligned triples, the alignment model adjusts axes of\n",
      "(cid:1) embeddingspacestowardsthegoalofcoincidingthevectors\n",
      "itale, Sacramento) ∈ δ(English,French). The alignment\n",
      "ofthesameentityindifferentlanguages.\n",
      "set commonly exists in a small portion in a multilingual\n",
      "knowledgebase[Vrandecˇic´,2012;Mahdisoltanietal.,2015; S =(cid:107)h−h(cid:48)(cid:107)+(cid:107)r−r(cid:48)(cid:107)+(cid:107)t−t(cid:48)(cid:107)\n",
      "Lehmannetal.,2015],andisonepartofknowledgewewant\n",
      "a2\n",
      "toextend. S a2 overlays the penalty of relation alignment to S a1 to ex-\n",
      "Our model consists of two components that learn on the plicitlyconvergecoordinatesofthesamerelation.\n",
      "twofacetsofKB: theknowledgemodelthatencodestheen- The alignment models based on axis calibration assume\n",
      "tities and relations from each language-specific graph struc- analogous spatial emergence of items in each language.\n",
      "ture, and the alignment model that learns the cross-lingual Therefore, itrealizesthecross-lingualtransitionbycarrying\n",
      "transitions from the existing alignment. We define a model forwardthevectorofagivenentityorrelationfromthespace\n",
      "for each language pair from L2 that has a non-empty align- oftheoriginallanguagetothatoftheotherlanguage.\n",
      "mentset.Thus,foraKB withmorethantwolanguages,aset TranslationVectors. Thismodelencodescross-lingualtran-\n",
      "ofmodelscomposesthesolution. Inthefollowing,weusea sitions into vectors. It consolidates alignment into graph\n",
      "language pair (L, L ) ∈ L2 as an example to describe how structures and characterizes cross-lingual transitions as reg-\n",
      "i j\n",
      "wedefineeachcomponentofamodel. ularrelationaltranslations. HenceS a3 asbelowisderived.\n",
      "3.2 KnowledgeModel S a3 =(cid:13) (cid:13)h+v ie j −h(cid:48)(cid:13) (cid:13)+(cid:13) (cid:13)r+v ir j −r(cid:48)(cid:13) (cid:13)+(cid:13) (cid:13)t+v ie j −t(cid:48)(cid:13) (cid:13)\n",
      "ForeachlanguageL∈L,adedicatedk-dimensionalembed- Thereof ve and vr are respectively deployed as the entity-\n",
      "ij ij\n",
      "dingspaceRk isassignedforvectorsofE andR, where dedicatedandrelation-dedicatedtranslationvectorsbetween\n",
      "L L L\n",
      "Risthefieldofrealnumbers. Weadoptthebasictranslation- L andL,suchthatwehavee+ve ≈e(cid:48)forembeddingvec-\n",
      "i j ij\n",
      "based method of TransE for each involved language, which tors e, e(cid:48) of the same entity e expressed in both languages,\n",
      "benefits the cross-lingual tasks by representing embeddings and r + vr ≈ r(cid:48) for those of the same relation. We de-\n",
      "ij\n",
      "uniformlyindifferentcontextsofrelations. Thereforeitsloss ploytwotranslationvectorsinsteadofone,becausethereare\n",
      "functionisgivenasbelow: farmoredistinctentitiesthanrelations,andusingonevector\n",
      "(cid:88) (cid:88) easilyleadstoimbalancedsignalsfromrelations.\n",
      "S K = (cid:107)h+r−t(cid:107) Such a model obtains a cross-lingual transition of an em-\n",
      "L∈{Li,Lj}(h,r,t)∈GL beddingvectorbyaddingthecorrespondingtranslationvec-\n",
      "tor. Moreover, it is easy to see that ve = −ve and vr =\n",
      "Itmeasurestheplausibilityofallgiventriples. Byminimiz- −vr hold. Therefore, as we obtain ti hj e translj ai tion vecij tors\n",
      "ingthelossfunction,theknowledgemodelpreservesmono- ji\n",
      "fromL toL,wecanalwaysusethesamevectorstotrans-\n",
      "lingual relations among entities, while also acts as a regu- i j\n",
      "lateintheoppositedirection.\n",
      "larizer for the alignment model. Meanwhile, the knowledge\n",
      "Linear Transformations. The last category of alignment\n",
      "modelpartitionstheknowledgebaseintodisjointsubsetsthat\n",
      "models deduce linear transformations between embedding\n",
      "canbetrainedinparallel.\n",
      "spaces. S as below learns a k ×k square matrix Me as\n",
      "a4 ij\n",
      "3.3 AlignmentModel alineartransformationonentityvectorsfromL itoL j,given\n",
      "kasthedimensionalityoftheembeddingspaces.\n",
      "Theobjectiveofthealignmentmodelistoconstructthetran-\n",
      "sitionsbetweenthevectorspacesofL iandL j. Itslossfunc- S a4 =(cid:13) (cid:13)Me ijh−h(cid:48)(cid:13) (cid:13)+(cid:13) (cid:13)Me ijt−t(cid:48)(cid:13) (cid:13)\n",
      "Table1: Summaryofmodelvariants. Table2: StatisticsoftheWK3ldatasets.\n",
      "Var ModelComplexity Cross-lingualTransition SearchComplexity Dataset #Entriples #Frtriples #Detriples #Alignedtriples\n",
      "Var1 O(nekl+nrkl) τ τi ij j( (e r) )= =e\n",
      "r\n",
      "OO (( nn rek k)\n",
      ")\n",
      "WK3l-15k 203,502 170,605 145,616 EE nn -- DFr e: :1 36 7,,4 17 70\n",
      "0\n",
      "Var2 O(nekl+nrkl) τ τi ij j( (e r) )= =e\n",
      "r\n",
      "OO (( nn rek k)\n",
      ")\n",
      "WK3l-120k 1,376,011 767,750 391,108 E En n- -F Dr e:1 :62 94,, 44 13 33\n",
      "Var3 O(nekl+ +n kr lk 2)l τ τi ij j( (e r) )= =e r++ vv irie jj OO (( nn rek k) ) Table3: Numberofentityinter-linguallinks(ILLs).\n",
      "Var4 O(ne +kl 0+.5kn 2r lk 2l ) τ τi ij j( (e r) )= =M Me i e ij je r OO (( nn rek k2 2+ +n ne rk k) ) WD Kat 3a l-S 1e 5t k E 3,n 7- 3F 3r F 3,r 8-E 15n E 1n,8- 4D 0e D 1,e 6- 1E 0n\n",
      "Var5 O(nekl ++ kn 2r l2k )l τ τi ij j( (e r) )= =M Me i r ij je r OO (( nn rek k2 2+ +n ne rk k) ) WK3l-120k 42,413 41,513 7,567 5,921\n",
      "Notation:eandrarerespectivelythevectorsofanentityeandarelationr,kis We enforce the constraint that the l 2 norm of any entity\n",
      "thedimensionoftheembeddingspaces,listhecardinalityofL,neandnrare embeddingvectoris1,thusregularizeembeddingvectorsto\n",
      "respectivelythenumberofentitiesandthenumberofrelations,wherene(cid:29)nr.\n",
      "a unit spherical surface. This constraint is employed in the\n",
      "S additionallybringsinasecondlineartransformationMr\n",
      "a5 ij literature[Bordesetal.,2013;Bordesetal.,2014;Jenattonet\n",
      "forrelationvectors,whichisofthesameshapeasMe. The\n",
      "ij al.,2012]andhastwoimportanteffects: (i)ithelpsavoidthe\n",
      "useofadifferentmatrixisagainduetodifferentredundancy\n",
      "case where the training process trivially minimizes the loss\n",
      "ofentitiesandrelations.\n",
      "functionbyshrinkingthenormofembeddingvectors,and(ii)\n",
      "S a5 =(cid:13) (cid:13)Me ijh−h(cid:48)(cid:13) (cid:13)+(cid:13) (cid:13)Mr ijr−r(cid:48)(cid:13) (cid:13)+(cid:13) (cid:13)Me ijt−t(cid:48)(cid:13) (cid:13) eit ti am l.p,l 2ie 0s 15th ]e foin rv Ve art ribi al nit dy Vo af rth.elineartransformations[Xing\n",
      "4 5\n",
      "Unlike axis calibration, linear-transformation-based align- We initialize vectors by drawing from a uniform distribu-\n",
      "ment model treats cross-lingual transitions as the topologi- tionontheunitsphericalsurface,andinitializematricesusing\n",
      "caltransformationofembeddingspaceswithoutassumingthe randomorthogonalinitialization[Saxeetal.,2014].Negative\n",
      "similarityofspatialemergence. samplingisnotemployedintraining,whichwefinddoesnot\n",
      "The cross-lingual transition of a vector is obtained by ap- noticeablyaffecttheresults.\n",
      "plyingthecorrespondinglineartransformation. Itisnotewor-\n",
      "thy that, regularization of embedding vectors in the training 4 Experiments\n",
      "process(whichwillbeintroducedsoonafter)ensuresthein-\n",
      "vertibility of the linear transformations such that Me −1 = In this section, we evaluate the proposed methods on two\n",
      "ij cross-lingualtasks: cross-lingualentitymatching,andtriple-\n",
      "Me andMr −1 = Mr. Thusthetransitionintherevertdi-\n",
      "ji ij ji wisealignmentverification. Wealsoconductexperimentson\n",
      "rectionisalwaysenabledeventhoughthemodelonlylearns twomonolingualtasks.Besides,acasestudywithknowledge\n",
      "thetransformationsofonedirection. alignmentexamplesisincludedintheAppendixof[Chenet\n",
      "3.4 VariantsofMTransE al.,2017].\n",
      "Data Sets. Experimental results on the trilingual data sets\n",
      "Combiningtheabovetwocomponentmodels,MTransEmin-\n",
      "WK3l are reported in this section. WK3l contains English\n",
      "imizesthefollowinglossfunctionJ = S +αS,whereα\n",
      "K A\n",
      "(En), French (Fr), and German (De) knowledge graphs un-\n",
      "isahyperparameterthatweightsS andS.\n",
      "K A der DBpedia’s dbo:Person domain, where a part of triples\n",
      "Aswehavegivenoutfivevariantsofthealignmentmodel,\n",
      "are aligned by verifying the ILLs on entities, and multi-\n",
      "each of which correspondingly defines its specific way of\n",
      "lingual labels of the DBpedia ontology on some relations.\n",
      "computing cross-lingual transitions of embedding vectors.\n",
      "The number of entities in each language is adjusted to ob-\n",
      "We denote Var as the variant of MTransE that adopts the\n",
      "k\n",
      "tain two data sets. For each of the three languages thereof,\n",
      "k-th alignment model which employs S. In practice, the\n",
      "ak\n",
      "WK3l-15kmatchesthenumberofnodes(about15,000)with\n",
      "searchingofacross-lingualcounterpartforasourceisalways\n",
      "FB15k—thelargestmonolingualgraphusedbymanyrecent\n",
      "done by querying the nearest neighbor from the result point\n",
      "works [Zhong et al., 2015; Lin et al., 2015; Ji et al., 2015;\n",
      "of the cross-lingual transition. We denote function τ that\n",
      "ij Jia et al., 2016], and the number of nodes in WK3l-120k is\n",
      "mapsacross-lingualtransitionofavectorfromL toL,or\n",
      "i j\n",
      "several times larger. For both data sets, German graphs are\n",
      "simply τ in a bilingual context. As stated, the solution in a\n",
      "sparserthanEnglishandFrenchgraphs.Wealsocollectextra\n",
      "multi-lingualscenarioconsistsofasetofmodelsofthesame\n",
      "variant defined on every language pair in L2. Table 1 sum- entity ILLs for the evaluation of cross-lingual entity match-\n",
      "ing,whosequantityisshowninTable3. Meanwhile,wede-\n",
      "marizesthemodelcomplexity,thedefinitionofcross-lingual\n",
      "riveanothertrilingualdatasetCN3lfromConceptNet[Speer\n",
      "transitions, and the complexity of searching a cross-lingual\n",
      "and Havasi, 2013]. Additional results on CN3l that lead to\n",
      "counterpartforeachvariant.\n",
      "similar evaluation conclusions are reported in the Appendix\n",
      "3.5 Training\n",
      "of[Chenetal.,2017].\n",
      "We optimize the loss function using on-line stochastic gra-\n",
      "4.1 Cross-lingualEntityMatching\n",
      "dient descent [Wilson and Martinez, 2003]. At each step,\n",
      "we update the parameter θ by setting θ ← θ − λ∇ J, The objective of this task is to match the same entities from\n",
      "θ\n",
      "where λ is the learning rate. Instead of directly updating differentlanguagesinKB. Duetothelargecandidatespace,\n",
      "J, our implementation optimizes S and αS alternately. this task emphasizes more on ranking a set of candidates\n",
      "K A\n",
      "In detail, at each epoch we optimize θ ← θ −λ∇ S and rather than acquiring the best answer. We perform this task\n",
      "θ K\n",
      "θ ←θ−λ∇ αS inseparatedgroupsofsteps. onbothdatasetstocomparefivevariantsofMTransE.\n",
      "θ A\n",
      "Table4: Cross-lingualentitymatchingresult.\n",
      "DataSet WK3l-15k WK3l-120k\n",
      "AlignedLanguages En-Fr Fr-En En-De De-En En-Fr Fr-En En-De De-En\n",
      "Metric Hits@10 Mean Hits@10 Mean Hits@10 Mean Hits@10 Mean Hits@10 Hits@10 Hits@10 Hits@10\n",
      "LM 12.31 3621.17 10.42 3660.98 22.17 5891.13 15.21 6114.08 11.74 14.26 24.52 13.58\n",
      "CCA 20.78 3094.25 19.44 3017.90 26.46 5550.89 22.30 5855.61 19.47 12.85 25.54 20.39\n",
      "OT 44.97 508.39 40.92 461.18 44.47 155.47 49.24 145.47 38.91 37.19 38.85 34.21\n",
      "Var1 51.05 470.29 46.64 436.47 48.67 146.13 50.60 167.02 38.58 36.52 42.06 47.79\n",
      "Var2 45.25 570.72 41.74 565.38 46.27 168.33 49.00 211.94 31.88 30.84 41.22 40.39\n",
      "Var3 38.64 587.46 36.44 464.64 50.82 125.15 52.16 151.84 38.26 36.45 50.48 52.24\n",
      "Var4 59.24 190.26 57.48 199.64 66.25 74.62 68.53 42.31 48.66 47.43 57.56 63.49\n",
      "Var5 59.52 191.36 57.07 204.45 60.25 99.48 66.03 54.69 45.65 47.48 64.22 67.85\n",
      "Figure1: Precision-recallcurvesforcross-lingualentitymatchingonWK3l-15k.\n",
      "ToshowthesuperiorityofMTransE,weadaptLM,CCA, indicatethattheinterferencecausedbylearninganadditional\n",
      "and OT (which are introduced in Section 2) to their knowl- relation-dedicatedtransformationinVar isnegligibletothe\n",
      "5\n",
      "edgegraphequivalences. entity-dedicatedtransformation.Correspondingly,webelieve\n",
      "thatthereasonforVar tobeoutperformedbyVar andVar\n",
      "Evaluation Protocol. Each MTransE variant is trained on 3 4 5\n",
      "is that it fails to differentiate well the over-frequent cross-\n",
      "a complete data set. LM and CCA are implemented by in-\n",
      "lingualalignmentfromregularrelations. Therefore,thechar-\n",
      "ducing the corresponding transformations across separately\n",
      "acterizationforcross-lingualalignmentisnegativelyaffected\n",
      "trainedknowledgemodelsonmonolingualgraphs,whileus-\n",
      "bythelearningprocessformonolingualrelationsinavisible\n",
      "ingthealignmentsetsasanchors. TrainingOTisquitesim-\n",
      "degree. Axis calibration appears to be unstable on this task.\n",
      "ilar to MTransE, we add the process of orthogonalization to\n",
      "Wehypothesizethatthissimpletechniqueisaffectedbytwo\n",
      "the training of the alignment model, since the regularization\n",
      "factors: coherence between language-specific versions, and\n",
      "of vectors has already been enforced. The entity ILLs are\n",
      "density of the graphs. Var is always outperformed by Var\n",
      "used as ground truth for test. We take these unidirectional 2 1\n",
      "due to the negative effect of the calibration based on rela-\n",
      "linksbetweenEnglish-FrenchandEnglish-German,i.e.,four\n",
      "tions. Webelievethisisbecausemulti-mappingrelationsare\n",
      "directions in total. For each ILL (e,e(cid:48)), we perform a kNN\n",
      "notsowell-capturedbyTransEasexplainedin[Wangetal.,\n",
      "searchfromthecross-lingualtransitionpointofe(i.e.,τ(e))\n",
      "2014], therefore disturb the calibration of the entire embed-\n",
      "and record the rank of e(cid:48). Following the convention [Xing\n",
      "ding spaces. Although Var still outperforms Var on entity\n",
      "etal.,2015;Jiaetal.,2016],weaggregatetwometricsover 1 3\n",
      "matchingbetweenEnglishandFrenchgraphsinWK3l-15k<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  23751,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Cross-lingual entity matching', 'Triple-wise alignment verification']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  by Var\n",
      "used as ground truth for test. We take these unidirectional 2 1\n",
      "due to the negative effect of the calibration based on rela-\n",
      "linksbetweenEnglish-FrenchandEnglish-German,i.e.,four\n",
      "tions. Webelievethisisbecausemulti-mappingrelationsare\n",
      "directions in total. For each ILL (e,e(cid:48)), we perform a kNN\n",
      "notsowell-capturedbyTransEasexplainedin[Wangetal.,\n",
      "searchfromthecross-lingualtransitionpointofe(i.e.,τ(e))\n",
      "2014], therefore disturb the calibration of the entire embed-\n",
      "and record the rank of e(cid:48). Following the convention [Xing\n",
      "ding spaces. Although Var still outperforms Var on entity\n",
      "etal.,2015;Jiaetal.,2016],weaggregatetwometricsover 1 3\n",
      "matchingbetweenEnglishandFrenchgraphsinWK3l-15k,\n",
      "all test cases, i.e., the proportion of ranks no larger than 10\n",
      "coherencesomewhatdropsalongsidewhenscalinguptothe\n",
      "Hits@10(inpercentage),andthemeanrankMean. Wepre-\n",
      "larger data set so as to hinder the calibration. The German\n",
      "fer higher Hits@10 and lower Mean that indicate a better\n",
      "graphsaresparse,thusshouldhavesetabarrierforprecisely\n",
      "outcome.\n",
      "constructing embedding vectors and hindered calibration on\n",
      "For training, we select the learning rate λ among {0.001,\n",
      "theotherside.ThereforeVar stillperformscloselytoVar in\n",
      "1 3\n",
      "0.01, 0.1}, α among {1, 2.5, 5, 7.5}, l or l norm in loss\n",
      "1 2 the English-German task on WK3l-15k and English-French\n",
      "functions, and dimensionality k among {50, 75, 100, 125}.\n",
      "task on WK3l-120k, but is outperformed by Var in the last\n",
      "3\n",
      "The best configuration on WK3l-15k is λ = 0.01, α = 5,\n",
      "setting. In general, the variants that use linear transforma-\n",
      "k = 75, l normforVar, Var, LM,andCCA,l normfor\n",
      "1 1 2 2 tions are the most desired. This conclusion is supported by\n",
      "othervariantsandOT.WhilethebestconfigurationonWK3l-\n",
      "their promising outcomeon this task, and it isalso reflected\n",
      "120k is λ = 0.01, α = 5, k = 100, and l norm for all\n",
      "2 intheprecision-recallcurvesshowninFigure1.\n",
      "models. Thetrainingonbothdatasetstakes400epochs.\n",
      "4.2 Triple-wiseAlignmentVerification\n",
      "Results. WereportHits@10andMean forWK3l-15k, and\n",
      "Hits@10 for WK3l-120k, on the four involved directions Thistaskistoverifywhetheragivenpairofalignedtriplesare\n",
      "of cross-lingual matching in Table 4. As expected, with- truly cross-lingual counterparts. It produces a classifier that\n",
      "out jointly adapting the monolingual vector spaces with the helpswithverifyingcandidatesoftriplematching[Nguyenet\n",
      "knowledgealignment,LMandCCAarelargelyoutperformed al.,2011;Rinseretal.,2013].\n",
      "by the rest. While the orthogonality constraint being too Evaluation Protocol. We create positive cases by isolating\n",
      "strong to be enforced in these cases, OT performs at most 20%ofthealignmentset. Similarto[Socheretal.,2013],we\n",
      "closelytothesimplestcasesofMTransE.ForMTransE,Var randomly corrupt positive cases to generate negative cases.\n",
      "4\n",
      "and Var outperform the other three variants under all set- Indetail,givenapairofcorrectlyalignedtriples(T,T(cid:48)),itis\n",
      "5\n",
      "tings. Thefairlycloseresultsobtainedbythesetwovariants corruptedby(i)randomlyreplacingoneofthesixelementsin\n",
      "Table5: AccuracyofTWAverification(%). Table6: Resultsoftailprediction Table7: Resultsofrelationprediction\n",
      "DataSet WK3l-15k WK3l-120k (Hits@10). (Hits@10).\n",
      "Languages En&Fr En&De En&Fr En&De DataSet WK3l-15k WK3l-120k DataSet WK3l-15k WK3l-120k\n",
      "LM 52.23 63.61 59.98 59.98 Language En Fr En Fr Language En Fr En Fr\n",
      "CCA 52.28 66.49 65.89 61.01 TransE 42.19 25.06 36.78 25.38 TransE 61.79 62.55 60.06 65.29\n",
      "V V V V VO a a a a aT r r r r r1 2 3 4 5 9 9 9 9 9 93 3 0 0 4 4......2 2 2 3 5 90 5 4 8 8 0 9 98 9 8 8 5 47 1 6 4...... 0 99 2 5 2 3 57 4 9 4 8 9 8 8 9 98 1 9 7 3 2......6 2 3 9 4 65 7 6 9 8 3 9 98 9 8 8 3 35 1 6 7...... 0 62 3 2 0 6 64 5 9 4 V V V V Va a a a ar r r r r1 2 3 4 5 4 4 4 4 40 0 0 1 1.....3 8 9 0 77 0 7 3 9 2 2 2 2 23 4 2 5 5.....4 7 2 4 75 7 6 6 7 3 3 3 3 39 6 5 9 8.....0 0 9 6 39 2 9 4 5 2 2 1 2 25 1 9 5 4.....5 1 6 5 62 3 9 9 8 V V VV Va a aa ar r rr r1 3 42 5 6 5 5 6 60 4 8 3 4.....1 3 3 7 78 3 2 4 9 6 6 5 6 60 2 9 4 3.....7 9 4 7 73 8 4 7 1 6 6 6 6 61 1 0 0 0.....7 1 1 2 75 1 4 6 7 6 4 66 61 8 65 7.....4 0 84 67 6 66 4\n",
      "thetwotripleswithanotherelementfromthesamelanguage, ofMTransE,butisstillleftbehindbyVar andVar.\n",
      "4 5\n",
      "or(ii)randomlysubstitutingeitherT orT(cid:48)withanothertriple\n",
      "4.3 MonolingualTasks\n",
      "fromthesamelanguage. Cases(i)and(ii)respectivelycon-\n",
      "tribute negative cases that are as many as 100% and 50% of The above experiments have shown the strong capability\n",
      "positivecases.Weuse10-foldcross-validationonthesecases of MTransE in handling cross-lingual tasks. Now we re-\n",
      "totrainandevaluatetheclassifier. port the results on comparing MTransE with its monolin-\n",
      "We use a simple threshold-based classifier similar to the gual counterpart TransE on two monolingual tasks intro-\n",
      "widely-usedonesfortripleclassification[Socheretal.,2013; duced in the literature [Bordes et al., 2013; Bordes et al.,\n",
      "Wang et al., 2014; Lin et al., 2015]. For a given pair of 2014], namely tail prediction (predicting t given h and r)\n",
      "aligned triples (T,T(cid:48)) = (cid:0) (h,r,t),(h(cid:48),r(cid:48),t(cid:48))(cid:1), the dissim- and relation prediction (predicting r given h and t), us-\n",
      "ilarity function is defined as f (T,T(cid:48)) = (cid:107)τ(h)−h(cid:48)(cid:107) + ing the English and French versions of our data sets. Like\n",
      "(cid:107)τ(r)−r(cid:48)(cid:107) + (cid:107)τ(t)−t(cid:48)(cid:107). Td he classifier finds a thre2 sh- previous works [Bordes et al., 2013; Wang et al., 2014;\n",
      "oldσ sucht2 hatf < σ impl2 iespositive,otherwisenegative. Jia et al., 2016], for each language version, 10% triples are\n",
      "d\n",
      "The value of σ is determined by maximizing the accuracy selectedasthetestset,andtheremainingbecomesthetrain-\n",
      "foreachfoldonthetrainingset. Suchasimpleclassification ingset. EachMTransEvariantistraineduponbothlanguage\n",
      "ruleadequatelyreliesonhowpreciselyeachmodelrepresents versions of the training set for the knowledge model, while\n",
      "cross-lingualtransitionsforbothentitiesandrelations. the intersection between the alignment set and the training\n",
      "setisusedforthealignmentmodels. TransEistrainedonei-\n",
      "Wecarryforwardthecorrespondingconfigurationfromthe\n",
      "ther language version of the training set. Again, we use the\n",
      "lastexperiment,justtoshowtheperformanceofeachvariant\n",
      "configurationfromthepreviousexperiment.\n",
      "undercontrolledvariables.\n",
      "Results. Table 5 shows the mean accuracy, with a standard Results. The results for Hits@10 are reported in Tables 6\n",
      "and 7. They imply that MTransE preserves well the char-\n",
      "deviation below 0.009 in cross-validation for all settings.\n",
      "acterization of monolingual knowledge. For each setting,\n",
      "Thus, the results are statistically sufficient to reflect the per-\n",
      "Var, Var, andVar performatleastaswellasTransE,and\n",
      "formance of classifiers. Note that the results appear to be 1 4 5\n",
      "some even outperforms TransE under certain settings. This\n",
      "better than those of the previous task since this is a binary\n",
      "signifies that the alignment model does not interfere much\n",
      "classificationproblem. Intuitively,thelinear-transformation-\n",
      "withtheknowledgemodelincharacterizingmonolingualre-\n",
      "basedMTransEperformsteadilyandtaketheleadonallset-\n",
      "lations,butmighthaveactuallystrengtheneditsincecoherent\n",
      "tings. WealsoobservethatVar,thoughlearnsanadditional\n",
      "5\n",
      "portions of knowledge are unified by the alignment model.\n",
      "relation-dedicatedtransformation,stillperformsconsiderably\n",
      "Since such coherence is currently not measured, this ques-\n",
      "close to Var (the difference is at most 0.85%). The simple\n",
      "4\n",
      "tionisleftasafuturework. Theotherquestionthatdeserves\n",
      "Var is the runner-up, and is between 1.65% and 3.79% to\n",
      "1\n",
      "further attention is, how other knowledge models involving\n",
      "the optimal solutions. However the relation-dedicated cali-\n",
      "brationinVar causesanotablesetback(4.12%∼8.44%from relation-specific entity transformations [Wang et al., 2014;\n",
      "2\n",
      "Linetal.,2015;Jietal.,2015;Jiaetal.,2016;Nguyenetal.,\n",
      "the optimal). The performance of Var falls behind slightly\n",
      "3\n",
      "more than Var (4.52%∼10.79% from the optimal) due to\n",
      "2016]mayinfluencemonolingualandcross-lingualtasks.\n",
      "2\n",
      "thefailureindistinguishingcross-lingualalignmentfromreg-\n",
      "5 ConclusionandFutureWork\n",
      "ular relations. Meanwhile, we single out the accuracy on\n",
      "the portion of negative cases where only the relation is cor- Atthebestofourknowledge,thispaperisthefirstworkthat\n",
      "rupted for English-French in WK3l-15k. The five variants generalizesknowledgegraphembeddingstothemultilingual\n",
      "receive 97.73%, 93.78%, 82.34%, 98.57%, and 98.54%, re- scenario. OurmodelMTransEcharacterizesmonolingualre-\n",
      "spectively. The close accuracy of Var and Var indicates lationsandcomparesthreedifferenttechniquestolearncross-\n",
      "4 5\n",
      "that the only transformation learnt from entities in Var is lingualalignmentforentitiesandrelations. Extensiveexperi-\n",
      "4\n",
      "enoughtosubstitutetherelation-dedicatedtransformationin mentsonthetasksofcross-lingualentitymatchingandtriple\n",
      "Var fordiscriminatingrelationalignment,whilelearningthe alignment verification show that the linear-transformation-\n",
      "5\n",
      "additional transformation in Var does not notably interfere technique is the best among the three. Moreover, MTransE\n",
      "5\n",
      "theoriginalone. However, itappliesdifferentlytoaxiscali- preservesthekeypropertiesofmonolingualknowledgegraph\n",
      "brationsinceVar doesnotimprovebutactuallyimpairsthe embeddingsonmonolingualtasks.\n",
      "2\n",
      "cross-lingualtransitionsforrelations.Forthesamereasonsas The results here are very encouraging, but we also point\n",
      "above,LMandCCAdonotmatchwithMTransEinthisex- outopportunitiesforfurtherworkandimprovements. Inpar-\n",
      "perimentaswell,whileOTperformscloselytosomevariants ticular, we should explore how to substitute the simple loss\n",
      "functionoftheknowledgemodelusedinMTransEwithmore [Linetal.,2015] Yankai Lin, Zhiyuan Liu, Maosong Sun,\n",
      "advanced ones involving relation-specific entity transforma- Yang Liu, and Xuan Zhu. Learning entity and relation\n",
      "tions. More sophisticated tasks of cross-lingual triple com- embeddings for knowledge graph completion. In AAAI,\n",
      "pletion may also be conducted. Combining MTransE with 2015.\n",
      "multilingualwordembeddings[Xingetal.,2015]isanother\n",
      "[Mahdisoltanietal.,2015] Farzaneh Mahdisoltani, Joanna\n",
      "meaningfuldirectionsinceitwillprovideausefultooltoex-\n",
      "Biega,FabianSuchanek,etal. Yago3: Aknowledgebase\n",
      "tractnewrelationsfrommultilingualtextcorpora.\n",
      "frommultilingualWikipedias. InCIDR,2015.\n",
      "References [Mikolovetal.,2013] TomasMikolov,QuocVLe,andIlya\n",
      "Sutskever. Exploiting similarities among languages for\n",
      "[BondandFoster,2013] Francis Bond and Ryan Foster. machinetranslation. arXiv,2013.\n",
      "Linking and extending an open multilingual Wordnet. In\n",
      "ACL,pages1352–1362,2013. [Nguyenetal.,2011] Thanh Nguyen, Viviane Moreira,\n",
      "HuongNguyen,HoaNguyen,andJulianaFreire.Multilin-\n",
      "[Bordesetal.,2011] Antoine Bordes, Jason Weston, Ronan\n",
      "gualschemamatchingforWikipediainfoboxes. PVLDB,\n",
      "Collobert, and Yoshua Bengio. Learning structured em-\n",
      "5(2):133–144,2011.\n",
      "beddingsofknowledgebases. InAAAI,2011.\n",
      "[Nguyenetal.,2016] DatQuocNguyen,KairitSirts,Lizhen\n",
      "[Bordesetal.,2012] Antoine Bordes, Xavier Glorot, Jason\n",
      "Qu, and Mark Johnson. Stranse: a novel embedding\n",
      "Weston,andYoshuaBengio. Jointlearningofwordsand\n",
      "modelofentitiesandrelationshipsinknowledgebases. In\n",
      "meaningrepresentationsforopen-textsemanticparsing.In\n",
      "NAACLHLT,pages460–466,2016.\n",
      "AISTATS,pages127–135,2012.\n",
      "[Nickeletal.,2016] Maximilian Nickel, Lorenzo Rosasco,\n",
      "[Bordesetal.,2013] Antoine Bordes, Nicolas Usunier,\n",
      "TomasoPoggio,etal. Holographicembeddingsofknowl-\n",
      "Alberto Garcia-Duran, Jason Weston, and Oksana\n",
      "edgegraphs. InAAAI,2016.\n",
      "Yakhnenko. Translating embeddings for modeling\n",
      "multi-relationaldata. InNIPS,pages2787–2795,2013. [Rinseretal.,2013] Daniel Rinser, Dustin Lange, Felix\n",
      "Naumann, and Gerhard Weikum. Cross-lingual entity\n",
      "[Bordesetal.,2014] Antoine Bordes, Jason Weston, and\n",
      "matching and infobox alignment in Wikipedia. Informa-\n",
      "Nicolas Usunier. Open question answering with weakly\n",
      "tionSystems,38(6):887–907,2013.\n",
      "supervised embedding models. In ECML-PKDD, pages\n",
      "165–180,2014. [Saxeetal.,2014] Andrew M Saxe, James L McClelland,\n",
      "and Surya Ganguli. Exact solutions to the nonlinear dy-\n",
      "[Chenetal.,2017] Muhao Chen, Yingtao Tian, Mohan\n",
      "namicsoflearningindeeplinearneuralnetworks. ICLR,\n",
      "Yang,andCarloZaniolo. Multi-lingualknowledgegraph\n",
      "2014.\n",
      "embeddingsforcross-lingualknowledgealignment. arXiv\n",
      "preprintarXiv:1611.03954,2017. [Socheretal.,2013] Richard Socher, Danqi Chen, Christo-\n",
      "pherDManning,andAndrewNg. Reasoningwithneural\n",
      "[CollobertandWeston,2008] Ronan Collobert and Jason\n",
      "tensornetworksforknowledgebasecompletion. InNIPS,\n",
      "Weston. A unified architecture for natural language pro-\n",
      "pages926–934,2013.\n",
      "cessing: Deepneuralnetworkswithmultitasklearning. In\n",
      "ICML,pages160–167,2008. [SpeerandHavasi,2013] Robert Speer and Catherine\n",
      "[CulottaandSorensen,2004] Aron Culotta and Jeffrey Havasi. Conceptnet 5: A large semantic network for\n",
      "relational knowledge. The People’s Web Meets NLP,\n",
      "Sorensen. Dependencytreekernelsforrelationextraction.\n",
      "pages161–176,2013.\n",
      "InACL,page423,2004.\n",
      "[FaruquiandDyer,2014] Manaal Faruqui and Chris Dyer. [Suchaneketal.,2011] Fabian M Suchanek, Serge Abite-\n",
      "Improving vector space word representations using mul- boul, Pierre Senellart, and Tom Mitchell. Paris: Prob-\n",
      "tilingualcorrelation. EACL,2014. abilistic alignment of relations, instances, and schema.\n",
      "PVLDB,5(3):157–168,2011.\n",
      "[Jenattonetal.,2012] Rodolphe Jenatton, Nicolas L Roux,\n",
      "AntoineBordes,andGuillaumeRObozinski.Alatentfac- [Sunetal.,2011] Ang Sun, Ralph Grishman, and Satoshi\n",
      "tormodelforhighlymulti-relationaldata. InNIPS,2012. Sekine. Semi-supervised relation extraction with large-\n",
      "scalewordclustering. InACL,pages521–529,2011.\n",
      "[Jietal.,2015] Guoliang Ji, Shizhu He, Liheng Xu, Kang\n",
      "Liu, andJunZhao. Knowledgegraphembeddingviady- [Vrandecˇic´,2012] DennyVrandecˇic´. Wikidata: Anewplat-\n",
      "namicmappingmatrix. InACL,pages687–696,2015. formforcollaborativedatacollection. InWWW,2012.\n",
      "[Jiaetal.,2016] Yantao Jia, Yuanzhuo Wang, Hailun Lin, [Wangetal.,2014] Zhen Wang, Jianwen Zhang, Jianlin\n",
      "Xiaolong Jin, and Xueqi Cheng. Locally adaptive trans- Feng, andZhengChen. Knowledgegraphembeddingby\n",
      "lationforknowledgegraphembedding. InAAAI,2016. translatingonhyperplanes. InAAAI,2014.\n",
      "[Lehmannetal.,2015] Jens Lehmann, Robert Isele, Max [Westonetal.,2013] Jason Weston, Antoine Bordes, Ok-\n",
      "Jakob,AnjaJentzsch,etal. Dbpedia–alarge-scale,multi- sana Yakhnenko, and Nicolas Usunier. Connecting lan-\n",
      "lingualknowledgebaseextractedfromWikipedia.Seman- guage and knowledge bases with embedding models for\n",
      "ticWeb,6(2):167–195,2015. relationextraction. InEMNLP,pages1366–1371,2013.\n",
      "[Wikipedia,2017] Wikipedia, 2017. https://www.\n",
      "wikipedia.org/.\n",
      "[WilsonandMartinez,2003] D Randall Wilson and Tony R\n",
      "Martinez. The general inefficiency of batch training for\n",
      "gradientdescentlearning. NeuralNetworks,16(10),2003.\n",
      "[Xingetal.,2015] Chao Xing, Dong Wang, Chao Liu, and\n",
      "Yiye Lin. Normalized word embedding and orthogonal\n",
      "transformforbilingualwordtranslation. InNAACLHLT,\n",
      "pages1006–1011,2015.\n",
      "[Yangetal.,2015a] Cheng Yang, Zhiyuan Liu, Deli Zhao,\n",
      "Maosong Sun, and Edward Chang. Network representa-\n",
      "tionlearningwithrichtextinformation. InIJCAI,2015.\n",
      "[Yangetal.,2015b] Yang Yang, Yizhou Sun, Jie Tang,\n",
      "Bo Ma, and Juanzi Li. Entity matching across heteroge-\n",
      "neoussources. InKDD,pages1395–1404,2015.\n",
      "[Zhongetal.,2015] Huaping Zhong, Jianwen Zhang, Zhen\n",
      "Wang, Hai Wan, and Zheng Chen. Aligning knowledge\n",
      "and text embeddings by entity descriptions. In EMNLP,\n",
      "pages267–272,2015.\n",
      "[Zhouetal.,2005] Guodong Zhou, Jian Su, Jie Zhang, and\n",
      "Min Zhang. Exploring various knowledge in relation ex-\n",
      "traction. InACL,pages427–434,2005.\n",
      "6 Appendix Table11: StatisticsoftheCN3ldataset.\n",
      "Typeoftriples Entriples Frtriples Detriples Alignedtriples\n",
      "6.1 ExamplesofKnowledgeAlignment En-Fr:3,668\n",
      "Numberoftriples 47,696 18,624 25,560\n",
      "En-De:8,588\n",
      "We have already shown the effectiveness of MTransE\n",
      "TypeofILLs En-Fr Fr-En En-De De-En\n",
      "in aligning cross-lingual knowledge, especially the linear- NumberofILLs 2,154 2,146 3,485 3,813\n",
      "transformation-based variants Var and Var. Now we dis-\n",
      "4 5\n",
      "cussseveralexamplestorevealinsightsonhowourmethods basic queries are already useful for aided cross-lingual aug-\n",
      "maybeusedincross-lingualknowledgeaugmentation. mentationofknowledge. However,developingajointmodel\n",
      "tosupportcomplexqueriesonmultilingualknowledgegraphs\n",
      "Table8: Examplesofcross-lingualentitymatching. basedonMTransEgeneratedfeaturesappearstobeapromis-\n",
      "ing future work to support Q&A on multilingual knowledge\n",
      "Entity Target Candidates(inascendingorderofrankbyEuclideandistance)\n",
      "Barack French BarackObama,GeorgeBush,JimmyCarter,GeorgeKalkoa bases.\n",
      "Obama German BarackObama,BillClinton,Georgeh.w.Bush,HamidKarzai Figure2showsthePCAprojectionofthesamesixEnglish\n",
      "French Paris,Amsterdam,a`Paris,Manchester,DeSmet\n",
      "Paris\n",
      "German Paris,Languedoc,Constantine,Saint-maurice,Nancy entities in their original English space and in French space\n",
      "French SanFrancisco,LosAngeles,SantaMonica,Californie\n",
      "California German Kalifornien,LosAngeles,PalmSprings,SantaMonica aftertransformation. WecanobservethatthevectorsofEn-\n",
      "French post-punk,rockalternatif,smoothjazz,souljazz glish entities show certain structures, where the U.S. cities\n",
      "rockmusic\n",
      "German rockmusik,soul,deathmetal,dance-pop\n",
      "aregroupedtogetherandothercountries’citiesarewellsep-\n",
      "Table9: Examplesofcross-lingualrelationmatching. arated. AftertransformationintoFrenchspace,theseEnglish\n",
      "entities not only keep their original spatial emergence, but\n",
      "Relation Target Candidates(inascendingorderofrankbyEuclideandistance)\n",
      "French capitale,territoire,paysaccre`ditant,lieudeveneration alsoareclosetotheircorrespondingentitiesinFrench. This\n",
      "capital\n",
      "German hauptstadt,hauptort,gru¨ndungsort,city\n",
      "illustrates the transformation preserves mono-lingual struc-\n",
      "French nationalie´,paysdenaissance,domicile,re´sidence\n",
      "nationality\n",
      "German nationalita¨t,nation,letzterstart,sterbeort ture and also it is able to capture cross-lingual information.\n",
      "French langue,re´alisations,lieudeces,nationalite`\n",
      "language We believe this example illustrates the good performance\n",
      "German sprache,originalsprache,lang,land\n",
      "French surnom,descendant,texte,nomdering wehavedemonstratedincross-lingualtasksincludingcross-\n",
      "nickname\n",
      "German spitzname,originaltitel,names,alternativnamen\n",
      "lingual entity matching and triple-wise alignment verifica-\n",
      "Table10: Examplesofcross-lingualtriplecompletion. tion.\n",
      "Query Target Candidates(inascendingorderofrank)\n",
      "6.2 AdditionalExperimentalResults\n",
      "musiqueinde`pendante,musiquealternative,\n",
      "(AdamLambert, French\n",
      "ode,glamrock\n",
      "genre,?t) German popmusik,dance-pop,nowave,soul We derive another data set CN3l from the MIT ConceptNet\n",
      "(Ronaldinho, French milieuoffensif,attaquant,quarterback,late`ralgauche toevaluateMTransE,whosestatisticsareshowninTable11.\n",
      "position,?t) German stu¨rmer,linkerflu¨gel,angriffsspieler,rechterflgel\n",
      "French capitale,plusgrandeville,chef-lieu,garnison ThoughbeingasmallerdatasetthanWK3l-15k,knowledge\n",
      "(Italy,?r,Rome)\n",
      "German hauptstadt,hauptort,verwaltungssitz,stadion graphs in CN3l are highly sparse. Thereof, each language\n",
      "ministre-pre`sident,pre`de`cesseur,premierministre,\n",
      "(BarackObama,?r, French pre`sidentduconseil version of CN3l contains around 7,500 nodes and less than\n",
      "GeorgeBush)\n",
      "German vorga¨nger,vorga¨ngerin,besetzung,lied 41 types of relations. The alignment sets are created based\n",
      "BrantBjork,ChrisGarneau,DavidDraiman,\n",
      "(?h,instrument, French IanMackaye on the relation TranslationOf of the ConceptNet. In this\n",
      "guitar)\n",
      "German PhilManzanera,StylesP.,TinaCharles,LukeBryan section we report the results of the two cross-lingual tasks\n",
      "We start with the search of cross-lingual counterparts of on the CN3l data set for MTransE as well as all baselines.\n",
      "entities and relations. We choose an entity (or relation) in Basically,theseresultsleadtosimilarconclusionsaswehave\n",
      "English and then show the nearest candidates in French and onWK3l.\n",
      "German,respectively.Thesecandidatesarelistedbydecreas- EvaluationProtocol. Themetricsandevaluationprocedures\n",
      "ingvaluesoftheEuclideandistancebetweentheirvectorsin arethesameasthoseonWK3l. Weselectλamong{0.001,\n",
      "thetargetlanguagespaceandtheresultpointofcross-lingual 0.01, 0.05}, α among {1, 2.5, 5, 7.5}, l 1 or l 2 norm in loss\n",
      "transition. Several examples are shown in Table 8 and Ta- functions,anddimensionalitykamong{25,50,75}.Optimal\n",
      "ble9. Inalltablesofthissubsection, wemarktheexactan- parameters are configured as λ = 0.001, α = 2.5, k = 50,\n",
      "swersasboldfaced,andtheconceptuallycloseonesasitalic. andl\n",
      "1\n",
      "normforallmodels. Toperformtheevaluationunder\n",
      "For example, in Table 8, besides boldfacing the exactly cor- controlledvariables,weagainuseoneconfigurationoneach\n",
      "rectanswersforBarackObamaandParis,weconsiderthose modelrespectivelyinthetwoexperiments. Sincethedataset\n",
      "whohavealsobeenU.S.presidentsasconceptuallycloseto issmaller,trainingislimitedto200epochs.\n",
      "BarackObama, andEuropeancitiesotherthanParisascon- Results of Cross-lingual Entity Matching. The results are\n",
      "ceptually close to Paris. Also, in Table 9, those French and reported in Table 12. For the baselines, LM and CCA are\n",
      "Germanrelationsthathavethemeaningofsettlementsofsig- againleftfarbehindforbeingdisjointedlytrained. OT,how-\n",
      "nificanceareconsideredasconceptuallyclosetocapital. ever,takesthepositionaheadofVar. Thisislikelybecause\n",
      "1\n",
      "We then move on to the more complicated cross-lingual the knowledge graphs in CN3l are highly sparse, therefore\n",
      "triplecompletiontask. Weconstructqueriesbyreplacingone fewer interference of monolingual relations among entities\n",
      "elementinanEnglishtriplewithaquestionmark,forwhich makestheorthogonalityconstrainteasiertofulfill. Evenso,\n",
      "weseekforanswersinanotherlanguage. Ourmethodsneed in all settings, OT is still largely outperformed by Var and\n",
      "4\n",
      "to transfer the remaining elements to the space of the target Var, which receives amazingly good results, thus steadily\n",
      "5\n",
      "language and pick the best answer for the missing element. being the optimal solutions. Interestingly, Var now ranks\n",
      "3\n",
      "Table10showssomequeryanswers. Itisnoteworthythatthe rightbehindthelinear-transformation-basedvariantsinmost\n",
      "Figure2: VisualizationoftheresultofVar forthesamesixEnglishentitiesintheiroriginalspace(left)andinFrenchspace\n",
      "4\n",
      "afterbeingtransformed(right). Englishentitiesarerenderedinblue,andthecorrespondingFrenchentitiesareinlightruby.\n",
      "Table12: Cross-lingualentitymatching(CN3l). Table13: Accuracyoftriple-wise\n",
      "alignmentverification(%).\n",
      "Languages En-Fr Fr-En En-De De-En\n",
      "Metric Hits@10 Mean Hits@10 Mean Hits@10 Mean Hits@10 Mean Languages En&Fr En&De\n",
      "LM 25.45 1302.83 20.16 1884.70 30.12 954.71 18.04 1487.90 LM 60.53 51.55\n",
      "CCA 27.96 1204.91 26.40 1740.83 28.76 1176.09 25.30 1834.21 CCA 81.57 79.54\n",
      "OT 68.43 42.30 67.06 33.86 72.34 74.98 69.47 44.38 OT 93.01 87.59\n",
      "V V V V Va a a a ar r r r r2 3 4 51 6 4 7 8 81 4 3 6 6.....3 0 7 8 27 6 3 3 1 25 2 1 125 9 6 66....1. 3 6 966 4 4 93 6 5 7 8 89 7 7 0 0.....2 1 0 6 17 5 2 2 9 3 9<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   9954,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Cross-lingual entity matching', 'Cross-lingual triple-wise alignment verification', 'Cross-lingual relation matching', 'Cross-lingual triple completion']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: 09 25.30 1834.21 CCA 81.57 79.54\n",
      "OT 68.43 42.30 67.06 33.86 72.34 74.98 69.47 44.38 OT 93.01 87.59\n",
      "V V V V Va a a a ar r r r r2 3 4 51 6 4 7 8 81 4 3 6 6.....3 0 7 8 27 6 3 3 1 25 2 1 125 9 6 66....1. 3 6 966 4 4 93 6 5 7 8 89 7 7 0 0.....2 1 0 6 17 5 2 2 9 3 9 1 7 73 5 4..... 8 36 1 8 6 40 3 2 6 4 7 8 83 9 0 8 9.....0 0 5 8 16 7 5 9 9 27 51 7 84 09.... 1 25. 89 6 74 37 6 4 7 9 93 9 0 5 5.....5 1 9 6 56 5 6 7 3 27 41 1 19 74.... 4 67. 95 7 39 98 V V V V Va a a a ar r r r r1 2 3 4 5 9 8 8 9 93 7 8 7 7.....9 3 9 4 12 0 5 6 8 9 8 8 9 91 2 4 6 5.....8 7 8 6 49 0 0 3 2\n",
      "settings. This is quite reasonable because the cross-lingual\n",
      "transitions, which are regarded as a type of relation by Var\n",
      "3\n",
      "in the graphs, are now way less frequent in CN3l than they\n",
      "areinthemuchdenserandmoreheterogeneousWK3l. Thus,\n",
      "thisexplainswhyitperformsbetterthanVar. Forthesame\n",
      "1\n",
      "reasonaswestatedinSection4.1,Var isplacedatlastofthe\n",
      "2\n",
      "fiveMTransEvariantsinmatchingcross-lingualentities.\n",
      "ResultsofTriple-wiseAlignmentVerification. Theresults\n",
      "shown in Table 13 reflect the same conclusions of the ex-\n",
      "perimentperformedonWK3lthat,thelinear-transformation-\n",
      "based variants takes the lead ahead of the rest MTransE\n",
      "variants and the baselines. While Var, despite being the\n",
      "1\n",
      "simplest, takes the second place with a satisfying accuracy\n",
      "in triple-wise alignment verification as well. The relation-\n",
      "dedicatedcalibrationstillcausesinterferenceintheoptimiza-\n",
      "tionprocessofVar,thereforeleadstoa4%∼9%dropofac-\n",
      "2\n",
      "curacy from Var. Var performs slightly better than Var.\n",
      "1 3 3\n",
      "On triple-wise alignment verification on CN3l, we receive\n",
      "exactly the same placement for evaluating the five MTransE\n",
      "variants. Meanwhile, for the baselines, OT is slightly worse\n",
      "than Var, CCA also receives acceptable accuracy which is\n",
      "1\n",
      "howeverworsethanallMTransEvariants,whiletheaccuracy\n",
      "ofLMisjustslightlybetterthanrandomguessing.\n",
      "Aboveall,theresultsinCN3lindicatesthatMTransEalso\n",
      "workspromisinglyonverysparsemultilingualgraphs,while\n",
      "thelinear-transformation-basedvariantsarethebestrepresen-\n",
      "tationtechniques.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "            220,   2545,    220,    914,     13,    966,    220,  10750,     19,\n",
      "             13,   1691,    356,   5158,    220,   5932,     13,   3226,    220,\n",
      "           4643,     13,   4370,    198,   1831,    220,   2614,     13,   3391,\n",
      "            220,   2983,     13,    966,    220,   3080,     13,   2705,    220,\n",
      "           1644,     13,   4218,    220,   5332,     13,   1958,    220,   5728,\n",
      "             13,   3264,    220,   3076,     13,   2618,    220,   2096,     13,\n",
      "           1987,   8775,    220,   6365,     13,   1721,    220,   4044,     13,\n",
      "           2946,    198,     53,    650,    650,    650,  27713,    264,    264,\n",
      "            264,    802,    436,    436,    436,    436,     17,    220,     18,\n",
      "            220,     19,    220,   3971,    220,     21,    220,     19,    220,\n",
      "             22,    220,     23,    220,   5932,    220,     19,    220,     18,\n",
      "            220,     21,    220,     21,  18575,     18,    220,     15,    220,\n",
      "             22,    220,     23,    220,   1544,    220,     21,    220,     18,\n",
      "            220,     18,    220,     16,    220,    914,    220,     17,    220,\n",
      "             16,    220,   6549,    220,     24,    220,     21,    220,   2287,\n",
      "           1975,     16,     13,    220,     18,    220,     21,    220,  25285,\n",
      "            220,     19,    220,     19,    220,   6365,    220,     21,    220,\n",
      "             20,    220,     22,    220,     23,    220,   4578,    220,     22,\n",
      "            220,     22,    220,     15,    220,     15,  18575,     17,    220,\n",
      "             16,    220,     15,    220,     21,    220,   1114,    220,     20,\n",
      "            220,     17,    220,     17,    220,     24,    220,     18,    220,\n",
      "             24,    220,     16,    220,     22,    220,   5958,    220,     20,\n",
      "            220,     19,  18575,    220,     23,    220,   1927,    220,     16,\n",
      "            220,     23,    220,     21,    220,   1272,    220,     18,    220,\n",
      "             17,    220,     21,    220,     19,    220,     22,    220,     23,\n",
      "            220,   6069,    220,     24,    220,     15,    220,     23,    220,\n",
      "             24,  18575,     15,    220,     15,    220,     20,    220,     23,\n",
      "            220,    845,    220,     22,    220,     20,    220,     24,    220,\n",
      "             24,    220,   1544,    220,   3971,    220,     22,    220,   5833,\n",
      "            220,   2545,   1975,    220,     16,    220,    914,     13,    220,\n",
      "           4578,    220,     21,    220,   5728,    220,   1806,    220,     21,\n",
      "            220,     19,    220,     22,    220,     24,    220,   6365,    220,\n",
      "             24,    220,     15,    220,     20,    220,     20,  18575,     20,\n",
      "            220,     16,    220,     24,    220,     21,    220,   3487,    220,\n",
      "             20,    220,     21,    220,     22,    220,     18,    220,   1544,\n",
      "            220,   3174,    220,     16,    220,    777,    220,   5728,   1975,\n",
      "            220,     19,    220,   3080,     13,    220,   2721,    220,     22,\n",
      "            220,   2137,    220,   3264,    650,    650,    650,    650,  27713,\n",
      "            264,    264,    264,    802,    436,    436,    436,    436,     16,\n",
      "            220,     17,    220,     18,    220,     19,    220,     20,    220,\n",
      "             24,    220,     23,    220,     23,    220,     24,    220,   6365,\n",
      "            220,     22,    220,     23,    220,     22,    220,     22,  18575,\n",
      "             24,    220,     18,    220,     24,    220,     19,    220,    717,\n",
      "            220,     15,    220,     20,    220,     21,    220,     23,    220,\n",
      "             24,    220,     23,    220,     23,    220,     24,    220,   5925,\n",
      "            220,     17,    220,     19,    220,     21,    220,     20,  18575,\n",
      "             23,    220,     22,    220,     23,    220,     21,    220,   2491,\n",
      "            220,     15,    220,     15,    220,     18,    220,     17,    198,\n",
      "           6648,     13,   1115,    374,   5115,  13579,   1606,    279,   5425,\n",
      "             12,   2785,    940,    198,   1485,   6055,     11,    902,    527,\n",
      "          27458,    439,    264,    955,    315,  12976,    555,   8909,    198,\n",
      "             18,    198,    258,    279,  40099,     11,    527,   1457,   1648,\n",
      "           2753,  21420,    304,  25914,     18,     75,   1109,    814,    198,\n",
      "            548,  48121,    336,   1412,  53321,    261,    438,   6518,     71,\n",
      "           1430,  49122,  69416,     18,     75,     13,  14636,    345,    576,\n",
      "          30992,   1771,  35734,    275,    716,  10008,  58234,  54895,   4050,\n",
      "             13,   1789,   6509,    373,    198,     16,    198,  20489,    300,\n",
      "          11285,    660,    258,   9817,     19,     13,     16,     11,   4050,\n",
      "            374,  37469,    266,   4354,   1073,   1820,    198,     17,    198,\n",
      "          53770,     44,   3246,     36,  55711,    258,  90143,  29942,     12,\n",
      "           2785,    940,  10720,    627,   2122,    708,     69,  83926,  45539,\n",
      "           7178,  63439,     13,    666,  13213, 122761,    198,  70463,    304,\n",
      "           6771,    220,   1032,   8881,    279,   1890,  31342,    315,    279,\n",
      "            506,   7058,  14666,    716,  10365,    263,  69416,     18,     75,\n",
      "           9210,     11,    339,   4939,    277,  39160,   1659,   7058,  31039,\n",
      "          27103,   5097,    279,   3063,   8469,    315,    279,   2800,    386,\n",
      "           3246,     36,    198,  55711,    323,    279,   3122,  11243,     13,\n",
      "           6104,   8909,     11,   8994,   1694,    279,    198,     16,    198,\n",
      "          23796,    267,     11,   5097,    279,   2132,   2035,    449,    264,\n",
      "          37154,  13708,    198,    258,  24657,  45539,  17632,  23751,    439,\n",
      "           1664,     13,    578,  12976,   7058,   9988,  10297,   5531,  18856,\n",
      "          44466,    936,   4881,   2295,   2251,    258,   1820,  19680,  17528,\n",
      "           7058,  28491,   4734,   1073,   4050,     11,  19041,   1348,  27152,\n",
      "            267,  20103,     19,      4,  22447,    120,     24,      4,   6861,\n",
      "           1073,    582,   7058,     17,    198,  22222,    505,   8909,     13,\n",
      "           8909,  27772,  10284,   2731,   1109,   8909,    627,     16,    220,\n",
      "             18,    220,     18,    198,   1966,  24657,  45539,  17632,  23751,\n",
      "            389,  25914,     18,     75,     11,    584,   5371,    198,    327,\n",
      "          33839,    279,   1890,  22165,    369,  38663,    279,   4330,    386,\n",
      "           3246,     36,    198,  55711,     13,  26982,     11,    369,    279,\n",
      "           3122,  11243,     11,   8775,    374,  10284,  11201,    198,  54895,\n",
      "           8909,     11,    356,   5158,   1101,  21879,  22281,  13708,    902,\n",
      "            374,    198,     16,    198,  98936,  50810,    751,  10118,    543,\n",
      "             44,   3246,     36,  55711,     11,   3556,   1820,  33829,    198,\n",
      "           1073,  11237,    285,   4345,   3306,  73048,  58234,  54895,  11719,\n",
      "          52851,    287,    627,  59907,    543,     11,    700,    288,    495,\n",
      "          16319,  29768,     18,     75,    485,   8630,    267,   9379,     44,\n",
      "           3246,     36,  19171,    198,   1816,   2203,    442,   3876,    398,\n",
      "            263,    424,   1065,   6534,  26961,  50923,  87286,     11,   3556,\n",
      "            198,    339,   4939,    277,  39160,   1659,   6108,  55711,    548,\n",
      "           1820,  16241,  10200,    417,    268,   7058,     83,    367,  26522,\n",
      "           8467,     13, 128009, 128006,    882, 128007,    271,   7184,     11,\n",
      "           2728,    420,   3488,     25,   3639,    527,    279,   9256,    430,\n",
      "            279,   1646,    374,  16572,    369,   4710,    220,  21335,   1203,\n",
      "            279,   4320,   1193,    304,    264,  13325,   1160,   3645,     11,\n",
      "            369,   3187,     25,   2570,     32,   1882,     33,   7352,   1442,\n",
      "            499,   1541,    956,   1440,    279,   4320,     11,   1120,    471,\n",
      "            459,   4384,   1160,     13, 128009, 128006,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "            220,   2545,    220,    914,     13,    966,    220,  10750,     19,\n",
      "             13,   1691,    356,   5158,    220,   5932,     13,   3226,    220,\n",
      "           4643,     13,   4370,    198,   1831,    220,   2614,     13,   3391,\n",
      "            220,   2983,     13,    966,    220,   3080,     13,   2705,    220,\n",
      "           1644,     13,   4218,    220,   5332,     13,   1958,    220,   5728,\n",
      "             13,   3264,    220,   3076,     13,   2618,    220,   2096,     13,\n",
      "           1987,   8775,    220,   6365,     13,   1721,    220,   4044,     13,\n",
      "           2946,    198,     53,    650,    650,    650,  27713,    264,    264,\n",
      "            264,    802,    436,    436,    436,    436,     17,    220,     18,\n",
      "            220,     19,    220,   3971,    220,     21,    220,     19,    220,\n",
      "             22,    220,     23,    220,   5932,    220,     19,    220,     18,\n",
      "            220,     21,    220,     21,  18575,     18,    220,     15,    220,\n",
      "             22,    220,     23,    220,   1544,    220,     21,    220,     18,\n",
      "            220,     18,    220,     16,    220,    914,    220,     17,    220,\n",
      "             16,    220,   6549,    220,     24,    220,     21,    220,   2287,\n",
      "           1975,     16,     13,    220,     18,    220,     21,    220,  25285,\n",
      "            220,     19,    220,     19,    220,   6365,    220,     21,    220,\n",
      "             20,    220,     22,    220,     23,    220,   4578,    220,     22,\n",
      "            220,     22,    220,     15,    220,     15,  18575,     17,    220,\n",
      "             16,    220,     15,    220,     21,    220,   1114,    220,     20,\n",
      "            220,     17,    220,     17,    220,     24,    220,     18,    220,\n",
      "             24,    220,     16,    220,     22,    220,   5958,    220,     20,\n",
      "            220,     19,  18575,    220,     23,    220,   1927,    220,     16,\n",
      "            220,     23,    220,     21,    220,   1272,    220,     18,    220,\n",
      "             17,    220,     21,    220,     19,    220,     22,    220,     23,\n",
      "            220,   6069,    220,     24,    220,     15,    220,     23,    220,\n",
      "             24,  18575,     15,    220,     15,    220,     20,    220,     23,\n",
      "            220,    845,    220,     22,    220,     20,    220,     24,    220,\n",
      "             24,    220,   1544,    220,   3971,    220,     22,    220,   5833,\n",
      "            220,   2545,   1975,    220,     16,    220,    914,     13,    220,\n",
      "           4578,    220,     21,    220,   5728,    220,   1806,    220,     21,\n",
      "            220,     19,    220,     22,    220,     24,    220,   6365,    220,\n",
      "             24,    220,     15,    220,     20,    220,     20,  18575,     20,\n",
      "            220,     16,    220,     24,    220,     21,    220,   3487,    220,\n",
      "             20,    220,     21,    220,     22,    220,     18,    220,   1544,\n",
      "            220,   3174,    220,     16,    220,    777,    220,   5728,   1975,\n",
      "            220,     19,    220,   3080,     13,    220,   2721,    220,     22,\n",
      "            220,   2137,    220,   3264,    650,    650,    650,    650,  27713,\n",
      "            264,    264,    264,    802,    436,    436,    436,    436,     16,\n",
      "            220,     17,    220,     18,    220,     19,    220,     20,    220,\n",
      "             24,    220,     23,    220,     23,    220,     24,    220,   6365,\n",
      "            220,     22,    220,     23,    220,     22,    220,     22,  18575,\n",
      "             24,    220,     18,    220,     24,    220,     19,    220,    717,\n",
      "            220,     15,    220,     20,    220,     21,    220,     23,    220,\n",
      "             24,    220,     23,    220,     23,    220,     24,    220,   5925,\n",
      "            220,     17,    220,     19,    220,     21,    220,     20,  18575,\n",
      "             23,    220,     22,    220,     23,    220,     21,    220,   2491,\n",
      "            220,     15,    220,     15,    220,     18,    220,     17,    198,\n",
      "           6648,     13,   1115,    374,   5115,  13579,   1606,    279,   5425,\n",
      "             12,   2785,    940,    198,   1485,   6055,     11,    902,    527,\n",
      "          27458,    439,    264,    955,    315,  12976,    555,   8909,    198,\n",
      "             18,    198,    258,    279,  40099,     11,    527,   1457,   1648,\n",
      "           2753,  21420,    304,  25914,     18,     75,   1109,    814,    198,\n",
      "            548,  48121,    336,   1412,  53321,    261,    438,   6518,     71,\n",
      "           1430,  49122,  69416,     18,     75,     13,  14636,    345,    576,\n",
      "          30992,   1771,  35734,    275,    716,  10008,  58234,  54895,   4050,\n",
      "             13,   1789,   6509,    373,    198,     16,    198,  20489,    300,\n",
      "          11285,    660,    258,   9817,     19,     13,     16,     11,   4050,\n",
      "            374,  37469,    266,   4354,   1073,   1820,    198,     17,    198,\n",
      "          53770,     44,   3246,     36,  55711,    258,  90143,  29942,     12,\n",
      "           2785,    940,  10720,    627,   2122,    708,     69,  83926,  45539,\n",
      "           7178,  63439,     13,    666,  13213, 122761,    198,  70463,    304,\n",
      "           6771,    220,   1032,   8881,    279,   1890,  31342,    315,    279,\n",
      "            506,   7058,  14666,    716,  10365,    263,  69416,     18,     75,\n",
      "           9210,     11,    339,   4939,    277,  39160,   1659,   7058,  31039,\n",
      "          27103,   5097,    279,   3063,   8469,    315,    279,   2800,    386,\n",
      "           3246,     36,    198,  55711,    323,    279,   3122,  11243,     13,\n",
      "           6104,   8909,     11,   8994,   1694,    279,    198,     16,    198,\n",
      "          23796,    267,     11,   5097,    279,   2132,   2035,    449,    264,\n",
      "          37154,  13708,    198,    258,  24657,  45539,  17632,  23751,    439,\n",
      "           1664,     13,    578,  12976,   7058,   9988,  10297,   5531,  18856,\n",
      "          44466,    936,   4881,   2295,   2251,    258,   1820,  19680,  17528,\n",
      "           7058,  28491,   4734,   1073,   4050,     11,  19041,   1348,  27152,\n",
      "            267,  20103,     19,      4,  22447,    120,     24,      4,   6861,\n",
      "           1073,    582,   7058,     17,    198,  22222,    505,   8909,     13,\n",
      "           8909,  27772,  10284,   2731,   1109,   8909,    627,     16,    220,\n",
      "             18,    220,     18,    198,   1966,  24657,  45539,  17632,  23751,\n",
      "            389,  25914,     18,     75,     11,    584,   5371,    198,    327,\n",
      "          33839,    279,   1890,  22165,    369,  38663,    279,   4330,    386,\n",
      "           3246,     36,    198,  55711,     13,  26982,     11,    369,    279,\n",
      "           3122,  11243,     11,   8775,    374,  10284,  11201,    198,  54895,\n",
      "           8909,     11,    356,   5158,   1101,  21879,  22281,  13708,    902,\n",
      "            374,    198,     16,    198,  98936,  50810,    751,  10118,    543,\n",
      "             44,   3246,     36,  55711,     11,   3556,   1820,  33829,    198,\n",
      "           1073,  11237,    285,   4345,   3306,  73048,  58234,  54895,  11719,\n",
      "          52851,    287,    627,  59907,    543,     11,    700,    288,    495,\n",
      "          16319,  29768,     18,     75,    485,   8630,    267,   9379,     44,\n",
      "           3246,     36,  19171,    198,   1816,   2203,    442,   3876,    398,\n",
      "            263,    424,   1065,   6534,  26961,  50923,  87286,     11,   3556,\n",
      "            198,    339,   4939,    277,  39160,   1659,   6108,  55711,    548,\n",
      "           1820,  16241,  10200,    417,    268,   7058,     83,    367,  26522,\n",
      "           8467,     13, 128009, 128006,    882, 128007,    271,   7184,     11,\n",
      "           2728,    420,   3488,     25,   3639,    527,    279,   9256,    430,\n",
      "            279,   1646,    374,  16572,    369,   4710,    220,  21335,   1203,\n",
      "            279,   4320,   1193,    304,    264,  13325,   1160,   3645,     11,\n",
      "            369,   3187,     25,   2570,     32,   1882,     33,   7352,   1442,\n",
      "            499,   1541,    956,   1440,    279,   4320,     11,   1120,    471,\n",
      "            459,   4384,   1160,     13, 128009, 128006,  78191, 128007,    271,\n",
      "            681,  83926,  45539,  17632,  23751,    518,    364,  29601,     12,\n",
      "           2785,    940,  34692,    663, 128009]], device='cuda:0')\n",
      "Decoded output:\n",
      " ['Triple-wise alignment verification', 'Cross-lingual transitions']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Multilingual Knowledge Graph Embeddings for\n",
      "Cross-lingual Knowledge Alignment\n",
      "MuhaoChen1,YingtaoTian2,MohanYang1,CarloZaniolo1\n",
      "{muhaochen,yang,zaniolo}@cs.ucla.edu;yittian@cs.stonybrook.edu\n",
      "DepartmentofComputerScience,UCLA1\n",
      "DepartmentofComputerScience,StonyBrookUniversity2\n",
      "Abstract and the cross-lingual knowledge that matches the monolin-\n",
      "gualknowledgeamongvarioushumanlanguages.\n",
      "Many recent works have demonstrated the bene-\n",
      "The coverage issue of monolingual knowledge has been\n",
      "fits of knowledge graph embeddings in complet-\n",
      "widelyaddressed,andparsing-basedtechniquesforcomplet-\n",
      "ing monolingual knowledge graphs. Inasmuch as\n",
      "ing monolingual knowledge bases have been well studied\n",
      "related knowledge bases are built in several dif-\n",
      "in the past [Culotta and Sorensen, 2004; Zhou et al., 2005;\n",
      "ferent languages, achieving cross-lingual knowl-\n",
      "Sun et al., 2011]. More recently, much attention has been\n",
      "edge alignment will help people in constructing\n",
      "paid to embedding-based techniques, which provide simple\n",
      "a coherent knowledge base, and assist machines\n",
      "methods to encode entities in low-dimensional embedding\n",
      "in dealing with different expressions of entity re-\n",
      "spacesandcapturerelationsasmeansoftranslationsamong\n",
      "lationships across diverse human languages. Un-\n",
      "entity vectors. Given a triple (h,r,t) where r is the rela-\n",
      "fortunately, achieving this highly desirable cross-\n",
      "tion between entities h and t, then h and t are represented\n",
      "lingual alignment by human labor is very costly\n",
      "astwok-dimensionalvectorshandt, respectively. Afunc-\n",
      "and error-prone. Thus, we propose MTransE, a\n",
      "tion f (h,t) is used to measure the plausibility of (h,r,t),\n",
      "translation-based model for multilingual knowl- r\n",
      "whichalsoimpliesthetransformationrthatcharacterizesr.\n",
      "edge graph embeddings, to provide a simple and\n",
      "Exemplarily, the translation-based model TransE [Bordes et\n",
      "automated solution. By encoding entities and re-\n",
      "al., 2013] uses the loss function f (h,t) = (cid:107)h+r−t(cid:107) 1,\n",
      "lationsofeachlanguageinaseparatedembedding r\n",
      "where r is characterized as a translation vector learnt from\n",
      "space, MTransE provides transitions for each em-\n",
      "thelatentconnectivitypatternsintheknowledgegraph. This\n",
      "bedding vector to its cross-lingual counterparts in\n",
      "modelprovidesaflexiblewayofpredictingamissingitemin\n",
      "other spaces, while preserving the functionalities\n",
      "atriple,orverifyingthevalidityofageneratedtriple. Other\n",
      "ofmonolingualembeddings. Wedeploythreedif-\n",
      "works like TransH [Wang et al., 2014] and TransR [Lin et\n",
      "ferent techniques to represent cross-lingual transi-\n",
      "al., 2015], introduce different loss functions that represent\n",
      "tions, namely axis calibration, translation vectors,\n",
      "the relational translation in other forms, and have achieved\n",
      "andlineartransformations,andderivefivevariants\n",
      "promisingresultsincompletingtheknowledgegraphs.\n",
      "for MTransE using different loss functions. Our\n",
      "While embedding-based techniques can help improve the\n",
      "models can be trained on partially aligned graphs,\n",
      "completenessofmonolingualknowledge,theproblemofap-\n",
      "wherejustasmallportionoftriplesarealignedwith\n",
      "plyingthese techniqueson cross-lingual knowledgeremains\n",
      "their cross-lingual counterparts. The experiments\n",
      "largelyunexplored. Suchknowledge,includinginter-lingual\n",
      "on cross-lingual entity matching and triple-wise\n",
      "links (ILLs) that match the same entities, and triple-wise\n",
      "alignmentverificationshowpromisingresults,with\n",
      "alignment (TWA) that represents the same relations, is very\n",
      "somevariantsconsistentlyoutperformingotherson\n",
      "helpfulinsynchronizingdifferentlanguage-specificversions\n",
      "differenttasks. WealsoexplorehowMTransEpre-\n",
      "of a knowledge base that evolve independently, as needed\n",
      "serves the key properties of its monolingual coun-\n",
      "to further improve applications built on knowledge bases,\n",
      "terpartTransE.\n",
      "such as Q&A systems, semantic Web, and Web search. In\n",
      "spiteofitsimportance,thiscross-lingualknowledgeremains\n",
      "1 Introduction largelyintact. Infact,inthemostsuccessfulknowledgebase\n",
      "MultilingualknowledgebasessuchasWikipedia[Wikipedia, Wikipedia,wefindthatILLscoverlessthan15%entityalign-\n",
      "2017], WordNet [Bond and Foster, 2013], and Concept- ment.\n",
      "Net[SpeerandHavasi,2013]arebecomingessentialsources Leveragingknowledgegraphembeddingstocross-lingual\n",
      "of knowledge for people and AI-related applications. These knowledge no doubt provides a generic way to help extract\n",
      "knowledgebasesaremodeledasknowledgegraphsthatstore and apply such knowledge. However, it is a non-trivial task\n",
      "two aspects of knowledge: the monolingual knowledge that\n",
      "includesentitiesandrelationsrecordedintheformoftriples, 1Hereafter,(cid:107)·(cid:107)meansl orl normunlessexplicitlyspecified.\n",
      "1 2\n",
      "7102\n",
      "yaM\n",
      "71\n",
      "]IA.sc[\n",
      "3v45930.1161:viXra\n",
      "tofindatractabletechniquetocapturethecross-lingualtran- TransD [Ji et al., 2015], and other forms [Jia et al., 2016;\n",
      "sitions2. Such transitions are more difficult to capture than Nguyen et al., 2016]. All these variants of TransE special-\n",
      "relationaltranslationsforseveralreasons: (i)across-lingual ize entity embeddings for different relations, therefore im-\n",
      "transition has a far larger domain than any monolingual re- proving knowledge graph completion on multi-mapping re-\n",
      "lational translation; (ii) it applies on both entities and rela- lationsatthecostofincreasedmodelcomplexity. Meanwhile\n",
      "tions, which have incoherent vocabularies among different translation-based models cooperate well with other models.\n",
      "languages;(iii)theknownalignmentfortrainingsuchtransi- Forexample,variantsofTransEarecombinedwithwordem-\n",
      "tionsusuallyaccountsforasmallpercentageofaknowledge beddingstohelprelationextractionfromtext[Westonetal.,\n",
      "base. Moreover, thecharacterizationofmonolingualknowl- 2013;Zhongetal.,2015].\n",
      "edge graph structures has to be well-preserved to ensure the\n",
      "correctrepresentationoftheknowledgetobealigned.\n",
      "To address the above issues, we propose a multilingual Inadditiontothese,therearenon-translation-basedmeth-\n",
      "knowledgegraphembeddingmodelMTransE,thatlearnsthe ods. Some of those including UM [Bordes et al., 2011], SE\n",
      "multilingualknowledgegraphstructureusingacombination [Bordes et al., 2012], Bilinear [Jenatton et al., 2012], and\n",
      "of two component models, namely knowledge model and HolE [Nickel et al., 2016], do not explicitly represent re-\n",
      "alignmentmodel. Theknowledgemodelencodesentitiesand lation embeddings. Others including neural-based models\n",
      "relationsinalanguage-specificversionofknowledgegraph.\n",
      "SLM[CollobertandWeston,2008]andNTN[Socheretal.,\n",
      "Weexplorethemethodthatorganizeseachlanguage-specific 2013], and random-walk-based model TADW [Yang et al.,\n",
      "version in a separated embedding space, in which MTransE\n",
      "2015a],areexpressiveandadaptableforbothstructuredand\n",
      "adopts TransE as the knowledge model. On top of that, the text corpora, but are too complex to be incorporated into an\n",
      "alignmentmodellearnscross-lingualtransitionsforbothen- architecturesupportingmultilingualknowledge.\n",
      "titiesandrelationsacrossdifferentembeddingspaces,where\n",
      "the following three representations of cross-lingual align-\n",
      "MultilingualWordEmbeddings. Severalapproacheslearn\n",
      "ment are considered: distance-based axis calibration, trans-\n",
      "multilingualwordembeddingsonparalleltextcorpora.Some\n",
      "lation vectors, and linear transformations. Thus, we obtain\n",
      "of those can be extended to multilingual knowledge graphs,\n",
      "five variants of MTransE based on different loss functions,\n",
      "such as LM [Mikolov et al., 2013] and CCA [Faruqui and\n",
      "and identify the best variant by comparing them on cross-\n",
      "Dyer, 2014] which induce offline transitions among pre-\n",
      "lingualalignmenttasksusingtwopartiallyalignedtrilingual\n",
      "trained monolingual embeddings in forms of linear trans-\n",
      "graphs constructed from Wikipedia triples. We also show\n",
      "formations and canonical component analysis respectively.\n",
      "that MTransE performs as well as its monolingual counter-\n",
      "Theseapproachesdonotadjusttheinconsistentvectorspaces\n",
      "partTransEonmonolingualtasks.\n",
      "via calibration or jointly training with the alignment model,\n",
      "Therestofthepaperisorganizedasfollows. Wefirstdis-\n",
      "thus fail to perform well on knowledge graphs as the par-\n",
      "cusstherelatedwork,andthenintroduceourapproachinthe\n",
      "allelism exists only in small portions. A better approach\n",
      "section that follows. After that we present the experimental\n",
      "OT[Xingetal.,2015]jointlylearnsregularizedembeddings\n",
      "results,andconcludethepaperinthelastsection.\n",
      "and orthogonal transformations, which is however found to\n",
      "2 RelatedWork beovercomplicatedduetotheinconsistencyofmonolingual\n",
      "vectorspacesandthelargediversityofrelationsamongenti-\n",
      "While,atthebestofourknowledge,thereisnopreviouswork ties.\n",
      "on learning multilingual knowledge graph embeddings, we\n",
      "will describe next three lines of work which are closely re-\n",
      "latedtothistopic. KnowledgeBasesAlignment. Someprojectsproducecross-\n",
      "Knowledge Graph Embeddings. Recently, significant ad- lingualalignmentinknowledgebasesatthecostofextensive\n",
      "vancement has been made in using the translation-based humaninvolvementanddesigninghand-craftedfeaturesded-\n",
      "method to train monolingual knowledge graph embeddings. icated to specific applications. Wikidata [Vrandecˇic´, 2012]\n",
      "To characterize a triple (h,r,t), models of this family fol- and DBpedia [Lehmann et al., 2015] rely on crowdsourc-\n",
      "low a common assumption h +r ≈ t, where h and t ing to create ILLs and relation alignment. YAGO [Mahdis-\n",
      "r r r r\n",
      "are either the original vectors of h and t, or the transformed oltanietal.,2015]minesassociationrulesonknownmatches,\n",
      "vectors under a certain transformation w.r.t. relation r. The which combines many confident scores and requires exten-\n",
      "forerunner TransE [Bordes et al., 2013] sets h and t as sively fine tuning. Many other works require sources that\n",
      "r r\n",
      "the original h and t, and achieves promising results in han- are external to the graphs, from well-established schemata\n",
      "dling1-to-1relations. LaterworksimproveTransEonmulti- or ontologies [Nguyen et al., 2011; Suchanek et al., 2011;\n",
      "mapping relations by introducing relation-specific transfor- Rinseretal.,2013]toentitydescriptions[Yangetal.,2015b],\n",
      "mations on entities to obtain different h and t, including which being unavailable to many knowledge bases such as\n",
      "r r\n",
      "projectionsonrelation-specifichyperplanesinTransH[Wang YAGO,WordNet,andConceptNet[SpeerandHavasi,2013].\n",
      "et al., 2014], linear transformations to heterogeneous rela- Such approaches also involve complicated model depen-\n",
      "tionspacesinTransR[Linetal.,2015],dynamicmatricesin dencies that are not tractable and reusable. By contrast,\n",
      "embedding-based methods are simple and general, require\n",
      "2Weusethewordtransitionheretodifferentiatefromtherela- littlehumaninvolvement,andgeneratetask-independentfea-\n",
      "tionaltranslationsamongentitiesintranslation-basedmethods. turesthatcancontributetootherNLPtasks.\n",
      "3 MultilingualKnowledgeGraph tionisgivenasbelow:\n",
      "Embeddings (cid:88)\n",
      "S = S (T,T(cid:48))\n",
      "A a\n",
      "Weherebybeginourmodelingwiththeformalizationofmul-\n",
      "(T,T(cid:48))∈δ(Li,Lj)\n",
      "tilingualknowledgegraphs.\n",
      "Thereof, the alignment score S (T,T(cid:48)) iterates through all\n",
      "a\n",
      "3.1 MultilingualKnowledgeGraphs pairs of aligned triples. Three different techniques to score\n",
      "InaknowledgebaseKB,weuseLtodenotethesetoflan- thealignmentareconsidered:distance-basedaxiscalibration,\n",
      "guages, and L2 to denote the 2-combination of L (i.e., the translationvectors,andlineartransformations. Eachofthem\n",
      "set of unordered language pairs). For a language L ∈ L, is based on a different assumption, and constitutes different\n",
      "G denotesthelanguage-specificknowledgegraphofL,and formsofS aalongside.\n",
      "L\n",
      "E and R respectively denote the corresponding vocabu- Distance-based Axis Calibration. This type of alignment\n",
      "L L\n",
      "laries of entity expression and relation expression. T = models penalize the alignment based on the distances of\n",
      "(h,r,t) denotes a triple in G such that h,t ∈ E and cross-lingualcounterparts. Eitherofthefollowingtwoscor-\n",
      "L L\n",
      "r ∈ R. Boldfaced h, r, t respectively represent the em- ingscanbeadoptedtothemodel.\n",
      "L\n",
      "bedding vectors of head h, relation r, and tail t. For a lan- S =(cid:107)h−h(cid:48)(cid:107)+(cid:107)t−t(cid:48)(cid:107)\n",
      "guage pair (L,L ) ∈ L2, δ(L,L ) denotes the alignment a1\n",
      "1 2 1 2\n",
      "setwhichcontainsthepairsoftriplesthathavealreadybeen S regulatesthatcorrectlyalignedmultilingualexpressions\n",
      "a1\n",
      "aligned between L and L. For example, across the lan- ofthesameentitytendtohavecloseembeddingvectors.Thus\n",
      "1 2\n",
      "(cid:0)\n",
      "guages English and French, we may have (State of Cal- byminimizingthelossfunctionthatinvolvesS onknown\n",
      "a1\n",
      "ifornia, capital city, Sacramento),(E´tat de Californie, cap- pairs of aligned triples, the alignment model adjusts axes of\n",
      "(cid:1) embeddingspacestowardsthegoalofcoincidingthevectors\n",
      "itale, Sacramento) ∈ δ(English,French). The alignment\n",
      "ofthesameentityindifferentlanguages.\n",
      "set commonly exists in a small portion in a multilingual\n",
      "knowledgebase[Vrandecˇic´,2012;Mahdisoltanietal.,2015; S =(cid:107)h−h(cid:48)(cid:107)+(cid:107)r−r(cid:48)(cid:107)+(cid:107)t−t(cid:48)(cid:107)\n",
      "Lehmannetal.,2015],andisonepartofknowledgewewant\n",
      "a2\n",
      "toextend. S a2 overlays the penalty of relation alignment to S a1 to ex-\n",
      "Our model consists of two components that learn on the plicitlyconvergecoordinatesofthesamerelation.\n",
      "twofacetsofKB: theknowledgemodelthatencodestheen- The alignment models based on axis calibration assume\n",
      "tities and relations from each language-specific graph struc- analogous spatial emergence of items in each language.\n",
      "ture, and the alignment model that learns the cross-lingual Therefore, itrealizesthecross-lingualtransitionbycarrying\n",
      "transitions from the existing alignment. We define a model forwardthevectorofagivenentityorrelationfromthespace\n",
      "for each language pair from L2 that has a non-empty align- oftheoriginallanguagetothatoftheotherlanguage.\n",
      "mentset.Thus,foraKB withmorethantwolanguages,aset TranslationVectors. Thismodelencodescross-lingualtran-\n",
      "ofmodelscomposesthesolution. Inthefollowing,weusea sitions into vectors. It consolidates alignment into graph\n",
      "language pair (L, L ) ∈ L2 as an example to describe how structures and characterizes cross-lingual transitions as reg-\n",
      "i j\n",
      "wedefineeachcomponentofamodel. ularrelationaltranslations. HenceS a3 asbelowisderived.\n",
      "3.2 KnowledgeModel S a3 =(cid:13) (cid:13)h+v ie j −h(cid:48)(cid:13) (cid:13)+(cid:13) (cid:13)r+v ir j −r(cid:48)(cid:13) (cid:13)+(cid:13) (cid:13)t+v ie j −t(cid:48)(cid:13) (cid:13)\n",
      "ForeachlanguageL∈L,adedicatedk-dimensionalembed- Thereof ve and vr are respectively deployed as the entity-\n",
      "ij ij\n",
      "dingspaceRk isassignedforvectorsofE andR, where dedicatedandrelation-dedicatedtranslationvectorsbetween\n",
      "L L L\n",
      "Risthefieldofrealnumbers. Weadoptthebasictranslation- L andL,suchthatwehavee+ve ≈e(cid:48)forembeddingvec-\n",
      "i j ij\n",
      "based method of TransE for each involved language, which tors e, e(cid:48) of the same entity e expressed in both languages,\n",
      "benefits the cross-lingual tasks by representing embeddings and r + vr ≈ r(cid:48) for those of the same relation. We de-\n",
      "ij\n",
      "uniformlyindifferentcontextsofrelations. Thereforeitsloss ploytwotranslationvectorsinsteadofone,becausethereare\n",
      "functionisgivenasbelow: farmoredistinctentitiesthanrelations,andusingonevector\n",
      "(cid:88) (cid:88) easilyleadstoimbalancedsignalsfromrelations.\n",
      "S K = (cid:107)h+r−t(cid:107) Such a model obtains a cross-lingual transition of an em-\n",
      "L∈{Li,Lj}(h,r,t)∈GL beddingvectorbyaddingthecorrespondingtranslationvec-\n",
      "tor. Moreover, it is easy to see that ve = −ve and vr =\n",
      "Itmeasurestheplausibilityofallgiventriples. Byminimiz- −vr hold. Therefore, as we obtain ti hj e translj ai tion vecij tors\n",
      "ingthelossfunction,theknowledgemodelpreservesmono- ji\n",
      "fromL toL,wecanalwaysusethesamevectorstotrans-\n",
      "lingual relations among entities, while also acts as a regu- i j\n",
      "lateintheoppositedirection.\n",
      "larizer for the alignment model. Meanwhile, the knowledge\n",
      "Linear Transformations. The last category of alignment\n",
      "modelpartitionstheknowledgebaseintodisjointsubsetsthat\n",
      "models deduce linear transformations between embedding\n",
      "canbetrainedinparallel.\n",
      "spaces. S as below learns a k ×k square matrix Me as\n",
      "a4 ij\n",
      "3.3 AlignmentModel alineartransformationonentityvectorsfromL itoL j,given\n",
      "kasthedimensionalityoftheembeddingspaces.\n",
      "Theobjectiveofthealignmentmodelistoconstructthetran-\n",
      "sitionsbetweenthevectorspacesofL iandL j. Itslossfunc- S a4 =(cid:13) (cid:13)Me ijh−h(cid:48)(cid:13) (cid:13)+(cid:13) (cid:13)Me ijt−t(cid:48)(cid:13) (cid:13)\n",
      "Table1: Summaryofmodelvariants. Table2: StatisticsoftheWK3ldatasets.\n",
      "Var ModelComplexity Cross-lingualTransition SearchComplexity Dataset #Entriples #Frtriples #Detriples #Alignedtriples\n",
      "Var1 O(nekl+nrkl) τ τi ij j( (e r) )= =e\n",
      "r\n",
      "OO (( nn rek k)\n",
      ")\n",
      "WK3l-15k 203,502 170,605 145,616 EE nn -- DFr e: :1 36 7,,4 17 70\n",
      "0\n",
      "Var2 O(nekl+nrkl) τ τi ij j( (e r) )= =e\n",
      "r\n",
      "OO (( nn rek k)\n",
      ")\n",
      "WK3l-120k 1,376,011 767,750 391,108 E En n- -F Dr e:1 :62 94,, 44 13 33\n",
      "Var3 O(nekl+ +n kr lk 2)l τ τi ij j( (e r) )= =e r++ vv irie jj OO (( nn rek k) ) Table3: Numberofentityinter-linguallinks(ILLs).\n",
      "Var4 O(ne +kl 0+.5kn 2r lk 2l ) τ τi ij j( (e r) )= =M Me i e ij je r OO (( nn rek k2 2+ +n ne rk k) ) WD Kat 3a l-S 1e 5t k E 3,n 7- 3F 3r F 3,r 8-E 15n E 1n,8- 4D 0e D 1,e 6- 1E 0n\n",
      "Var5 O(nekl ++ kn 2r l2k )l τ τi ij j( (e r) )= =M Me i r ij je r OO (( nn rek k2 2+ +n ne rk k) ) WK3l-120k 42,413 41,513 7,567 5,921\n",
      "Notation:eandrarerespectivelythevectorsofanentityeandarelationr,kis We enforce the constraint that the l 2 norm of any entity\n",
      "thedimensionoftheembeddingspaces,listhecardinalityofL,neandnrare embeddingvectoris1,thusregularizeembeddingvectorsto\n",
      "respectivelythenumberofentitiesandthenumberofrelations,wherene(cid:29)nr.\n",
      "a unit spherical surface. This constraint is employed in the\n",
      "S additionallybringsinasecondlineartransformationMr\n",
      "a5 ij literature[Bordesetal.,2013;Bordesetal.,2014;Jenattonet\n",
      "forrelationvectors,whichisofthesameshapeasMe. The\n",
      "ij al.,2012]andhastwoimportanteffects: (i)ithelpsavoidthe\n",
      "useofadifferentmatrixisagainduetodifferentredundancy\n",
      "case where the training process trivially minimizes the loss\n",
      "ofentitiesandrelations.\n",
      "functionbyshrinkingthenormofembeddingvectors,and(ii)\n",
      "S a5 =(cid:13) (cid:13)Me ijh−h(cid:48)(cid:13) (cid:13)+(cid:13) (cid:13)Mr ijr−r(cid:48)(cid:13) (cid:13)+(cid:13) (cid:13)Me ijt−t(cid:48)(cid:13) (cid:13) eit ti am l.p,l 2ie 0s 15th ]e foin rv Ve art ribi al nit dy Vo af rth.elineartransformations[Xing\n",
      "4 5\n",
      "Unlike axis calibration, linear-transformation-based align- We initialize vectors by drawing from a uniform distribu-\n",
      "ment model treats cross-lingual transitions as the topologi- tionontheunitsphericalsurface,andinitializematricesusing\n",
      "caltransformationofembeddingspaceswithoutassumingthe randomorthogonalinitialization[Saxeetal.,2014].Negative\n",
      "similarityofspatialemergence. samplingisnotemployedintraining,whichwefinddoesnot\n",
      "The cross-lingual transition of a vector is obtained by ap- noticeablyaffecttheresults.\n",
      "plyingthecorrespondinglineartransformation. Itisnotewor-\n",
      "thy that, regularization of embedding vectors in the training 4 Experiments\n",
      "process(whichwillbeintroducedsoonafter)ensuresthein-\n",
      "vertibility of the linear transformations such that Me −1 = In this section, we evaluate the proposed methods on two\n",
      "ij cross-lingualtasks: cross-lingualentitymatching,andtriple-\n",
      "Me andMr −1 = Mr. Thusthetransitionintherevertdi-\n",
      "ji ij ji wisealignmentverification. Wealsoconductexperimentson\n",
      "rectionisalwaysenabledeventhoughthemodelonlylearns twomonolingualtasks.Besides,acasestudywithknowledge\n",
      "thetransformationsofonedirection. alignmentexamplesisincludedintheAppendixof[Chenet\n",
      "3.4 VariantsofMTransE al.,2017].\n",
      "Data Sets. Experimental results on the trilingual data sets\n",
      "Combiningtheabovetwocomponentmodels,MTransEmin-\n",
      "WK3l are reported in this section. WK3l contains English\n",
      "imizesthefollowinglossfunctionJ = S +αS,whereα\n",
      "K A\n",
      "(En), French (Fr), and German (De) knowledge graphs un-\n",
      "isahyperparameterthatweightsS andS.\n",
      "K A der DBpedia’s dbo:Person domain, where a part of triples\n",
      "Aswehavegivenoutfivevariantsofthealignmentmodel,\n",
      "are aligned by verifying the ILLs on entities, and multi-\n",
      "each of which correspondingly defines its specific way of\n",
      "lingual labels of the DBpedia ontology on some relations.\n",
      "computing cross-lingual transitions of embedding vectors.\n",
      "The number of entities in each language is adjusted to ob-\n",
      "We denote Var as the variant of MTransE that adopts the\n",
      "k\n",
      "tain two data sets. For each of the three languages thereof,\n",
      "k-th alignment model which employs S. In practice, the\n",
      "ak\n",
      "WK3l-15kmatchesthenumberofnodes(about15,000)with\n",
      "searchingofacross-lingualcounterpartforasourceisalways\n",
      "FB15k—thelargestmonolingualgraphusedbymanyrecent\n",
      "done by querying the nearest neighbor from the result point\n",
      "works [Zhong et al., 2015; Lin et al., 2015; Ji et al., 2015;\n",
      "of the cross-lingual transition. We denote function τ that\n",
      "ij Jia et al., 2016], and the number of nodes in WK3l-120k is\n",
      "mapsacross-lingualtransitionofavectorfromL toL,or\n",
      "i j\n",
      "several times larger. For both data sets, German graphs are\n",
      "simply τ in a bilingual context. As stated, the solution in a\n",
      "sparserthanEnglishandFrenchgraphs.Wealsocollectextra\n",
      "multi-lingualscenarioconsistsofasetofmodelsofthesame\n",
      "variant defined on every language pair in L2. Table 1 sum- entity ILLs for the evaluation of cross-lingual entity match-\n",
      "ing,whosequantityisshowninTable3. Meanwhile,wede-\n",
      "marizesthemodelcomplexity,thedefinitionofcross-lingual\n",
      "riveanothertrilingualdatasetCN3lfromConceptNet[Speer\n",
      "transitions, and the complexity of searching a cross-lingual\n",
      "and Havasi, 2013]. Additional results on CN3l that lead to\n",
      "counterpartforeachvariant.\n",
      "similar evaluation conclusions are reported in the Appendix\n",
      "3.5 Training\n",
      "of[Chenetal.,2017].\n",
      "We optimize the loss function using on-line stochastic gra-\n",
      "4.1 Cross-lingualEntityMatching\n",
      "dient descent [Wilson and Martinez, 2003]. At each step,\n",
      "we update the parameter θ by setting θ ← θ − λ∇ J, The objective of this task is to match the same entities from\n",
      "θ\n",
      "where λ is the learning rate. Instead of directly updating differentlanguagesinKB. Duetothelargecandidatespace,\n",
      "J, our implementation optimizes S and αS alternately. this task emphasizes more on ranking a set of candidates\n",
      "K A\n",
      "In detail, at each epoch we optimize θ ← θ −λ∇ S and rather than acquiring the best answer. We perform this task\n",
      "θ K\n",
      "θ ←θ−λ∇ αS inseparatedgroupsofsteps. onbothdatasetstocomparefivevariantsofMTransE.\n",
      "θ A\n",
      "Table4: Cross-lingualentitymatchingresult.\n",
      "DataSet WK3l-15k WK3l-120k\n",
      "AlignedLanguages En-Fr Fr-En En-De De-En En-Fr Fr-En En-De De-En\n",
      "Metric Hits@10 Mean Hits@10 Mean Hits@10 Mean Hits@10 Mean Hits@10 Hits@10 Hits@10 Hits@10\n",
      "LM 12.31 3621.17 10.42 3660.98 22.17 5891.13 15.21 6114.08 11.74 14.26 24.52 13.58\n",
      "CCA 20.78 3094.25 19.44 3017.90 26.46 5550.89 22.30 5855.61 19.47 12.85 25.54 20.39\n",
      "OT 44.97 508.39 40.92 461.18 44.47 155.47 49.24 145.47 38.91 37.19 38.85 34.21\n",
      "Var1 51.05 470.29 46.64 436.47 48.67 146.13 50.60 167.02 38.58 36.52 42.06 47.79\n",
      "Var2 45.25 570.72 41.74 565.38 46.27 168.33 49.00 211.94 31.88 30.84 41.22 40.39\n",
      "Var3 38.64 587.46 36.44 464.64 50.82 125.15 52.16 151.84 38.26 36.45 50.48 52.24\n",
      "Var4 59.24 190.26 57.48 199.64 66.25 74.62 68.53 42.31 48.66 47.43 57.56 63.49\n",
      "Var5 59.52 191.36 57.07 204.45 60.25 99.48 66.03 54.69 45.65 47.48 64.22 67.85\n",
      "Figure1: Precision-recallcurvesforcross-lingualentitymatchingonWK3l-15k.\n",
      "ToshowthesuperiorityofMTransE,weadaptLM,CCA, indicatethattheinterferencecausedbylearninganadditional\n",
      "and OT (which are introduced in Section 2) to their knowl- relation-dedicatedtransformationinVar isnegligibletothe\n",
      "5\n",
      "edgegraphequivalences. entity-dedicatedtransformation.Correspondingly,webelieve\n",
      "thatthereasonforVar tobeoutperformedbyVar andVar\n",
      "Evaluation Protocol. Each MTransE variant is trained on 3 4 5\n",
      "is that it fails to differentiate well the over-frequent cross-\n",
      "a complete data set. LM and CCA are implemented by in-\n",
      "lingualalignmentfromregularrelations. Therefore,thechar-\n",
      "ducing the corresponding transformations across separately\n",
      "acterizationforcross-lingualalignmentisnegativelyaffected\n",
      "trainedknowledgemodelsonmonolingualgraphs,whileus-\n",
      "bythelearningprocessformonolingualrelationsinavisible\n",
      "ingthealignmentsetsasanchors. TrainingOTisquitesim-\n",
      "degree. Axis calibration appears to be unstable on this task.\n",
      "ilar to MTransE, we add the process of orthogonalization to\n",
      "Wehypothesizethatthissimpletechniqueisaffectedbytwo\n",
      "the training of the alignment model, since the regularization\n",
      "factors: coherence between language-specific versions, and\n",
      "of vectors has already been enforced. The entity ILLs are\n",
      "density of the graphs. Var is always outperformed by Var\n",
      "used as ground truth for test. We take these unidirectional 2 1\n",
      "due to the negative effect of the calibration based on rela-\n",
      "linksbetweenEnglish-FrenchandEnglish-German,i.e.,four\n",
      "tions. Webelievethisisbecausemulti-mappingrelationsare\n",
      "directions in total. For each ILL (e,e(cid:48)), we perform a kNN\n",
      "notsowell-capturedbyTransEasexplainedin[Wangetal.,\n",
      "searchfromthecross-lingualtransitionpointofe(i.e.,τ(e))\n",
      "2014], therefore disturb the calibration of the entire embed-\n",
      "and record the rank of e(cid:48). Following the convention [Xing\n",
      "ding spaces. Although Var still outperforms Var on entity\n",
      "etal.,2015;Jiaetal.,2016],weaggregatetwometricsover 1 3\n",
      "matchingbetweenEnglishandFrenchgraphsinWK3l-15k<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  10216,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Muhao Chen', 'Yingtao Tian', 'Mohan Yang', 'Carlo Zaniolo']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  by Var\n",
      "used as ground truth for test. We take these unidirectional 2 1\n",
      "due to the negative effect of the calibration based on rela-\n",
      "linksbetweenEnglish-FrenchandEnglish-German,i.e.,four\n",
      "tions. Webelievethisisbecausemulti-mappingrelationsare\n",
      "directions in total. For each ILL (e,e(cid:48)), we perform a kNN\n",
      "notsowell-capturedbyTransEasexplainedin[Wangetal.,\n",
      "searchfromthecross-lingualtransitionpointofe(i.e.,τ(e))\n",
      "2014], therefore disturb the calibration of the entire embed-\n",
      "and record the rank of e(cid:48). Following the convention [Xing\n",
      "ding spaces. Although Var still outperforms Var on entity\n",
      "etal.,2015;Jiaetal.,2016],weaggregatetwometricsover 1 3\n",
      "matchingbetweenEnglishandFrenchgraphsinWK3l-15k,\n",
      "all test cases, i.e., the proportion of ranks no larger than 10\n",
      "coherencesomewhatdropsalongsidewhenscalinguptothe\n",
      "Hits@10(inpercentage),andthemeanrankMean. Wepre-\n",
      "larger data set so as to hinder the calibration. The German\n",
      "fer higher Hits@10 and lower Mean that indicate a better\n",
      "graphsaresparse,thusshouldhavesetabarrierforprecisely\n",
      "outcome.\n",
      "constructing embedding vectors and hindered calibration on\n",
      "For training, we select the learning rate λ among {0.001,\n",
      "theotherside.ThereforeVar stillperformscloselytoVar in\n",
      "1 3\n",
      "0.01, 0.1}, α among {1, 2.5, 5, 7.5}, l or l norm in loss\n",
      "1 2 the English-German task on WK3l-15k and English-French\n",
      "functions, and dimensionality k among {50, 75, 100, 125}.\n",
      "task on WK3l-120k, but is outperformed by Var in the last\n",
      "3\n",
      "The best configuration on WK3l-15k is λ = 0.01, α = 5,\n",
      "setting. In general, the variants that use linear transforma-\n",
      "k = 75, l normforVar, Var, LM,andCCA,l normfor\n",
      "1 1 2 2 tions are the most desired. This conclusion is supported by\n",
      "othervariantsandOT.WhilethebestconfigurationonWK3l-\n",
      "their promising outcomeon this task, and it isalso reflected\n",
      "120k is λ = 0.01, α = 5, k = 100, and l norm for all\n",
      "2 intheprecision-recallcurvesshowninFigure1.\n",
      "models. Thetrainingonbothdatasetstakes400epochs.\n",
      "4.2 Triple-wiseAlignmentVerification\n",
      "Results. WereportHits@10andMean forWK3l-15k, and\n",
      "Hits@10 for WK3l-120k, on the four involved directions Thistaskistoverifywhetheragivenpairofalignedtriplesare\n",
      "of cross-lingual matching in Table 4. As expected, with- truly cross-lingual counterparts. It produces a classifier that\n",
      "out jointly adapting the monolingual vector spaces with the helpswithverifyingcandidatesoftriplematching[Nguyenet\n",
      "knowledgealignment,LMandCCAarelargelyoutperformed al.,2011;Rinseretal.,2013].\n",
      "by the rest. While the orthogonality constraint being too Evaluation Protocol. We create positive cases by isolating\n",
      "strong to be enforced in these cases, OT performs at most 20%ofthealignmentset. Similarto[Socheretal.,2013],we\n",
      "closelytothesimplestcasesofMTransE.ForMTransE,Var randomly corrupt positive cases to generate negative cases.\n",
      "4\n",
      "and Var outperform the other three variants under all set- Indetail,givenapairofcorrectlyalignedtriples(T,T(cid:48)),itis\n",
      "5\n",
      "tings. Thefairlycloseresultsobtainedbythesetwovariants corruptedby(i)randomlyreplacingoneofthesixelementsin\n",
      "Table5: AccuracyofTWAverification(%). Table6: Resultsoftailprediction Table7: Resultsofrelationprediction\n",
      "DataSet WK3l-15k WK3l-120k (Hits@10). (Hits@10).\n",
      "Languages En&Fr En&De En&Fr En&De DataSet WK3l-15k WK3l-120k DataSet WK3l-15k WK3l-120k\n",
      "LM 52.23 63.61 59.98 59.98 Language En Fr En Fr Language En Fr En Fr\n",
      "CCA 52.28 66.49 65.89 61.01 TransE 42.19 25.06 36.78 25.38 TransE 61.79 62.55 60.06 65.29\n",
      "V V V V VO a a a a aT r r r r r1 2 3 4 5 9 9 9 9 9 93 3 0 0 4 4......2 2 2 3 5 90 5 4 8 8 0 9 98 9 8 8 5 47 1 6 4...... 0 99 2 5 2 3 57 4 9 4 8 9 8 8 9 98 1 9 7 3 2......6 2 3 9 4 65 7 6 9 8 3 9 98 9 8 8 3 35 1 6 7...... 0 62 3 2 0 6 64 5 9 4 V V V V Va a a a ar r r r r1 2 3 4 5 4 4 4 4 40 0 0 1 1.....3 8 9 0 77 0 7 3 9 2 2 2 2 23 4 2 5 5.....4 7 2 4 75 7 6 6 7 3 3 3 3 39 6 5 9 8.....0 0 9 6 39 2 9 4 5 2 2 1 2 25 1 9 5 4.....5 1 6 5 62 3 9 9 8 V V VV Va a aa ar r rr r1 3 42 5 6 5 5 6 60 4 8 3 4.....1 3 3 7 78 3 2 4 9 6 6 5 6 60 2 9 4 3.....7 9 4 7 73 8 4 7 1 6 6 6 6 61 1 0 0 0.....7 1 1 2 75 1 4 6 7 6 4 66 61 8 65 7.....4 0 84 67 6 66 4\n",
      "thetwotripleswithanotherelementfromthesamelanguage, ofMTransE,butisstillleftbehindbyVar andVar.\n",
      "4 5\n",
      "or(ii)randomlysubstitutingeitherT orT(cid:48)withanothertriple\n",
      "4.3 MonolingualTasks\n",
      "fromthesamelanguage. Cases(i)and(ii)respectivelycon-\n",
      "tribute negative cases that are as many as 100% and 50% of The above experiments have shown the strong capability\n",
      "positivecases.Weuse10-foldcross-validationonthesecases of MTransE in handling cross-lingual tasks. Now we re-\n",
      "totrainandevaluatetheclassifier. port the results on comparing MTransE with its monolin-\n",
      "We use a simple threshold-based classifier similar to the gual counterpart TransE on two monolingual tasks intro-\n",
      "widely-usedonesfortripleclassification[Socheretal.,2013; duced in the literature [Bordes et al., 2013; Bordes et al.,\n",
      "Wang et al., 2014; Lin et al., 2015]. For a given pair of 2014], namely tail prediction (predicting t given h and r)\n",
      "aligned triples (T,T(cid:48)) = (cid:0) (h,r,t),(h(cid:48),r(cid:48),t(cid:48))(cid:1), the dissim- and relation prediction (predicting r given h and t), us-\n",
      "ilarity function is defined as f (T,T(cid:48)) = (cid:107)τ(h)−h(cid:48)(cid:107) + ing the English and French versions of our data sets. Like\n",
      "(cid:107)τ(r)−r(cid:48)(cid:107) + (cid:107)τ(t)−t(cid:48)(cid:107). Td he classifier finds a thre2 sh- previous works [Bordes et al., 2013; Wang et al., 2014;\n",
      "oldσ sucht2 hatf < σ impl2 iespositive,otherwisenegative. Jia et al., 2016], for each language version, 10% triples are\n",
      "d\n",
      "The value of σ is determined by maximizing the accuracy selectedasthetestset,andtheremainingbecomesthetrain-\n",
      "foreachfoldonthetrainingset. Suchasimpleclassification ingset. EachMTransEvariantistraineduponbothlanguage\n",
      "ruleadequatelyreliesonhowpreciselyeachmodelrepresents versions of the training set for the knowledge model, while\n",
      "cross-lingualtransitionsforbothentitiesandrelations. the intersection between the alignment set and the training\n",
      "setisusedforthealignmentmodels. TransEistrainedonei-\n",
      "Wecarryforwardthecorrespondingconfigurationfromthe\n",
      "ther language version of the training set. Again, we use the\n",
      "lastexperiment,justtoshowtheperformanceofeachvariant\n",
      "configurationfromthepreviousexperiment.\n",
      "undercontrolledvariables.\n",
      "Results. Table 5 shows the mean accuracy, with a standard Results. The results for Hits@10 are reported in Tables 6\n",
      "and 7. They imply that MTransE preserves well the char-\n",
      "deviation below 0.009 in cross-validation for all settings.\n",
      "acterization of monolingual knowledge. For each setting,\n",
      "Thus, the results are statistically sufficient to reflect the per-\n",
      "Var, Var, andVar performatleastaswellasTransE,and\n",
      "formance of classifiers. Note that the results appear to be 1 4 5\n",
      "some even outperforms TransE under certain settings. This\n",
      "better than those of the previous task since this is a binary\n",
      "signifies that the alignment model does not interfere much\n",
      "classificationproblem. Intuitively,thelinear-transformation-\n",
      "withtheknowledgemodelincharacterizingmonolingualre-\n",
      "basedMTransEperformsteadilyandtaketheleadonallset-\n",
      "lations,butmighthaveactuallystrengtheneditsincecoherent\n",
      "tings. WealsoobservethatVar,thoughlearnsanadditional\n",
      "5\n",
      "portions of knowledge are unified by the alignment model.\n",
      "relation-dedicatedtransformation,stillperformsconsiderably\n",
      "Since such coherence is currently not measured, this ques-\n",
      "close to Var (the difference is at most 0.85%). The simple\n",
      "4\n",
      "tionisleftasafuturework. Theotherquestionthatdeserves\n",
      "Var is the runner-up, and is between 1.65% and 3.79% to\n",
      "1\n",
      "further attention is, how other knowledge models involving\n",
      "the optimal solutions. However the relation-dedicated cali-\n",
      "brationinVar causesanotablesetback(4.12%∼8.44%from relation-specific entity transformations [Wang et al., 2014;\n",
      "2\n",
      "Linetal.,2015;Jietal.,2015;Jiaetal.,2016;Nguyenetal.,\n",
      "the optimal). The performance of Var falls behind slightly\n",
      "3\n",
      "more than Var (4.52%∼10.79% from the optimal) due to\n",
      "2016]mayinfluencemonolingualandcross-lingualtasks.\n",
      "2\n",
      "thefailureindistinguishingcross-lingualalignmentfromreg-\n",
      "5 ConclusionandFutureWork\n",
      "ular relations. Meanwhile, we single out the accuracy on\n",
      "the portion of negative cases where only the relation is cor- Atthebestofourknowledge,thispaperisthefirstworkthat\n",
      "rupted for English-French in WK3l-15k. The five variants generalizesknowledgegraphembeddingstothemultilingual\n",
      "receive 97.73%, 93.78%, 82.34%, 98.57%, and 98.54%, re- scenario. OurmodelMTransEcharacterizesmonolingualre-\n",
      "spectively. The close accuracy of Var and Var indicates lationsandcomparesthreedifferenttechniquestolearncross-\n",
      "4 5\n",
      "that the only transformation learnt from entities in Var is lingualalignmentforentitiesandrelations. Extensiveexperi-\n",
      "4\n",
      "enoughtosubstitutetherelation-dedicatedtransformationin mentsonthetasksofcross-lingualentitymatchingandtriple\n",
      "Var fordiscriminatingrelationalignment,whilelearningthe alignment verification show that the linear-transformation-\n",
      "5\n",
      "additional transformation in Var does not notably interfere technique is the best among the three. Moreover, MTransE\n",
      "5\n",
      "theoriginalone. However, itappliesdifferentlytoaxiscali- preservesthekeypropertiesofmonolingualknowledgegraph\n",
      "brationsinceVar doesnotimprovebutactuallyimpairsthe embeddingsonmonolingualtasks.\n",
      "2\n",
      "cross-lingualtransitionsforrelations.Forthesamereasonsas The results here are very encouraging, but we also point\n",
      "above,LMandCCAdonotmatchwithMTransEinthisex- outopportunitiesforfurtherworkandimprovements. Inpar-\n",
      "perimentaswell,whileOTperformscloselytosomevariants ticular, we should explore how to substitute the simple loss\n",
      "functionoftheknowledgemodelusedinMTransEwithmore [Linetal.,2015] Yankai Lin, Zhiyuan Liu, Maosong Sun,\n",
      "advanced ones involving relation-specific entity transforma- Yang Liu, and Xuan Zhu. Learning entity and relation\n",
      "tions. More sophisticated tasks of cross-lingual triple com- embeddings for knowledge graph completion. In AAAI,\n",
      "pletion may also be conducted. Combining MTransE with 2015.\n",
      "multilingualwordembeddings[Xingetal.,2015]isanother\n",
      "[Mahdisoltanietal.,2015] Farzaneh Mahdisoltani, Joanna\n",
      "meaningfuldirectionsinceitwillprovideausefultooltoex-\n",
      "Biega,FabianSuchanek,etal. Yago3: Aknowledgebase\n",
      "tractnewrelationsfrommultilingualtextcorpora.\n",
      "frommultilingualWikipedias. InCIDR,2015.\n",
      "References [Mikolovetal.,2013] TomasMikolov,QuocVLe,andIlya\n",
      "Sutskever. Exploiting similarities among languages for\n",
      "[BondandFoster,2013] Francis Bond and Ryan Foster. machinetranslation. arXiv,2013.\n",
      "Linking and extending an open multilingual Wordnet. In\n",
      "ACL,pages1352–1362,2013. [Nguyenetal.,2011] Thanh Nguyen, Viviane Moreira,\n",
      "HuongNguyen,HoaNguyen,andJulianaFreire.Multilin-\n",
      "[Bordesetal.,2011] Antoine Bordes, Jason Weston, Ronan\n",
      "gualschemamatchingforWikipediainfoboxes. PVLDB,\n",
      "Collobert, and Yoshua Bengio. Learning structured em-\n",
      "5(2):133–144,2011.\n",
      "beddingsofknowledgebases. InAAAI,2011.\n",
      "[Nguyenetal.,2016] DatQuocNguyen,KairitSirts,Lizhen\n",
      "[Bordesetal.,2012] Antoine Bordes, Xavier Glorot, Jason\n",
      "Qu, and Mark Johnson. Stranse: a novel embedding\n",
      "Weston,andYoshuaBengio. Jointlearningofwordsand\n",
      "modelofentitiesandrelationshipsinknowledgebases. In\n",
      "meaningrepresentationsforopen-textsemanticparsing.In\n",
      "NAACLHLT,pages460–466,2016.\n",
      "AISTATS,pages127–135,2012.\n",
      "[Nickeletal.,2016] Maximilian Nickel, Lorenzo Rosasco,\n",
      "[Bordesetal.,2013] Antoine Bordes, Nicolas Usunier,\n",
      "TomasoPoggio,etal. Holographicembeddingsofknowl-\n",
      "Alberto Garcia-Duran, Jason Weston, and Oksana\n",
      "edgegraphs. InAAAI,2016.\n",
      "Yakhnenko. Translating embeddings for modeling\n",
      "multi-relationaldata. InNIPS,pages2787–2795,2013. [Rinseretal.,2013] Daniel Rinser, Dustin Lange, Felix\n",
      "Naumann, and Gerhard Weikum. Cross-lingual entity\n",
      "[Bordesetal.,2014] Antoine Bordes, Jason Weston, and\n",
      "matching and infobox alignment in Wikipedia. Informa-\n",
      "Nicolas Usunier. Open question answering with weakly\n",
      "tionSystems,38(6):887–907,2013.\n",
      "supervised embedding models. In ECML-PKDD, pages\n",
      "165–180,2014. [Saxeetal.,2014] Andrew M Saxe, James L McClelland,\n",
      "and Surya Ganguli. Exact solutions to the nonlinear dy-\n",
      "[Chenetal.,2017] Muhao Chen, Yingtao Tian, Mohan\n",
      "namicsoflearningindeeplinearneuralnetworks. ICLR,\n",
      "Yang,andCarloZaniolo. Multi-lingualknowledgegraph\n",
      "2014.\n",
      "embeddingsforcross-lingualknowledgealignment. arXiv\n",
      "preprintarXiv:1611.03954,2017. [Socheretal.,2013] Richard Socher, Danqi Chen, Christo-\n",
      "pherDManning,andAndrewNg. Reasoningwithneural\n",
      "[CollobertandWeston,2008] Ronan Collobert and Jason\n",
      "tensornetworksforknowledgebasecompletion. InNIPS,\n",
      "Weston. A unified architecture for natural language pro-\n",
      "pages926–934,2013.\n",
      "cessing: Deepneuralnetworkswithmultitasklearning. In\n",
      "ICML,pages160–167,2008. [SpeerandHavasi,2013] Robert Speer and Catherine\n",
      "[CulottaandSorensen,2004] Aron Culotta and Jeffrey Havasi. Conceptnet 5: A large semantic network for\n",
      "relational knowledge. The People’s Web Meets NLP,\n",
      "Sorensen. Dependencytreekernelsforrelationextraction.\n",
      "pages161–176,2013.\n",
      "InACL,page423,2004.\n",
      "[FaruquiandDyer,2014] Manaal Faruqui and Chris Dyer. [Suchaneketal.,2011] Fabian M Suchanek, Serge Abite-\n",
      "Improving vector space word representations using mul- boul, Pierre Senellart, and Tom Mitchell. Paris: Prob-\n",
      "tilingualcorrelation. EACL,2014. abilistic alignment of relations, instances, and schema.\n",
      "PVLDB,5(3):157–168,2011.\n",
      "[Jenattonetal.,2012] Rodolphe Jenatton, Nicolas L Roux,\n",
      "AntoineBordes,andGuillaumeRObozinski.Alatentfac- [Sunetal.,2011] Ang Sun, Ralph Grishman, and Satoshi\n",
      "tormodelforhighlymulti-relationaldata. InNIPS,2012. Sekine. Semi-supervised relation extraction with large-\n",
      "scalewordclustering. InACL,pages521–529,2011.\n",
      "[Jietal.,2015] Guoliang Ji, Shizhu He, Liheng Xu, Kang\n",
      "Liu, andJunZhao. Knowledgegraphembeddingviady- [Vrandecˇic´,2012] DennyVrandecˇic´. Wikidata: Anewplat-\n",
      "namicmappingmatrix. InACL,pages687–696,2015. formforcollaborativedatacollection. InWWW,2012.\n",
      "[Jiaetal.,2016] Yantao Jia, Yuanzhuo Wang, Hailun Lin, [Wangetal.,2014] Zhen Wang, Jianwen Zhang, Jianlin\n",
      "Xiaolong Jin, and Xueqi Cheng. Locally adaptive trans- Feng, andZhengChen. Knowledgegraphembeddingby\n",
      "lationforknowledgegraphembedding. InAAAI,2016. translatingonhyperplanes. InAAAI,2014.\n",
      "[Lehmannetal.,2015] Jens Lehmann, Robert Isele, Max [Westonetal.,2013] Jason Weston, Antoine Bordes, Ok-\n",
      "Jakob,AnjaJentzsch,etal. Dbpedia–alarge-scale,multi- sana Yakhnenko, and Nicolas Usunier. Connecting lan-\n",
      "lingualknowledgebaseextractedfromWikipedia.Seman- guage and knowledge bases with embedding models for\n",
      "ticWeb,6(2):167–195,2015. relationextraction. InEMNLP,pages1366–1371,2013.\n",
      "[Wikipedia,2017] Wikipedia, 2017. https://www.\n",
      "wikipedia.org/.\n",
      "[WilsonandMartinez,2003] D Randall Wilson and Tony R\n",
      "Martinez. The general inefficiency of batch training for\n",
      "gradientdescentlearning. NeuralNetworks,16(10),2003.\n",
      "[Xingetal.,2015] Chao Xing, Dong Wang, Chao Liu, and\n",
      "Yiye Lin. Normalized word embedding and orthogonal\n",
      "transformforbilingualwordtranslation. InNAACLHLT,\n",
      "pages1006–1011,2015.\n",
      "[Yangetal.,2015a] Cheng Yang, Zhiyuan Liu, Deli Zhao,\n",
      "Maosong Sun, and Edward Chang. Network representa-\n",
      "tionlearningwithrichtextinformation. InIJCAI,2015.\n",
      "[Yangetal.,2015b] Yang Yang, Yizhou Sun, Jie Tang,\n",
      "Bo Ma, and Juanzi Li. Entity matching across heteroge-\n",
      "neoussources. InKDD,pages1395–1404,2015.\n",
      "[Zhongetal.,2015] Huaping Zhong, Jianwen Zhang, Zhen\n",
      "Wang, Hai Wan, and Zheng Chen. Aligning knowledge\n",
      "and text embeddings by entity descriptions. In EMNLP,\n",
      "pages267–272,2015.\n",
      "[Zhouetal.,2005] Guodong Zhou, Jian Su, Jie Zhang, and\n",
      "Min Zhang. Exploring various knowledge in relation ex-\n",
      "traction. InACL,pages427–434,2005.\n",
      "6 Appendix Table11: StatisticsoftheCN3ldataset.\n",
      "Typeoftriples Entriples Frtriples Detriples Alignedtriples\n",
      "6.1 ExamplesofKnowledgeAlignment En-Fr:3,668\n",
      "Numberoftriples 47,696 18,624 25,560\n",
      "En-De:8,588\n",
      "We have already shown the effectiveness of MTransE\n",
      "TypeofILLs En-Fr Fr-En En-De De-En\n",
      "in aligning cross-lingual knowledge, especially the linear- NumberofILLs 2,154 2,146 3,485 3,813\n",
      "transformation-based variants Var and Var. Now we dis-\n",
      "4 5\n",
      "cussseveralexamplestorevealinsightsonhowourmethods basic queries are already useful for aided cross-lingual aug-\n",
      "maybeusedincross-lingualknowledgeaugmentation. mentationofknowledge. However,developingajointmodel\n",
      "tosupportcomplexqueriesonmultilingualknowledgegraphs\n",
      "Table8: Examplesofcross-lingualentitymatching. basedonMTransEgeneratedfeaturesappearstobeapromis-\n",
      "ing future work to support Q&A on multilingual knowledge\n",
      "Entity Target Candidates(inascendingorderofrankbyEuclideandistance)\n",
      "Barack French BarackObama,GeorgeBush,JimmyCarter,GeorgeKalkoa bases.\n",
      "Obama German BarackObama,BillClinton,Georgeh.w.Bush,HamidKarzai Figure2showsthePCAprojectionofthesamesixEnglish\n",
      "French Paris,Amsterdam,a`Paris,Manchester,DeSmet\n",
      "Paris\n",
      "German Paris,Languedoc,Constantine,Saint-maurice,Nancy entities in their original English space and in French space\n",
      "French SanFrancisco,LosAngeles,SantaMonica,Californie\n",
      "California German Kalifornien,LosAngeles,PalmSprings,SantaMonica aftertransformation. WecanobservethatthevectorsofEn-\n",
      "French post-punk,rockalternatif,smoothjazz,souljazz glish entities show certain structures, where the U.S. cities\n",
      "rockmusic\n",
      "German rockmusik,soul,deathmetal,dance-pop\n",
      "aregroupedtogetherandothercountries’citiesarewellsep-\n",
      "Table9: Examplesofcross-lingualrelationmatching. arated. AftertransformationintoFrenchspace,theseEnglish\n",
      "entities not only keep their original spatial emergence, but\n",
      "Relation Target Candidates(inascendingorderofrankbyEuclideandistance)\n",
      "French capitale,territoire,paysaccre`ditant,lieudeveneration alsoareclosetotheircorrespondingentitiesinFrench. This\n",
      "capital\n",
      "German hauptstadt,hauptort,gru¨ndungsort,city\n",
      "illustrates the transformation preserves mono-lingual struc-\n",
      "French nationalie´,paysdenaissance,domicile,re´sidence\n",
      "nationality\n",
      "German nationalita¨t,nation,letzterstart,sterbeort ture and also it is able to capture cross-lingual information.\n",
      "French langue,re´alisations,lieudeces,nationalite`\n",
      "language We believe this example illustrates the good performance\n",
      "German sprache,originalsprache,lang,land\n",
      "French surnom,descendant,texte,nomdering wehavedemonstratedincross-lingualtasksincludingcross-\n",
      "nickname\n",
      "German spitzname,originaltitel,names,alternativnamen\n",
      "lingual entity matching and triple-wise alignment verifica-\n",
      "Table10: Examplesofcross-lingualtriplecompletion. tion.\n",
      "Query Target Candidates(inascendingorderofrank)\n",
      "6.2 AdditionalExperimentalResults\n",
      "musiqueinde`pendante,musiquealternative,\n",
      "(AdamLambert, French\n",
      "ode,glamrock\n",
      "genre,?t) German popmusik,dance-pop,nowave,soul We derive another data set CN3l from the MIT ConceptNet\n",
      "(Ronaldinho, French milieuoffensif,attaquant,quarterback,late`ralgauche toevaluateMTransE,whosestatisticsareshowninTable11.\n",
      "position,?t) German stu¨rmer,linkerflu¨gel,angriffsspieler,rechterflgel\n",
      "French capitale,plusgrandeville,chef-lieu,garnison ThoughbeingasmallerdatasetthanWK3l-15k,knowledge\n",
      "(Italy,?r,Rome)\n",
      "German hauptstadt,hauptort,verwaltungssitz,stadion graphs in CN3l are highly sparse. Thereof, each language\n",
      "ministre-pre`sident,pre`de`cesseur,premierministre,\n",
      "(BarackObama,?r, French pre`sidentduconseil version of CN3l contains around 7,500 nodes and less than\n",
      "GeorgeBush)\n",
      "German vorga¨nger,vorga¨ngerin,besetzung,lied 41 types of relations. The alignment sets are created based\n",
      "BrantBjork,ChrisGarneau,DavidDraiman,\n",
      "(?h,instrument, French IanMackaye on the relation TranslationOf of the ConceptNet. In this\n",
      "guitar)\n",
      "German PhilManzanera,StylesP.,TinaCharles,LukeBryan section we report the results of the two cross-lingual tasks\n",
      "We start with the search of cross-lingual counterparts of on the CN3l data set for MTransE as well as all baselines.\n",
      "entities and relations. We choose an entity (or relation) in Basically,theseresultsleadtosimilarconclusionsaswehave\n",
      "English and then show the nearest candidates in French and onWK3l.\n",
      "German,respectively.Thesecandidatesarelistedbydecreas- EvaluationProtocol. Themetricsandevaluationprocedures\n",
      "ingvaluesoftheEuclideandistancebetweentheirvectorsin arethesameasthoseonWK3l. Weselectλamong{0.001,\n",
      "thetargetlanguagespaceandtheresultpointofcross-lingual 0.01, 0.05}, α among {1, 2.5, 5, 7.5}, l 1 or l 2 norm in loss\n",
      "transition. Several examples are shown in Table 8 and Ta- functions,anddimensionalitykamong{25,50,75}.Optimal\n",
      "ble9. Inalltablesofthissubsection, wemarktheexactan- parameters are configured as λ = 0.001, α = 2.5, k = 50,\n",
      "swersasboldfaced,andtheconceptuallycloseonesasitalic. andl\n",
      "1\n",
      "normforallmodels. Toperformtheevaluationunder\n",
      "For example, in Table 8, besides boldfacing the exactly cor- controlledvariables,weagainuseoneconfigurationoneach\n",
      "rectanswersforBarackObamaandParis,weconsiderthose modelrespectivelyinthetwoexperiments. Sincethedataset\n",
      "whohavealsobeenU.S.presidentsasconceptuallycloseto issmaller,trainingislimitedto200epochs.\n",
      "BarackObama, andEuropeancitiesotherthanParisascon- Results of Cross-lingual Entity Matching. The results are\n",
      "ceptually close to Paris. Also, in Table 9, those French and reported in Table 12. For the baselines, LM and CCA are\n",
      "Germanrelationsthathavethemeaningofsettlementsofsig- againleftfarbehindforbeingdisjointedlytrained. OT,how-\n",
      "nificanceareconsideredasconceptuallyclosetocapital. ever,takesthepositionaheadofVar. Thisislikelybecause\n",
      "1\n",
      "We then move on to the more complicated cross-lingual the knowledge graphs in CN3l are highly sparse, therefore\n",
      "triplecompletiontask. Weconstructqueriesbyreplacingone fewer interference of monolingual relations among entities\n",
      "elementinanEnglishtriplewithaquestionmark,forwhich makestheorthogonalityconstrainteasiertofulfill. Evenso,\n",
      "weseekforanswersinanotherlanguage. Ourmethodsneed in all settings, OT is still largely outperformed by Var and\n",
      "4\n",
      "to transfer the remaining elements to the space of the target Var, which receives amazingly good results, thus steadily\n",
      "5\n",
      "language and pick the best answer for the missing element. being the optimal solutions. Interestingly, Var now ranks\n",
      "3\n",
      "Table10showssomequeryanswers. Itisnoteworthythatthe rightbehindthelinear-transformation-basedvariantsinmost\n",
      "Figure2: VisualizationoftheresultofVar forthesamesixEnglishentitiesintheiroriginalspace(left)andinFrenchspace\n",
      "4\n",
      "afterbeingtransformed(right). Englishentitiesarerenderedinblue,andthecorrespondingFrenchentitiesareinlightruby.\n",
      "Table12: Cross-lingualentitymatching(CN3l). Table13: Accuracyoftriple-wise\n",
      "alignmentverification(%).\n",
      "Languages En-Fr Fr-En En-De De-En\n",
      "Metric Hits@10 Mean Hits@10 Mean Hits@10 Mean Hits@10 Mean Languages En&Fr En&De\n",
      "LM 25.45 1302.83 20.16 1884.70 30.12 954.71 18.04 1487.90 LM 60.53 51.55\n",
      "CCA 27.96 1204.91 26.40 1740.83 28.76 1176.09 25.30 1834.21 CCA 81.57 79.54\n",
      "OT 68.43 42.30 67.06 33.86 72.34 74.98 69.47 44.38 OT 93.01 87.59\n",
      "V V V V Va a a a ar r r r r2 3 4 51 6 4 7 8 81 4 3 6 6.....3 0 7 8 27 6 3 3 1 25 2 1 125 9 6 66....1. 3 6 966 4 4 93 6 5 7 8 89 7 7 0 0.....2 1 0 6 17 5 2 2 9 3 9<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  68844,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Yankai Lin', 'Zhiyuan Liu', 'Maosong Sun', 'Yang Liu', 'Xuan Zhu']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: 09 25.30 1834.21 CCA 81.57 79.54\n",
      "OT 68.43 42.30 67.06 33.86 72.34 74.98 69.47 44.38 OT 93.01 87.59\n",
      "V V V V Va a a a ar r r r r2 3 4 51 6 4 7 8 81 4 3 6 6.....3 0 7 8 27 6 3 3 1 25 2 1 125 9 6 66....1. 3 6 966 4 4 93 6 5 7 8 89 7 7 0 0.....2 1 0 6 17 5 2 2 9 3 9 1 7 73 5 4..... 8 36 1 8 6 40 3 2 6 4 7 8 83 9 0 8 9.....0 0 5 8 16 7 5 9 9 27 51 7 84 09.... 1 25. 89 6 74 37 6 4 7 9 93 9 0 5 5.....5 1 9 6 56 5 6 7 3 27 41 1 19 74.... 4 67. 95 7 39 98 V V V V Va a a a ar r r r r1 2 3 4 5 9 8 8 9 93 7 8 7 7.....9 3 9 4 12 0 5 6 8 9 8 8 9 91 2 4 6 5.....8 7 8 6 49 0 0 3 2\n",
      "settings. This is quite reasonable because the cross-lingual\n",
      "transitions, which are regarded as a type of relation by Var\n",
      "3\n",
      "in the graphs, are now way less frequent in CN3l than they\n",
      "areinthemuchdenserandmoreheterogeneousWK3l. Thus,\n",
      "thisexplainswhyitperformsbetterthanVar. Forthesame\n",
      "1\n",
      "reasonaswestatedinSection4.1,Var isplacedatlastofthe\n",
      "2\n",
      "fiveMTransEvariantsinmatchingcross-lingualentities.\n",
      "ResultsofTriple-wiseAlignmentVerification. Theresults\n",
      "shown in Table 13 reflect the same conclusions of the ex-\n",
      "perimentperformedonWK3lthat,thelinear-transformation-\n",
      "based variants takes the lead ahead of the rest MTransE\n",
      "variants and the baselines. While Var, despite being the\n",
      "1\n",
      "simplest, takes the second place with a satisfying accuracy\n",
      "in triple-wise alignment verification as well. The relation-\n",
      "dedicatedcalibrationstillcausesinterferenceintheoptimiza-\n",
      "tionprocessofVar,thereforeleadstoa4%∼9%dropofac-\n",
      "2\n",
      "curacy from Var. Var performs slightly better than Var.\n",
      "1 3 3\n",
      "On triple-wise alignment verification on CN3l, we receive\n",
      "exactly the same placement for evaluating the five MTransE\n",
      "variants. Meanwhile, for the baselines, OT is slightly worse\n",
      "than Var, CCA also receives acceptable accuracy which is\n",
      "1\n",
      "howeverworsethanallMTransEvariants,whiletheaccuracy\n",
      "ofLMisjustslightlybetterthanrandomguessing.\n",
      "Aboveall,theresultsinCN3lindicatesthatMTransEalso\n",
      "workspromisinglyonverysparsemultilingualgraphs,while\n",
      "thelinear-transformation-basedvariantsarethebestrepresen-\n",
      "tationtechniques.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "            220,   2545,    220,    914,     13,    966,    220,  10750,     19,\n",
      "             13,   1691,    356,   5158,    220,   5932,     13,   3226,    220,\n",
      "           4643,     13,   4370,    198,   1831,    220,   2614,     13,   3391,\n",
      "            220,   2983,     13,    966,    220,   3080,     13,   2705,    220,\n",
      "           1644,     13,   4218,    220,   5332,     13,   1958,    220,   5728,\n",
      "             13,   3264,    220,   3076,     13,   2618,    220,   2096,     13,\n",
      "           1987,   8775,    220,   6365,     13,   1721,    220,   4044,     13,\n",
      "           2946,    198,     53,    650,    650,    650,  27713,    264,    264,\n",
      "            264,    802,    436,    436,    436,    436,     17,    220,     18,\n",
      "            220,     19,    220,   3971,    220,     21,    220,     19,    220,\n",
      "             22,    220,     23,    220,   5932,    220,     19,    220,     18,\n",
      "            220,     21,    220,     21,  18575,     18,    220,     15,    220,\n",
      "             22,    220,     23,    220,   1544,    220,     21,    220,     18,\n",
      "            220,     18,    220,     16,    220,    914,    220,     17,    220,\n",
      "             16,    220,   6549,    220,     24,    220,     21,    220,   2287,\n",
      "           1975,     16,     13,    220,     18,    220,     21,    220,  25285,\n",
      "            220,     19,    220,     19,    220,   6365,    220,     21,    220,\n",
      "             20,    220,     22,    220,     23,    220,   4578,    220,     22,\n",
      "            220,     22,    220,     15,    220,     15,  18575,     17,    220,\n",
      "             16,    220,     15,    220,     21,    220,   1114,    220,     20,\n",
      "            220,     17,    220,     17,    220,     24,    220,     18,    220,\n",
      "             24,    220,     16,    220,     22,    220,   5958,    220,     20,\n",
      "            220,     19,  18575,    220,     23,    220,   1927,    220,     16,\n",
      "            220,     23,    220,     21,    220,   1272,    220,     18,    220,\n",
      "             17,    220,     21,    220,     19,    220,     22,    220,     23,\n",
      "            220,   6069,    220,     24,    220,     15,    220,     23,    220,\n",
      "             24,  18575,     15,    220,     15,    220,     20,    220,     23,\n",
      "            220,    845,    220,     22,    220,     20,    220,     24,    220,\n",
      "             24,    220,   1544,    220,   3971,    220,     22,    220,   5833,\n",
      "            220,   2545,   1975,    220,     16,    220,    914,     13,    220,\n",
      "           4578,    220,     21,    220,   5728,    220,   1806,    220,     21,\n",
      "            220,     19,    220,     22,    220,     24,    220,   6365,    220,\n",
      "             24,    220,     15,    220,     20,    220,     20,  18575,     20,\n",
      "            220,     16,    220,     24,    220,     21,    220,   3487,    220,\n",
      "             20,    220,     21,    220,     22,    220,     18,    220,   1544,\n",
      "            220,   3174,    220,     16,    220,    777,    220,   5728,   1975,\n",
      "            220,     19,    220,   3080,     13,    220,   2721,    220,     22,\n",
      "            220,   2137,    220,   3264,    650,    650,    650,    650,  27713,\n",
      "            264,    264,    264,    802,    436,    436,    436,    436,     16,\n",
      "            220,     17,    220,     18,    220,     19,    220,     20,    220,\n",
      "             24,    220,     23,    220,     23,    220,     24,    220,   6365,\n",
      "            220,     22,    220,     23,    220,     22,    220,     22,  18575,\n",
      "             24,    220,     18,    220,     24,    220,     19,    220,    717,\n",
      "            220,     15,    220,     20,    220,     21,    220,     23,    220,\n",
      "             24,    220,     23,    220,     23,    220,     24,    220,   5925,\n",
      "            220,     17,    220,     19,    220,     21,    220,     20,  18575,\n",
      "             23,    220,     22,    220,     23,    220,     21,    220,   2491,\n",
      "            220,     15,    220,     15,    220,     18,    220,     17,    198,\n",
      "           6648,     13,   1115,    374,   5115,  13579,   1606,    279,   5425,\n",
      "             12,   2785,    940,    198,   1485,   6055,     11,    902,    527,\n",
      "          27458,    439,    264,    955,    315,  12976,    555,   8909,    198,\n",
      "             18,    198,    258,    279,  40099,     11,    527,   1457,   1648,\n",
      "           2753,  21420,    304,  25914,     18,     75,   1109,    814,    198,\n",
      "            548,  48121,    336,   1412,  53321,    261,    438,   6518,     71,\n",
      "           1430,  49122,  69416,     18,     75,     13,  14636,    345,    576,\n",
      "          30992,   1771,  35734,    275,    716,  10008,  58234,  54895,   4050,\n",
      "             13,   1789,   6509,    373,    198,     16,    198,  20489,    300,\n",
      "          11285,    660,    258,   9817,     19,     13,     16,     11,   4050,\n",
      "            374,  37469,    266,   4354,   1073,   1820,    198,     17,    198,\n",
      "          53770,     44,   3246,     36,  55711,    258,  90143,  29942,     12,\n",
      "           2785,    940,  10720,    627,   2122,    708,     69,  83926,  45539,\n",
      "           7178,  63439,     13,    666,  13213, 122761,    198,  70463,    304,\n",
      "           6771,    220,   1032,   8881,    279,   1890,  31342,    315,    279,\n",
      "            506,   7058,  14666,    716,  10365,    263,  69416,     18,     75,\n",
      "           9210,     11,    339,   4939,    277,  39160,   1659,   7058,  31039,\n",
      "          27103,   5097,    279,   3063,   8469,    315,    279,   2800,    386,\n",
      "           3246,     36,    198,  55711,    323,    279,   3122,  11243,     13,\n",
      "           6104,   8909,     11,   8994,   1694,    279,    198,     16,    198,\n",
      "          23796,    267,     11,   5097,    279,   2132,   2035,    449,    264,\n",
      "          37154,  13708,    198,    258,  24657,  45539,  17632,  23751,    439,\n",
      "           1664,     13,    578,  12976,   7058,   9988,  10297,   5531,  18856,\n",
      "          44466,    936,   4881,   2295,   2251,    258,   1820,  19680,  17528,\n",
      "           7058,  28491,   4734,   1073,   4050,     11,  19041,   1348,  27152,\n",
      "            267,  20103,     19,      4,  22447,    120,     24,      4,   6861,\n",
      "           1073,    582,   7058,     17,    198,  22222,    505,   8909,     13,\n",
      "           8909,  27772,  10284,   2731,   1109,   8909,    627,     16,    220,\n",
      "             18,    220,     18,    198,   1966,  24657,  45539,  17632,  23751,\n",
      "            389,  25914,     18,     75,     11,    584,   5371,    198,    327,\n",
      "          33839,    279,   1890,  22165,    369,  38663,    279,   4330,    386,\n",
      "           3246,     36,    198,  55711,     13,  26982,     11,    369,    279,\n",
      "           3122,  11243,     11,   8775,    374,  10284,  11201,    198,  54895,\n",
      "           8909,     11,    356,   5158,   1101,  21879,  22281,  13708,    902,\n",
      "            374,    198,     16,    198,  98936,  50810,    751,  10118,    543,\n",
      "             44,   3246,     36,  55711,     11,   3556,   1820,  33829,    198,\n",
      "           1073,  11237,    285,   4345,   3306,  73048,  58234,  54895,  11719,\n",
      "          52851,    287,    627,  59907,    543,     11,    700,    288,    495,\n",
      "          16319,  29768,     18,     75,    485,   8630,    267,   9379,     44,\n",
      "           3246,     36,  19171,    198,   1816,   2203,    442,   3876,    398,\n",
      "            263,    424,   1065,   6534,  26961,  50923,  87286,     11,   3556,\n",
      "            198,    339,   4939,    277,  39160,   1659,   6108,  55711,    548,\n",
      "           1820,  16241,  10200,    417,    268,   7058,     83,    367,  26522,\n",
      "           8467,     13, 128009, 128006,    882, 128007,    271,   7184,     11,\n",
      "           2728,    420,   3488,     25,  10699,    527,    279,  12283,    315,\n",
      "            279,   5684,   4710,    220,  21335,   1203,    279,   4320,   1193,\n",
      "            304,    264,  13325,   1160,   3645,     11,    369,   3187,     25,\n",
      "           2570,     32,   1882,     33,   7352,   1442,    499,   1541,    956,\n",
      "           1440,    279,   4320,     11,   1120,    471,    459,   4384,   1160,\n",
      "             13, 128009, 128006,  78191, 128007,    271]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    459,  18328,\n",
      "            369,   3488,     12,    598,     86,   4776,   9256,     13,   5560,\n",
      "           1193,    279,   3984,   2317,   2038,    311,   1376,    701,   2077,\n",
      "             13, 128009, 128006,    882, 128007,    271,   2014,  12143,     25,\n",
      "            220,   2545,    220,    914,     13,    966,    220,  10750,     19,\n",
      "             13,   1691,    356,   5158,    220,   5932,     13,   3226,    220,\n",
      "           4643,     13,   4370,    198,   1831,    220,   2614,     13,   3391,\n",
      "            220,   2983,     13,    966,    220,   3080,     13,   2705,    220,\n",
      "           1644,     13,   4218,    220,   5332,     13,   1958,    220,   5728,\n",
      "             13,   3264,    220,   3076,     13,   2618,    220,   2096,     13,\n",
      "           1987,   8775,    220,   6365,     13,   1721,    220,   4044,     13,\n",
      "           2946,    198,     53,    650,    650,    650,  27713,    264,    264,\n",
      "            264,    802,    436,    436,    436,    436,     17,    220,     18,\n",
      "            220,     19,    220,   3971,    220,     21,    220,     19,    220,\n",
      "             22,    220,     23,    220,   5932,    220,     19,    220,     18,\n",
      "            220,     21,    220,     21,  18575,     18,    220,     15,    220,\n",
      "             22,    220,     23,    220,   1544,    220,     21,    220,     18,\n",
      "            220,     18,    220,     16,    220,    914,    220,     17,    220,\n",
      "             16,    220,   6549,    220,     24,    220,     21,    220,   2287,\n",
      "           1975,     16,     13,    220,     18,    220,     21,    220,  25285,\n",
      "            220,     19,    220,     19,    220,   6365,    220,     21,    220,\n",
      "             20,    220,     22,    220,     23,    220,   4578,    220,     22,\n",
      "            220,     22,    220,     15,    220,     15,  18575,     17,    220,\n",
      "             16,    220,     15,    220,     21,    220,   1114,    220,     20,\n",
      "            220,     17,    220,     17,    220,     24,    220,     18,    220,\n",
      "             24,    220,     16,    220,     22,    220,   5958,    220,     20,\n",
      "            220,     19,  18575,    220,     23,    220,   1927,    220,     16,\n",
      "            220,     23,    220,     21,    220,   1272,    220,     18,    220,\n",
      "             17,    220,     21,    220,     19,    220,     22,    220,     23,\n",
      "            220,   6069,    220,     24,    220,     15,    220,     23,    220,\n",
      "             24,  18575,     15,    220,     15,    220,     20,    220,     23,\n",
      "            220,    845,    220,     22,    220,     20,    220,     24,    220,\n",
      "             24,    220,   1544,    220,   3971,    220,     22,    220,   5833,\n",
      "            220,   2545,   1975,    220,     16,    220,    914,     13,    220,\n",
      "           4578,    220,     21,    220,   5728,    220,   1806,    220,     21,\n",
      "            220,     19,    220,     22,    220,     24,    220,   6365,    220,\n",
      "             24,    220,     15,    220,     20,    220,     20,  18575,     20,\n",
      "            220,     16,    220,     24,    220,     21,    220,   3487,    220,\n",
      "             20,    220,     21,    220,     22,    220,     18,    220,   1544,\n",
      "            220,   3174,    220,     16,    220,    777,    220,   5728,   1975,\n",
      "            220,     19,    220,   3080,     13,    220,   2721,    220,     22,\n",
      "            220,   2137,    220,   3264,    650,    650,    650,    650,  27713,\n",
      "            264,    264,    264,    802,    436,    436,    436,    436,     16,\n",
      "            220,     17,    220,     18,    220,     19,    220,     20,    220,\n",
      "             24,    220,     23,    220,     23,    220,     24,    220,   6365,\n",
      "            220,     22,    220,     23,    220,     22,    220,     22,  18575,\n",
      "             24,    220,     18,    220,     24,    220,     19,    220,    717,\n",
      "            220,     15,    220,     20,    220,     21,    220,     23,    220,\n",
      "             24,    220,     23,    220,     23,    220,     24,    220,   5925,\n",
      "            220,     17,    220,     19,    220,     21,    220,     20,  18575,\n",
      "             23,    220,     22,    220,     23,    220,     21,    220,   2491,\n",
      "            220,     15,    220,     15,    220,     18,    220,     17,    198,\n",
      "           6648,     13,   1115,    374,   5115,  13579,   1606,    279,   5425,\n",
      "             12,   2785,    940,    198,   1485,   6055,     11,    902,    527,\n",
      "          27458,    439,    264,    955,    315,  12976,    555,   8909,    198,\n",
      "             18,    198,    258,    279,  40099,     11,    527,   1457,   1648,\n",
      "           2753,  21420,    304,  25914,     18,     75,   1109,    814,    198,\n",
      "            548,  48121,    336,   1412,  53321,    261,    438,   6518,     71,\n",
      "           1430,  49122,  69416,     18,     75,     13,  14636,    345,    576,\n",
      "          30992,   1771,  35734,    275,    716,  10008,  58234,  54895,   4050,\n",
      "             13,   1789,   6509,    373,    198,     16,    198,  20489,    300,\n",
      "          11285,    660,    258,   9817,     19,     13,     16,     11,   4050,\n",
      "            374,  37469,    266,   4354,   1073,   1820,    198,     17,    198,\n",
      "          53770,     44,   3246,     36,  55711,    258,  90143,  29942,     12,\n",
      "           2785,    940,  10720,    627,   2122,    708,     69,  83926,  45539,\n",
      "           7178,  63439,     13,    666,  13213, 122761,    198,  70463,    304,\n",
      "           6771,    220,   1032,   8881,    279,   1890,  31342,    315,    279,\n",
      "            506,   7058,  14666,    716,  10365,    263,  69416,     18,     75,\n",
      "           9210,     11,    339,   4939,    277,  39160,   1659,   7058,  31039,\n",
      "          27103,   5097,    279,   3063,   8469,    315,    279,   2800,    386,\n",
      "           3246,     36,    198,  55711,    323,    279,   3122,  11243,     13,\n",
      "           6104,   8909,     11,   8994,   1694,    279,    198,     16,    198,\n",
      "          23796,    267,     11,   5097,    279,   2132,   2035,    449,    264,\n",
      "          37154,  13708,    198,    258,  24657,  45539,  17632,  23751,    439,\n",
      "           1664,     13,    578,  12976,   7058,   9988,  10297,   5531,  18856,\n",
      "          44466,    936,   4881,   2295,   2251,    258,   1820,  19680,  17528,\n",
      "           7058,  28491,   4734,   1073,   4050,     11,  19041,   1348,  27152,\n",
      "            267,  20103,     19,      4,  22447,    120,     24,      4,   6861,\n",
      "           1073,    582,   7058,     17,    198,  22222,    505,   8909,     13,\n",
      "           8909,  27772,  10284,   2731,   1109,   8909,    627,     16,    220,\n",
      "             18,    220,     18,    198,   1966,  24657,  45539,  17632,  23751,\n",
      "            389,  25914,     18,     75,     11,    584,   5371,    198,    327,\n",
      "          33839,    279,   1890,  22165,    369,  38663,    279,   4330,    386,\n",
      "           3246,     36,    198,  55711,     13,  26982,     11,    369,    279,\n",
      "           3122,  11243,     11,   8775,    374,  10284,  11201,    198,  54895,\n",
      "           8909,     11,    356,   5158,   1101,  21879,  22281,  13708,    902,\n",
      "            374,    198,     16,    198,  98936,  50810,    751,  10118,    543,\n",
      "             44,   3246,     36,  55711,     11,   3556,   1820,  33829,    198,\n",
      "           1073,  11237,    285,   4345,   3306,  73048,  58234,  54895,  11719,\n",
      "          52851,    287,    627,  59907,    543,     11,    700,    288,    495,\n",
      "          16319,  29768,     18,     75,    485,   8630,    267,   9379,     44,\n",
      "           3246,     36,  19171,    198,   1816,   2203,    442,   3876,    398,\n",
      "            263,    424,   1065,   6534,  26961,  50923,  87286,     11,   3556,\n",
      "            198,    339,   4939,    277,  39160,   1659,   6108,  55711,    548,\n",
      "           1820,  16241,  10200,    417,    268,   7058,     83,    367,  26522,\n",
      "           8467,     13, 128009, 128006,    882, 128007,    271,   7184,     11,\n",
      "           2728,    420,   3488,     25,  10699,    527,    279,  12283,    315,\n",
      "            279,   5684,   4710,    220,  21335,   1203,    279,   4320,   1193,\n",
      "            304,    264,  13325,   1160,   3645,     11,    369,   3187,     25,\n",
      "           2570,     32,   1882,     33,   7352,   1442,    499,   1541,    956,\n",
      "           1440,    279,   4320,     11,   1120,    471,    459,   4384,   1160,\n",
      "             13, 128009, 128006,  78191, 128007,    271,   1318, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Learning Symmetric Collaborative Dialogue Agents with Dynamic\n",
      "Knowledge Graph Embeddings\n",
      "HeHe and AnushaBalakrishnan and MihailEric and PercyLiang\n",
      "ComputerScienceDepartment,StanfordUniversity\n",
      "{hehe,anusha28,meric,pliang}@cs.stanford.edu\n",
      "Abstract FriendsofagentA:\n",
      "Name School Major Company\n",
      "We study a symmetric collaborative dia-\n",
      "Jessica Columbia ComputerScience Google\n",
      "logue setting in which two agents, each\n",
      "Josh Columbia Linguistics Google\n",
      "withprivateknowledge,muststrategically............\n",
      "communicate to achieve a common goal.\n",
      "A:Hi!MostofmyfriendsworkforGoogle\n",
      "The open-ended dialogue state in this set-\n",
      "B:doyouhaveanyonewhowenttocolumbia?\n",
      "ting poses new challenges for existing di- A:Hello?\n",
      "A:IhaveJessicaafriendofmine\n",
      "alogue systems. We collected a dataset\n",
      "A:andJosh,bothwenttocolumbia\n",
      "of 11K human-human dialogues, which B:oranyoneworkingatapple?\n",
      "exhibits interesting lexical, semantic, and B:SELECT(Jessica,Columbia,ComputerScience,Google)\n",
      "A:SELECT(Jessica,Columbia,ComputerScience,Google)\n",
      "strategic elements. To model both struc-\n",
      "tured knowledge and unstructured lan-\n",
      "Figure1: AnexampledialoguefromtheMutual-\n",
      "guage,weproposeaneuralmodelwithdy-\n",
      "Friends task in which two agents, A and B, each\n",
      "namic knowledge graph embeddings that\n",
      "givenaprivatelistofafriends,trytoidentifytheir\n",
      "evolve as the dialogue progresses. Au-\n",
      "mutual friend. Our objective is to build an agent\n",
      "tomatic and human evaluations show that\n",
      "that can perform the task with a human. Cross-\n",
      "ourmodelisbothmoreeffectiveatachiev-\n",
      "talk(Section2.3)isitalicized.\n",
      "ing the goal and more human-like than\n",
      "baselineneuralandrule-basedmodels.\n",
      "of systems, we focus on a symmetric collabora-\n",
      "1 Introduction\n",
      "tive dialogue setting, which is task-oriented but\n",
      "Current task-oriented dialogue systems (Young encourages open-ended dialogue acts. In our set-\n",
      "etal.,2013;Wenetal.,2017;Dhingraetal.,2017) ting, two agents, each with a private list of items\n",
      "require a pre-defined dialogue state (e.g., slots with attributes, must communicate to identify the\n",
      "such as food type and price range for a restau- uniqueshareditem. ConsiderthedialogueinFig-\n",
      "rantsearchingtask)andafixedsetofdialogueacts ure 1, in which two people are trying to find their\n",
      "(e.g.,request,inform). However,humanconversa- mutual friend. By asking “do you have anyone\n",
      "tionoftenrequiresricherdialoguestatesandmore who went to columbia?”, B is suggesting that she\n",
      "nuanced, pragmatic dialogue acts. Recent open- hassomeColumbiafriends,andthattheyprobably\n",
      "domain chat systems (Shang et al., 2015; Serban work at Google. Such conversational implicature\n",
      "etal.,2015b;Sordonietal.,2015;Lietal.,2016a; is lost when interpreting the utterance as simply\n",
      "Lowe et al., 2017; Mei et al., 2017) learn a map- an information request. In addition, it is hard to\n",
      "ping directly from previous utterances to the next define a structured state that captures the diverse\n",
      "utterance. Whilethesemodelscaptureopen-ended semanticsinmanyutterances(e.g.,defining“most\n",
      "aspectsofdialogue,thelackofstructureddialogue of”,“mightbe”;seedetailsinTable1).\n",
      "state prevents them from being directly applied To model both structured and open-ended con-\n",
      "to settings that require interfacing with structured text, we propose the Dynamic Knowledge Graph\n",
      "knowledge. Network(DynoNet),inwhichthedialoguestateis\n",
      "Inordertobridgethegapbetweenthetwotypes modeledasaknowledgegraphwithanembedding\n",
      "7102\n",
      "rpA\n",
      "42\n",
      "]LC.sc[\n",
      "1v03170.4071:viXra\n",
      "for each node (Section 3). Our model is similar 2.1 TaskDefinition\n",
      "to EntNet (Henaff et al., 2017) in that node/entity\n",
      "In the symmetric collaborative dialogue setting,\n",
      "embeddings are updated recurrently given new\n",
      "there are two agents, A and B, each with a pri-\n",
      "utterances. The difference is that we structure\n",
      "vate knowledge base—KB and KB, respec-\n",
      "A B\n",
      "entities as a knowledge graph; as the dialogue\n",
      "tively. Each knowledge base includes a list of\n",
      "proceeds, new nodes are added and new context\n",
      "items, where each item has a value for each at-\n",
      "is propagated on the graph. An attention-based\n",
      "tribute. For example, in the MutualFriends set-\n",
      "mechanism (Bahdanau et al., 2015) over the node\n",
      "ting, Figure 1, items are friends and attributes are\n",
      "embeddings drives generation of new utterances.\n",
      "name, school, etc. There is a shared item that A\n",
      "Ourmodel’suseofknowledgegraphscapturesthe\n",
      "and B both have; their goal is to converse with\n",
      "grounding capability of classic task-oriented sys-\n",
      "eachothertodeterminetheshareditemandselect\n",
      "temsandthegraphembeddingprovidestherepre-\n",
      "it. Formally, an agent is a mapping from its pri-\n",
      "sentationalflexibilityofneuralmodels.\n",
      "vateKBandthedialoguethusfar(sequenceofut-\n",
      "The naturalness of communication in the sym-\n",
      "terances)tothenextutterancetogenerateorase-\n",
      "metric collaborative setting enables large-scale\n",
      "lection. Adialogueisconsideredsuccessfulwhen\n",
      "data collection: We were able to crowdsource\n",
      "both agents correctly select the shared item. This\n",
      "around 11K human-human dialogues on Amazon\n",
      "settinghasparallelsinhuman-computercollabora-\n",
      "Mechanical Turk (AMT) in less than 15 hours.1\n",
      "tion where each agent has complementary exper-\n",
      "We show that the new dataset calls for more flex-\n",
      "tise.\n",
      "iblerepresentationsbeyondfully-structuredstates\n",
      "(Section2.2). 2.2 Datacollection\n",
      "Inadditiontoconductingthethird-partyhuman\n",
      "Wecreatedaschemawith7attributesandapprox-\n",
      "evaluationadoptedbymostwork(Liuetal.,2016;\n",
      "imately3Kentities(attributevalues). Toelicitlin-\n",
      "Lietal.,2016b,c), wealsoconductpartnerevalu-\n",
      "guistic and strategic variants, we generate a ran-\n",
      "ation (Wen et al., 2017) where AMT workers rate\n",
      "dom scenario for each task by varying the num-\n",
      "theirconversationalpartners(otherworkersorour\n",
      "ber of items (5 to 12), the number attributes (3 or\n",
      "models) based on fluency, correctness, coopera-\n",
      "4),andthedistributionofvaluesforeachattribute\n",
      "tion, and human-likeness. We compare DynoNet\n",
      "(skewed to uniform). See Appendix A and B for\n",
      "with baseline neural models and a strong rule-\n",
      "detailsofschemaandscenariogeneration.\n",
      "basedsystem. TheresultsshowthatDynoNetcan\n",
      "perform the task with humans efficiently and nat-\n",
      "urally; it also captures some strategic aspects of\n",
      "human-humandialogues.\n",
      "The contributions of this work are: (i) a new\n",
      "symmetric collaborative dialogue setting and a\n",
      "large dialogue corpus that pushes the boundaries\n",
      "ofexistingdialoguesystems;(ii)DynoNet,which\n",
      "integrates semantically rich utterances with struc-\n",
      "turedknowledgetorepresentopen-endeddialogue\n",
      "states; (iii) multiple automatic metrics based on\n",
      "bot-bot chat and a comparison of third-party and\n",
      "partnerevaluation. Figure2: Screenshotofthechatinterface.\n",
      "2 SymmetricCollaborativeDialogue We crowdsourced dialogues on AMT by ran-\n",
      "domly pairing up workers to perform the task\n",
      "We begin by introducing a collaborative task be- within 5 minutes.2 Our chat interface is shown in\n",
      "tween two agents and describe the human-human\n",
      "Figure2. Todiscouragerandomguessing,wepre-\n",
      "dialoguecollectionprocess. Weshowthatourdata\n",
      "ventworkersfromselectingmorethanonceevery\n",
      "exhibitsdiverse,interestinglanguagephenomena.\n",
      "10seconds. Ourtaskwasverypopularandwecol-\n",
      "1The dataset is available publicly at https:// 2If the workers exceed the time limit, the dialogue is\n",
      "stanfordnlp.github.io/cocoa/. markedasunsuccessful(butstilllogged).\n",
      "Type % Easyexample Hardexample\n",
      "Iknowajudy./Ihavesomeonewho Aboutequalindoorandoutdoorfriends/metoo.his\n",
      "Inform 30.4\n",
      "studiedthebibleintheafternoon. majorisforestry/mightbekelly\n",
      "DoanyofthemlikePoi?/Whatdoesyourhenry Whatcanyoutellmeaboutourfriend?/Ormaybe\n",
      "Ask 17.7\n",
      "do? northparkcollege?\n",
      "Answer 7.4 Noneofminedid/Yup/Theydo./Samehere. yes3ofthem/Nohelikespoi/yesifbostoncollege\n",
      "Table 1: Main utterance types and examples. We show both standard utterances whose meaning can\n",
      "be represented by simple logical forms (e.g., ask(indoor)), and open-ended ones which require more\n",
      "complexlogicalforms(difficultpartsinbold). Textspanscorrespondingtoentitiesareunderlined.\n",
      "Phenomenon Example\n",
      "Coreference (IknowoneDebra)doessheliketheindoors?/(IhavetwofriendsnamedTIffany)atWorldairways?\n",
      "Coordination keepongoingwiththefashion/Ok.let’strysomethingelse./gobyhobby/great.selecthim.thanks!\n",
      "Chit-chat Yes,thatisgoodoleTerry./Allindoorsers!myfriendshatenature\n",
      "Categorization same,mostofminearefemaletoo/DoesanyofthemnamesstartwithB\n",
      "Correction IknowonefriendintoEmbroidery-hernameisEmily.Sorry–EmbroideryfriendisnamedMichelle\n",
      "Table2: Communicationphenomenainthedataset. Evidentpartsisinboldandtextspanscorresponding\n",
      "toanentityareunderlined. Forcoreference,theantecedentisinparentheses.\n",
      "lected11Kdialoguesoveraperiodof13.5hours.3 Someofthestandardonesarealsonon-trivialdue\n",
      "Of these, over 9K dialogues are successful. Un- tocoreferenceandlogicalcompositionality.\n",
      "successfuldialoguesareusuallytheresultofeither Ourdatasetalsoexhibitssomeinterestingcom-\n",
      "workerleavingthechatprematurely. munication phenomena. Coreference occurs fre-\n",
      "quently when people check multiple attributes\n",
      "2.3 Datasetstatistics\n",
      "of one item. Sometimes mentions are dropped,\n",
      "We show the basic statistics of our dataset in Ta- as an utterance simply continues from the part-\n",
      "ble 3. An utterance is defined as a message sent ner’s utterance. People occasionally use exter-\n",
      "byoneoftheagents. Theaverageutterancelength nalknowledgetogroupitemswithout-of-schema\n",
      "is short due to the informality of the chat, how- attributes (e.g., gender based on names, location\n",
      "ever,anagentusuallysendsmultipleutterancesin based on schools). We summarize these phenom-\n",
      "one turn. Some example dialogues are shown in ena in Table 2. In addition, we find 30% utter-\n",
      "Table6andAppendixI. ances involve cross-talk where the conversation\n",
      "does not progress linearly (e.g., italic utterances\n",
      "#dialogues 11157 in Figure 1), a common characteristic of online\n",
      "#completeddialogues 9041\n",
      "chat(Ivanovic,2005).\n",
      "Vocabularysize 5325\n",
      "Average#ofutterances 11.41 Onestrategicaspectofthistaskischoosingthe\n",
      "Averagetimetakenpertask(sec.) 91.18\n",
      "orderofattributestomention. Wefindthatpeople\n",
      "Averageutterancelength(tokens) 5.08\n",
      "Numberoflinguistictemplates4 41561 tendtostartfromattributeswithfeweruniqueval-\n",
      "ues, e.g., “all my friends like morning” given the\n",
      "Table3: StatisticsoftheMutualFriendsdataset. KB in Table 6, as intuitively it would help ex-\n",
      "B\n",
      "clude items quickly given fewer values to check.5\n",
      "We categorize utterances into coarse types— Weprovideamoredetailedanalysisofstrategyin\n",
      "inform, ask, answer, greeting, apology—by pattern Section4.2andAppendixF.\n",
      "matching (Appendix E). There are 7.4% multi-\n",
      "type utterances, and 30.9% utterances contain 3 DynamicKnowledgeGraphNetwork\n",
      "more than one entity. In Table 1, we show exam-\n",
      "The diverse semantics in our data motivates us\n",
      "ple utterances with rich semantics that cannot be\n",
      "to combine unstructured representation of the di-\n",
      "sufficiently represented by traditional slot-values.\n",
      "alogue history with structured knowledge. Our\n",
      "3Tasksareputupinbatches;thetotaltimeexcludesinter-\n",
      "valsbetweenbatches. 5Ourgoalistomodelhumanbehaviorthuswedonotdis-\n",
      "4Entitynamesarereplacedbytheirentitytypes. cusstheoptimalstrategyhere.\n",
      "Dynamic knowledge graph Graph Generator\n",
      "embedding\n",
      "anyonewentcolumbia Yes jessica and josh\n",
      "KB + Dialogue history\n",
      "… …\n",
      "jessica\n",
      "Name School Company\n",
      "1 S\n",
      "Item 1 Jessica Columbia Google columbia columbia\n",
      "Item 2 Josh Columbia Google N jessica\n",
      "google josh\n",
      "B: anyone went to columbia? 2 C google …\n",
      "…\n",
      "josh\n",
      "Attention + Copy\n",
      "Message passing path of columbia\n",
      "Figure3: Overviewofourapproach. First,theKBanddialoguehistory(entitiesinbold)ismappedto\n",
      "a graph. Here, an item node is labeled by the item ID and an attribute node is labeled by the attribute’s\n",
      "firstletter. Next,eachnodeisembeddedusingrelevantutteranceembeddingsthroughmessagepassing.\n",
      "Finally,anLSTMgeneratesthenextutterancebasedonattentionoverthenodeembeddings.\n",
      "modelconsistsofthreecomponentsshowninFig- is columbia. An example graph is shown in Fig-\n",
      "ure3: (i)adynamicknowledgegraph,whichrep- ure3. ThegraphG isupdatedbasedonutterance\n",
      "t\n",
      "resentstheagent’sprivateKBandshareddialogue t by taking G and adding a new node for any\n",
      "t−1\n",
      "history as a graph (Section 3.1), (ii) a graph em- entitymentionedinutterancetbutnotinKB.7\n",
      "A\n",
      "bedding over the nodes (Section 3.2), and (iii) an\n",
      "3.2 GraphEmbedding\n",
      "utterancegenerator(Section3.3).\n",
      "Theknowledgegraphrepresentsentitiesandre- Given a knowledge graph, we are interested in\n",
      "lations in the agent’s private KB, e.g., item-1’s computing a vector representation for each node\n",
      "company is google. As the conversation unfolds, v that captures both its unstructured context from\n",
      "utterances are embedded and incorporated into the dialogue history and its structured context in\n",
      "node embeddings of mentioned entities. For in- the KB. A node embedding V t(v) for each node\n",
      "stance, in Figure 3, “anyone went to columbia” v ∈ G t is built from three parts: structural prop-\n",
      "updates the embedding of columbia. Next, each ertiesofanentitydefinedbytheKB,embeddings\n",
      "node recursively passes its embedding to neigh- ofutterancesinthedialoguehistory,andmessage\n",
      "boring nodes so that related entities (e.g., those passingbetweenneighboringnodes.\n",
      "in the same row or column) also receive informa-\n",
      "Node Features. Simple structural properties of\n",
      "tion from the most recent utterance. In our exam-\n",
      "the KB often govern what is talked about; e.g.,\n",
      "ple, jessica and josh both receive new context\n",
      "a high-frequency entity is usually interesting to\n",
      "when columbia is mentioned. Finally, the utter-\n",
      "mention(consider“Allmyfriendslikedancing.”).\n",
      "ancegenerator,anLSTM,producesthenextutter-\n",
      "We represent this type of information as a fea-\n",
      "ancebyattendingtothenodeembeddings.\n",
      "ture vector F (v), which includes the degree and\n",
      "t\n",
      "type(item,attribute,orentitytype)ofnodev,and\n",
      "3.1 KnowledgeGraph\n",
      "whetherithasbeenmentionedinthecurrentturn.\n",
      "Given a dialogue of T utterances, we construct Each feature is encoded as a one-hot vector and\n",
      "graphs (G t)T\n",
      "t=1\n",
      "over the KB and dialogue history theyareconcatenatedtoformF t(v).\n",
      "foragentA.6 Therearethreetypesofnodes: item\n",
      "MentionVectors. AmentionvectorM (v)con-\n",
      "nodes, attribute nodes, and entity nodes. Edges t\n",
      "tainsunstructuredcontextfromutterancesrelevant\n",
      "between nodes represent their relations. For ex-\n",
      "to node v up to turn t. To compute it, we first de-\n",
      "ample,(item-1, hasSchool, columbia)means\n",
      "fine the utterance representation u˜ and the set of\n",
      "thatthefirstitemhasattributeschoolwhosevalue t\n",
      "relevant entities E. Let u be the embedding of\n",
      "t t\n",
      "6 It is important to differentiate perspectives of the two utterancet(Section3.3). Todifferentiatebetween\n",
      "agentsastheyhavedifferentKBs. Thereafterweassumethe\n",
      "perspectiveofagentA,i.e., accessingKB forAonly, and 7Weusearule-basedlexicontolinktextspanstoentities.\n",
      "A\n",
      "refertoBasthepartner. SeedetailsinAppendixD.\n",
      "the agent’s and the partner’s utterances, we repre- 3.3 UtteranceEmbeddingandGeneration\n",
      "sent it as u˜ = (cid:2) u ·1,u ·1 (cid:3),\n",
      "t t {ut∈U self} t {ut∈Upartner} We embed and generate utterances using Long\n",
      "where U and U denote sets of utterances\n",
      "self partner Short Term Memory (LSTM) networks that take\n",
      "generated by the agent and the partner, and [·,·]\n",
      "thegraphembeddingsintoaccount.\n",
      "denotes concatenation. Let E be the set of entity\n",
      "t\n",
      "nodesmentionedinutterancetifutterancetmen- Embedding. On turn t, upon receiving an\n",
      "tionssomeentities, orutterancet−1otherwise.8 utterance consisting of n t tokens, x t =\n",
      "ThementionvectorM t(v)ofnodev incorporates (x t,1,...,x t,nt), the LSTM maps it to a vector as\n",
      "thecurrentutteranceifvismentionedandinherits follows:\n",
      "M t−1(v)ifnot: h\n",
      "t,j\n",
      "= LSTM enc(h t,j−1,A t(x t,j)), (4)\n",
      "M t(v) = λ tM t−1(v)+(1−λ t)u˜ t; (1) where h\n",
      "t,0\n",
      "= h t−1,nt−1, and A\n",
      "t\n",
      "is an entity ab-\n",
      "(cid:40) σ(cid:0) Winc[M (v),u˜ ](cid:1) ifv ∈ E, stractionfunction,explainedbelow. Thefinalhid-\n",
      "t−1 t t\n",
      "λ t = 1 otherwise. den state h t,nt is used as the utterance embed-\n",
      "dingu,whichupdatesthementionvectorsasde-\n",
      "t\n",
      "Here, σ is the sigmoid function and Winc is a pa- scribedinSection3.2.\n",
      "rametermatrix. In our dialogue task, the identity of an entity\n",
      "is unimportant. For example, replacing google\n",
      "Recursive Node Embeddings. We propagate\n",
      "with alphabet in Figure 1 should make little dif-\n",
      "informationbetweennodesaccordingtothestruc-\n",
      "ference to the conversation. The role of an entity\n",
      "ture of the knowledge graph. In Figure 3, given\n",
      "is determined instead by its relation to other en-\n",
      "“anyonewenttocolumbia?”,theagentshouldfo-\n",
      "tities and relevant utterances. Therefore, we de-\n",
      "cusonherfriendswhowenttoColumbiaUniver-\n",
      "finetheabstractionA (y)forawordy asfollows:\n",
      "t\n",
      "sity. Therefore, we want this utterance to be sent\n",
      "if y is linked to an entity v, then we represent an\n",
      "toitemnodesconnectedtocolumbia,andonestep\n",
      "entity by its type (school, company etc.) embed-\n",
      "further to other attributes of these items because\n",
      "ding concatenated with its current node embed-\n",
      "theymightbementionednextasrelevantinforma-\n",
      "ding: A (y) = [E,V (v)]. Note that V (v)\n",
      "tion,e.g.,jessicaandjosh.\n",
      "t type(y) t t\n",
      "isdeterminedonlybyitsstructuralfeaturesandits\n",
      "We compute the node embeddings recursively,\n",
      "context. Ifyisanon-entity,thenA (y)istheword\n",
      "t\n",
      "analogoustobeliefpropagation:\n",
      "embedding of y concatenated with a zero vector\n",
      "Vk(v) = max tanh (2) of the same dimensionality as V (v). This way,\n",
      "t t\n",
      "v(cid:48)∈Nt(v)\n",
      "therepresentationofanentityonlydependsonits\n",
      "(cid:16) (cid:104) (cid:105)(cid:17)\n",
      "Wmp Vk−1(v(cid:48)),R(e ), structural properties given by the KB and the dia-\n",
      "t v→v(cid:48)\n",
      "logue context, which enables the model to gener-\n",
      "where Vk(v) is the depth-k node embedding at\n",
      "t alizetounseenentitiesattesttime.\n",
      "turntandN (v)denotesthesetofnodesadjacent\n",
      "t\n",
      "tov. Themessagefromaneighboringnodev(cid:48) de- Generation. Now,assumingwehaveembedded\n",
      "utterance x into h as described above,\n",
      "pendsonitsembeddingatdepth-(k−1),theedge t−1 t−1,nt−1\n",
      "we use another LSTM to generate utterance x.\n",
      "label e (embedded by a relation embedding t\n",
      "v→v(cid:48)\n",
      "function R), and a parameter matrix Wmp. Mes- Formally, we carryover the lastutterance embed-\n",
      "dingh = h anddefine:\n",
      "sages from all neighbors are aggregated by max, t,0 t−1,nt−1\n",
      "the element-wise max operation.9 Example mes- h = LSTM (h,[A (x ),c ]), (5)\n",
      "t,j dec t,j−1 t t,j t,j\n",
      "sagepassingpathsareshowninFigure3.\n",
      "wherec isaweightedsumofnodeembeddings\n",
      "The final node embedding is the concatenation t,j\n",
      "(cid:80)\n",
      "in the current turn: c = α V (v),\n",
      "ofembeddingsateachdepth: t,j v∈Gt t,j,v t\n",
      "where α are the attention weights over the\n",
      "t,j,v\n",
      "V t(v) = (cid:2) V t0(v),...,V tK(v)(cid:3), (3) nodes. Intuitively, high weight should be given to\n",
      "relevant entity nodes as shown in Figure 3. We\n",
      "whereK isahyperparameter(weexperimentwith\n",
      "K ∈ {0,1,2})andV0(v) = [F (v),M (v)]. compute the weights through standard attention\n",
      "t t t\n",
      "mechanism(Bahdanauetal.,2015):\n",
      "8 Relying on utterance t−1 is useful when utterance t\n",
      "answersaquestion,e.g.,“doyouhaveanygooglefriends?” α t,j = softmax(s t,j),\n",
      "“No.” s = wattn·tanh(cid:0) Wattn[h,V (v)](cid:1),\n",
      "9Usingsumormeanslightlyhurtsperformance. t,j,v t,j−1 t\n",
      "wherevectorwattn andWattn areparameters. what to talk about and which item to select. It\n",
      "Finally,wedefineadistributionoverbothwords has a pattern-matching semantic parser, a rule-\n",
      "inthevocabularyandnodesinG usingthecopy- based policy, and a templated generator. See Ap-\n",
      "t\n",
      "ingmechanismofJiaandLiang(2016): pendixGfordetails.\n",
      "p(x t,j+1 = y|G t,x t,≤j) ∝ exp(cid:0) Wvocabh t,j +b(cid:1), 4.2 Evaluation\n",
      "p(x t,j+1 = r(v)|G t,x t,≤j) ∝ exp(s t,j,v), We test our systems in two interactive settings:\n",
      "bot-botchatandbot-humanchat. Weperformboth\n",
      "where y is a word in the vocabulary, Wvocab and\n",
      "automaticevaluationandhumanevaluation.\n",
      "bareparameters,andr(v)istherealizationofthe\n",
      "entity represented by node v, e.g., google is real- Automatic Evaluation. First, we compute the\n",
      "izedto“Google”duringcopying.10\n",
      "cross-entropy ((cid:96)) of a model on test data. As\n",
      "shown in Table 4, DynoNet has the lowest test\n",
      "4 Experiments\n",
      "loss. Next, we have a model chat with itself on\n",
      "We compare our model with a rule-based sys- the scenarios from the test set.12 We evaluate the\n",
      "tem and a baseline neural model. Both automatic chatswithrespecttolanguagevariation,effective-\n",
      "and human evaluations are conducted to test the ness,andstrategy.\n",
      "models in terms of fluency, correctness, coopera- For language variation, we report the average\n",
      "tion, and human-likeness. The results show that utterance length L u and the unigram entropy H\n",
      "DynoNetisabletoconversewithhumansinaco- in Table 4. Compared to Rule, the neural mod-\n",
      "herentandstrategicway. els tend to generate shorter utterances (Li et al.,\n",
      "2016b; Serban et al., 2017b). However, they are\n",
      "4.1 Setup\n",
      "more diverse; for example, questions are asked\n",
      "Werandomlysplitthedataintotrain,dev,andtest in multiple ways such as “Do you have...”, “Any\n",
      "sets (8:1:1). We use a one-layer LSTM with 100 friendslike...”,“Whatabout...”.\n",
      "hidden units and 100-dimensional word vectors At the discourse level, we expect the distribu-\n",
      "forboththeencoderandthedecoder(Section3.3). tion of a bot’s utterance types to match the distri-\n",
      "Eachsuccessfuldialogueisturnedintotwoexam- bution of human’s. We show percentages of each\n",
      "ples, each from the perspective of one of the two utterance type in Table 4. For Rule, the decision\n",
      "agents. We maximize the log-likelihood of all ut- about which action to take is written in the rules,\n",
      "terancesinthedialogues. Theparametersareopti- whileStanoNetandDynoNetlearnedtobehavein\n",
      "mizedbyAdaGrad(Duchietal.,2010)withanini- amorehuman-likeway,frequentlyinformingand\n",
      "tiallearningrateof0.5. Wetrainedforatleast10 askingquestions.\n",
      "epochs; after that, training stops if there is no im- Tomeasureeffectiveness,wecomputetheover-\n",
      "provementonthedevsetfor5epochs. Bydefault, all success rate (C) and the success rate per turn\n",
      "we perform K = 2 iterations of message passing (C )andperselection(C ). AsshowninTable4,\n",
      "T S\n",
      "to compute node embeddings (Section 3.2). For humansarethebestatthisgame,followedbyRule\n",
      "decoding,wesequentiallysamplefromtheoutput whichiscomparabletoDynoNet.\n",
      "distribution with a softmax temperature of 0.5.11 Next, we investigate the strategies leading to\n",
      "Hyperparametersaretunedonthedevset. these results. An agent needs to decide which\n",
      "We compare DynoNet with its static cou- entity/attribute to check first to quickly reduce\n",
      "sion (StanoNet) and a rule-based system (Rule). the search space. We hypothesize that humans\n",
      "StanoNet uses G 0 throughout the dialogue, thus tend to first focus on a majority entity and an\n",
      "the dialogue history is completely contained in attribute with fewer unique values (Section 2.3).\n",
      "the LSTM states instead of being injected into For example, in the scenario in Table 6, time and\n",
      "the knowledge graph. Rule maintains weights for location are likely to be mentioned first. We\n",
      "each entity and each item in the KB to decide showtheaveragefrequencyoffirst-mentioneden-\n",
      "tities (#Ent ) and the average number of unique\n",
      "10Werealizeanentitybysamplingfromtheempiricaldis- 1\n",
      "tributionofitssurfaceformsfoundinthetrainingdata. valuesforfirst-mentionedattributes(|Attr 1|)inTa-\n",
      "11 Sinceselectionisacommon‘utterance’inourdataset\n",
      "and neural generation models are susceptible to over- 12 Welimitthenumberofturnsinbot-botchattobethe\n",
      "generatingcommonsentences, wehalveitsprobabilitydur- maximum number of turns humans took in the test set (46\n",
      "ingsampling. turns).\n",
      "System (cid:96)↓ L H C ↑ C ↑ C ↑ Sel Inf Ask Ans Greet #Ent |Attr | #Ent #Attr\n",
      "u T S 1 1\n",
      "Human - 5.10 4.57.82.07.38.21.31.17.08.08.55.35 6.1 2.6\n",
      "Rule - 7.61 3.37.90.05.29.18.34.23.00.12.24.61 9.9 3.0\n",
      "StanoNet 2.20 4.01 4.05.78.04.18.19.26.12.23.09.61.19 7.1 2.9\n",
      "DynoNet 2.13 3.37 3.90.96.06.25.22.26.13.20.12.55.18 5.2 2.5\n",
      "Table 4: Automatic evaluation on human-human and bot-bot chats on test scenarios. We use ↑ / ↓ to\n",
      "indicatethathigher/lowervaluesarebetter;otherwisetheobjectiveistomatchhumans’statistics. Best\n",
      "results(exceptHuman)areinbold. Neuralmodelsgenerateshorter(lowerL )butmorediverse(higher\n",
      "u\n",
      "H)utterances. Overall,theirdistributionsofutterancetypesmatchthoseofthehumans’. (Weonlyshow\n",
      "the most frequent speech acts therefore the numbers do not sum to 1.) Rule is effective in completing\n",
      "the task (higher C ), but it is not information-efficient given the large number of attributes (#Attr) and\n",
      "S\n",
      "entities(#Ent)mentioned.\n",
      "ble4.13 BothDynoNetandStanoNetsuccessfully Noticeably, DynoNet is more cooperative than\n",
      "match human’s starting strategy by favoring enti- the other models. As shown in the example dia-\n",
      "ties of higher frequency and attributes of smaller logues in Table 6, DynoNet cooperates smoothly\n",
      "domainsize. with the human partner, e.g., replying with rel-\n",
      "To examine the overall strategy, we show the evant information about morning/indoor friends\n",
      "average number of attributes (#Attr) and entities when the partner mentioned that all her friends\n",
      "(#Ent) mentioned during the conversation in Ta- prefer morning and most like indoor. StanoNet\n",
      "ble4. HumansandDynoNetstrategicallyfocuson starts well but doesn’t follow up on the morn-\n",
      "a few attributes and entities, whereas Rule needs ingfriend, presumablybecausethemorningnode\n",
      "almost twice entities to achieve similar success is not updated dynamically when mentioned by\n",
      "rates. This suggests that the effectiveness of Rule the partner. Rule follows the partner poorly. In\n",
      "mainly comes from large amounts of unselective the comments, the biggest complaint about Rule\n",
      "information, which is consistent with comments was that it was not ‘listening’ or ‘understanding’.\n",
      "fromtheirhumanpartners. Overall,DynoNetachievesbetterpartnersatisfac-\n",
      "tion,especiallyincooperation.\n",
      "Partner Evaluation. We generated 200 new\n",
      "scenarios and put up the bots on AMT using the Third-party Evaluation. We also created a\n",
      "same chat interface that was used for data col- third-partyevaluationtask,whereanindependent\n",
      "lection. The bots follow simple turn-taking rules AMTworkerisshownaconversationandtheKB\n",
      "explained in Appendix H. Each AMT worker is ofoneoftheagents; sheisaskedtoratethesame\n",
      "randomly paired with Rule, StanoNet, DynoNet, aspects of the agent as in the partner evaluation\n",
      "or another human (but the worker doesn’t know and provide justifications. Each agent in a dia-\n",
      "which), and we make sure that all four types of logueisratedbyatleast5people.\n",
      "agentsaretestedineachscenarioatleastonce. At The average ratings and histograms are shown\n",
      "theendofeachdialogue,humansareaskedtor<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  45443,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['MutualFriends']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: ,especiallyincooperation.\n",
      "Partner Evaluation. We generated 200 new\n",
      "scenarios and put up the bots on AMT using the Third-party Evaluation. We also created a\n",
      "same chat interface that was used for data col- third-partyevaluationtask,whereanindependent\n",
      "lection. The bots follow simple turn-taking rules AMTworkerisshownaconversationandtheKB\n",
      "explained in Appendix H. Each AMT worker is ofoneoftheagents; sheisaskedtoratethesame\n",
      "randomly paired with Rule, StanoNet, DynoNet, aspects of the agent as in the partner evaluation\n",
      "or another human (but the worker doesn’t know and provide justifications. Each agent in a dia-\n",
      "which), and we make sure that all four types of logueisratedbyatleast5people.\n",
      "agentsaretestedineachscenarioatleastonce. At The average ratings and histograms are shown\n",
      "theendofeachdialogue,humansareaskedtorate inTable5andAppendixJ.Forcorrectness,wesee\n",
      "their partner in terms of fluency, correctness, co- thatRulehasthebestperformancesinceitalways\n",
      "operation, and human-likeness from 1 (very bad) tellsthetruth,whereashumanscanmakemistakes\n",
      "to5(verygood),alongwithoptionalcomments. duetocarelessnessandtheneuralmodelscangen-\n",
      "We show the average ratings (with significance erate false information. For example, in Table 6,\n",
      "tests)inTable5andthehistogramsinAppendixJ. DynoNet‘lied’whensayingthatithasamorning\n",
      "In terms of fluency, the models have similar per- friendwholikesoutdoor.\n",
      "formance since the utterances are usually short. Surprisingly,thereisadiscrepancybetweenthe\n",
      "Judgmentoncorrectnessisamereguesssincethe twoevaluationmodesintermsofcooperationand\n",
      "evaluatorcannotseethepartner’sKB;wewillan- human-likeness. Manual analysis of the com-\n",
      "alyze correctness more meaningfully in the third- ments indicates that third-party evaluators focus\n",
      "partyevaluationbelow. lessonthedialoguestrategyandmoreonlinguis-\n",
      "tic features, probably because they were not fully\n",
      "13Bothnumbersarenormalizedto[0,1]withrespecttoall\n",
      "entities/attributesinthecorrespondingKB. engagedinthedialogue. Forexample,justification\n",
      "Partnereval Third-partyeval\n",
      "System C C C\n",
      "T S Flnt Crct Coop Human Flnt Crct Coop Human\n",
      "Human.89.07.36 4.2rds 4.3rds 4.2rds 4.1rds 4.0 4.3ds 4.0ds 4.1rds\n",
      "Rule.88.06.29 3.6 4.0 3.5 3.5 4.0 4.4hds 3.9s 4.0s\n",
      "StanoNet.76.04.23 3.5 3.8 3.4 3.3 4.0 4.0 3.8 3.8\n",
      "DynoNet.87.05.27 3.8s 4.0 3.8rs 3.6s 4.0 4.1 3.9 3.9\n",
      "Table 5: Results on human-bot/human chats. Best results (except Human) in each column are in bold.\n",
      "We report the average ratings of each system. For third-party evaluation, we first take mean of each\n",
      "question then average the ratings. DynoNet has the best partner satisfaction in terms of fluency (Flnt),\n",
      "correctness(Crct),cooperation(Coop),humanlikeness(Human). Thesuperscriptofaresultindicatesthat\n",
      "its advantage over other systems (r: Rule, s: StanoNet, d: DynoNet) is statistically significant with\n",
      "p < 0.05givenbypairedt-tests.\n",
      "forcooperationoftenmentionsfrequentquestions dialogues are easy to collect as natural human\n",
      "and timely answers, less attention is paid to what conversations, and are also challenging enough\n",
      "isaskedaboutthough. given the large number of scenarios and diverse\n",
      "For human-likeness, partner evaluation is conversation phenomena. There are some in-\n",
      "largelycorrelatedwithcoherence(e.g.,notrepeat- teresting strategic dialogue datasets—settlers of\n",
      "ing or ignoring past information) and task suc- Catan (Afantenos et al., 2012) (2K turns) and the\n",
      "cess, whereas third-party evaluators often rely on cards corpus (Potts, 2012) (1.3K dialogues), as\n",
      "informality (e.g., usage of colloquia like “hiya”, well as work on dialogue strategies (Keizer et al.,\n",
      "capitalization, and abbreviation) or intuition. In- 2017; Vogel et al., 2013), though no full dialogue\n",
      "terestingly,third-partyevaluatorsnotedmostphe- systemhasbeenbuiltforthesedatasets.\n",
      "nomena listed in Table 2 as indicators of human-\n",
      "Mosttask-orienteddialoguesystemsfollowthe\n",
      "beings, e.g., correcting oneself, making chit-chat\n",
      "POMDP-based approach (Williams and Young,\n",
      "other than simply finishing the task. See example\n",
      "2007; Young et al., 2013). Despite their suc-\n",
      "commentsinAppendixK.\n",
      "cess (Wen et al., 2017; Dhingra et al., 2017; Su\n",
      "etal.,2016),therequirementforhandcraftedslots\n",
      "4.3 AblationStudies\n",
      "limits their scalability to new domains and bur-\n",
      "Our model has two novel designs: entity abstrac- dens data collection with extra state labeling. To\n",
      "tion and message passing for node embeddings. go past this limit, Bordes and Weston (2017) pro-\n",
      "Table 7 shows what happens if we ablate these. posed a Memory-Networks-based approach with-\n",
      "When the number of message passing iterations, out domain-specific features. However, the mem-\n",
      "K, is reduced from 2 to 0, the loss consistently oryisunstructuredandinterfacingwithKBsrelies\n",
      "increases. Removingentityabstraction—meaning onAPIcalls,whereasourmodelembedsboththe\n",
      "addingentityembeddingstonodeembeddingsand dialoguehistoryandtheKBstructurally. Williams\n",
      "the LSTM input embeddings—also degrades per- et al. (2017) use an LSTM to automatically infer\n",
      "formance. ThisshowsthatDynoNetbenefitsfrom the dialogue state, but as they focus on dialogue\n",
      "contextually-defined, structural node embeddings controlratherthanthefullproblem,theresponseis\n",
      "ratherthanonesbasedonaclassiclookuptable. modeledasatemplatedaction,whichrestrictsthe\n",
      "generation of richer utterances. Our network ar-\n",
      "5 DiscussionandRelatedWork\n",
      "chitectureismostsimilartoEntNet(Henaffetal.,\n",
      "2017), where memories are also updated by input\n",
      "There has been a recent surge of interest in\n",
      "sentences recurrently. The main difference is that\n",
      "end-to-endtask-orienteddialoguesystems,though\n",
      "ourmodelallowsinformationtobepropagatedbe-\n",
      "progress has been limited by the size of available\n",
      "tween structured entities, which is shown to be\n",
      "datasets(Serbanetal.,2015a). Mostworkfocuses\n",
      "crucialinoursetting(Section4.3).\n",
      "on information-querying tasks, using Wizard-of-\n",
      "Oz data collection (Williams et al., 2016; Asri Ourworkisalsorelatedtolanguagegeneration\n",
      "et al., 2016) or simulators (Bordes and Weston, conditionedonknowledgebases(Meietal.,2016;\n",
      "2017; Li et al., 2016d), In contrast, collaborative Kiddon et al., 2016). One challenge here is to\n",
      "FriendsofA FriendsofB\n",
      "ID Name Company Time Location ID Name Company Time Location\n",
      "1 Kathy TRTHoldings afternoon indoor 1 Justin NewEraTickets morning indoor\n",
      "2 Jason DollarGeneral afternoon indoor 2 Kathleen TRTHoldings morning indoor\n",
      "3 Johnny TRTHoldings afternoon outdoor 3 Gloria L&LHawaiianBarbecue morning indoor\n",
      "4 Frank SFNGroup afternoon indoor 4 Kathleen AdvanceAutoParts morning outdoor\n",
      "5 Catherine DollarGeneral afternoon indoor 5 Justin ArcticCat morning indoor\n",
      "6 Catherine WeisMarkets afternoon indoor 6 Anna DollarGeneral morning indoor\n",
      "7 Kathleen TRTHoldings morning indoor 7 Steven SFNGroup morning indoor\n",
      "8 Lori TRTHoldings afternoon indoor 8 Wayne R.J.CormanRailroadGroup morning indoor\n",
      "9 Frank L&LHawaiianBarbecue afternoon outdoor 9 Alexander R.J.CormanRailroadGroup morning indoor\n",
      "A:Human B:Human A:DynoNet B:Human\n",
      "A:hi\n",
      "A:Hi\n",
      "B:Hello||allmyfriendsprefermorning\n",
      "B:hey||ihaveoneoutdoor\n",
      "A:1ofmymorninglikestheoutdoors\n",
      "A:Ihave4TRTHoldings||Ihave2outdoorone\n",
      "B:andalllikeindoorexceptforone\n",
      "Johnny,otherFrank\n",
      "A:dotheyworkfortrtholdings?\n",
      "B:ionlyhaveoneTRTHoldings-Kathleen\n",
      "B:Kathleen?\n",
      "A:SELECT7 B:SELECT2\n",
      "A:SELECT7 B:SELECT2\n",
      "A:StanoNet B:Human A:Human B:Rule\n",
      "A:Hello B:hiya\n",
      "B:hi A:hEY\n",
      "A:Hello||Ihaveonemorningperson. B:Ihave1indoorsandkathleen.\n",
      "B:allofmyfriendslikemornings A:Mostofmineareindoors.\n",
      "A:Myfriendprefersafternoonworksattrtholdings. B:SELECT1\n",
      "B:whatistheirname? A:Ihaveonemorningandrestafternoon.\n",
      "A:Likesindoors. B:Doyouhaveanyfriendworkingatlhawaiian?\n",
      "B:whatisyourfiendwholikesmorningname? A:Idon’tknowJustin\n",
      "A:Theyworkfortrtholdings. B:Ihave1alexander.\n",
      "B:SELECT2 A:SELECT7...\n",
      "Table 6: Examples of human-bot chats. The mutual friend is highlighted in blue in each KB. Bots’\n",
      "utterances are in bold and selected items are represented by item IDs. Only the first half of the human-\n",
      "Rulechatisshownduetolimitedspace. Multipleutterancesofoneagentraeseparatedby||.\n",
      "Model (cid:96) vide unique opportunities at the interface of tra-\n",
      "ditional task-oriented dialogue and open-domain\n",
      "DynoNet(K=2) 2.16\n",
      "DynoNet(K=1) 2.20 chat. We also offered DynoNet as a promising\n",
      "DynoNet(K=0) 2.26\n",
      "means for open-ended dialogue state representa-\n",
      "DynoNet(K=2)w/oentityabstraction 2.21\n",
      "tion. Our dataset facilitates the study of prag-\n",
      "Table 7: Ablations of our model on the dev matics and human strategies in dialogue—a good\n",
      "set show the importance of entity abstraction and steppingstonetowardslearningmorecomplexdi-\n",
      "messagepassing(K = 2). aloguessuchasnegotiation.\n",
      "Acknowledgments. This work is supported by\n",
      "avoidgeneratingfalseorcontradictingstatements, DARPA Communicating with Computers (CwC)\n",
      "which is currently a weakness of neural models. programunderAROprimecontractno. W911NF-\n",
      "Our model is mostly accurate when generating 15-1-0462. Mike Kayser worked on an early ver-\n",
      "facts and answering existence questions about a sion of the project while he was at Stanford. We\n",
      "single entity, but will need a more advanced at- alsothankmembersoftheStanfordNLPgroupfor\n",
      "tention mechanism for generating utterances in- insightfuldiscussions.\n",
      "volving multiple entities, e.g., attending to items\n",
      "or attributes first, then selecting entities; generat- Reproducibility. All code, data, and\n",
      "inghigh-levelconceptsbeforecomposingthemto experiments for this paper are avail-\n",
      "naturaltokens(Serbanetal.,2017a). able on the CodaLab platform: https:\n",
      "In conclusion, we believe the symmetric col- //worksheets.codalab.org/worksheets/\n",
      "laborative dialogue setting and our dataset pro- 0xc757f29f5c794e5eb7bfa8ca9c945573.\n",
      "References J.Li,M.Galley,C.Brockett,J.Gao,andW.B.Dolan.\n",
      "2016b. A diversity-promoting objective function\n",
      "S. Afantenos, N. Asher, F. Benamara, A. Cadilhac, for neural conversation models. In Human Lan-\n",
      "C.De´gremont,P.Denis,M.Guhe,S.Keizer,A.Las- guageTechnologyandNorthAmericanAssociation\n",
      "carides,O.Lemon,P.Muller,S.Paul,V.Rieser,and forComputationalLinguistics(HLT/NAACL).\n",
      "L.Vieu.2012. Developingacorpusofstrategiccon-\n",
      "versationinthesettlersofcatan. InSeineDial2012- J.Li,W.Monroe,A.Ritter,D.Jurafsky,M.Galley,and\n",
      "The16thWorkshopontheSemanticsandPragmat- J.Gao.2016c. Deepreinforcementlearningfordia-\n",
      "icsofDialogue. loguegeneration. InEmpiricalMethodsinNatural\n",
      "LanguageProcessing(EMNLP).\n",
      "L. E. Asri, H. Schulz, S. Sharma, J. Zumer, J. Har-\n",
      "ris, E. Fine, R. Mehrotra, and K. Suleman. 2016. X. Li, Z. C. Lipton, B. Dhingra, L. Li, J. Gao,\n",
      "Frames: A corpus for adding memory to goal- and Y. Chen. 2016d. A user simulator for task-\n",
      "oriented dialogue systems. Maluuba Technical Re- completiondialogues. arXiv.\n",
      "port.\n",
      "C.Liu,R.Lowe,I.V.Serban,M.Noseworthy,L.Char-\n",
      "D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural lin,andJ.Pineau.2016. HowNOTtoevaluateyour\n",
      "machinetranslationbyjointlylearningtoalignand dialogue system: An empirical study of unsuper-\n",
      "translate. In International Conference on Learning visedevaluationmetricsfordialogueresponsegen-\n",
      "Representations(ICLR). eration. InEmpiricalMethodsinNaturalLanguage\n",
      "Processing(EMNLP).\n",
      "A. Bordes and J. Weston. 2017. Learning end-to-end\n",
      "goal-oriented dialog. In International Conference R.T.Lowe,N.Pow,I.Serban,L.Charlin,C.Liu,and\n",
      "onLearningRepresentations(ICLR). J.Pineau.2017. TrainingEnd-to-Enddialoguesys-\n",
      "temswiththeubuntudialoguecorpus. Dialogueand\n",
      "B. Dhingra, L. Li, X. Li, J. Gao, Y. Chen, F. Ahmed, Discourse8.\n",
      "andL.Deng.2017. End-to-endreinforcementlearn-\n",
      "ingofdialogueagentsforinformationaccess. InAs- H. Mei, M. Bansal, and M. R. Walter. 2016. What\n",
      "sociationforComputationalLinguistics(ACL). to talk about and how? selective generation using\n",
      "LSTMs with coarse-to-fine alignment. In Human\n",
      "LanguageTechnologyandNorthAmericanAssocia-\n",
      "J.Duchi,E.Hazan,andY.Singer.2010. Adaptivesub-\n",
      "tionforComputationalLinguistics(HLT/NAACL).\n",
      "gradient methods for online learning and stochastic\n",
      "optimization. In Conference on Learning Theory\n",
      "H.Mei,M.Bansal,andM.R.Walter.2017. Coherent\n",
      "(COLT).\n",
      "dialogue with attention-based language models. In\n",
      "Association for the Advancement of Artificial Intel-\n",
      "M.Henaff,J.Weston,A.Szlam,A.Bordes,andY.Le-\n",
      "ligence(AAAI).\n",
      "Cun. 2017. Tracking the world state with recur-\n",
      "rent entity networks. In International Conference\n",
      "C.Potts.2012. Goal-drivenanswersintheCardsdia-\n",
      "onLearningRepresentations(ICLR).\n",
      "loguecorpus. InProceedingsofthe30thWestCoast\n",
      "ConferenceonFormalLinguistics.\n",
      "E. Ivanovic. 2005. Dialogue act tagging for instant\n",
      "messaging chat sessions. In Association for Com-\n",
      "I. Serban, T. Klinger, G. Tesauro, K. Talamadupula,\n",
      "putationalLinguistics(ACL).\n",
      "B. Zhou, Y. Bengio, and A. C. Courville. 2017a.\n",
      "Multiresolution recurrent neural networks: An ap-\n",
      "R.JiaandP.Liang.2016. Datarecombinationforneu-\n",
      "plication to dialogue response generation. In Asso-\n",
      "ral semantic parsing. In Association for Computa-\n",
      "ciationfortheAdvancementofArtificialIntelligence\n",
      "tionalLinguistics(ACL).\n",
      "(AAAI).\n",
      "S. Keizer, M. Guhe, H. Cuayahuitl, I. Efstathiou, I. Serban, A. Sordoni, R. Lowe, L. Charlin, J. Pineau,\n",
      "K. Engelbrecht, M. Dobre, A. Lascarides, and A.C.Courville,andY.Bengio.2017b. Ahierarchi-\n",
      "O. Lemon. 2017. Evaluating persuasion strategies callatentvariableencoder-decodermodelforgener-\n",
      "anddeepreinforcementlearningmethodsfornego- atingdialogues. InAssociationfortheAdvancement\n",
      "tiationdialogueagents. InEuropeanAssociationfor ofArtificialIntelligence(AAAI).\n",
      "ComputationalLinguistics(EACL).\n",
      "I. V. Serban, R. Lowe, L. Charlin, and J. Pineau.\n",
      "C.Kiddon,L.S.Zettlemoyer,andY.Choi.2016. Glob- 2015a. A survey of available corpora for build-\n",
      "ally coherent text generation with neural checklist ing data-driven dialogue systems. arXiv preprint\n",
      "models. InEmpiricalMethodsinNaturalLanguage arXiv:1512.05742.\n",
      "Processing(EMNLP).\n",
      "I.V.Serban,A.Sordoni,Y.Bengio,A.Courville,and\n",
      "J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan. J.Pineau.2015b. Buildingend-to-enddialoguesys-\n",
      "2016a. Apersona-basedneuralconversationmodel. tems using generative hierarchical neural network\n",
      "InAssociationforComputationalLinguistics(ACL). models. arXivpreprintarXiv:1507.04808.\n",
      "L. Shang, Z. Lu, and H. Li. 2015. Neural responding\n",
      "machineforshort-textconversation. InAssociation\n",
      "forComputationalLinguistics(ACL).\n",
      "A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji,\n",
      "M. Mitchell, J. Nie, J. Gao, and B. Dolan. 2015.\n",
      "A neural network approach to context-sensitive\n",
      "generation of conversational responses. In North\n",
      "American Association for Computational Linguis-\n",
      "tics(NAACL).\n",
      "P. Su, M. Gasic, N. Mrksic, L. M. Rojas-Barahona,\n",
      "S.Ultes,D.Vandyke,T.Wen,andS.J.Young.2016.\n",
      "Continuouslylearningneuraldialoguemanagement.\n",
      "arXivpreprintarXiv:1606.02689.\n",
      "A.Vogel, M.Bodoia, C.Potts, andD.Jurafsky.2013.\n",
      "Emergenceofgriceanmaximsfrommulti-agentde-\n",
      "cision theory. In North American Association for\n",
      "Computational Linguistics (NAACL). pages 1072–\n",
      "1081.\n",
      "T. Wen, M. Gasic, N. Mrksic, L. M. Rojas-Barahona,\n",
      "P. Su, S. Ultes, D. Vandyke, and S. Young. 2017.\n",
      "Anetwork-basedend-to-endtrainabletask-oriented\n",
      "dialoguesystem. InEuropeanAssociationforCom-\n",
      "putationalLinguistics(EACL).\n",
      "J. D. Williams, K. Asadi, and G. Zweig. 2017. Hy-\n",
      "brid code networks: Practical and efficient end-to-\n",
      "end dialog control with supervised and reinforce-\n",
      "ment learning. In Association for Computational\n",
      "Linguistics(ACL).\n",
      "J.D.Williams,A.Raux,andM.Henderson.2016. The\n",
      "dialogstatetrackingchallengeseries: Areview. Di-\n",
      "alogueandDiscourse7.\n",
      "J. D. Williams and S. Young. 2007. Partially observ-\n",
      "able Markov decision processes for spoken dialog\n",
      "systems. ComputerSpeech&Language21(2):393–\n",
      "422.\n",
      "S.Young, M.Gasic, B.Thomson, andJ.D.Williams.\n",
      "2013. POMDP-based statistical spoken dialog\n",
      "systems: A review. Proceedings of the IEEE\n",
      "101(5):1160–1179.\n",
      "A KnowledgeBaseSchema each dialogue participant. We instruct people to\n",
      "play intelligently, to refrain from brute-force tac-\n",
      "TheattributesetAfortheMutualFriendstaskcon-\n",
      "tics (e.g., mentioning every attribute value), and\n",
      "tainsname, school, major, company, hobby, time-\n",
      "to use grammatical sentences. To discourage ran-\n",
      "of-day preference, and location preference. Each\n",
      "dom guessing, we prevent users from selecting a\n",
      "attributeahasasetofpossiblevalues(entities)E.\n",
      "a friend (item) more than once every 10 seconds.\n",
      "Forname,school,major,company,andhobby,we\n",
      "Each worker was paid $0.35 for a successful di-\n",
      "collectedalargesetofvaluesfromvariousonline\n",
      "alogue within a 5-minute time limit. We log each\n",
      "sources.14 We used three possible values (morn-\n",
      "utterance in the dialogue along with timing infor-\n",
      "ing, afternoon, and evening) for the time-of-day\n",
      "mation.\n",
      "preference, and two possible values (indoors and\n",
      "outdoors)forthelocationpreference.\n",
      "D EntityLinkingandRealization\n",
      "B ScenarioGeneration\n",
      "We use a rule-based lexicon to link text spans to\n",
      "entities. For every entity in the schema, we com-\n",
      "Wegeneratescenariosrandomlytovarytaskcom-\n",
      "putedifferentvariationsofitscanonicalname,in-\n",
      "plexity and elicit linguistic and strategic variants.\n",
      "A scenario S is characterized by the number of cluding acronyms, strings with a certain edit dis-\n",
      "items (N ), the attribute set (A ) whose size is tance,prefixes,andmorphologicalvariants. Given\n",
      "S S\n",
      "M, and the values for each attribute a ∈ A in a text span, a set of candidate entities is returned\n",
      "S S\n",
      "bystringmatching. Aheuristicrankerthenscores\n",
      "thetwoKBs.\n",
      "eachcandidate(e.g.,consideringwhetherthespan\n",
      "Ascenarioisgeneratedasfollows.\n",
      "is a substring of a candidate, the edit distance be-\n",
      "1. Sample N S and M S uniformly from tweenthespanandacandidateetc.). Thehighest-\n",
      "{5,...,12}and{3,4}respectively. scoringcandidateisreturned.\n",
      "A linked entity is considered as a single token\n",
      "2. Generate A by sampling M attributes\n",
      "S S and its surface form is ignored in all models. At\n",
      "withoutreplacementfromA.\n",
      "generation time, we realize an entity by sampling\n",
      "3. For each attribute a ∈ A, sample the con- fromtheempiricaldistributionofitssurfaceforms\n",
      "S\n",
      "centration parameter α uniformly from the inthetrainingset.\n",
      "a\n",
      "set{0.3,1,3}.\n",
      "E UtteranceCategorization\n",
      "4. GeneratetwoKBsbysamplingN valuesfor\n",
      "S\n",
      "eachattributeafromaDirichlet-multinomial\n",
      "We categorize utterances into inform, ask, answer,\n",
      "distribution over the value set E with the\n",
      "greeting,apologyheuristicallybypatternmatching.\n",
      "a\n",
      "concentrationparameterα.\n",
      "a\n",
      "• Anaskutteranceasksforinformationregard-\n",
      "WerepeatthelaststepuntilthetwoKBshaveone ing the partner’s KB. We detect these utter-\n",
      "uniquecommonitem. ances by checking for the presence of a ‘?’\n",
      "and/or a question word like “do”, “does”,\n",
      "C ChatInterface\n",
      "“what”,etc.\n",
      "Inordertocollectreal-timedialoguebetweenhu-\n",
      "• An inform utterance provides information\n",
      "mans, we set up a web server and redirect AMT\n",
      "about the agent’s KB. We define it as an ut-\n",
      "workers to our website. Visitors are randomly\n",
      "terancesthatmentionsentitiesintheKBand\n",
      "pairedupastheyarrive. Foreachpair,wechoose\n",
      "isnotanaskutterance.\n",
      "a random scenario, and randomly assign a KB to\n",
      "14Names: https://www.ssa.gov/oact/ • An answer utterance simply provides a posi-\n",
      "babynames/decades/century.html tive/negativeresponsetoaquestion,contain-\n",
      "Schools: http://doors.stanford.edu/˜sr/\n",
      "ingwordslike“yes”,“no”,“nope”,etc.\n",
      "universities.html\n",
      "Majors:http://www.a2zcolleges.com/majors\n",
      "Companies: https://en.wikipedia.org/wiki/ • A greeting utterance contains words like “hi”\n",
      "List_of_companies_of_the_United_States\n",
      "or“hello”;itoftenoccursatthebeginningof\n",
      "Hobbies: https://en.wikipedia.org/wiki/\n",
      "List_of_hobbies adialogue.\n",
      "• An apology utterance contains the word G Rule-basedSystem\n",
      "“sorry”, which is typically associated with\n",
      "The rule-based bot takes the following actions:\n",
      "correctionsandwrongselections.\n",
      "greeting, informing or asking about a set of en-\n",
      "tities,answeringaquestion,andselectinganitem.\n",
      "See Table 2 and Table 1 for examples of these ut-\n",
      "The set of entities to inform/ask is sampled ran-\n",
      "terancetypes.\n",
      "domlygiventheentityweights. Initially,eachen-\n",
      "tity is weighted by its count in the KB. We then\n",
      "F Strategy\n",
      "increment or decrement weights of entities men-\n",
      "tioned by the partner and its related entities (in\n",
      "During scenario generation, we varied the num-\n",
      "the same row or column), depending on whether\n",
      "berofattributes,thenumberofitemsineachKB,\n",
      "the mention is positive or negative. A negative\n",
      "and the distribution of values for each attribute.\n",
      "mention contains words like “no”, “none”, “n’t”\n",
      "We find that as the number of items and/or at-\n",
      "etc. Similarly,eachitemhasaninitialweightof1,\n",
      "tributes grows, the dialogue length and the com-\n",
      "whichisupdateddependingonthepartner’smen-\n",
      "pletion time also increase, indicating that the task\n",
      "tionofitsattributes.\n",
      "becomes harder. We also anticipated that vary-\n",
      "If there exists an item with weight larger than\n",
      "ing the value of α would impact the overall strat-\n",
      "1, the bot selects the highest-weighted item with\n",
      "egy(forexample,theorderinwhichattributesare\n",
      "probability 0.3. If a question is received, the\n",
      "mentioned) since α controls the skewness of the\n",
      "bot informs facts of the entities being asked, e.g.,\n",
      "distributionofvaluesforanattribute.\n",
      "“anyone went to columbia?”, “I have 2 friends\n",
      "On examining the data, we find that humans\n",
      "who went to columbia”. Otherwise, the bot sam-\n",
      "tendtofirstmentionattributeswithamoreskewed\n",
      "ples an entity set and randomly chooses between\n",
      "(i.e., less uniform) distribution of values. Specif-\n",
      "informingandaskingabouttheentities.\n",
      "ically, we rank the α values of all attributes in a\n",
      "All utterances are generated by sentence tem-\n",
      "scenario (see step 3 in Section B), and bin them\n",
      "plates, and parsing of the partner’s utterance is\n",
      "into 3 distribution groups—least uniform, medium,\n",
      "donebyentitylinkingandpatternmatching(Sec-\n",
      "and most uniform, according to the ranking where\n",
      "tionE).\n",
      "higherαvaluescorrespondstomoreuniformdis-\n",
      "tributions.15 In Figure 4, we plot the histogram\n",
      "H Turn-takingRules\n",
      "ofthedistributiongroupofthefirst-mentionedat-\n",
      "tribute in a dialogues, which shows that skewed Turn-taking is universal in human conversations\n",
      "attributesarementionedmuchmorefrequently. andthebotneedstodecidewhento‘talk’(sendan\n",
      "utterance). Topreventthebotfromgeneratingut-\n",
      "terances continuously and forming a monologue,\n",
      "we allow it to send at most one utterance if the\n",
      "utterance contains any entity, and two utterances\n",
      "otherwise. Whensendingmorethanoneutterance\n",
      "inaturn,thebotmustwaitfor1to2secondsinbe-\n",
      "tween. Inaddition, afteranutteranceisgenerated\n",
      "bythemodel(almostinstantly),thebotmusthold\n",
      "on for some time to simulate message typing be-\n",
      "fore sending. We used a typing speed of 7 chars /\n",
      "secandaddedanadditionalrandomdelaybetween\n",
      "Figure 4: Histogram of the first attribute men- 0to1.5safter‘typing’. Therulesareappliedtoall\n",
      "tioned in a dialogue. People tend to first mention models.\n",
      "attributes from very skewed (non-uniform) distri-\n",
      "I AdditionalHuman-BotDialogue\n",
      "butions.\n",
      "We show another set of human-bot/human chats\n",
      "in Table 8. In this scenario, the distribution of\n",
      "15Forscenarioswith3attributes,eachgroupcontainsone\n",
      "values are more uniform compared to Table 6.\n",
      "attributes. For scenarios with 4 attributes, we put the two\n",
      "attributeswithrankingsinthemiddletomedium. Nevertheless, we see that StanoNet and DynoNet\n",
      "stilllearnedtostartfromrelativelyhigh-frequency\n",
      "entities. They also appear more cooperative and\n",
      "mentions relevant entities in the dialogue context\n",
      "comparedtoRule.\n",
      "J HistogramsofRatingsfromHuman\n",
      "Evaluations\n",
      "The histograms of ratings from partner and third-\n",
      "party evaluations is shown in Figure 5 and Fig-\n",
      "ure 6 respectively. As these figures show, there\n",
      "are some obvious discrepancies between the rat-\n",
      "ingsmadebyagentswhochattedwiththebotand\n",
      "those made by an ‘objective’ third party. These\n",
      "ratingsprovidesomeinterestinginsightsintohow\n",
      "dialogue participants in this task setting perceive\n",
      "theirpartners,andwhatconstitutesa‘human-like’\n",
      "ora‘fluent’partner.\n",
      "K ExampleCommentsfromPartnerand\n",
      "Third-partyEvaluations\n",
      "In Table 9, we show several pairs of ratings and\n",
      "comments on human-likeness for the same dia-\n",
      "logue from both the partner evaluation and the\n",
      "third-party evaluation. As a conversation partic-\n",
      "ipant, the dialogue partner often judges from the\n",
      "cooperation and strategy perspective, whereas the\n",
      "third-partyevaluatorreliesmoreonlinguisticfea-\n",
      "tures(e.g.,length,spelling,formality).\n",
      "60\n",
      "50\n",
      "40\n",
      "30\n",
      "20\n",
      "10\n",
      "0\n",
      "1 2 3 4 5\n",
      "egatnecreP\n",
      "Fluency Correctness\n",
      "Human\n",
      "Rule\n",
      "StanoNet\n",
      "DynoNet\n",
      "1 2 3 4 5\n",
      "60\n",
      "50\n",
      "40\n",
      "30\n",
      "20\n",
      "10\n",
      "0\n",
      "1 2 3 4 5\n",
      "egatnecreP\n",
      "Cooperation Human-likeness\n",
      "1 2 3 4 5\n",
      "Figure5: Histogramofratings(higherisbetter)fromdialoguepartners. DynoNetisbetterthanallother\n",
      "systems,especiallyincooperation.\n",
      "FriendsofB\n",
      "FriendsofA ID Major Company Hobby\n",
      "ID Major Company Hobby ForeignLanguage\n",
      "1 GannettCompany Roadbiking\n",
      "TeacherEducation\n",
      "Metallurgical\n",
      "1 GannettCompany Candlemaking Mathematics\n",
      "Engineering 2 ElectronicArts Astronomy\n",
      "Education\n",
      "2 BusinessEducation ElectronicArts Gunsmithing\n",
      "Petroleum WesternSugar\n",
      "Parks 3 Candlemaking\n",
      "3 Kenworth Watersports Engineering Cooperative\n",
      "Administration\n",
      "American\n",
      "Mathematics Mathematics\n",
      "4 ElectronicArts Astronomy 4 Broadcasting Roadbiking\n",
      "Education Education\n",
      "Company\n",
      "Agricultural\n",
      "5 AVST Fieldhockey Petroleum WesternSugar\n",
      "Mechanization 5 Roadbiking\n",
      "Engineering Cooperative\n",
      "Mathematics\n",
      "6 AVST Shopping Petroleum\n",
      "Education 6 A&WRestaurants Golfing\n",
      "Engineering\n",
      "Parks Foreignlanguage\n",
      "7 AdobeSystems American\n",
      "Administration learning Petroleum\n",
      "7 Broadcasting Origami\n",
      "Agricultural BroncoWine Engineering\n",
      "8 Shopping Company\n",
      "Mechanization Company\n",
      "TheWaltDisney\n",
      "Metallurgical Foreignlanguage 8 Russian Astronomy\n",
      "9 ElectronicArts Company\n",
      "Engineering learning\n",
      "Petroleum TheWaltDisney\n",
      "Mathematics 9 Origami\n",
      "10 ElectronicArts Poi Engineering Company\n",
      "Education\n",
      "Protestant\n",
      "10 AcmeBrick Astronomy\n",
      "Affiliation\n",
      "A:Human B:Human A:Human B:DynoNet\n",
      "A:hi B:hi\n",
      "B:hi A:hey\n",
      "A:AnyfriendsworkatAVST? B:ihavethreefriendswholikeroadbiking\n",
      "B:petroleumengi??||no A:ihavet<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  18299,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['MutualFriends', 'CardsDialogueCorpus', 'UbuntuDialogueCorpus']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  WesternSugar\n",
      "Mechanization 5 Roadbiking\n",
      "Engineering Cooperative\n",
      "Mathematics\n",
      "6 AVST Shopping Petroleum\n",
      "Education 6 A&WRestaurants Golfing\n",
      "Engineering\n",
      "Parks Foreignlanguage\n",
      "7 AdobeSystems American\n",
      "Administration learning Petroleum\n",
      "7 Broadcasting Origami\n",
      "Agricultural BroncoWine Engineering\n",
      "8 Shopping Company\n",
      "Mechanization Company\n",
      "TheWaltDisney\n",
      "Metallurgical Foreignlanguage 8 Russian Astronomy\n",
      "9 ElectronicArts Company\n",
      "Engineering learning\n",
      "Petroleum TheWaltDisney\n",
      "Mathematics 9 Origami\n",
      "10 ElectronicArts Poi Engineering Company\n",
      "Education\n",
      "Protestant\n",
      "10 AcmeBrick Astronomy\n",
      "Affiliation\n",
      "A:Human B:Human A:Human B:DynoNet\n",
      "A:hi B:hi\n",
      "B:hi A:hey\n",
      "A:AnyfriendsworkatAVST? B:ihavethreefriendswholikeroadbiking\n",
      "B:petroleumengi??||no A:ihavetwofriendswholikeforeignlanguage\n",
      "A:nopetroleum learning\n",
      "B:russianasmajor? B:noforeignlanguagelearninghere\n",
      "A:Nope A:Ihavenofriendswholikeroadbiking\n",
      "B:protestantafiil?||math? B:dotheymajorinforeignlanguage?\n",
      "A:Ihavetwomath A:No||no\n",
      "B:SELECT2 B:petroleumengineering?\n",
      "A:oneworksatElectronicArts||SELECT10 A:NobutIhavefourfriendswhoworkforthe\n",
      "B:SELECT1 ElectronicArtscompany\n",
      "A:SELECT4 B:SELECT2 B:SELECT2 A:SELECT4\n",
      "A:StanoNet B:Human A:Human B:Rule\n",
      "B:hiya||ihaveoneforeignlanguageandgannett\n",
      "A:ihavetwoavst\n",
      "B:doyouhaveanyacmebrickandastronomy?\n",
      "A:hi||doyouhaveanyfriendsthatworkat A:manymathspeople||twoareforeignlanguage\n",
      "electronicarts? B:doyouhaveanypetroleumengineeringand\n",
      "B:Oneandtheylikeastronomy americanbroadcastingcompany?\n",
      "A:SELECT10 A:no||electronicarts\n",
      "B:SELECT2 A:SELECT4 B:SELECT1\n",
      "A:avst\n",
      "B:doyouhaveanydisneyorrestaurant?\n",
      "...\n",
      "Table8: Examplehuman-botchats. ThemutualfriendishighlightedinblueineachKB.Bots’utterances\n",
      "areinboldandselecteditemsarerepresentedbyitemIDs. Onlythefirsthalfofthehuman-Rulechatis\n",
      "shownduetospacelimit. Multipleutterancesofoneagentisseparatedby||.\n",
      "70\n",
      "60\n",
      "50\n",
      "40\n",
      "30\n",
      "20\n",
      "10\n",
      "0\n",
      "1 2 3 4 5\n",
      "egatnecreP\n",
      "Fluency Correctness\n",
      "Human\n",
      "Rule\n",
      "StanoNet\n",
      "DynoNet\n",
      "1 2 3 4 5\n",
      "70\n",
      "60\n",
      "50\n",
      "40\n",
      "30\n",
      "20\n",
      "10\n",
      "0\n",
      "1 2 3 4 5\n",
      "egatnecreP\n",
      "Cooperation Human-likeness\n",
      "1 2 3 4 5\n",
      "Figure 6: Histogram of ratings (higher is better) from third-party evaluators. Differences between\n",
      "systemsarelesssignificant.\n",
      "Partnerevaluation(1perdialogue) Third-partyevaluation(5perdialogue)\n",
      "System\n",
      "Human Comments Human Justifications\n",
      "-youhaveanyfriendswhowenttomonmouth?\n",
      "-Theflowwasniceandtheywereabletodiscernthecorrect\n",
      "answers.\n",
      "Goodpartner.Easyto\n",
      "Human 4 4.6 -humanlikebecauseofinteractiontalking\n",
      "workwith\n",
      "-Answersarehumanlike,notrobotic.Uses”hiya”tobegin\n",
      "conversation,moreofawarmtone.\n",
      "-morehumanthancomputerAgent2:hiyaAgent1:Hey\n",
      "-agent2lookedhumantome\n",
      "-definitelyhuman\n",
      "-A2couldbereplacedwitharobotwithoutnoticeable\n",
      "difference.\n",
      "Rule 2 Didn’tlistentome 4 -TheyspokeandbehavedasIoranyhumanwouldinthis\n",
      "situation.\n",
      "-Theagentjustseemstobegoingthroughthemotions,which\n",
      "givesmetheideathattheagentdoesn’texbithumanlike\n",
      "characteristics.\n",
      "-Nodjarum–Thisdoesn’tmakesenseinthiscontext,so\n",
      "doesn’tseemtobewrittenbyahuman.\n",
      "Tookforeveranddidn’t -humanlikebecauseofslightmispellingss\n",
      "StanoNet 5 reallyrespondcorrectly 3.5 -Cantelltheyarelikelyhumanbutjustnotveryverbose\n",
      "toquestions. -Theirterseconversionleanstothinkingtheywereeithernot\n",
      "payingattentionornothuman.\n",
      "-Theshortvaguesentencesareveryhumanlikemistakes.\n",
      "-Agent1isveryhumanlikebasedonthewaytheytypedand\n",
      "thefactthattheywerebeingdeceiving.\n",
      "IrepliedtwicethatI -Prettyresponsiveandlogicalprogression,butit’sverystilted\n",
      "DynoNet 4 onlyhadindoorfriends 3.8 sounding\n",
      "andwasignored. -idonothaveajose\n",
      "-Agentgivesnormalhumanresponses,“noangelaidon’t”\n",
      "-agent1waslookinglikeahumanlike\n",
      "Table 9: Comparison of ratings and comments on human-likeness from partners and third-party eval-\n",
      "uators. Each row contains results for the same dialogue. For the partner evaluation, we ask the human\n",
      "partnertoprovideasingle,optionalcommentattheendoftheconversation. Forthethird-partyevalua-\n",
      "tion,weaskfiveTurkerstorateeachdialogueandreportthemeanscore;theymustprovidejustification\n",
      "forratingsineachaspect. Fromthecomments,weseethatdialoguepartnersfocusmoreoncooperation\n",
      "andeffectiveness,whereasthird-partyevaluatorsfocusmoreonlinguisticfeaturessuchasverbosityand\n",
      "informality.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  95803,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['WesternSugar', 'Mechanization', 'Engineering Cooperative', 'Mathematics', 'AVST Shopping Petroleum', 'Education', 'Parks Foreignlanguage', 'AdobeSystems American', 'Administration learning Petroleum', 'Broadcasting Origami', 'Agricultural BroncoWine Engineering', 'Shopping Company', 'Mechanization Company', 'TheWaltDisney', 'Metallurgical Foreignlanguage', 'ElectronicArts Company', 'Protestant', 'AcmeBrick Astronomy']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Learning Symmetric Collaborative Dialogue Agents with Dynamic\n",
      "Knowledge Graph Embeddings\n",
      "HeHe and AnushaBalakrishnan and MihailEric and PercyLiang\n",
      "ComputerScienceDepartment,StanfordUniversity\n",
      "{hehe,anusha28,meric,pliang}@cs.stanford.edu\n",
      "Abstract FriendsofagentA:\n",
      "Name School Major Company\n",
      "We study a symmetric collaborative dia-\n",
      "Jessica Columbia ComputerScience Google\n",
      "logue setting in which two agents, each\n",
      "Josh Columbia Linguistics Google\n",
      "withprivateknowledge,muststrategically............\n",
      "communicate to achieve a common goal.\n",
      "A:Hi!MostofmyfriendsworkforGoogle\n",
      "The open-ended dialogue state in this set-\n",
      "B:doyouhaveanyonewhowenttocolumbia?\n",
      "ting poses new challenges for existing di- A:Hello?\n",
      "A:IhaveJessicaafriendofmine\n",
      "alogue systems. We collected a dataset\n",
      "A:andJosh,bothwenttocolumbia\n",
      "of 11K human-human dialogues, which B:oranyoneworkingatapple?\n",
      "exhibits interesting lexical, semantic, and B:SELECT(Jessica,Columbia,ComputerScience,Google)\n",
      "A:SELECT(Jessica,Columbia,ComputerScience,Google)\n",
      "strategic elements. To model both struc-\n",
      "tured knowledge and unstructured lan-\n",
      "Figure1: AnexampledialoguefromtheMutual-\n",
      "guage,weproposeaneuralmodelwithdy-\n",
      "Friends task in which two agents, A and B, each\n",
      "namic knowledge graph embeddings that\n",
      "givenaprivatelistofafriends,trytoidentifytheir\n",
      "evolve as the dialogue progresses. Au-\n",
      "mutual friend. Our objective is to build an agent\n",
      "tomatic and human evaluations show that\n",
      "that can perform the task with a human. Cross-\n",
      "ourmodelisbothmoreeffectiveatachiev-\n",
      "talk(Section2.3)isitalicized.\n",
      "ing the goal and more human-like than\n",
      "baselineneuralandrule-basedmodels.\n",
      "of systems, we focus on a symmetric collabora-\n",
      "1 Introduction\n",
      "tive dialogue setting, which is task-oriented but\n",
      "Current task-oriented dialogue systems (Young encourages open-ended dialogue acts. In our set-\n",
      "etal.,2013;Wenetal.,2017;Dhingraetal.,2017) ting, two agents, each with a private list of items\n",
      "require a pre-defined dialogue state (e.g., slots with attributes, must communicate to identify the\n",
      "such as food type and price range for a restau- uniqueshareditem. ConsiderthedialogueinFig-\n",
      "rantsearchingtask)andafixedsetofdialogueacts ure 1, in which two people are trying to find their\n",
      "(e.g.,request,inform). However,humanconversa- mutual friend. By asking “do you have anyone\n",
      "tionoftenrequiresricherdialoguestatesandmore who went to columbia?”, B is suggesting that she\n",
      "nuanced, pragmatic dialogue acts. Recent open- hassomeColumbiafriends,andthattheyprobably\n",
      "domain chat systems (Shang et al., 2015; Serban work at Google. Such conversational implicature\n",
      "etal.,2015b;Sordonietal.,2015;Lietal.,2016a; is lost when interpreting the utterance as simply\n",
      "Lowe et al., 2017; Mei et al., 2017) learn a map- an information request. In addition, it is hard to\n",
      "ping directly from previous utterances to the next define a structured state that captures the diverse\n",
      "utterance. Whilethesemodelscaptureopen-ended semanticsinmanyutterances(e.g.,defining“most\n",
      "aspectsofdialogue,thelackofstructureddialogue of”,“mightbe”;seedetailsinTable1).\n",
      "state prevents them from being directly applied To model both structured and open-ended con-\n",
      "to settings that require interfacing with structured text, we propose the Dynamic Knowledge Graph\n",
      "knowledge. Network(DynoNet),inwhichthedialoguestateis\n",
      "Inordertobridgethegapbetweenthetwotypes modeledasaknowledgegraphwithanembedding\n",
      "7102\n",
      "rpA\n",
      "42\n",
      "]LC.sc[\n",
      "1v03170.4071:viXra\n",
      "for each node (Section 3). Our model is similar 2.1 TaskDefinition\n",
      "to EntNet (Henaff et al., 2017) in that node/entity\n",
      "In the symmetric collaborative dialogue setting,\n",
      "embeddings are updated recurrently given new\n",
      "there are two agents, A and B, each with a pri-\n",
      "utterances. The difference is that we structure\n",
      "vate knowledge base—KB and KB, respec-\n",
      "A B\n",
      "entities as a knowledge graph; as the dialogue\n",
      "tively. Each knowledge base includes a list of\n",
      "proceeds, new nodes are added and new context\n",
      "items, where each item has a value for each at-\n",
      "is propagated on the graph. An attention-based\n",
      "tribute. For example, in the MutualFriends set-\n",
      "mechanism (Bahdanau et al., 2015) over the node\n",
      "ting, Figure 1, items are friends and attributes are\n",
      "embeddings drives generation of new utterances.\n",
      "name, school, etc. There is a shared item that A\n",
      "Ourmodel’suseofknowledgegraphscapturesthe\n",
      "and B both have; their goal is to converse with\n",
      "grounding capability of classic task-oriented sys-\n",
      "eachothertodeterminetheshareditemandselect\n",
      "temsandthegraphembeddingprovidestherepre-\n",
      "it. Formally, an agent is a mapping from its pri-\n",
      "sentationalflexibilityofneuralmodels.\n",
      "vateKBandthedialoguethusfar(sequenceofut-\n",
      "The naturalness of communication in the sym-\n",
      "terances)tothenextutterancetogenerateorase-\n",
      "metric collaborative setting enables large-scale\n",
      "lection. Adialogueisconsideredsuccessfulwhen\n",
      "data collection: We were able to crowdsource\n",
      "both agents correctly select the shared item. This\n",
      "around 11K human-human dialogues on Amazon\n",
      "settinghasparallelsinhuman-computercollabora-\n",
      "Mechanical Turk (AMT) in less than 15 hours.1\n",
      "tion where each agent has complementary exper-\n",
      "We show that the new dataset calls for more flex-\n",
      "tise.\n",
      "iblerepresentationsbeyondfully-structuredstates\n",
      "(Section2.2). 2.2 Datacollection\n",
      "Inadditiontoconductingthethird-partyhuman\n",
      "Wecreatedaschemawith7attributesandapprox-\n",
      "evaluationadoptedbymostwork(Liuetal.,2016;\n",
      "imately3Kentities(attributevalues). Toelicitlin-\n",
      "Lietal.,2016b,c), wealsoconductpartnerevalu-\n",
      "guistic and strategic variants, we generate a ran-\n",
      "ation (Wen et al., 2017) where AMT workers rate\n",
      "dom scenario for each task by varying the num-\n",
      "theirconversationalpartners(otherworkersorour\n",
      "ber of items (5 to 12), the number attributes (3 or\n",
      "models) based on fluency, correctness, coopera-\n",
      "4),andthedistributionofvaluesforeachattribute\n",
      "tion, and human-likeness. We compare DynoNet\n",
      "(skewed to uniform). See Appendix A and B for\n",
      "with baseline neural models and a strong rule-\n",
      "detailsofschemaandscenariogeneration.\n",
      "basedsystem. TheresultsshowthatDynoNetcan\n",
      "perform the task with humans efficiently and nat-\n",
      "urally; it also captures some strategic aspects of\n",
      "human-humandialogues.\n",
      "The contributions of this work are: (i) a new\n",
      "symmetric collaborative dialogue setting and a\n",
      "large dialogue corpus that pushes the boundaries\n",
      "ofexistingdialoguesystems;(ii)DynoNet,which\n",
      "integrates semantically rich utterances with struc-\n",
      "turedknowledgetorepresentopen-endeddialogue\n",
      "states; (iii) multiple automatic metrics based on\n",
      "bot-bot chat and a comparison of third-party and\n",
      "partnerevaluation. Figure2: Screenshotofthechatinterface.\n",
      "2 SymmetricCollaborativeDialogue We crowdsourced dialogues on AMT by ran-\n",
      "domly pairing up workers to perform the task\n",
      "We begin by introducing a collaborative task be- within 5 minutes.2 Our chat interface is shown in\n",
      "tween two agents and describe the human-human\n",
      "Figure2. Todiscouragerandomguessing,wepre-\n",
      "dialoguecollectionprocess. Weshowthatourdata\n",
      "ventworkersfromselectingmorethanonceevery\n",
      "exhibitsdiverse,interestinglanguagephenomena.\n",
      "10seconds. Ourtaskwasverypopularandwecol-\n",
      "1The dataset is available publicly at https:// 2If the workers exceed the time limit, the dialogue is\n",
      "stanfordnlp.github.io/cocoa/. markedasunsuccessful(butstilllogged).\n",
      "Type % Easyexample Hardexample\n",
      "Iknowajudy./Ihavesomeonewho Aboutequalindoorandoutdoorfriends/metoo.his\n",
      "Inform 30.4\n",
      "studiedthebibleintheafternoon. majorisforestry/mightbekelly\n",
      "DoanyofthemlikePoi?/Whatdoesyourhenry Whatcanyoutellmeaboutourfriend?/Ormaybe\n",
      "Ask 17.7\n",
      "do? northparkcollege?\n",
      "Answer 7.4 Noneofminedid/Yup/Theydo./Samehere. yes3ofthem/Nohelikespoi/yesifbostoncollege\n",
      "Table 1: Main utterance types and examples. We show both standard utterances whose meaning can\n",
      "be represented by simple logical forms (e.g., ask(indoor)), and open-ended ones which require more\n",
      "complexlogicalforms(difficultpartsinbold). Textspanscorrespondingtoentitiesareunderlined.\n",
      "Phenomenon Example\n",
      "Coreference (IknowoneDebra)doessheliketheindoors?/(IhavetwofriendsnamedTIffany)atWorldairways?\n",
      "Coordination keepongoingwiththefashion/Ok.let’strysomethingelse./gobyhobby/great.selecthim.thanks!\n",
      "Chit-chat Yes,thatisgoodoleTerry./Allindoorsers!myfriendshatenature\n",
      "Categorization same,mostofminearefemaletoo/DoesanyofthemnamesstartwithB\n",
      "Correction IknowonefriendintoEmbroidery-hernameisEmily.Sorry–EmbroideryfriendisnamedMichelle\n",
      "Table2: Communicationphenomenainthedataset. Evidentpartsisinboldandtextspanscorresponding\n",
      "toanentityareunderlined. Forcoreference,theantecedentisinparentheses.\n",
      "lected11Kdialoguesoveraperiodof13.5hours.3 Someofthestandardonesarealsonon-trivialdue\n",
      "Of these, over 9K dialogues are successful. Un- tocoreferenceandlogicalcompositionality.\n",
      "successfuldialoguesareusuallytheresultofeither Ourdatasetalsoexhibitssomeinterestingcom-\n",
      "workerleavingthechatprematurely. munication phenomena. Coreference occurs fre-\n",
      "quently when people check multiple attributes\n",
      "2.3 Datasetstatistics\n",
      "of one item. Sometimes mentions are dropped,\n",
      "We show the basic statistics of our dataset in Ta- as an utterance simply continues from the part-\n",
      "ble 3. An utterance is defined as a message sent ner’s utterance. People occasionally use exter-\n",
      "byoneoftheagents. Theaverageutterancelength nalknowledgetogroupitemswithout-of-schema\n",
      "is short due to the informality of the chat, how- attributes (e.g., gender based on names, location\n",
      "ever,anagentusuallysendsmultipleutterancesin based on schools). We summarize these phenom-\n",
      "one turn. Some example dialogues are shown in ena in Table 2. In addition, we find 30% utter-\n",
      "Table6andAppendixI. ances involve cross-talk where the conversation\n",
      "does not progress linearly (e.g., italic utterances\n",
      "#dialogues 11157 in Figure 1), a common characteristic of online\n",
      "#completeddialogues 9041\n",
      "chat(Ivanovic,2005).\n",
      "Vocabularysize 5325\n",
      "Average#ofutterances 11.41 Onestrategicaspectofthistaskischoosingthe\n",
      "Averagetimetakenpertask(sec.) 91.18\n",
      "orderofattributestomention. Wefindthatpeople\n",
      "Averageutterancelength(tokens) 5.08\n",
      "Numberoflinguistictemplates4 41561 tendtostartfromattributeswithfeweruniqueval-\n",
      "ues, e.g., “all my friends like morning” given the\n",
      "Table3: StatisticsoftheMutualFriendsdataset. KB in Table 6, as intuitively it would help ex-\n",
      "B\n",
      "clude items quickly given fewer values to check.5\n",
      "We categorize utterances into coarse types— Weprovideamoredetailedanalysisofstrategyin\n",
      "inform, ask, answer, greeting, apology—by pattern Section4.2andAppendixF.\n",
      "matching (Appendix E). There are 7.4% multi-\n",
      "type utterances, and 30.9% utterances contain 3 DynamicKnowledgeGraphNetwork\n",
      "more than one entity. In Table 1, we show exam-\n",
      "The diverse semantics in our data motivates us\n",
      "ple utterances with rich semantics that cannot be\n",
      "to combine unstructured representation of the di-\n",
      "sufficiently represented by traditional slot-values.\n",
      "alogue history with structured knowledge. Our\n",
      "3Tasksareputupinbatches;thetotaltimeexcludesinter-\n",
      "valsbetweenbatches. 5Ourgoalistomodelhumanbehaviorthuswedonotdis-\n",
      "4Entitynamesarereplacedbytheirentitytypes. cusstheoptimalstrategyhere.\n",
      "Dynamic knowledge graph Graph Generator\n",
      "embedding\n",
      "anyonewentcolumbia Yes jessica and josh\n",
      "KB + Dialogue history\n",
      "… …\n",
      "jessica\n",
      "Name School Company\n",
      "1 S\n",
      "Item 1 Jessica Columbia Google columbia columbia\n",
      "Item 2 Josh Columbia Google N jessica\n",
      "google josh\n",
      "B: anyone went to columbia? 2 C google …\n",
      "…\n",
      "josh\n",
      "Attention + Copy\n",
      "Message passing path of columbia\n",
      "Figure3: Overviewofourapproach. First,theKBanddialoguehistory(entitiesinbold)ismappedto\n",
      "a graph. Here, an item node is labeled by the item ID and an attribute node is labeled by the attribute’s\n",
      "firstletter. Next,eachnodeisembeddedusingrelevantutteranceembeddingsthroughmessagepassing.\n",
      "Finally,anLSTMgeneratesthenextutterancebasedonattentionoverthenodeembeddings.\n",
      "modelconsistsofthreecomponentsshowninFig- is columbia. An example graph is shown in Fig-\n",
      "ure3: (i)adynamicknowledgegraph,whichrep- ure3. ThegraphG isupdatedbasedonutterance\n",
      "t\n",
      "resentstheagent’sprivateKBandshareddialogue t by taking G and adding a new node for any\n",
      "t−1\n",
      "history as a graph (Section 3.1), (ii) a graph em- entitymentionedinutterancetbutnotinKB.7\n",
      "A\n",
      "bedding over the nodes (Section 3.2), and (iii) an\n",
      "3.2 GraphEmbedding\n",
      "utterancegenerator(Section3.3).\n",
      "Theknowledgegraphrepresentsentitiesandre- Given a knowledge graph, we are interested in\n",
      "lations in the agent’s private KB, e.g., item-1’s computing a vector representation for each node\n",
      "company is google. As the conversation unfolds, v that captures both its unstructured context from\n",
      "utterances are embedded and incorporated into the dialogue history and its structured context in\n",
      "node embeddings of mentioned entities. For in- the KB. A node embedding V t(v) for each node\n",
      "stance, in Figure 3, “anyone went to columbia” v ∈ G t is built from three parts: structural prop-\n",
      "updates the embedding of columbia. Next, each ertiesofanentitydefinedbytheKB,embeddings\n",
      "node recursively passes its embedding to neigh- ofutterancesinthedialoguehistory,andmessage\n",
      "boring nodes so that related entities (e.g., those passingbetweenneighboringnodes.\n",
      "in the same row or column) also receive informa-\n",
      "Node Features. Simple structural properties of\n",
      "tion from the most recent utterance. In our exam-\n",
      "the KB often govern what is talked about; e.g.,\n",
      "ple, jessica and josh both receive new context\n",
      "a high-frequency entity is usually interesting to\n",
      "when columbia is mentioned. Finally, the utter-\n",
      "mention(consider“Allmyfriendslikedancing.”).\n",
      "ancegenerator,anLSTM,producesthenextutter-\n",
      "We represent this type of information as a fea-\n",
      "ancebyattendingtothenodeembeddings.\n",
      "ture vector F (v), which includes the degree and\n",
      "t\n",
      "type(item,attribute,orentitytype)ofnodev,and\n",
      "3.1 KnowledgeGraph\n",
      "whetherithasbeenmentionedinthecurrentturn.\n",
      "Given a dialogue of T utterances, we construct Each feature is encoded as a one-hot vector and\n",
      "graphs (G t)T\n",
      "t=1\n",
      "over the KB and dialogue history theyareconcatenatedtoformF t(v).\n",
      "foragentA.6 Therearethreetypesofnodes: item\n",
      "MentionVectors. AmentionvectorM (v)con-\n",
      "nodes, attribute nodes, and entity nodes. Edges t\n",
      "tainsunstructuredcontextfromutterancesrelevant\n",
      "between nodes represent their relations. For ex-\n",
      "to node v up to turn t. To compute it, we first de-\n",
      "ample,(item-1, hasSchool, columbia)means\n",
      "fine the utterance representation u˜ and the set of\n",
      "thatthefirstitemhasattributeschoolwhosevalue t\n",
      "relevant entities E. Let u be the embedding of\n",
      "t t\n",
      "6 It is important to differentiate perspectives of the two utterancet(Section3.3). Todifferentiatebetween\n",
      "agentsastheyhavedifferentKBs. Thereafterweassumethe\n",
      "perspectiveofagentA,i.e., accessingKB forAonly, and 7Weusearule-basedlexicontolinktextspanstoentities.\n",
      "A\n",
      "refertoBasthepartner. SeedetailsinAppendixD.\n",
      "the agent’s and the partner’s utterances, we repre- 3.3 UtteranceEmbeddingandGeneration\n",
      "sent it as u˜ = (cid:2) u ·1,u ·1 (cid:3),\n",
      "t t {ut∈U self} t {ut∈Upartner} We embed and generate utterances using Long\n",
      "where U and U denote sets of utterances\n",
      "self partner Short Term Memory (LSTM) networks that take\n",
      "generated by the agent and the partner, and [·,·]\n",
      "thegraphembeddingsintoaccount.\n",
      "denotes concatenation. Let E be the set of entity\n",
      "t\n",
      "nodesmentionedinutterancetifutterancetmen- Embedding. On turn t, upon receiving an\n",
      "tionssomeentities, orutterancet−1otherwise.8 utterance consisting of n t tokens, x t =\n",
      "ThementionvectorM t(v)ofnodev incorporates (x t,1,...,x t,nt), the LSTM maps it to a vector as\n",
      "thecurrentutteranceifvismentionedandinherits follows:\n",
      "M t−1(v)ifnot: h\n",
      "t,j\n",
      "= LSTM enc(h t,j−1,A t(x t,j)), (4)\n",
      "M t(v) = λ tM t−1(v)+(1−λ t)u˜ t; (1) where h\n",
      "t,0\n",
      "= h t−1,nt−1, and A\n",
      "t\n",
      "is an entity ab-\n",
      "(cid:40) σ(cid:0) Winc[M (v),u˜ ](cid:1) ifv ∈ E, stractionfunction,explainedbelow. Thefinalhid-\n",
      "t−1 t t\n",
      "λ t = 1 otherwise. den state h t,nt is used as the utterance embed-\n",
      "dingu,whichupdatesthementionvectorsasde-\n",
      "t\n",
      "Here, σ is the sigmoid function and Winc is a pa- scribedinSection3.2.\n",
      "rametermatrix. In our dialogue task, the identity of an entity\n",
      "is unimportant. For example, replacing google\n",
      "Recursive Node Embeddings. We propagate\n",
      "with alphabet in Figure 1 should make little dif-\n",
      "informationbetweennodesaccordingtothestruc-\n",
      "ference to the conversation. The role of an entity\n",
      "ture of the knowledge graph. In Figure 3, given\n",
      "is determined instead by its relation to other en-\n",
      "“anyonewenttocolumbia?”,theagentshouldfo-\n",
      "tities and relevant utterances. Therefore, we de-\n",
      "cusonherfriendswhowenttoColumbiaUniver-\n",
      "finetheabstractionA (y)forawordy asfollows:\n",
      "t\n",
      "sity. Therefore, we want this utterance to be sent\n",
      "if y is linked to an entity v, then we represent an\n",
      "toitemnodesconnectedtocolumbia,andonestep\n",
      "entity by its type (school, company etc.) embed-\n",
      "further to other attributes of these items because\n",
      "ding concatenated with its current node embed-\n",
      "theymightbementionednextasrelevantinforma-\n",
      "ding: A (y) = [E,V (v)]. Note that V (v)\n",
      "tion,e.g.,jessicaandjosh.\n",
      "t type(y) t t\n",
      "isdeterminedonlybyitsstructuralfeaturesandits\n",
      "We compute the node embeddings recursively,\n",
      "context. Ifyisanon-entity,thenA (y)istheword\n",
      "t\n",
      "analogoustobeliefpropagation:\n",
      "embedding of y concatenated with a zero vector\n",
      "Vk(v) = max tanh (2) of the same dimensionality as V (v). This way,\n",
      "t t\n",
      "v(cid:48)∈Nt(v)\n",
      "therepresentationofanentityonlydependsonits\n",
      "(cid:16) (cid:104) (cid:105)(cid:17)\n",
      "Wmp Vk−1(v(cid:48)),R(e ), structural properties given by the KB and the dia-\n",
      "t v→v(cid:48)\n",
      "logue context, which enables the model to gener-\n",
      "where Vk(v) is the depth-k node embedding at\n",
      "t alizetounseenentitiesattesttime.\n",
      "turntandN (v)denotesthesetofnodesadjacent\n",
      "t\n",
      "tov. Themessagefromaneighboringnodev(cid:48) de- Generation. Now,assumingwehaveembedded\n",
      "utterance x into h as described above,\n",
      "pendsonitsembeddingatdepth-(k−1),theedge t−1 t−1,nt−1\n",
      "we use another LSTM to generate utterance x.\n",
      "label e (embedded by a relation embedding t\n",
      "v→v(cid:48)\n",
      "function R), and a parameter matrix Wmp. Mes- Formally, we carryover the lastutterance embed-\n",
      "dingh = h anddefine:\n",
      "sages from all neighbors are aggregated by max, t,0 t−1,nt−1\n",
      "the element-wise max operation.9 Example mes- h = LSTM (h,[A (x ),c ]), (5)\n",
      "t,j dec t,j−1 t t,j t,j\n",
      "sagepassingpathsareshowninFigure3.\n",
      "wherec isaweightedsumofnodeembeddings\n",
      "The final node embedding is the concatenation t,j\n",
      "(cid:80)\n",
      "in the current turn: c = α V (v),\n",
      "ofembeddingsateachdepth: t,j v∈Gt t,j,v t\n",
      "where α are the attention weights over the\n",
      "t,j,v\n",
      "V t(v) = (cid:2) V t0(v),...,V tK(v)(cid:3), (3) nodes. Intuitively, high weight should be given to\n",
      "relevant entity nodes as shown in Figure 3. We\n",
      "whereK isahyperparameter(weexperimentwith\n",
      "K ∈ {0,1,2})andV0(v) = [F (v),M (v)]. compute the weights through standard attention\n",
      "t t t\n",
      "mechanism(Bahdanauetal.,2015):\n",
      "8 Relying on utterance t−1 is useful when utterance t\n",
      "answersaquestion,e.g.,“doyouhaveanygooglefriends?” α t,j = softmax(s t,j),\n",
      "“No.” s = wattn·tanh(cid:0) Wattn[h,V (v)](cid:1),\n",
      "9Usingsumormeanslightlyhurtsperformance. t,j,v t,j−1 t\n",
      "wherevectorwattn andWattn areparameters. what to talk about and which item to select. It\n",
      "Finally,wedefineadistributionoverbothwords has a pattern-matching semantic parser, a rule-\n",
      "inthevocabularyandnodesinG usingthecopy- based policy, and a templated generator. See Ap-\n",
      "t\n",
      "ingmechanismofJiaandLiang(2016): pendixGfordetails.\n",
      "p(x t,j+1 = y|G t,x t,≤j) ∝ exp(cid:0) Wvocabh t,j +b(cid:1), 4.2 Evaluation\n",
      "p(x t,j+1 = r(v)|G t,x t,≤j) ∝ exp(s t,j,v), We test our systems in two interactive settings:\n",
      "bot-botchatandbot-humanchat. Weperformboth\n",
      "where y is a word in the vocabulary, Wvocab and\n",
      "automaticevaluationandhumanevaluation.\n",
      "bareparameters,andr(v)istherealizationofthe\n",
      "entity represented by node v, e.g., google is real- Automatic Evaluation. First, we compute the\n",
      "izedto“Google”duringcopying.10\n",
      "cross-entropy ((cid:96)) of a model on test data. As\n",
      "shown in Table 4, DynoNet has the lowest test\n",
      "4 Experiments\n",
      "loss. Next, we have a model chat with itself on\n",
      "We compare our model with a rule-based sys- the scenarios from the test set.12 We evaluate the\n",
      "tem and a baseline neural model. Both automatic chatswithrespecttolanguagevariation,effective-\n",
      "and human evaluations are conducted to test the ness,andstrategy.\n",
      "models in terms of fluency, correctness, coopera- For language variation, we report the average\n",
      "tion, and human-likeness. The results show that utterance length L u and the unigram entropy H\n",
      "DynoNetisabletoconversewithhumansinaco- in Table 4. Compared to Rule, the neural mod-\n",
      "herentandstrategicway. els tend to generate shorter utterances (Li et al.,\n",
      "2016b; Serban et al., 2017b). However, they are\n",
      "4.1 Setup\n",
      "more diverse; for example, questions are asked\n",
      "Werandomlysplitthedataintotrain,dev,andtest in multiple ways such as “Do you have...”, “Any\n",
      "sets (8:1:1). We use a one-layer LSTM with 100 friendslike...”,“Whatabout...”.\n",
      "hidden units and 100-dimensional word vectors At the discourse level, we expect the distribu-\n",
      "forboththeencoderandthedecoder(Section3.3). tion of a bot’s utterance types to match the distri-\n",
      "Eachsuccessfuldialogueisturnedintotwoexam- bution of human’s. We show percentages of each\n",
      "ples, each from the perspective of one of the two utterance type in Table 4. For Rule, the decision\n",
      "agents. We maximize the log-likelihood of all ut- about which action to take is written in the rules,\n",
      "terancesinthedialogues. Theparametersareopti- whileStanoNetandDynoNetlearnedtobehavein\n",
      "mizedbyAdaGrad(Duchietal.,2010)withanini- amorehuman-likeway,frequentlyinformingand\n",
      "tiallearningrateof0.5. Wetrainedforatleast10 askingquestions.\n",
      "epochs; after that, training stops if there is no im- Tomeasureeffectiveness,wecomputetheover-\n",
      "provementonthedevsetfor5epochs. Bydefault, all success rate (C) and the success rate per turn\n",
      "we perform K = 2 iterations of message passing (C )andperselection(C ). AsshowninTable4,\n",
      "T S\n",
      "to compute node embeddings (Section 3.2). For humansarethebestatthisgame,followedbyRule\n",
      "decoding,wesequentiallysamplefromtheoutput whichiscomparabletoDynoNet.\n",
      "distribution with a softmax temperature of 0.5.11 Next, we investigate the strategies leading to\n",
      "Hyperparametersaretunedonthedevset. these results. An agent needs to decide which\n",
      "We compare DynoNet with its static cou- entity/attribute to check first to quickly reduce\n",
      "sion (StanoNet) and a rule-based system (Rule). the search space. We hypothesize that humans\n",
      "StanoNet uses G 0 throughout the dialogue, thus tend to first focus on a majority entity and an\n",
      "the dialogue history is completely contained in attribute with fewer unique values (Section 2.3).\n",
      "the LSTM states instead of being injected into For example, in the scenario in Table 6, time and\n",
      "the knowledge graph. Rule maintains weights for location are likely to be mentioned first. We\n",
      "each entity and each item in the KB to decide showtheaveragefrequencyoffirst-mentioneden-\n",
      "tities (#Ent ) and the average number of unique\n",
      "10Werealizeanentitybysamplingfromtheempiricaldis- 1\n",
      "tributionofitssurfaceformsfoundinthetrainingdata. valuesforfirst-mentionedattributes(|Attr 1|)inTa-\n",
      "11 Sinceselectionisacommon‘utterance’inourdataset\n",
      "and neural generation models are susceptible to over- 12 Welimitthenumberofturnsinbot-botchattobethe\n",
      "generatingcommonsentences, wehalveitsprobabilitydur- maximum number of turns humans took in the test set (46\n",
      "ingsampling. turns).\n",
      "System (cid:96)↓ L H C ↑ C ↑ C ↑ Sel Inf Ask Ans Greet #Ent |Attr | #Ent #Attr\n",
      "u T S 1 1\n",
      "Human - 5.10 4.57.82.07.38.21.31.17.08.08.55.35 6.1 2.6\n",
      "Rule - 7.61 3.37.90.05.29.18.34.23.00.12.24.61 9.9 3.0\n",
      "StanoNet 2.20 4.01 4.05.78.04.18.19.26.12.23.09.61.19 7.1 2.9\n",
      "DynoNet 2.13 3.37 3.90.96.06.25.22.26.13.20.12.55.18 5.2 2.5\n",
      "Table 4: Automatic evaluation on human-human and bot-bot chats on test scenarios. We use ↑ / ↓ to\n",
      "indicatethathigher/lowervaluesarebetter;otherwisetheobjectiveistomatchhumans’statistics. Best\n",
      "results(exceptHuman)areinbold. Neuralmodelsgenerateshorter(lowerL )butmorediverse(higher\n",
      "u\n",
      "H)utterances. Overall,theirdistributionsofutterancetypesmatchthoseofthehumans’. (Weonlyshow\n",
      "the most frequent speech acts therefore the numbers do not sum to 1.) Rule is effective in completing\n",
      "the task (higher C ), but it is not information-efficient given the large number of attributes (#Attr) and\n",
      "S\n",
      "entities(#Ent)mentioned.\n",
      "ble4.13 BothDynoNetandStanoNetsuccessfully Noticeably, DynoNet is more cooperative than\n",
      "match human’s starting strategy by favoring enti- the other models. As shown in the example dia-\n",
      "ties of higher frequency and attributes of smaller logues in Table 6, DynoNet cooperates smoothly\n",
      "domainsize. with the human partner, e.g., replying with rel-\n",
      "To examine the overall strategy, we show the evant information about morning/indoor friends\n",
      "average number of attributes (#Attr) and entities when the partner mentioned that all her friends\n",
      "(#Ent) mentioned during the conversation in Ta- prefer morning and most like indoor. StanoNet\n",
      "ble4. HumansandDynoNetstrategicallyfocuson starts well but doesn’t follow up on the morn-\n",
      "a few attributes and entities, whereas Rule needs ingfriend, presumablybecausethemorningnode\n",
      "almost twice entities to achieve similar success is not updated dynamically when mentioned by\n",
      "rates. This suggests that the effectiveness of Rule the partner. Rule follows the partner poorly. In\n",
      "mainly comes from large amounts of unselective the comments, the biggest complaint about Rule\n",
      "information, which is consistent with comments was that it was not ‘listening’ or ‘understanding’.\n",
      "fromtheirhumanpartners. Overall,DynoNetachievesbetterpartnersatisfac-\n",
      "tion,especiallyincooperation.\n",
      "Partner Evaluation. We generated 200 new\n",
      "scenarios and put up the bots on AMT using the Third-party Evaluation. We also created a\n",
      "same chat interface that was used for data col- third-partyevaluationtask,whereanindependent\n",
      "lection. The bots follow simple turn-taking rules AMTworkerisshownaconversationandtheKB\n",
      "explained in Appendix H. Each AMT worker is ofoneoftheagents; sheisaskedtoratethesame\n",
      "randomly paired with Rule, StanoNet, DynoNet, aspects of the agent as in the partner evaluation\n",
      "or another human (but the worker doesn’t know and provide justifications. Each agent in a dia-\n",
      "which), and we make sure that all four types of logueisratedbyatleast5people.\n",
      "agentsaretestedineachscenarioatleastonce. At The average ratings and histograms are shown\n",
      "theendofeachdialogue,humansareaskedtor<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  45443,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['MutualFriends']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: ,especiallyincooperation.\n",
      "Partner Evaluation. We generated 200 new\n",
      "scenarios and put up the bots on AMT using the Third-party Evaluation. We also created a\n",
      "same chat interface that was used for data col- third-partyevaluationtask,whereanindependent\n",
      "lection. The bots follow simple turn-taking rules AMTworkerisshownaconversationandtheKB\n",
      "explained in Appendix H. Each AMT worker is ofoneoftheagents; sheisaskedtoratethesame\n",
      "randomly paired with Rule, StanoNet, DynoNet, aspects of the agent as in the partner evaluation\n",
      "or another human (but the worker doesn’t know and provide justifications. Each agent in a dia-\n",
      "which), and we make sure that all four types of logueisratedbyatleast5people.\n",
      "agentsaretestedineachscenarioatleastonce. At The average ratings and histograms are shown\n",
      "theendofeachdialogue,humansareaskedtorate inTable5andAppendixJ.Forcorrectness,wesee\n",
      "their partner in terms of fluency, correctness, co- thatRulehasthebestperformancesinceitalways\n",
      "operation, and human-likeness from 1 (very bad) tellsthetruth,whereashumanscanmakemistakes\n",
      "to5(verygood),alongwithoptionalcomments. duetocarelessnessandtheneuralmodelscangen-\n",
      "We show the average ratings (with significance erate false information. For example, in Table 6,\n",
      "tests)inTable5andthehistogramsinAppendixJ. DynoNet‘lied’whensayingthatithasamorning\n",
      "In terms of fluency, the models have similar per- friendwholikesoutdoor.\n",
      "formance since the utterances are usually short. Surprisingly,thereisadiscrepancybetweenthe\n",
      "Judgmentoncorrectnessisamereguesssincethe twoevaluationmodesintermsofcooperationand\n",
      "evaluatorcannotseethepartner’sKB;wewillan- human-likeness. Manual analysis of the com-\n",
      "alyze correctness more meaningfully in the third- ments indicates that third-party evaluators focus\n",
      "partyevaluationbelow. lessonthedialoguestrategyandmoreonlinguis-\n",
      "tic features, probably because they were not fully\n",
      "13Bothnumbersarenormalizedto[0,1]withrespecttoall\n",
      "entities/attributesinthecorrespondingKB. engagedinthedialogue. Forexample,justification\n",
      "Partnereval Third-partyeval\n",
      "System C C C\n",
      "T S Flnt Crct Coop Human Flnt Crct Coop Human\n",
      "Human.89.07.36 4.2rds 4.3rds 4.2rds 4.1rds 4.0 4.3ds 4.0ds 4.1rds\n",
      "Rule.88.06.29 3.6 4.0 3.5 3.5 4.0 4.4hds 3.9s 4.0s\n",
      "StanoNet.76.04.23 3.5 3.8 3.4 3.3 4.0 4.0 3.8 3.8\n",
      "DynoNet.87.05.27 3.8s 4.0 3.8rs 3.6s 4.0 4.1 3.9 3.9\n",
      "Table 5: Results on human-bot/human chats. Best results (except Human) in each column are in bold.\n",
      "We report the average ratings of each system. For third-party evaluation, we first take mean of each\n",
      "question then average the ratings. DynoNet has the best partner satisfaction in terms of fluency (Flnt),\n",
      "correctness(Crct),cooperation(Coop),humanlikeness(Human). Thesuperscriptofaresultindicatesthat\n",
      "its advantage over other systems (r: Rule, s: StanoNet, d: DynoNet) is statistically significant with\n",
      "p < 0.05givenbypairedt-tests.\n",
      "forcooperationoftenmentionsfrequentquestions dialogues are easy to collect as natural human\n",
      "and timely answers, less attention is paid to what conversations, and are also challenging enough\n",
      "isaskedaboutthough. given the large number of scenarios and diverse\n",
      "For human-likeness, partner evaluation is conversation phenomena. There are some in-\n",
      "largelycorrelatedwithcoherence(e.g.,notrepeat- teresting strategic dialogue datasets—settlers of\n",
      "ing or ignoring past information) and task suc- Catan (Afantenos et al., 2012) (2K turns) and the\n",
      "cess, whereas third-party evaluators often rely on cards corpus (Potts, 2012) (1.3K dialogues), as\n",
      "informality (e.g., usage of colloquia like “hiya”, well as work on dialogue strategies (Keizer et al.,\n",
      "capitalization, and abbreviation) or intuition. In- 2017; Vogel et al., 2013), though no full dialogue\n",
      "terestingly,third-partyevaluatorsnotedmostphe- systemhasbeenbuiltforthesedatasets.\n",
      "nomena listed in Table 2 as indicators of human-\n",
      "Mosttask-orienteddialoguesystemsfollowthe\n",
      "beings, e.g., correcting oneself, making chit-chat\n",
      "POMDP-based approach (Williams and Young,\n",
      "other than simply finishing the task. See example\n",
      "2007; Young et al., 2013). Despite their suc-\n",
      "commentsinAppendixK.\n",
      "cess (Wen et al., 2017; Dhingra et al., 2017; Su\n",
      "etal.,2016),therequirementforhandcraftedslots\n",
      "4.3 AblationStudies\n",
      "limits their scalability to new domains and bur-\n",
      "Our model has two novel designs: entity abstrac- dens data collection with extra state labeling. To\n",
      "tion and message passing for node embeddings. go past this limit, Bordes and Weston (2017) pro-\n",
      "Table 7 shows what happens if we ablate these. posed a Memory-Networks-based approach with-\n",
      "When the number of message passing iterations, out domain-specific features. However, the mem-\n",
      "K, is reduced from 2 to 0, the loss consistently oryisunstructuredandinterfacingwithKBsrelies\n",
      "increases. Removingentityabstraction—meaning onAPIcalls,whereasourmodelembedsboththe\n",
      "addingentityembeddingstonodeembeddingsand dialoguehistoryandtheKBstructurally. Williams\n",
      "the LSTM input embeddings—also degrades per- et al. (2017) use an LSTM to automatically infer\n",
      "formance. ThisshowsthatDynoNetbenefitsfrom the dialogue state, but as they focus on dialogue\n",
      "contextually-defined, structural node embeddings controlratherthanthefullproblem,theresponseis\n",
      "ratherthanonesbasedonaclassiclookuptable. modeledasatemplatedaction,whichrestrictsthe\n",
      "generation of richer utterances. Our network ar-\n",
      "5 DiscussionandRelatedWork\n",
      "chitectureismostsimilartoEntNet(Henaffetal.,\n",
      "2017), where memories are also updated by input\n",
      "There has been a recent surge of interest in\n",
      "sentences recurrently. The main difference is that\n",
      "end-to-endtask-orienteddialoguesystems,though\n",
      "ourmodelallowsinformationtobepropagatedbe-\n",
      "progress has been limited by the size of available\n",
      "tween structured entities, which is shown to be\n",
      "datasets(Serbanetal.,2015a). Mostworkfocuses\n",
      "crucialinoursetting(Section4.3).\n",
      "on information-querying tasks, using Wizard-of-\n",
      "Oz data collection (Williams et al., 2016; Asri Ourworkisalsorelatedtolanguagegeneration\n",
      "et al., 2016) or simulators (Bordes and Weston, conditionedonknowledgebases(Meietal.,2016;\n",
      "2017; Li et al., 2016d), In contrast, collaborative Kiddon et al., 2016). One challenge here is to\n",
      "FriendsofA FriendsofB\n",
      "ID Name Company Time Location ID Name Company Time Location\n",
      "1 Kathy TRTHoldings afternoon indoor 1 Justin NewEraTickets morning indoor\n",
      "2 Jason DollarGeneral afternoon indoor 2 Kathleen TRTHoldings morning indoor\n",
      "3 Johnny TRTHoldings afternoon outdoor 3 Gloria L&LHawaiianBarbecue morning indoor\n",
      "4 Frank SFNGroup afternoon indoor 4 Kathleen AdvanceAutoParts morning outdoor\n",
      "5 Catherine DollarGeneral afternoon indoor 5 Justin ArcticCat morning indoor\n",
      "6 Catherine WeisMarkets afternoon indoor 6 Anna DollarGeneral morning indoor\n",
      "7 Kathleen TRTHoldings morning indoor 7 Steven SFNGroup morning indoor\n",
      "8 Lori TRTHoldings afternoon indoor 8 Wayne R.J.CormanRailroadGroup morning indoor\n",
      "9 Frank L&LHawaiianBarbecue afternoon outdoor 9 Alexander R.J.CormanRailroadGroup morning indoor\n",
      "A:Human B:Human A:DynoNet B:Human\n",
      "A:hi\n",
      "A:Hi\n",
      "B:Hello||allmyfriendsprefermorning\n",
      "B:hey||ihaveoneoutdoor\n",
      "A:1ofmymorninglikestheoutdoors\n",
      "A:Ihave4TRTHoldings||Ihave2outdoorone\n",
      "B:andalllikeindoorexceptforone\n",
      "Johnny,otherFrank\n",
      "A:dotheyworkfortrtholdings?\n",
      "B:ionlyhaveoneTRTHoldings-Kathleen\n",
      "B:Kathleen?\n",
      "A:SELECT7 B:SELECT2\n",
      "A:SELECT7 B:SELECT2\n",
      "A:StanoNet B:Human A:Human B:Rule\n",
      "A:Hello B:hiya\n",
      "B:hi A:hEY\n",
      "A:Hello||Ihaveonemorningperson. B:Ihave1indoorsandkathleen.\n",
      "B:allofmyfriendslikemornings A:Mostofmineareindoors.\n",
      "A:Myfriendprefersafternoonworksattrtholdings. B:SELECT1\n",
      "B:whatistheirname? A:Ihaveonemorningandrestafternoon.\n",
      "A:Likesindoors. B:Doyouhaveanyfriendworkingatlhawaiian?\n",
      "B:whatisyourfiendwholikesmorningname? A:Idon’tknowJustin\n",
      "A:Theyworkfortrtholdings. B:Ihave1alexander.\n",
      "B:SELECT2 A:SELECT7...\n",
      "Table 6: Examples of human-bot chats. The mutual friend is highlighted in blue in each KB. Bots’\n",
      "utterances are in bold and selected items are represented by item IDs. Only the first half of the human-\n",
      "Rulechatisshownduetolimitedspace. Multipleutterancesofoneagentraeseparatedby||.\n",
      "Model (cid:96) vide unique opportunities at the interface of tra-\n",
      "ditional task-oriented dialogue and open-domain\n",
      "DynoNet(K=2) 2.16\n",
      "DynoNet(K=1) 2.20 chat. We also offered DynoNet as a promising\n",
      "DynoNet(K=0) 2.26\n",
      "means for open-ended dialogue state representa-\n",
      "DynoNet(K=2)w/oentityabstraction 2.21\n",
      "tion. Our dataset facilitates the study of prag-\n",
      "Table 7: Ablations of our model on the dev matics and human strategies in dialogue—a good\n",
      "set show the importance of entity abstraction and steppingstonetowardslearningmorecomplexdi-\n",
      "messagepassing(K = 2). aloguessuchasnegotiation.\n",
      "Acknowledgments. This work is supported by\n",
      "avoidgeneratingfalseorcontradictingstatements, DARPA Communicating with Computers (CwC)\n",
      "which is currently a weakness of neural models. programunderAROprimecontractno. W911NF-\n",
      "Our model is mostly accurate when generating 15-1-0462. Mike Kayser worked on an early ver-\n",
      "facts and answering existence questions about a sion of the project while he was at Stanford. We\n",
      "single entity, but will need a more advanced at- alsothankmembersoftheStanfordNLPgroupfor\n",
      "tention mechanism for generating utterances in- insightfuldiscussions.\n",
      "volving multiple entities, e.g., attending to items\n",
      "or attributes first, then selecting entities; generat- Reproducibility. All code, data, and\n",
      "inghigh-levelconceptsbeforecomposingthemto experiments for this paper are avail-\n",
      "naturaltokens(Serbanetal.,2017a). able on the CodaLab platform: https:\n",
      "In conclusion, we believe the symmetric col- //worksheets.codalab.org/worksheets/\n",
      "laborative dialogue setting and our dataset pro- 0xc757f29f5c794e5eb7bfa8ca9c945573.\n",
      "References J.Li,M.Galley,C.Brockett,J.Gao,andW.B.Dolan.\n",
      "2016b. A diversity-promoting objective function\n",
      "S. Afantenos, N. Asher, F. Benamara, A. Cadilhac, for neural conversation models. In Human Lan-\n",
      "C.De´gremont,P.Denis,M.Guhe,S.Keizer,A.Las- guageTechnologyandNorthAmericanAssociation\n",
      "carides,O.Lemon,P.Muller,S.Paul,V.Rieser,and forComputationalLinguistics(HLT/NAACL).\n",
      "L.Vieu.2012. Developingacorpusofstrategiccon-\n",
      "versationinthesettlersofcatan. InSeineDial2012- J.Li,W.Monroe,A.Ritter,D.Jurafsky,M.Galley,and\n",
      "The16thWorkshopontheSemanticsandPragmat- J.Gao.2016c. Deepreinforcementlearningfordia-\n",
      "icsofDialogue. loguegeneration. InEmpiricalMethodsinNatural\n",
      "LanguageProcessing(EMNLP).\n",
      "L. E. Asri, H. Schulz, S. Sharma, J. Zumer, J. Har-\n",
      "ris, E. Fine, R. Mehrotra, and K. Suleman. 2016. X. Li, Z. C. Lipton, B. Dhingra, L. Li, J. Gao,\n",
      "Frames: A corpus for adding memory to goal- and Y. Chen. 2016d. A user simulator for task-\n",
      "oriented dialogue systems. Maluuba Technical Re- completiondialogues. arXiv.\n",
      "port.\n",
      "C.Liu,R.Lowe,I.V.Serban,M.Noseworthy,L.Char-\n",
      "D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural lin,andJ.Pineau.2016. HowNOTtoevaluateyour\n",
      "machinetranslationbyjointlylearningtoalignand dialogue system: An empirical study of unsuper-\n",
      "translate. In International Conference on Learning visedevaluationmetricsfordialogueresponsegen-\n",
      "Representations(ICLR). eration. InEmpiricalMethodsinNaturalLanguage\n",
      "Processing(EMNLP).\n",
      "A. Bordes and J. Weston. 2017. Learning end-to-end\n",
      "goal-oriented dialog. In International Conference R.T.Lowe,N.Pow,I.Serban,L.Charlin,C.Liu,and\n",
      "onLearningRepresentations(ICLR). J.Pineau.2017. TrainingEnd-to-Enddialoguesys-\n",
      "temswiththeubuntudialoguecorpus. Dialogueand\n",
      "B. Dhingra, L. Li, X. Li, J. Gao, Y. Chen, F. Ahmed, Discourse8.\n",
      "andL.Deng.2017. End-to-endreinforcementlearn-\n",
      "ingofdialogueagentsforinformationaccess. InAs- H. Mei, M. Bansal, and M. R. Walter. 2016. What\n",
      "sociationforComputationalLinguistics(ACL). to talk about and how? selective generation using\n",
      "LSTMs with coarse-to-fine alignment. In Human\n",
      "LanguageTechnologyandNorthAmericanAssocia-\n",
      "J.Duchi,E.Hazan,andY.Singer.2010. Adaptivesub-\n",
      "tionforComputationalLinguistics(HLT/NAACL).\n",
      "gradient methods for online learning and stochastic\n",
      "optimization. In Conference on Learning Theory\n",
      "H.Mei,M.Bansal,andM.R.Walter.2017. Coherent\n",
      "(COLT).\n",
      "dialogue with attention-based language models. In\n",
      "Association for the Advancement of Artificial Intel-\n",
      "M.Henaff,J.Weston,A.Szlam,A.Bordes,andY.Le-\n",
      "ligence(AAAI).\n",
      "Cun. 2017. Tracking the world state with recur-\n",
      "rent entity networks. In International Conference\n",
      "C.Potts.2012. Goal-drivenanswersintheCardsdia-\n",
      "onLearningRepresentations(ICLR).\n",
      "loguecorpus. InProceedingsofthe30thWestCoast\n",
      "ConferenceonFormalLinguistics.\n",
      "E. Ivanovic. 2005. Dialogue act tagging for instant\n",
      "messaging chat sessions. In Association for Com-\n",
      "I. Serban, T. Klinger, G. Tesauro, K. Talamadupula,\n",
      "putationalLinguistics(ACL).\n",
      "B. Zhou, Y. Bengio, and A. C. Courville. 2017a.\n",
      "Multiresolution recurrent neural networks: An ap-\n",
      "R.JiaandP.Liang.2016. Datarecombinationforneu-\n",
      "plication to dialogue response generation. In Asso-\n",
      "ral semantic parsing. In Association for Computa-\n",
      "ciationfortheAdvancementofArtificialIntelligence\n",
      "tionalLinguistics(ACL).\n",
      "(AAAI).\n",
      "S. Keizer, M. Guhe, H. Cuayahuitl, I. Efstathiou, I. Serban, A. Sordoni, R. Lowe, L. Charlin, J. Pineau,\n",
      "K. Engelbrecht, M. Dobre, A. Lascarides, and A.C.Courville,andY.Bengio.2017b. Ahierarchi-\n",
      "O. Lemon. 2017. Evaluating persuasion strategies callatentvariableencoder-decodermodelforgener-\n",
      "anddeepreinforcementlearningmethodsfornego- atingdialogues. InAssociationfortheAdvancement\n",
      "tiationdialogueagents. InEuropeanAssociationfor ofArtificialIntelligence(AAAI).\n",
      "ComputationalLinguistics(EACL).\n",
      "I. V. Serban, R. Lowe, L. Charlin, and J. Pineau.\n",
      "C.Kiddon,L.S.Zettlemoyer,andY.Choi.2016. Glob- 2015a. A survey of available corpora for build-\n",
      "ally coherent text generation with neural checklist ing data-driven dialogue systems. arXiv preprint\n",
      "models. InEmpiricalMethodsinNaturalLanguage arXiv:1512.05742.\n",
      "Processing(EMNLP).\n",
      "I.V.Serban,A.Sordoni,Y.Bengio,A.Courville,and\n",
      "J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan. J.Pineau.2015b. Buildingend-to-enddialoguesys-\n",
      "2016a. Apersona-basedneuralconversationmodel. tems using generative hierarchical neural network\n",
      "InAssociationforComputationalLinguistics(ACL). models. arXivpreprintarXiv:1507.04808.\n",
      "L. Shang, Z. Lu, and H. Li. 2015. Neural responding\n",
      "machineforshort-textconversation. InAssociation\n",
      "forComputationalLinguistics(ACL).\n",
      "A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji,\n",
      "M. Mitchell, J. Nie, J. Gao, and B. Dolan. 2015.\n",
      "A neural network approach to context-sensitive\n",
      "generation of conversational responses. In North\n",
      "American Association for Computational Linguis-\n",
      "tics(NAACL).\n",
      "P. Su, M. Gasic, N. Mrksic, L. M. Rojas-Barahona,\n",
      "S.Ultes,D.Vandyke,T.Wen,andS.J.Young.2016.\n",
      "Continuouslylearningneuraldialoguemanagement.\n",
      "arXivpreprintarXiv:1606.02689.\n",
      "A.Vogel, M.Bodoia, C.Potts, andD.Jurafsky.2013.\n",
      "Emergenceofgriceanmaximsfrommulti-agentde-\n",
      "cision theory. In North American Association for\n",
      "Computational Linguistics (NAACL). pages 1072–\n",
      "1081.\n",
      "T. Wen, M. Gasic, N. Mrksic, L. M. Rojas-Barahona,\n",
      "P. Su, S. Ultes, D. Vandyke, and S. Young. 2017.\n",
      "Anetwork-basedend-to-endtrainabletask-oriented\n",
      "dialoguesystem. InEuropeanAssociationforCom-\n",
      "putationalLinguistics(EACL).\n",
      "J. D. Williams, K. Asadi, and G. Zweig. 2017. Hy-\n",
      "brid code networks: Practical and efficient end-to-\n",
      "end dialog control with supervised and reinforce-\n",
      "ment learning. In Association for Computational\n",
      "Linguistics(ACL).\n",
      "J.D.Williams,A.Raux,andM.Henderson.2016. The\n",
      "dialogstatetrackingchallengeseries: Areview. Di-\n",
      "alogueandDiscourse7.\n",
      "J. D. Williams and S. Young. 2007. Partially observ-\n",
      "able Markov decision processes for spoken dialog\n",
      "systems. ComputerSpeech&Language21(2):393–\n",
      "422.\n",
      "S.Young, M.Gasic, B.Thomson, andJ.D.Williams.\n",
      "2013. POMDP-based statistical spoken dialog\n",
      "systems: A review. Proceedings of the IEEE\n",
      "101(5):1160–1179.\n",
      "A KnowledgeBaseSchema each dialogue participant. We instruct people to\n",
      "play intelligently, to refrain from brute-force tac-\n",
      "TheattributesetAfortheMutualFriendstaskcon-\n",
      "tics (e.g., mentioning every attribute value), and\n",
      "tainsname, school, major, company, hobby, time-\n",
      "to use grammatical sentences. To discourage ran-\n",
      "of-day preference, and location preference. Each\n",
      "dom guessing, we prevent users from selecting a\n",
      "attributeahasasetofpossiblevalues(entities)E.\n",
      "a friend (item) more than once every 10 seconds.\n",
      "Forname,school,major,company,andhobby,we\n",
      "Each worker was paid $0.35 for a successful di-\n",
      "collectedalargesetofvaluesfromvariousonline\n",
      "alogue within a 5-minute time limit. We log each\n",
      "sources.14 We used three possible values (morn-\n",
      "utterance in the dialogue along with timing infor-\n",
      "ing, afternoon, and evening) for the time-of-day\n",
      "mation.\n",
      "preference, and two possible values (indoors and\n",
      "outdoors)forthelocationpreference.\n",
      "D EntityLinkingandRealization\n",
      "B ScenarioGeneration\n",
      "We use a rule-based lexicon to link text spans to\n",
      "entities. For every entity in the schema, we com-\n",
      "Wegeneratescenariosrandomlytovarytaskcom-\n",
      "putedifferentvariationsofitscanonicalname,in-\n",
      "plexity and elicit linguistic and strategic variants.\n",
      "A scenario S is characterized by the number of cluding acronyms, strings with a certain edit dis-\n",
      "items (N ), the attribute set (A ) whose size is tance,prefixes,andmorphologicalvariants. Given\n",
      "S S\n",
      "M, and the values for each attribute a ∈ A in a text span, a set of candidate entities is returned\n",
      "S S\n",
      "bystringmatching. Aheuristicrankerthenscores\n",
      "thetwoKBs.\n",
      "eachcandidate(e.g.,consideringwhetherthespan\n",
      "Ascenarioisgeneratedasfollows.\n",
      "is a substring of a candidate, the edit distance be-\n",
      "1. Sample N S and M S uniformly from tweenthespanandacandidateetc.). Thehighest-\n",
      "{5,...,12}and{3,4}respectively. scoringcandidateisreturned.\n",
      "A linked entity is considered as a single token\n",
      "2. Generate A by sampling M attributes\n",
      "S S and its surface form is ignored in all models. At\n",
      "withoutreplacementfromA.\n",
      "generation time, we realize an entity by sampling\n",
      "3. For each attribute a ∈ A, sample the con- fromtheempiricaldistributionofitssurfaceforms\n",
      "S\n",
      "centration parameter α uniformly from the inthetrainingset.\n",
      "a\n",
      "set{0.3,1,3}.\n",
      "E UtteranceCategorization\n",
      "4. GeneratetwoKBsbysamplingN valuesfor\n",
      "S\n",
      "eachattributeafromaDirichlet-multinomial\n",
      "We categorize utterances into inform, ask, answer,\n",
      "distribution over the value set E with the\n",
      "greeting,apologyheuristicallybypatternmatching.\n",
      "a\n",
      "concentrationparameterα.\n",
      "a\n",
      "• Anaskutteranceasksforinformationregard-\n",
      "WerepeatthelaststepuntilthetwoKBshaveone ing the partner’s KB. We detect these utter-\n",
      "uniquecommonitem. ances by checking for the presence of a ‘?’\n",
      "and/or a question word like “do”, “does”,\n",
      "C ChatInterface\n",
      "“what”,etc.\n",
      "Inordertocollectreal-timedialoguebetweenhu-\n",
      "• An inform utterance provides information\n",
      "mans, we set up a web server and redirect AMT\n",
      "about the agent’s KB. We define it as an ut-\n",
      "workers to our website. Visitors are randomly\n",
      "terancesthatmentionsentitiesintheKBand\n",
      "pairedupastheyarrive. Foreachpair,wechoose\n",
      "isnotanaskutterance.\n",
      "a random scenario, and randomly assign a KB to\n",
      "14Names: https://www.ssa.gov/oact/ • An answer utterance simply provides a posi-\n",
      "babynames/decades/century.html tive/negativeresponsetoaquestion,contain-\n",
      "Schools: http://doors.stanford.edu/˜sr/\n",
      "ingwordslike“yes”,“no”,“nope”,etc.\n",
      "universities.html\n",
      "Majors:http://www.a2zcolleges.com/majors\n",
      "Companies: https://en.wikipedia.org/wiki/ • A greeting utterance contains words like “hi”\n",
      "List_of_companies_of_the_United_States\n",
      "or“hello”;itoftenoccursatthebeginningof\n",
      "Hobbies: https://en.wikipedia.org/wiki/\n",
      "List_of_hobbies adialogue.\n",
      "• An apology utterance contains the word G Rule-basedSystem\n",
      "“sorry”, which is typically associated with\n",
      "The rule-based bot takes the following actions:\n",
      "correctionsandwrongselections.\n",
      "greeting, informing or asking about a set of en-\n",
      "tities,answeringaquestion,andselectinganitem.\n",
      "See Table 2 and Table 1 for examples of these ut-\n",
      "The set of entities to inform/ask is sampled ran-\n",
      "terancetypes.\n",
      "domlygiventheentityweights. Initially,eachen-\n",
      "tity is weighted by its count in the KB. We then\n",
      "F Strategy\n",
      "increment or decrement weights of entities men-\n",
      "tioned by the partner and its related entities (in\n",
      "During scenario generation, we varied the num-\n",
      "the same row or column), depending on whether\n",
      "berofattributes,thenumberofitemsineachKB,\n",
      "the mention is positive or negative. A negative\n",
      "and the distribution of values for each attribute.\n",
      "mention contains words like “no”, “none”, “n’t”\n",
      "We find that as the number of items and/or at-\n",
      "etc. Similarly,eachitemhasaninitialweightof1,\n",
      "tributes grows, the dialogue length and the com-\n",
      "whichisupdateddependingonthepartner’smen-\n",
      "pletion time also increase, indicating that the task\n",
      "tionofitsattributes.\n",
      "becomes harder. We also anticipated that vary-\n",
      "If there exists an item with weight larger than\n",
      "ing the value of α would impact the overall strat-\n",
      "1, the bot selects the highest-weighted item with\n",
      "egy(forexample,theorderinwhichattributesare\n",
      "probability 0.3. If a question is received, the\n",
      "mentioned) since α controls the skewness of the\n",
      "bot informs facts of the entities being asked, e.g.,\n",
      "distributionofvaluesforanattribute.\n",
      "“anyone went to columbia?”, “I have 2 friends\n",
      "On examining the data, we find that humans\n",
      "who went to columbia”. Otherwise, the bot sam-\n",
      "tendtofirstmentionattributeswithamoreskewed\n",
      "ples an entity set and randomly chooses between\n",
      "(i.e., less uniform) distribution of values. Specif-\n",
      "informingandaskingabouttheentities.\n",
      "ically, we rank the α values of all attributes in a\n",
      "All utterances are generated by sentence tem-\n",
      "scenario (see step 3 in Section B), and bin them\n",
      "plates, and parsing of the partner’s utterance is\n",
      "into 3 distribution groups—least uniform, medium,\n",
      "donebyentitylinkingandpatternmatching(Sec-\n",
      "and most uniform, according to the ranking where\n",
      "tionE).\n",
      "higherαvaluescorrespondstomoreuniformdis-\n",
      "tributions.15 In Figure 4, we plot the histogram\n",
      "H Turn-takingRules\n",
      "ofthedistributiongroupofthefirst-mentionedat-\n",
      "tribute in a dialogues, which shows that skewed Turn-taking is universal in human conversations\n",
      "attributesarementionedmuchmorefrequently. andthebotneedstodecidewhento‘talk’(sendan\n",
      "utterance). Topreventthebotfromgeneratingut-\n",
      "terances continuously and forming a monologue,\n",
      "we allow it to send at most one utterance if the\n",
      "utterance contains any entity, and two utterances\n",
      "otherwise. Whensendingmorethanoneutterance\n",
      "inaturn,thebotmustwaitfor1to2secondsinbe-\n",
      "tween. Inaddition, afteranutteranceisgenerated\n",
      "bythemodel(almostinstantly),thebotmusthold\n",
      "on for some time to simulate message typing be-\n",
      "fore sending. We used a typing speed of 7 chars /\n",
      "secandaddedanadditionalrandomdelaybetween\n",
      "Figure 4: Histogram of the first attribute men- 0to1.5safter‘typing’. Therulesareappliedtoall\n",
      "tioned in a dialogue. People tend to first mention models.\n",
      "attributes from very skewed (non-uniform) distri-\n",
      "I AdditionalHuman-BotDialogue\n",
      "butions.\n",
      "We show another set of human-bot/human chats\n",
      "in Table 8. In this scenario, the distribution of\n",
      "15Forscenarioswith3attributes,eachgroupcontainsone\n",
      "values are more uniform compared to Table 6.\n",
      "attributes. For scenarios with 4 attributes, we put the two\n",
      "attributeswithrankingsinthemiddletomedium. Nevertheless, we see that StanoNet and DynoNet\n",
      "stilllearnedtostartfromrelativelyhigh-frequency\n",
      "entities. They also appear more cooperative and\n",
      "mentions relevant entities in the dialogue context\n",
      "comparedtoRule.\n",
      "J HistogramsofRatingsfromHuman\n",
      "Evaluations\n",
      "The histograms of ratings from partner and third-\n",
      "party evaluations is shown in Figure 5 and Fig-\n",
      "ure 6 respectively. As these figures show, there\n",
      "are some obvious discrepancies between the rat-\n",
      "ingsmadebyagentswhochattedwiththebotand\n",
      "those made by an ‘objective’ third party. These\n",
      "ratingsprovidesomeinterestinginsightsintohow\n",
      "dialogue participants in this task setting perceive\n",
      "theirpartners,andwhatconstitutesa‘human-like’\n",
      "ora‘fluent’partner.\n",
      "K ExampleCommentsfromPartnerand\n",
      "Third-partyEvaluations\n",
      "In Table 9, we show several pairs of ratings and\n",
      "comments on human-likeness for the same dia-\n",
      "logue from both the partner evaluation and the\n",
      "third-party evaluation. As a conversation partic-\n",
      "ipant, the dialogue partner often judges from the\n",
      "cooperation and strategy perspective, whereas the\n",
      "third-partyevaluatorreliesmoreonlinguisticfea-\n",
      "tures(e.g.,length,spelling,formality).\n",
      "60\n",
      "50\n",
      "40\n",
      "30\n",
      "20\n",
      "10\n",
      "0\n",
      "1 2 3 4 5\n",
      "egatnecreP\n",
      "Fluency Correctness\n",
      "Human\n",
      "Rule\n",
      "StanoNet\n",
      "DynoNet\n",
      "1 2 3 4 5\n",
      "60\n",
      "50\n",
      "40\n",
      "30\n",
      "20\n",
      "10\n",
      "0\n",
      "1 2 3 4 5\n",
      "egatnecreP\n",
      "Cooperation Human-likeness\n",
      "1 2 3 4 5\n",
      "Figure5: Histogramofratings(higherisbetter)fromdialoguepartners. DynoNetisbetterthanallother\n",
      "systems,especiallyincooperation.\n",
      "FriendsofB\n",
      "FriendsofA ID Major Company Hobby\n",
      "ID Major Company Hobby ForeignLanguage\n",
      "1 GannettCompany Roadbiking\n",
      "TeacherEducation\n",
      "Metallurgical\n",
      "1 GannettCompany Candlemaking Mathematics\n",
      "Engineering 2 ElectronicArts Astronomy\n",
      "Education\n",
      "2 BusinessEducation ElectronicArts Gunsmithing\n",
      "Petroleum WesternSugar\n",
      "Parks 3 Candlemaking\n",
      "3 Kenworth Watersports Engineering Cooperative\n",
      "Administration\n",
      "American\n",
      "Mathematics Mathematics\n",
      "4 ElectronicArts Astronomy 4 Broadcasting Roadbiking\n",
      "Education Education\n",
      "Company\n",
      "Agricultural\n",
      "5 AVST Fieldhockey Petroleum WesternSugar\n",
      "Mechanization 5 Roadbiking\n",
      "Engineering Cooperative\n",
      "Mathematics\n",
      "6 AVST Shopping Petroleum\n",
      "Education 6 A&WRestaurants Golfing\n",
      "Engineering\n",
      "Parks Foreignlanguage\n",
      "7 AdobeSystems American\n",
      "Administration learning Petroleum\n",
      "7 Broadcasting Origami\n",
      "Agricultural BroncoWine Engineering\n",
      "8 Shopping Company\n",
      "Mechanization Company\n",
      "TheWaltDisney\n",
      "Metallurgical Foreignlanguage 8 Russian Astronomy\n",
      "9 ElectronicArts Company\n",
      "Engineering learning\n",
      "Petroleum TheWaltDisney\n",
      "Mathematics 9 Origami\n",
      "10 ElectronicArts Poi Engineering Company\n",
      "Education\n",
      "Protestant\n",
      "10 AcmeBrick Astronomy\n",
      "Affiliation\n",
      "A:Human B:Human A:Human B:DynoNet\n",
      "A:hi B:hi\n",
      "B:hi A:hey\n",
      "A:AnyfriendsworkatAVST? B:ihavethreefriendswholikeroadbiking\n",
      "B:petroleumengi??||no A:ihavet<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  70589,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['MutualFriend', 'Task-Oriented Dialogue']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  WesternSugar\n",
      "Mechanization 5 Roadbiking\n",
      "Engineering Cooperative\n",
      "Mathematics\n",
      "6 AVST Shopping Petroleum\n",
      "Education 6 A&WRestaurants Golfing\n",
      "Engineering\n",
      "Parks Foreignlanguage\n",
      "7 AdobeSystems American\n",
      "Administration learning Petroleum\n",
      "7 Broadcasting Origami\n",
      "Agricultural BroncoWine Engineering\n",
      "8 Shopping Company\n",
      "Mechanization Company\n",
      "TheWaltDisney\n",
      "Metallurgical Foreignlanguage 8 Russian Astronomy\n",
      "9 ElectronicArts Company\n",
      "Engineering learning\n",
      "Petroleum TheWaltDisney\n",
      "Mathematics 9 Origami\n",
      "10 ElectronicArts Poi Engineering Company\n",
      "Education\n",
      "Protestant\n",
      "10 AcmeBrick Astronomy\n",
      "Affiliation\n",
      "A:Human B:Human A:Human B:DynoNet\n",
      "A:hi B:hi\n",
      "B:hi A:hey\n",
      "A:AnyfriendsworkatAVST? B:ihavethreefriendswholikeroadbiking\n",
      "B:petroleumengi??||no A:ihavetwofriendswholikeforeignlanguage\n",
      "A:nopetroleum learning\n",
      "B:russianasmajor? B:noforeignlanguagelearninghere\n",
      "A:Nope A:Ihavenofriendswholikeroadbiking\n",
      "B:protestantafiil?||math? B:dotheymajorinforeignlanguage?\n",
      "A:Ihavetwomath A:No||no\n",
      "B:SELECT2 B:petroleumengineering?\n",
      "A:oneworksatElectronicArts||SELECT10 A:NobutIhavefourfriendswhoworkforthe\n",
      "B:SELECT1 ElectronicArtscompany\n",
      "A:SELECT4 B:SELECT2 B:SELECT2 A:SELECT4\n",
      "A:StanoNet B:Human A:Human B:Rule\n",
      "B:hiya||ihaveoneforeignlanguageandgannett\n",
      "A:ihavetwoavst\n",
      "B:doyouhaveanyacmebrickandastronomy?\n",
      "A:hi||doyouhaveanyfriendsthatworkat A:manymathspeople||twoareforeignlanguage\n",
      "electronicarts? B:doyouhaveanypetroleumengineeringand\n",
      "B:Oneandtheylikeastronomy americanbroadcastingcompany?\n",
      "A:SELECT10 A:no||electronicarts\n",
      "B:SELECT2 A:SELECT4 B:SELECT1\n",
      "A:avst\n",
      "B:doyouhaveanydisneyorrestaurant?\n",
      "...\n",
      "Table8: Examplehuman-botchats. ThemutualfriendishighlightedinblueineachKB.Bots’utterances\n",
      "areinboldandselecteditemsarerepresentedbyitemIDs. Onlythefirsthalfofthehuman-Rulechatis\n",
      "shownduetospacelimit. Multipleutterancesofoneagentisseparatedby||.\n",
      "70\n",
      "60\n",
      "50\n",
      "40\n",
      "30\n",
      "20\n",
      "10\n",
      "0\n",
      "1 2 3 4 5\n",
      "egatnecreP\n",
      "Fluency Correctness\n",
      "Human\n",
      "Rule\n",
      "StanoNet\n",
      "DynoNet\n",
      "1 2 3 4 5\n",
      "70\n",
      "60\n",
      "50\n",
      "40\n",
      "30\n",
      "20\n",
      "10\n",
      "0\n",
      "1 2 3 4 5\n",
      "egatnecreP\n",
      "Cooperation Human-likeness\n",
      "1 2 3 4 5\n",
      "Figure 6: Histogram of ratings (higher is better) from third-party evaluators. Differences between\n",
      "systemsarelesssignificant.\n",
      "Partnerevaluation(1perdialogue) Third-partyevaluation(5perdialogue)\n",
      "System\n",
      "Human Comments Human Justifications\n",
      "-youhaveanyfriendswhowenttomonmouth?\n",
      "-Theflowwasniceandtheywereabletodiscernthecorrect\n",
      "answers.\n",
      "Goodpartner.Easyto\n",
      "Human 4 4.6 -humanlikebecauseofinteractiontalking\n",
      "workwith\n",
      "-Answersarehumanlike,notrobotic.Uses”hiya”tobegin\n",
      "conversation,moreofawarmtone.\n",
      "-morehumanthancomputerAgent2:hiyaAgent1:Hey\n",
      "-agent2lookedhumantome\n",
      "-definitelyhuman\n",
      "-A2couldbereplacedwitharobotwithoutnoticeable\n",
      "difference.\n",
      "Rule 2 Didn’tlistentome 4 -TheyspokeandbehavedasIoranyhumanwouldinthis\n",
      "situation.\n",
      "-Theagentjustseemstobegoingthroughthemotions,which\n",
      "givesmetheideathattheagentdoesn’texbithumanlike\n",
      "characteristics.\n",
      "-Nodjarum–Thisdoesn’tmakesenseinthiscontext,so\n",
      "doesn’tseemtobewrittenbyahuman.\n",
      "Tookforeveranddidn’t -humanlikebecauseofslightmispellingss\n",
      "StanoNet 5 reallyrespondcorrectly 3.5 -Cantelltheyarelikelyhumanbutjustnotveryverbose\n",
      "toquestions. -Theirterseconversionleanstothinkingtheywereeithernot\n",
      "payingattentionornothuman.\n",
      "-Theshortvaguesentencesareveryhumanlikemistakes.\n",
      "-Agent1isveryhumanlikebasedonthewaytheytypedand\n",
      "thefactthattheywerebeingdeceiving.\n",
      "IrepliedtwicethatI -Prettyresponsiveandlogicalprogression,butit’sverystilted\n",
      "DynoNet 4 onlyhadindoorfriends 3.8 sounding\n",
      "andwasignored. -idonothaveajose\n",
      "-Agentgivesnormalhumanresponses,“noangelaidon’t”\n",
      "-agent1waslookinglikeahumanlike\n",
      "Table 9: Comparison of ratings and comments on human-likeness from partners and third-party eval-\n",
      "uators. Each row contains results for the same dialogue. For the partner evaluation, we ask the human\n",
      "partnertoprovideasingle,optionalcommentattheendoftheconversation. Forthethird-partyevalua-\n",
      "tion,weaskfiveTurkerstorateeachdialogueandreportthemeanscore;theymustprovidejustification\n",
      "forratingsineachaspect. Fromthecomments,weseethatdialoguepartnersfocusmoreoncooperation\n",
      "andeffectiveness,whereasthird-partyevaluatorsfocusmoreonlinguisticfeaturessuchasverbosityand\n",
      "informality.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   4776,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Question-answering']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Learning Symmetric Collaborative Dialogue Agents with Dynamic\n",
      "Knowledge Graph Embeddings\n",
      "HeHe and AnushaBalakrishnan and MihailEric and PercyLiang\n",
      "ComputerScienceDepartment,StanfordUniversity\n",
      "{hehe,anusha28,meric,pliang}@cs.stanford.edu\n",
      "Abstract FriendsofagentA:\n",
      "Name School Major Company\n",
      "We study a symmetric collaborative dia-\n",
      "Jessica Columbia ComputerScience Google\n",
      "logue setting in which two agents, each\n",
      "Josh Columbia Linguistics Google\n",
      "withprivateknowledge,muststrategically............\n",
      "communicate to achieve a common goal.\n",
      "A:Hi!MostofmyfriendsworkforGoogle\n",
      "The open-ended dialogue state in this set-\n",
      "B:doyouhaveanyonewhowenttocolumbia?\n",
      "ting poses new challenges for existing di- A:Hello?\n",
      "A:IhaveJessicaafriendofmine\n",
      "alogue systems. We collected a dataset\n",
      "A:andJosh,bothwenttocolumbia\n",
      "of 11K human-human dialogues, which B:oranyoneworkingatapple?\n",
      "exhibits interesting lexical, semantic, and B:SELECT(Jessica,Columbia,ComputerScience,Google)\n",
      "A:SELECT(Jessica,Columbia,ComputerScience,Google)\n",
      "strategic elements. To model both struc-\n",
      "tured knowledge and unstructured lan-\n",
      "Figure1: AnexampledialoguefromtheMutual-\n",
      "guage,weproposeaneuralmodelwithdy-\n",
      "Friends task in which two agents, A and B, each\n",
      "namic knowledge graph embeddings that\n",
      "givenaprivatelistofafriends,trytoidentifytheir\n",
      "evolve as the dialogue progresses. Au-\n",
      "mutual friend. Our objective is to build an agent\n",
      "tomatic and human evaluations show that\n",
      "that can perform the task with a human. Cross-\n",
      "ourmodelisbothmoreeffectiveatachiev-\n",
      "talk(Section2.3)isitalicized.\n",
      "ing the goal and more human-like than\n",
      "baselineneuralandrule-basedmodels.\n",
      "of systems, we focus on a symmetric collabora-\n",
      "1 Introduction\n",
      "tive dialogue setting, which is task-oriented but\n",
      "Current task-oriented dialogue systems (Young encourages open-ended dialogue acts. In our set-\n",
      "etal.,2013;Wenetal.,2017;Dhingraetal.,2017) ting, two agents, each with a private list of items\n",
      "require a pre-defined dialogue state (e.g., slots with attributes, must communicate to identify the\n",
      "such as food type and price range for a restau- uniqueshareditem. ConsiderthedialogueinFig-\n",
      "rantsearchingtask)andafixedsetofdialogueacts ure 1, in which two people are trying to find their\n",
      "(e.g.,request,inform). However,humanconversa- mutual friend. By asking “do you have anyone\n",
      "tionoftenrequiresricherdialoguestatesandmore who went to columbia?”, B is suggesting that she\n",
      "nuanced, pragmatic dialogue acts. Recent open- hassomeColumbiafriends,andthattheyprobably\n",
      "domain chat systems (Shang et al., 2015; Serban work at Google. Such conversational implicature\n",
      "etal.,2015b;Sordonietal.,2015;Lietal.,2016a; is lost when interpreting the utterance as simply\n",
      "Lowe et al., 2017; Mei et al., 2017) learn a map- an information request. In addition, it is hard to\n",
      "ping directly from previous utterances to the next define a structured state that captures the diverse\n",
      "utterance. Whilethesemodelscaptureopen-ended semanticsinmanyutterances(e.g.,defining“most\n",
      "aspectsofdialogue,thelackofstructureddialogue of”,“mightbe”;seedetailsinTable1).\n",
      "state prevents them from being directly applied To model both structured and open-ended con-\n",
      "to settings that require interfacing with structured text, we propose the Dynamic Knowledge Graph\n",
      "knowledge. Network(DynoNet),inwhichthedialoguestateis\n",
      "Inordertobridgethegapbetweenthetwotypes modeledasaknowledgegraphwithanembedding\n",
      "7102\n",
      "rpA\n",
      "42\n",
      "]LC.sc[\n",
      "1v03170.4071:viXra\n",
      "for each node (Section 3). Our model is similar 2.1 TaskDefinition\n",
      "to EntNet (Henaff et al., 2017) in that node/entity\n",
      "In the symmetric collaborative dialogue setting,\n",
      "embeddings are updated recurrently given new\n",
      "there are two agents, A and B, each with a pri-\n",
      "utterances. The difference is that we structure\n",
      "vate knowledge base—KB and KB, respec-\n",
      "A B\n",
      "entities as a knowledge graph; as the dialogue\n",
      "tively. Each knowledge base includes a list of\n",
      "proceeds, new nodes are added and new context\n",
      "items, where each item has a value for each at-\n",
      "is propagated on the graph. An attention-based\n",
      "tribute. For example, in the MutualFriends set-\n",
      "mechanism (Bahdanau et al., 2015) over the node\n",
      "ting, Figure 1, items are friends and attributes are\n",
      "embeddings drives generation of new utterances.\n",
      "name, school, etc. There is a shared item that A\n",
      "Ourmodel’suseofknowledgegraphscapturesthe\n",
      "and B both have; their goal is to converse with\n",
      "grounding capability of classic task-oriented sys-\n",
      "eachothertodeterminetheshareditemandselect\n",
      "temsandthegraphembeddingprovidestherepre-\n",
      "it. Formally, an agent is a mapping from its pri-\n",
      "sentationalflexibilityofneuralmodels.\n",
      "vateKBandthedialoguethusfar(sequenceofut-\n",
      "The naturalness of communication in the sym-\n",
      "terances)tothenextutterancetogenerateorase-\n",
      "metric collaborative setting enables large-scale\n",
      "lection. Adialogueisconsideredsuccessfulwhen\n",
      "data collection: We were able to crowdsource\n",
      "both agents correctly select the shared item. This\n",
      "around 11K human-human dialogues on Amazon\n",
      "settinghasparallelsinhuman-computercollabora-\n",
      "Mechanical Turk (AMT) in less than 15 hours.1\n",
      "tion where each agent has complementary exper-\n",
      "We show that the new dataset calls for more flex-\n",
      "tise.\n",
      "iblerepresentationsbeyondfully-structuredstates\n",
      "(Section2.2). 2.2 Datacollection\n",
      "Inadditiontoconductingthethird-partyhuman\n",
      "Wecreatedaschemawith7attributesandapprox-\n",
      "evaluationadoptedbymostwork(Liuetal.,2016;\n",
      "imately3Kentities(attributevalues). Toelicitlin-\n",
      "Lietal.,2016b,c), wealsoconductpartnerevalu-\n",
      "guistic and strategic variants, we generate a ran-\n",
      "ation (Wen et al., 2017) where AMT workers rate\n",
      "dom scenario for each task by varying the num-\n",
      "theirconversationalpartners(otherworkersorour\n",
      "ber of items (5 to 12), the number attributes (3 or\n",
      "models) based on fluency, correctness, coopera-\n",
      "4),andthedistributionofvaluesforeachattribute\n",
      "tion, and human-likeness. We compare DynoNet\n",
      "(skewed to uniform). See Appendix A and B for\n",
      "with baseline neural models and a strong rule-\n",
      "detailsofschemaandscenariogeneration.\n",
      "basedsystem. TheresultsshowthatDynoNetcan\n",
      "perform the task with humans efficiently and nat-\n",
      "urally; it also captures some strategic aspects of\n",
      "human-humandialogues.\n",
      "The contributions of this work are: (i) a new\n",
      "symmetric collaborative dialogue setting and a\n",
      "large dialogue corpus that pushes the boundaries\n",
      "ofexistingdialoguesystems;(ii)DynoNet,which\n",
      "integrates semantically rich utterances with struc-\n",
      "turedknowledgetorepresentopen-endeddialogue\n",
      "states; (iii) multiple automatic metrics based on\n",
      "bot-bot chat and a comparison of third-party and\n",
      "partnerevaluation. Figure2: Screenshotofthechatinterface.\n",
      "2 SymmetricCollaborativeDialogue We crowdsourced dialogues on AMT by ran-\n",
      "domly pairing up workers to perform the task\n",
      "We begin by introducing a collaborative task be- within 5 minutes.2 Our chat interface is shown in\n",
      "tween two agents and describe the human-human\n",
      "Figure2. Todiscouragerandomguessing,wepre-\n",
      "dialoguecollectionprocess. Weshowthatourdata\n",
      "ventworkersfromselectingmorethanonceevery\n",
      "exhibitsdiverse,interestinglanguagephenomena.\n",
      "10seconds. Ourtaskwasverypopularandwecol-\n",
      "1The dataset is available publicly at https:// 2If the workers exceed the time limit, the dialogue is\n",
      "stanfordnlp.github.io/cocoa/. markedasunsuccessful(butstilllogged).\n",
      "Type % Easyexample Hardexample\n",
      "Iknowajudy./Ihavesomeonewho Aboutequalindoorandoutdoorfriends/metoo.his\n",
      "Inform 30.4\n",
      "studiedthebibleintheafternoon. majorisforestry/mightbekelly\n",
      "DoanyofthemlikePoi?/Whatdoesyourhenry Whatcanyoutellmeaboutourfriend?/Ormaybe\n",
      "Ask 17.7\n",
      "do? northparkcollege?\n",
      "Answer 7.4 Noneofminedid/Yup/Theydo./Samehere. yes3ofthem/Nohelikespoi/yesifbostoncollege\n",
      "Table 1: Main utterance types and examples. We show both standard utterances whose meaning can\n",
      "be represented by simple logical forms (e.g., ask(indoor)), and open-ended ones which require more\n",
      "complexlogicalforms(difficultpartsinbold). Textspanscorrespondingtoentitiesareunderlined.\n",
      "Phenomenon Example\n",
      "Coreference (IknowoneDebra)doessheliketheindoors?/(IhavetwofriendsnamedTIffany)atWorldairways?\n",
      "Coordination keepongoingwiththefashion/Ok.let’strysomethingelse./gobyhobby/great.selecthim.thanks!\n",
      "Chit-chat Yes,thatisgoodoleTerry./Allindoorsers!myfriendshatenature\n",
      "Categorization same,mostofminearefemaletoo/DoesanyofthemnamesstartwithB\n",
      "Correction IknowonefriendintoEmbroidery-hernameisEmily.Sorry–EmbroideryfriendisnamedMichelle\n",
      "Table2: Communicationphenomenainthedataset. Evidentpartsisinboldandtextspanscorresponding\n",
      "toanentityareunderlined. Forcoreference,theantecedentisinparentheses.\n",
      "lected11Kdialoguesoveraperiodof13.5hours.3 Someofthestandardonesarealsonon-trivialdue\n",
      "Of these, over 9K dialogues are successful. Un- tocoreferenceandlogicalcompositionality.\n",
      "successfuldialoguesareusuallytheresultofeither Ourdatasetalsoexhibitssomeinterestingcom-\n",
      "workerleavingthechatprematurely. munication phenomena. Coreference occurs fre-\n",
      "quently when people check multiple attributes\n",
      "2.3 Datasetstatistics\n",
      "of one item. Sometimes mentions are dropped,\n",
      "We show the basic statistics of our dataset in Ta- as an utterance simply continues from the part-\n",
      "ble 3. An utterance is defined as a message sent ner’s utterance. People occasionally use exter-\n",
      "byoneoftheagents. Theaverageutterancelength nalknowledgetogroupitemswithout-of-schema\n",
      "is short due to the informality of the chat, how- attributes (e.g., gender based on names, location\n",
      "ever,anagentusuallysendsmultipleutterancesin based on schools). We summarize these phenom-\n",
      "one turn. Some example dialogues are shown in ena in Table 2. In addition, we find 30% utter-\n",
      "Table6andAppendixI. ances involve cross-talk where the conversation\n",
      "does not progress linearly (e.g., italic utterances\n",
      "#dialogues 11157 in Figure 1), a common characteristic of online\n",
      "#completeddialogues 9041\n",
      "chat(Ivanovic,2005).\n",
      "Vocabularysize 5325\n",
      "Average#ofutterances 11.41 Onestrategicaspectofthistaskischoosingthe\n",
      "Averagetimetakenpertask(sec.) 91.18\n",
      "orderofattributestomention. Wefindthatpeople\n",
      "Averageutterancelength(tokens) 5.08\n",
      "Numberoflinguistictemplates4 41561 tendtostartfromattributeswithfeweruniqueval-\n",
      "ues, e.g., “all my friends like morning” given the\n",
      "Table3: StatisticsoftheMutualFriendsdataset. KB in Table 6, as intuitively it would help ex-\n",
      "B\n",
      "clude items quickly given fewer values to check.5\n",
      "We categorize utterances into coarse types— Weprovideamoredetailedanalysisofstrategyin\n",
      "inform, ask, answer, greeting, apology—by pattern Section4.2andAppendixF.\n",
      "matching (Appendix E). There are 7.4% multi-\n",
      "type utterances, and 30.9% utterances contain 3 DynamicKnowledgeGraphNetwork\n",
      "more than one entity. In Table 1, we show exam-\n",
      "The diverse semantics in our data motivates us\n",
      "ple utterances with rich semantics that cannot be\n",
      "to combine unstructured representation of the di-\n",
      "sufficiently represented by traditional slot-values.\n",
      "alogue history with structured knowledge. Our\n",
      "3Tasksareputupinbatches;thetotaltimeexcludesinter-\n",
      "valsbetweenbatches. 5Ourgoalistomodelhumanbehaviorthuswedonotdis-\n",
      "4Entitynamesarereplacedbytheirentitytypes. cusstheoptimalstrategyhere.\n",
      "Dynamic knowledge graph Graph Generator\n",
      "embedding\n",
      "anyonewentcolumbia Yes jessica and josh\n",
      "KB + Dialogue history\n",
      "… …\n",
      "jessica\n",
      "Name School Company\n",
      "1 S\n",
      "Item 1 Jessica Columbia Google columbia columbia\n",
      "Item 2 Josh Columbia Google N jessica\n",
      "google josh\n",
      "B: anyone went to columbia? 2 C google …\n",
      "…\n",
      "josh\n",
      "Attention + Copy\n",
      "Message passing path of columbia\n",
      "Figure3: Overviewofourapproach. First,theKBanddialoguehistory(entitiesinbold)ismappedto\n",
      "a graph. Here, an item node is labeled by the item ID and an attribute node is labeled by the attribute’s\n",
      "firstletter. Next,eachnodeisembeddedusingrelevantutteranceembeddingsthroughmessagepassing.\n",
      "Finally,anLSTMgeneratesthenextutterancebasedonattentionoverthenodeembeddings.\n",
      "modelconsistsofthreecomponentsshowninFig- is columbia. An example graph is shown in Fig-\n",
      "ure3: (i)adynamicknowledgegraph,whichrep- ure3. ThegraphG isupdatedbasedonutterance\n",
      "t\n",
      "resentstheagent’sprivateKBandshareddialogue t by taking G and adding a new node for any\n",
      "t−1\n",
      "history as a graph (Section 3.1), (ii) a graph em- entitymentionedinutterancetbutnotinKB.7\n",
      "A\n",
      "bedding over the nodes (Section 3.2), and (iii) an\n",
      "3.2 GraphEmbedding\n",
      "utterancegenerator(Section3.3).\n",
      "Theknowledgegraphrepresentsentitiesandre- Given a knowledge graph, we are interested in\n",
      "lations in the agent’s private KB, e.g., item-1’s computing a vector representation for each node\n",
      "company is google. As the conversation unfolds, v that captures both its unstructured context from\n",
      "utterances are embedded and incorporated into the dialogue history and its structured context in\n",
      "node embeddings of mentioned entities. For in- the KB. A node embedding V t(v) for each node\n",
      "stance, in Figure 3, “anyone went to columbia” v ∈ G t is built from three parts: structural prop-\n",
      "updates the embedding of columbia. Next, each ertiesofanentitydefinedbytheKB,embeddings\n",
      "node recursively passes its embedding to neigh- ofutterancesinthedialoguehistory,andmessage\n",
      "boring nodes so that related entities (e.g., those passingbetweenneighboringnodes.\n",
      "in the same row or column) also receive informa-\n",
      "Node Features. Simple structural properties of\n",
      "tion from the most recent utterance. In our exam-\n",
      "the KB often govern what is talked about; e.g.,\n",
      "ple, jessica and josh both receive new context\n",
      "a high-frequency entity is usually interesting to\n",
      "when columbia is mentioned. Finally, the utter-\n",
      "mention(consider“Allmyfriendslikedancing.”).\n",
      "ancegenerator,anLSTM,producesthenextutter-\n",
      "We represent this type of information as a fea-\n",
      "ancebyattendingtothenodeembeddings.\n",
      "ture vector F (v), which includes the degree and\n",
      "t\n",
      "type(item,attribute,orentitytype)ofnodev,and\n",
      "3.1 KnowledgeGraph\n",
      "whetherithasbeenmentionedinthecurrentturn.\n",
      "Given a dialogue of T utterances, we construct Each feature is encoded as a one-hot vector and\n",
      "graphs (G t)T\n",
      "t=1\n",
      "over the KB and dialogue history theyareconcatenatedtoformF t(v).\n",
      "foragentA.6 Therearethreetypesofnodes: item\n",
      "MentionVectors. AmentionvectorM (v)con-\n",
      "nodes, attribute nodes, and entity nodes. Edges t\n",
      "tainsunstructuredcontextfromutterancesrelevant\n",
      "between nodes represent their relations. For ex-\n",
      "to node v up to turn t. To compute it, we first de-\n",
      "ample,(item-1, hasSchool, columbia)means\n",
      "fine the utterance representation u˜ and the set of\n",
      "thatthefirstitemhasattributeschoolwhosevalue t\n",
      "relevant entities E. Let u be the embedding of\n",
      "t t\n",
      "6 It is important to differentiate perspectives of the two utterancet(Section3.3). Todifferentiatebetween\n",
      "agentsastheyhavedifferentKBs. Thereafterweassumethe\n",
      "perspectiveofagentA,i.e., accessingKB forAonly, and 7Weusearule-basedlexicontolinktextspanstoentities.\n",
      "A\n",
      "refertoBasthepartner. SeedetailsinAppendixD.\n",
      "the agent’s and the partner’s utterances, we repre- 3.3 UtteranceEmbeddingandGeneration\n",
      "sent it as u˜ = (cid:2) u ·1,u ·1 (cid:3),\n",
      "t t {ut∈U self} t {ut∈Upartner} We embed and generate utterances using Long\n",
      "where U and U denote sets of utterances\n",
      "self partner Short Term Memory (LSTM) networks that take\n",
      "generated by the agent and the partner, and [·,·]\n",
      "thegraphembeddingsintoaccount.\n",
      "denotes concatenation. Let E be the set of entity\n",
      "t\n",
      "nodesmentionedinutterancetifutterancetmen- Embedding. On turn t, upon receiving an\n",
      "tionssomeentities, orutterancet−1otherwise.8 utterance consisting of n t tokens, x t =\n",
      "ThementionvectorM t(v)ofnodev incorporates (x t,1,...,x t,nt), the LSTM maps it to a vector as\n",
      "thecurrentutteranceifvismentionedandinherits follows:\n",
      "M t−1(v)ifnot: h\n",
      "t,j\n",
      "= LSTM enc(h t,j−1,A t(x t,j)), (4)\n",
      "M t(v) = λ tM t−1(v)+(1−λ t)u˜ t; (1) where h\n",
      "t,0\n",
      "= h t−1,nt−1, and A\n",
      "t\n",
      "is an entity ab-\n",
      "(cid:40) σ(cid:0) Winc[M (v),u˜ ](cid:1) ifv ∈ E, stractionfunction,explainedbelow. Thefinalhid-\n",
      "t−1 t t\n",
      "λ t = 1 otherwise. den state h t,nt is used as the utterance embed-\n",
      "dingu,whichupdatesthementionvectorsasde-\n",
      "t\n",
      "Here, σ is the sigmoid function and Winc is a pa- scribedinSection3.2.\n",
      "rametermatrix. In our dialogue task, the identity of an entity\n",
      "is unimportant. For example, replacing google\n",
      "Recursive Node Embeddings. We propagate\n",
      "with alphabet in Figure 1 should make little dif-\n",
      "informationbetweennodesaccordingtothestruc-\n",
      "ference to the conversation. The role of an entity\n",
      "ture of the knowledge graph. In Figure 3, given\n",
      "is determined instead by its relation to other en-\n",
      "“anyonewenttocolumbia?”,theagentshouldfo-\n",
      "tities and relevant utterances. Therefore, we de-\n",
      "cusonherfriendswhowenttoColumbiaUniver-\n",
      "finetheabstractionA (y)forawordy asfollows:\n",
      "t\n",
      "sity. Therefore, we want this utterance to be sent\n",
      "if y is linked to an entity v, then we represent an\n",
      "toitemnodesconnectedtocolumbia,andonestep\n",
      "entity by its type (school, company etc.) embed-\n",
      "further to other attributes of these items because\n",
      "ding concatenated with its current node embed-\n",
      "theymightbementionednextasrelevantinforma-\n",
      "ding: A (y) = [E,V (v)]. Note that V (v)\n",
      "tion,e.g.,jessicaandjosh.\n",
      "t type(y) t t\n",
      "isdeterminedonlybyitsstructuralfeaturesandits\n",
      "We compute the node embeddings recursively,\n",
      "context. Ifyisanon-entity,thenA (y)istheword\n",
      "t\n",
      "analogoustobeliefpropagation:\n",
      "embedding of y concatenated with a zero vector\n",
      "Vk(v) = max tanh (2) of the same dimensionality as V (v). This way,\n",
      "t t\n",
      "v(cid:48)∈Nt(v)\n",
      "therepresentationofanentityonlydependsonits\n",
      "(cid:16) (cid:104) (cid:105)(cid:17)\n",
      "Wmp Vk−1(v(cid:48)),R(e ), structural properties given by the KB and the dia-\n",
      "t v→v(cid:48)\n",
      "logue context, which enables the model to gener-\n",
      "where Vk(v) is the depth-k node embedding at\n",
      "t alizetounseenentitiesattesttime.\n",
      "turntandN (v)denotesthesetofnodesadjacent\n",
      "t\n",
      "tov. Themessagefromaneighboringnodev(cid:48) de- Generation. Now,assumingwehaveembedded\n",
      "utterance x into h as described above,\n",
      "pendsonitsembeddingatdepth-(k−1),theedge t−1 t−1,nt−1\n",
      "we use another LSTM to generate utterance x.\n",
      "label e (embedded by a relation embedding t\n",
      "v→v(cid:48)\n",
      "function R), and a parameter matrix Wmp. Mes- Formally, we carryover the lastutterance embed-\n",
      "dingh = h anddefine:\n",
      "sages from all neighbors are aggregated by max, t,0 t−1,nt−1\n",
      "the element-wise max operation.9 Example mes- h = LSTM (h,[A (x ),c ]), (5)\n",
      "t,j dec t,j−1 t t,j t,j\n",
      "sagepassingpathsareshowninFigure3.\n",
      "wherec isaweightedsumofnodeembeddings\n",
      "The final node embedding is the concatenation t,j\n",
      "(cid:80)\n",
      "in the current turn: c = α V (v),\n",
      "ofembeddingsateachdepth: t,j v∈Gt t,j,v t\n",
      "where α are the attention weights over the\n",
      "t,j,v\n",
      "V t(v) = (cid:2) V t0(v),...,V tK(v)(cid:3), (3) nodes. Intuitively, high weight should be given to\n",
      "relevant entity nodes as shown in Figure 3. We\n",
      "whereK isahyperparameter(weexperimentwith\n",
      "K ∈ {0,1,2})andV0(v) = [F (v),M (v)]. compute the weights through standard attention\n",
      "t t t\n",
      "mechanism(Bahdanauetal.,2015):\n",
      "8 Relying on utterance t−1 is useful when utterance t\n",
      "answersaquestion,e.g.,“doyouhaveanygooglefriends?” α t,j = softmax(s t,j),\n",
      "“No.” s = wattn·tanh(cid:0) Wattn[h,V (v)](cid:1),\n",
      "9Usingsumormeanslightlyhurtsperformance. t,j,v t,j−1 t\n",
      "wherevectorwattn andWattn areparameters. what to talk about and which item to select. It\n",
      "Finally,wedefineadistributionoverbothwords has a pattern-matching semantic parser, a rule-\n",
      "inthevocabularyandnodesinG usingthecopy- based policy, and a templated generator. See Ap-\n",
      "t\n",
      "ingmechanismofJiaandLiang(2016): pendixGfordetails.\n",
      "p(x t,j+1 = y|G t,x t,≤j) ∝ exp(cid:0) Wvocabh t,j +b(cid:1), 4.2 Evaluation\n",
      "p(x t,j+1 = r(v)|G t,x t,≤j) ∝ exp(s t,j,v), We test our systems in two interactive settings:\n",
      "bot-botchatandbot-humanchat. Weperformboth\n",
      "where y is a word in the vocabulary, Wvocab and\n",
      "automaticevaluationandhumanevaluation.\n",
      "bareparameters,andr(v)istherealizationofthe\n",
      "entity represented by node v, e.g., google is real- Automatic Evaluation. First, we compute the\n",
      "izedto“Google”duringcopying.10\n",
      "cross-entropy ((cid:96)) of a model on test data. As\n",
      "shown in Table 4, DynoNet has the lowest test\n",
      "4 Experiments\n",
      "loss. Next, we have a model chat with itself on\n",
      "We compare our model with a rule-based sys- the scenarios from the test set.12 We evaluate the\n",
      "tem and a baseline neural model. Both automatic chatswithrespecttolanguagevariation,effective-\n",
      "and human evaluations are conducted to test the ness,andstrategy.\n",
      "models in terms of fluency, correctness, coopera- For language variation, we report the average\n",
      "tion, and human-likeness. The results show that utterance length L u and the unigram entropy H\n",
      "DynoNetisabletoconversewithhumansinaco- in Table 4. Compared to Rule, the neural mod-\n",
      "herentandstrategicway. els tend to generate shorter utterances (Li et al.,\n",
      "2016b; Serban et al., 2017b). However, they are\n",
      "4.1 Setup\n",
      "more diverse; for example, questions are asked\n",
      "Werandomlysplitthedataintotrain,dev,andtest in multiple ways such as “Do you have...”, “Any\n",
      "sets (8:1:1). We use a one-layer LSTM with 100 friendslike...”,“Whatabout...”.\n",
      "hidden units and 100-dimensional word vectors At the discourse level, we expect the distribu-\n",
      "forboththeencoderandthedecoder(Section3.3). tion of a bot’s utterance types to match the distri-\n",
      "Eachsuccessfuldialogueisturnedintotwoexam- bution of human’s. We show percentages of each\n",
      "ples, each from the perspective of one of the two utterance type in Table 4. For Rule, the decision\n",
      "agents. We maximize the log-likelihood of all ut- about which action to take is written in the rules,\n",
      "terancesinthedialogues. Theparametersareopti- whileStanoNetandDynoNetlearnedtobehavein\n",
      "mizedbyAdaGrad(Duchietal.,2010)withanini- amorehuman-likeway,frequentlyinformingand\n",
      "tiallearningrateof0.5. Wetrainedforatleast10 askingquestions.\n",
      "epochs; after that, training stops if there is no im- Tomeasureeffectiveness,wecomputetheover-\n",
      "provementonthedevsetfor5epochs. Bydefault, all success rate (C) and the success rate per turn\n",
      "we perform K = 2 iterations of message passing (C )andperselection(C ). AsshowninTable4,\n",
      "T S\n",
      "to compute node embeddings (Section 3.2). For humansarethebestatthisgame,followedbyRule\n",
      "decoding,wesequentiallysamplefromtheoutput whichiscomparabletoDynoNet.\n",
      "distribution with a softmax temperature of 0.5.11 Next, we investigate the strategies leading to\n",
      "Hyperparametersaretunedonthedevset. these results. An agent needs to decide which\n",
      "We compare DynoNet with its static cou- entity/attribute to check first to quickly reduce\n",
      "sion (StanoNet) and a rule-based system (Rule). the search space. We hypothesize that humans\n",
      "StanoNet uses G 0 throughout the dialogue, thus tend to first focus on a majority entity and an\n",
      "the dialogue history is completely contained in attribute with fewer unique values (Section 2.3).\n",
      "the LSTM states instead of being injected into For example, in the scenario in Table 6, time and\n",
      "the knowledge graph. Rule maintains weights for location are likely to be mentioned first. We\n",
      "each entity and each item in the KB to decide showtheaveragefrequencyoffirst-mentioneden-\n",
      "tities (#Ent ) and the average number of unique\n",
      "10Werealizeanentitybysamplingfromtheempiricaldis- 1\n",
      "tributionofitssurfaceformsfoundinthetrainingdata. valuesforfirst-mentionedattributes(|Attr 1|)inTa-\n",
      "11 Sinceselectionisacommon‘utterance’inourdataset\n",
      "and neural generation models are susceptible to over- 12 Welimitthenumberofturnsinbot-botchattobethe\n",
      "generatingcommonsentences, wehalveitsprobabilitydur- maximum number of turns humans took in the test set (46\n",
      "ingsampling. turns).\n",
      "System (cid:96)↓ L H C ↑ C ↑ C ↑ Sel Inf Ask Ans Greet #Ent |Attr | #Ent #Attr\n",
      "u T S 1 1\n",
      "Human - 5.10 4.57.82.07.38.21.31.17.08.08.55.35 6.1 2.6\n",
      "Rule - 7.61 3.37.90.05.29.18.34.23.00.12.24.61 9.9 3.0\n",
      "StanoNet 2.20 4.01 4.05.78.04.18.19.26.12.23.09.61.19 7.1 2.9\n",
      "DynoNet 2.13 3.37 3.90.96.06.25.22.26.13.20.12.55.18 5.2 2.5\n",
      "Table 4: Automatic evaluation on human-human and bot-bot chats on test scenarios. We use ↑ / ↓ to\n",
      "indicatethathigher/lowervaluesarebetter;otherwisetheobjectiveistomatchhumans’statistics. Best\n",
      "results(exceptHuman)areinbold. Neuralmodelsgenerateshorter(lowerL )butmorediverse(higher\n",
      "u\n",
      "H)utterances. Overall,theirdistributionsofutterancetypesmatchthoseofthehumans’. (Weonlyshow\n",
      "the most frequent speech acts therefore the numbers do not sum to 1.) Rule is effective in completing\n",
      "the task (higher C ), but it is not information-efficient given the large number of attributes (#Attr) and\n",
      "S\n",
      "entities(#Ent)mentioned.\n",
      "ble4.13 BothDynoNetandStanoNetsuccessfully Noticeably, DynoNet is more cooperative than\n",
      "match human’s starting strategy by favoring enti- the other models. As shown in the example dia-\n",
      "ties of higher frequency and attributes of smaller logues in Table 6, DynoNet cooperates smoothly\n",
      "domainsize. with the human partner, e.g., replying with rel-\n",
      "To examine the overall strategy, we show the evant information about morning/indoor friends\n",
      "average number of attributes (#Attr) and entities when the partner mentioned that all her friends\n",
      "(#Ent) mentioned during the conversation in Ta- prefer morning and most like indoor. StanoNet\n",
      "ble4. HumansandDynoNetstrategicallyfocuson starts well but doesn’t follow up on the morn-\n",
      "a few attributes and entities, whereas Rule needs ingfriend, presumablybecausethemorningnode\n",
      "almost twice entities to achieve similar success is not updated dynamically when mentioned by\n",
      "rates. This suggests that the effectiveness of Rule the partner. Rule follows the partner poorly. In\n",
      "mainly comes from large amounts of unselective the comments, the biggest complaint about Rule\n",
      "information, which is consistent with comments was that it was not ‘listening’ or ‘understanding’.\n",
      "fromtheirhumanpartners. Overall,DynoNetachievesbetterpartnersatisfac-\n",
      "tion,especiallyincooperation.\n",
      "Partner Evaluation. We generated 200 new\n",
      "scenarios and put up the bots on AMT using the Third-party Evaluation. We also created a\n",
      "same chat interface that was used for data col- third-partyevaluationtask,whereanindependent\n",
      "lection. The bots follow simple turn-taking rules AMTworkerisshownaconversationandtheKB\n",
      "explained in Appendix H. Each AMT worker is ofoneoftheagents; sheisaskedtoratethesame\n",
      "randomly paired with Rule, StanoNet, DynoNet, aspects of the agent as in the partner evaluation\n",
      "or another human (but the worker doesn’t know and provide justifications. Each agent in a dia-\n",
      "which), and we make sure that all four types of logueisratedbyatleast5people.\n",
      "agentsaretestedineachscenarioatleastonce. At The average ratings and histograms are shown\n",
      "theendofeachdialogue,humansareaskedtor<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  28323,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['HeHe', 'AnushaBalakrishnan', 'MihailEric', 'PercyLiang']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: ,especiallyincooperation.\n",
      "Partner Evaluation. We generated 200 new\n",
      "scenarios and put up the bots on AMT using the Third-party Evaluation. We also created a\n",
      "same chat interface that was used for data col- third-partyevaluationtask,whereanindependent\n",
      "lection. The bots follow simple turn-taking rules AMTworkerisshownaconversationandtheKB\n",
      "explained in Appendix H. Each AMT worker is ofoneoftheagents; sheisaskedtoratethesame\n",
      "randomly paired with Rule, StanoNet, DynoNet, aspects of the agent as in the partner evaluation\n",
      "or another human (but the worker doesn’t know and provide justifications. Each agent in a dia-\n",
      "which), and we make sure that all four types of logueisratedbyatleast5people.\n",
      "agentsaretestedineachscenarioatleastonce. At The average ratings and histograms are shown\n",
      "theendofeachdialogue,humansareaskedtorate inTable5andAppendixJ.Forcorrectness,wesee\n",
      "their partner in terms of fluency, correctness, co- thatRulehasthebestperformancesinceitalways\n",
      "operation, and human-likeness from 1 (very bad) tellsthetruth,whereashumanscanmakemistakes\n",
      "to5(verygood),alongwithoptionalcomments. duetocarelessnessandtheneuralmodelscangen-\n",
      "We show the average ratings (with significance erate false information. For example, in Table 6,\n",
      "tests)inTable5andthehistogramsinAppendixJ. DynoNet‘lied’whensayingthatithasamorning\n",
      "In terms of fluency, the models have similar per- friendwholikesoutdoor.\n",
      "formance since the utterances are usually short. Surprisingly,thereisadiscrepancybetweenthe\n",
      "Judgmentoncorrectnessisamereguesssincethe twoevaluationmodesintermsofcooperationand\n",
      "evaluatorcannotseethepartner’sKB;wewillan- human-likeness. Manual analysis of the com-\n",
      "alyze correctness more meaningfully in the third- ments indicates that third-party evaluators focus\n",
      "partyevaluationbelow. lessonthedialoguestrategyandmoreonlinguis-\n",
      "tic features, probably because they were not fully\n",
      "13Bothnumbersarenormalizedto[0,1]withrespecttoall\n",
      "entities/attributesinthecorrespondingKB. engagedinthedialogue. Forexample,justification\n",
      "Partnereval Third-partyeval\n",
      "System C C C\n",
      "T S Flnt Crct Coop Human Flnt Crct Coop Human\n",
      "Human.89.07.36 4.2rds 4.3rds 4.2rds 4.1rds 4.0 4.3ds 4.0ds 4.1rds\n",
      "Rule.88.06.29 3.6 4.0 3.5 3.5 4.0 4.4hds 3.9s 4.0s\n",
      "StanoNet.76.04.23 3.5 3.8 3.4 3.3 4.0 4.0 3.8 3.8\n",
      "DynoNet.87.05.27 3.8s 4.0 3.8rs 3.6s 4.0 4.1 3.9 3.9\n",
      "Table 5: Results on human-bot/human chats. Best results (except Human) in each column are in bold.\n",
      "We report the average ratings of each system. For third-party evaluation, we first take mean of each\n",
      "question then average the ratings. DynoNet has the best partner satisfaction in terms of fluency (Flnt),\n",
      "correctness(Crct),cooperation(Coop),humanlikeness(Human). Thesuperscriptofaresultindicatesthat\n",
      "its advantage over other systems (r: Rule, s: StanoNet, d: DynoNet) is statistically significant with\n",
      "p < 0.05givenbypairedt-tests.\n",
      "forcooperationoftenmentionsfrequentquestions dialogues are easy to collect as natural human\n",
      "and timely answers, less attention is paid to what conversations, and are also challenging enough\n",
      "isaskedaboutthough. given the large number of scenarios and diverse\n",
      "For human-likeness, partner evaluation is conversation phenomena. There are some in-\n",
      "largelycorrelatedwithcoherence(e.g.,notrepeat- teresting strategic dialogue datasets—settlers of\n",
      "ing or ignoring past information) and task suc- Catan (Afantenos et al., 2012) (2K turns) and the\n",
      "cess, whereas third-party evaluators often rely on cards corpus (Potts, 2012) (1.3K dialogues), as\n",
      "informality (e.g., usage of colloquia like “hiya”, well as work on dialogue strategies (Keizer et al.,\n",
      "capitalization, and abbreviation) or intuition. In- 2017; Vogel et al., 2013), though no full dialogue\n",
      "terestingly,third-partyevaluatorsnotedmostphe- systemhasbeenbuiltforthesedatasets.\n",
      "nomena listed in Table 2 as indicators of human-\n",
      "Mosttask-orienteddialoguesystemsfollowthe\n",
      "beings, e.g., correcting oneself, making chit-chat\n",
      "POMDP-based approach (Williams and Young,\n",
      "other than simply finishing the task. See example\n",
      "2007; Young et al., 2013). Despite their suc-\n",
      "commentsinAppendixK.\n",
      "cess (Wen et al., 2017; Dhingra et al., 2017; Su\n",
      "etal.,2016),therequirementforhandcraftedslots\n",
      "4.3 AblationStudies\n",
      "limits their scalability to new domains and bur-\n",
      "Our model has two novel designs: entity abstrac- dens data collection with extra state labeling. To\n",
      "tion and message passing for node embeddings. go past this limit, Bordes and Weston (2017) pro-\n",
      "Table 7 shows what happens if we ablate these. posed a Memory-Networks-based approach with-\n",
      "When the number of message passing iterations, out domain-specific features. However, the mem-\n",
      "K, is reduced from 2 to 0, the loss consistently oryisunstructuredandinterfacingwithKBsrelies\n",
      "increases. Removingentityabstraction—meaning onAPIcalls,whereasourmodelembedsboththe\n",
      "addingentityembeddingstonodeembeddingsand dialoguehistoryandtheKBstructurally. Williams\n",
      "the LSTM input embeddings—also degrades per- et al. (2017) use an LSTM to automatically infer\n",
      "formance. ThisshowsthatDynoNetbenefitsfrom the dialogue state, but as they focus on dialogue\n",
      "contextually-defined, structural node embeddings controlratherthanthefullproblem,theresponseis\n",
      "ratherthanonesbasedonaclassiclookuptable. modeledasatemplatedaction,whichrestrictsthe\n",
      "generation of richer utterances. Our network ar-\n",
      "5 DiscussionandRelatedWork\n",
      "chitectureismostsimilartoEntNet(Henaffetal.,\n",
      "2017), where memories are also updated by input\n",
      "There has been a recent surge of interest in\n",
      "sentences recurrently. The main difference is that\n",
      "end-to-endtask-orienteddialoguesystems,though\n",
      "ourmodelallowsinformationtobepropagatedbe-\n",
      "progress has been limited by the size of available\n",
      "tween structured entities, which is shown to be\n",
      "datasets(Serbanetal.,2015a). Mostworkfocuses\n",
      "crucialinoursetting(Section4.3).\n",
      "on information-querying tasks, using Wizard-of-\n",
      "Oz data collection (Williams et al., 2016; Asri Ourworkisalsorelatedtolanguagegeneration\n",
      "et al., 2016) or simulators (Bordes and Weston, conditionedonknowledgebases(Meietal.,2016;\n",
      "2017; Li et al., 2016d), In contrast, collaborative Kiddon et al., 2016). One challenge here is to\n",
      "FriendsofA FriendsofB\n",
      "ID Name Company Time Location ID Name Company Time Location\n",
      "1 Kathy TRTHoldings afternoon indoor 1 Justin NewEraTickets morning indoor\n",
      "2 Jason DollarGeneral afternoon indoor 2 Kathleen TRTHoldings morning indoor\n",
      "3 Johnny TRTHoldings afternoon outdoor 3 Gloria L&LHawaiianBarbecue morning indoor\n",
      "4 Frank SFNGroup afternoon indoor 4 Kathleen AdvanceAutoParts morning outdoor\n",
      "5 Catherine DollarGeneral afternoon indoor 5 Justin ArcticCat morning indoor\n",
      "6 Catherine WeisMarkets afternoon indoor 6 Anna DollarGeneral morning indoor\n",
      "7 Kathleen TRTHoldings morning indoor 7 Steven SFNGroup morning indoor\n",
      "8 Lori TRTHoldings afternoon indoor 8 Wayne R.J.CormanRailroadGroup morning indoor\n",
      "9 Frank L&LHawaiianBarbecue afternoon outdoor 9 Alexander R.J.CormanRailroadGroup morning indoor\n",
      "A:Human B:Human A:DynoNet B:Human\n",
      "A:hi\n",
      "A:Hi\n",
      "B:Hello||allmyfriendsprefermorning\n",
      "B:hey||ihaveoneoutdoor\n",
      "A:1ofmymorninglikestheoutdoors\n",
      "A:Ihave4TRTHoldings||Ihave2outdoorone\n",
      "B:andalllikeindoorexceptforone\n",
      "Johnny,otherFrank\n",
      "A:dotheyworkfortrtholdings?\n",
      "B:ionlyhaveoneTRTHoldings-Kathleen\n",
      "B:Kathleen?\n",
      "A:SELECT7 B:SELECT2\n",
      "A:SELECT7 B:SELECT2\n",
      "A:StanoNet B:Human A:Human B:Rule\n",
      "A:Hello B:hiya\n",
      "B:hi A:hEY\n",
      "A:Hello||Ihaveonemorningperson. B:Ihave1indoorsandkathleen.\n",
      "B:allofmyfriendslikemornings A:Mostofmineareindoors.\n",
      "A:Myfriendprefersafternoonworksattrtholdings. B:SELECT1\n",
      "B:whatistheirname? A:Ihaveonemorningandrestafternoon.\n",
      "A:Likesindoors. B:Doyouhaveanyfriendworkingatlhawaiian?\n",
      "B:whatisyourfiendwholikesmorningname? A:Idon’tknowJustin\n",
      "A:Theyworkfortrtholdings. B:Ihave1alexander.\n",
      "B:SELECT2 A:SELECT7...\n",
      "Table 6: Examples of human-bot chats. The mutual friend is highlighted in blue in each KB. Bots’\n",
      "utterances are in bold and selected items are represented by item IDs. Only the first half of the human-\n",
      "Rulechatisshownduetolimitedspace. Multipleutterancesofoneagentraeseparatedby||.\n",
      "Model (cid:96) vide unique opportunities at the interface of tra-\n",
      "ditional task-oriented dialogue and open-domain\n",
      "DynoNet(K=2) 2.16\n",
      "DynoNet(K=1) 2.20 chat. We also offered DynoNet as a promising\n",
      "DynoNet(K=0) 2.26\n",
      "means for open-ended dialogue state representa-\n",
      "DynoNet(K=2)w/oentityabstraction 2.21\n",
      "tion. Our dataset facilitates the study of prag-\n",
      "Table 7: Ablations of our model on the dev matics and human strategies in dialogue—a good\n",
      "set show the importance of entity abstraction and steppingstonetowardslearningmorecomplexdi-\n",
      "messagepassing(K = 2). aloguessuchasnegotiation.\n",
      "Acknowledgments. This work is supported by\n",
      "avoidgeneratingfalseorcontradictingstatements, DARPA Communicating with Computers (CwC)\n",
      "which is currently a weakness of neural models. programunderAROprimecontractno. W911NF-\n",
      "Our model is mostly accurate when generating 15-1-0462. Mike Kayser worked on an early ver-\n",
      "facts and answering existence questions about a sion of the project while he was at Stanford. We\n",
      "single entity, but will need a more advanced at- alsothankmembersoftheStanfordNLPgroupfor\n",
      "tention mechanism for generating utterances in- insightfuldiscussions.\n",
      "volving multiple entities, e.g., attending to items\n",
      "or attributes first, then selecting entities; generat- Reproducibility. All code, data, and\n",
      "inghigh-levelconceptsbeforecomposingthemto experiments for this paper are avail-\n",
      "naturaltokens(Serbanetal.,2017a). able on the CodaLab platform: https:\n",
      "In conclusion, we believe the symmetric col- //worksheets.codalab.org/worksheets/\n",
      "laborative dialogue setting and our dataset pro- 0xc757f29f5c794e5eb7bfa8ca9c945573.\n",
      "References J.Li,M.Galley,C.Brockett,J.Gao,andW.B.Dolan.\n",
      "2016b. A diversity-promoting objective function\n",
      "S. Afantenos, N. Asher, F. Benamara, A. Cadilhac, for neural conversation models. In Human Lan-\n",
      "C.De´gremont,P.Denis,M.Guhe,S.Keizer,A.Las- guageTechnologyandNorthAmericanAssociation\n",
      "carides,O.Lemon,P.Muller,S.Paul,V.Rieser,and forComputationalLinguistics(HLT/NAACL).\n",
      "L.Vieu.2012. Developingacorpusofstrategiccon-\n",
      "versationinthesettlersofcatan. InSeineDial2012- J.Li,W.Monroe,A.Ritter,D.Jurafsky,M.Galley,and\n",
      "The16thWorkshopontheSemanticsandPragmat- J.Gao.2016c. Deepreinforcementlearningfordia-\n",
      "icsofDialogue. loguegeneration. InEmpiricalMethodsinNatural\n",
      "LanguageProcessing(EMNLP).\n",
      "L. E. Asri, H. Schulz, S. Sharma, J. Zumer, J. Har-\n",
      "ris, E. Fine, R. Mehrotra, and K. Suleman. 2016. X. Li, Z. C. Lipton, B. Dhingra, L. Li, J. Gao,\n",
      "Frames: A corpus for adding memory to goal- and Y. Chen. 2016d. A user simulator for task-\n",
      "oriented dialogue systems. Maluuba Technical Re- completiondialogues. arXiv.\n",
      "port.\n",
      "C.Liu,R.Lowe,I.V.Serban,M.Noseworthy,L.Char-\n",
      "D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural lin,andJ.Pineau.2016. HowNOTtoevaluateyour\n",
      "machinetranslationbyjointlylearningtoalignand dialogue system: An empirical study of unsuper-\n",
      "translate. In International Conference on Learning visedevaluationmetricsfordialogueresponsegen-\n",
      "Representations(ICLR). eration. InEmpiricalMethodsinNaturalLanguage\n",
      "Processing(EMNLP).\n",
      "A. Bordes and J. Weston. 2017. Learning end-to-end\n",
      "goal-oriented dialog. In International Conference R.T.Lowe,N.Pow,I.Serban,L.Charlin,C.Liu,and\n",
      "onLearningRepresentations(ICLR). J.Pineau.2017. TrainingEnd-to-Enddialoguesys-\n",
      "temswiththeubuntudialoguecorpus. Dialogueand\n",
      "B. Dhingra, L. Li, X. Li, J. Gao, Y. Chen, F. Ahmed, Discourse8.\n",
      "andL.Deng.2017. End-to-endreinforcementlearn-\n",
      "ingofdialogueagentsforinformationaccess. InAs- H. Mei, M. Bansal, and M. R. Walter. 2016. What\n",
      "sociationforComputationalLinguistics(ACL). to talk about and how? selective generation using\n",
      "LSTMs with coarse-to-fine alignment. In Human\n",
      "LanguageTechnologyandNorthAmericanAssocia-\n",
      "J.Duchi,E.Hazan,andY.Singer.2010. Adaptivesub-\n",
      "tionforComputationalLinguistics(HLT/NAACL).\n",
      "gradient methods for online learning and stochastic\n",
      "optimization. In Conference on Learning Theory\n",
      "H.Mei,M.Bansal,andM.R.Walter.2017. Coherent\n",
      "(COLT).\n",
      "dialogue with attention-based language models. In\n",
      "Association for the Advancement of Artificial Intel-\n",
      "M.Henaff,J.Weston,A.Szlam,A.Bordes,andY.Le-\n",
      "ligence(AAAI).\n",
      "Cun. 2017. Tracking the world state with recur-\n",
      "rent entity networks. In International Conference\n",
      "C.Potts.2012. Goal-drivenanswersintheCardsdia-\n",
      "onLearningRepresentations(ICLR).\n",
      "loguecorpus. InProceedingsofthe30thWestCoast\n",
      "ConferenceonFormalLinguistics.\n",
      "E. Ivanovic. 2005. Dialogue act tagging for instant\n",
      "messaging chat sessions. In Association for Com-\n",
      "I. Serban, T. Klinger, G. Tesauro, K. Talamadupula,\n",
      "putationalLinguistics(ACL).\n",
      "B. Zhou, Y. Bengio, and A. C. Courville. 2017a.\n",
      "Multiresolution recurrent neural networks: An ap-\n",
      "R.JiaandP.Liang.2016. Datarecombinationforneu-\n",
      "plication to dialogue response generation. In Asso-\n",
      "ral semantic parsing. In Association for Computa-\n",
      "ciationfortheAdvancementofArtificialIntelligence\n",
      "tionalLinguistics(ACL).\n",
      "(AAAI).\n",
      "S. Keizer, M. Guhe, H. Cuayahuitl, I. Efstathiou, I. Serban, A. Sordoni, R. Lowe, L. Charlin, J. Pineau,\n",
      "K. Engelbrecht, M. Dobre, A. Lascarides, and A.C.Courville,andY.Bengio.2017b. Ahierarchi-\n",
      "O. Lemon. 2017. Evaluating persuasion strategies callatentvariableencoder-decodermodelforgener-\n",
      "anddeepreinforcementlearningmethodsfornego- atingdialogues. InAssociationfortheAdvancement\n",
      "tiationdialogueagents. InEuropeanAssociationfor ofArtificialIntelligence(AAAI).\n",
      "ComputationalLinguistics(EACL).\n",
      "I. V. Serban, R. Lowe, L. Charlin, and J. Pineau.\n",
      "C.Kiddon,L.S.Zettlemoyer,andY.Choi.2016. Glob- 2015a. A survey of available corpora for build-\n",
      "ally coherent text generation with neural checklist ing data-driven dialogue systems. arXiv preprint\n",
      "models. InEmpiricalMethodsinNaturalLanguage arXiv:1512.05742.\n",
      "Processing(EMNLP).\n",
      "I.V.Serban,A.Sordoni,Y.Bengio,A.Courville,and\n",
      "J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan. J.Pineau.2015b. Buildingend-to-enddialoguesys-\n",
      "2016a. Apersona-basedneuralconversationmodel. tems using generative hierarchical neural network\n",
      "InAssociationforComputationalLinguistics(ACL). models. arXivpreprintarXiv:1507.04808.\n",
      "L. Shang, Z. Lu, and H. Li. 2015. Neural responding\n",
      "machineforshort-textconversation. InAssociation\n",
      "forComputationalLinguistics(ACL).\n",
      "A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji,\n",
      "M. Mitchell, J. Nie, J. Gao, and B. Dolan. 2015.\n",
      "A neural network approach to context-sensitive\n",
      "generation of conversational responses. In North\n",
      "American Association for Computational Linguis-\n",
      "tics(NAACL).\n",
      "P. Su, M. Gasic, N. Mrksic, L. M. Rojas-Barahona,\n",
      "S.Ultes,D.Vandyke,T.Wen,andS.J.Young.2016.\n",
      "Continuouslylearningneuraldialoguemanagement.\n",
      "arXivpreprintarXiv:1606.02689.\n",
      "A.Vogel, M.Bodoia, C.Potts, andD.Jurafsky.2013.\n",
      "Emergenceofgriceanmaximsfrommulti-agentde-\n",
      "cision theory. In North American Association for\n",
      "Computational Linguistics (NAACL). pages 1072–\n",
      "1081.\n",
      "T. Wen, M. Gasic, N. Mrksic, L. M. Rojas-Barahona,\n",
      "P. Su, S. Ultes, D. Vandyke, and S. Young. 2017.\n",
      "Anetwork-basedend-to-endtrainabletask-oriented\n",
      "dialoguesystem. InEuropeanAssociationforCom-\n",
      "putationalLinguistics(EACL).\n",
      "J. D. Williams, K. Asadi, and G. Zweig. 2017. Hy-\n",
      "brid code networks: Practical and efficient end-to-\n",
      "end dialog control with supervised and reinforce-\n",
      "ment learning. In Association for Computational\n",
      "Linguistics(ACL).\n",
      "J.D.Williams,A.Raux,andM.Henderson.2016. The\n",
      "dialogstatetrackingchallengeseries: Areview. Di-\n",
      "alogueandDiscourse7.\n",
      "J. D. Williams and S. Young. 2007. Partially observ-\n",
      "able Markov decision processes for spoken dialog\n",
      "systems. ComputerSpeech&Language21(2):393–\n",
      "422.\n",
      "S.Young, M.Gasic, B.Thomson, andJ.D.Williams.\n",
      "2013. POMDP-based statistical spoken dialog\n",
      "systems: A review. Proceedings of the IEEE\n",
      "101(5):1160–1179.\n",
      "A KnowledgeBaseSchema each dialogue participant. We instruct people to\n",
      "play intelligently, to refrain from brute-force tac-\n",
      "TheattributesetAfortheMutualFriendstaskcon-\n",
      "tics (e.g., mentioning every attribute value), and\n",
      "tainsname, school, major, company, hobby, time-\n",
      "to use grammatical sentences. To discourage ran-\n",
      "of-day preference, and location preference. Each\n",
      "dom guessing, we prevent users from selecting a\n",
      "attributeahasasetofpossiblevalues(entities)E.\n",
      "a friend (item) more than once every 10 seconds.\n",
      "Forname,school,major,company,andhobby,we\n",
      "Each worker was paid $0.35 for a successful di-\n",
      "collectedalargesetofvaluesfromvariousonline\n",
      "alogue within a 5-minute time limit. We log each\n",
      "sources.14 We used three possible values (morn-\n",
      "utterance in the dialogue along with timing infor-\n",
      "ing, afternoon, and evening) for the time-of-day\n",
      "mation.\n",
      "preference, and two possible values (indoors and\n",
      "outdoors)forthelocationpreference.\n",
      "D EntityLinkingandRealization\n",
      "B ScenarioGeneration\n",
      "We use a rule-based lexicon to link text spans to\n",
      "entities. For every entity in the schema, we com-\n",
      "Wegeneratescenariosrandomlytovarytaskcom-\n",
      "putedifferentvariationsofitscanonicalname,in-\n",
      "plexity and elicit linguistic and strategic variants.\n",
      "A scenario S is characterized by the number of cluding acronyms, strings with a certain edit dis-\n",
      "items (N ), the attribute set (A ) whose size is tance,prefixes,andmorphologicalvariants. Given\n",
      "S S\n",
      "M, and the values for each attribute a ∈ A in a text span, a set of candidate entities is returned\n",
      "S S\n",
      "bystringmatching. Aheuristicrankerthenscores\n",
      "thetwoKBs.\n",
      "eachcandidate(e.g.,consideringwhetherthespan\n",
      "Ascenarioisgeneratedasfollows.\n",
      "is a substring of a candidate, the edit distance be-\n",
      "1. Sample N S and M S uniformly from tweenthespanandacandidateetc.). Thehighest-\n",
      "{5,...,12}and{3,4}respectively. scoringcandidateisreturned.\n",
      "A linked entity is considered as a single token\n",
      "2. Generate A by sampling M attributes\n",
      "S S and its surface form is ignored in all models. At\n",
      "withoutreplacementfromA.\n",
      "generation time, we realize an entity by sampling\n",
      "3. For each attribute a ∈ A, sample the con- fromtheempiricaldistributionofitssurfaceforms\n",
      "S\n",
      "centration parameter α uniformly from the inthetrainingset.\n",
      "a\n",
      "set{0.3,1,3}.\n",
      "E UtteranceCategorization\n",
      "4. GeneratetwoKBsbysamplingN valuesfor\n",
      "S\n",
      "eachattributeafromaDirichlet-multinomial\n",
      "We categorize utterances into inform, ask, answer,\n",
      "distribution over the value set E with the\n",
      "greeting,apologyheuristicallybypatternmatching.\n",
      "a\n",
      "concentrationparameterα.\n",
      "a\n",
      "• Anaskutteranceasksforinformationregard-\n",
      "WerepeatthelaststepuntilthetwoKBshaveone ing the partner’s KB. We detect these utter-\n",
      "uniquecommonitem. ances by checking for the presence of a ‘?’\n",
      "and/or a question word like “do”, “does”,\n",
      "C ChatInterface\n",
      "“what”,etc.\n",
      "Inordertocollectreal-timedialoguebetweenhu-\n",
      "• An inform utterance provides information\n",
      "mans, we set up a web server and redirect AMT\n",
      "about the agent’s KB. We define it as an ut-\n",
      "workers to our website. Visitors are randomly\n",
      "terancesthatmentionsentitiesintheKBand\n",
      "pairedupastheyarrive. Foreachpair,wechoose\n",
      "isnotanaskutterance.\n",
      "a random scenario, and randomly assign a KB to\n",
      "14Names: https://www.ssa.gov/oact/ • An answer utterance simply provides a posi-\n",
      "babynames/decades/century.html tive/negativeresponsetoaquestion,contain-\n",
      "Schools: http://doors.stanford.edu/˜sr/\n",
      "ingwordslike“yes”,“no”,“nope”,etc.\n",
      "universities.html\n",
      "Majors:http://www.a2zcolleges.com/majors\n",
      "Companies: https://en.wikipedia.org/wiki/ • A greeting utterance contains words like “hi”\n",
      "List_of_companies_of_the_United_States\n",
      "or“hello”;itoftenoccursatthebeginningof\n",
      "Hobbies: https://en.wikipedia.org/wiki/\n",
      "List_of_hobbies adialogue.\n",
      "• An apology utterance contains the word G Rule-basedSystem\n",
      "“sorry”, which is typically associated with\n",
      "The rule-based bot takes the following actions:\n",
      "correctionsandwrongselections.\n",
      "greeting, informing or asking about a set of en-\n",
      "tities,answeringaquestion,andselectinganitem.\n",
      "See Table 2 and Table 1 for examples of these ut-\n",
      "The set of entities to inform/ask is sampled ran-\n",
      "terancetypes.\n",
      "domlygiventheentityweights. Initially,eachen-\n",
      "tity is weighted by its count in the KB. We then\n",
      "F Strategy\n",
      "increment or decrement weights of entities men-\n",
      "tioned by the partner and its related entities (in\n",
      "During scenario generation, we varied the num-\n",
      "the same row or column), depending on whether\n",
      "berofattributes,thenumberofitemsineachKB,\n",
      "the mention is positive or negative. A negative\n",
      "and the distribution of values for each attribute.\n",
      "mention contains words like “no”, “none”, “n’t”\n",
      "We find that as the number of items and/or at-\n",
      "etc. Similarly,eachitemhasaninitialweightof1,\n",
      "tributes grows, the dialogue length and the com-\n",
      "whichisupdateddependingonthepartner’smen-\n",
      "pletion time also increase, indicating that the task\n",
      "tionofitsattributes.\n",
      "becomes harder. We also anticipated that vary-\n",
      "If there exists an item with weight larger than\n",
      "ing the value of α would impact the overall strat-\n",
      "1, the bot selects the highest-weighted item with\n",
      "egy(forexample,theorderinwhichattributesare\n",
      "probability 0.3. If a question is received, the\n",
      "mentioned) since α controls the skewness of the\n",
      "bot informs facts of the entities being asked, e.g.,\n",
      "distributionofvaluesforanattribute.\n",
      "“anyone went to columbia?”, “I have 2 friends\n",
      "On examining the data, we find that humans\n",
      "who went to columbia”. Otherwise, the bot sam-\n",
      "tendtofirstmentionattributeswithamoreskewed\n",
      "ples an entity set and randomly chooses between\n",
      "(i.e., less uniform) distribution of values. Specif-\n",
      "informingandaskingabouttheentities.\n",
      "ically, we rank the α values of all attributes in a\n",
      "All utterances are generated by sentence tem-\n",
      "scenario (see step 3 in Section B), and bin them\n",
      "plates, and parsing of the partner’s utterance is\n",
      "into 3 distribution groups—least uniform, medium,\n",
      "donebyentitylinkingandpatternmatching(Sec-\n",
      "and most uniform, according to the ranking where\n",
      "tionE).\n",
      "higherαvaluescorrespondstomoreuniformdis-\n",
      "tributions.15 In Figure 4, we plot the histogram\n",
      "H Turn-takingRules\n",
      "ofthedistributiongroupofthefirst-mentionedat-\n",
      "tribute in a dialogues, which shows that skewed Turn-taking is universal in human conversations\n",
      "attributesarementionedmuchmorefrequently. andthebotneedstodecidewhento‘talk’(sendan\n",
      "utterance). Topreventthebotfromgeneratingut-\n",
      "terances continuously and forming a monologue,\n",
      "we allow it to send at most one utterance if the\n",
      "utterance contains any entity, and two utterances\n",
      "otherwise. Whensendingmorethanoneutterance\n",
      "inaturn,thebotmustwaitfor1to2secondsinbe-\n",
      "tween. Inaddition, afteranutteranceisgenerated\n",
      "bythemodel(almostinstantly),thebotmusthold\n",
      "on for some time to simulate message typing be-\n",
      "fore sending. We used a typing speed of 7 chars /\n",
      "secandaddedanadditionalrandomdelaybetween\n",
      "Figure 4: Histogram of the first attribute men- 0to1.5safter‘typing’. Therulesareappliedtoall\n",
      "tioned in a dialogue. People tend to first mention models.\n",
      "attributes from very skewed (non-uniform) distri-\n",
      "I AdditionalHuman-BotDialogue\n",
      "butions.\n",
      "We show another set of human-bot/human chats\n",
      "in Table 8. In this scenario, the distribution of\n",
      "15Forscenarioswith3attributes,eachgroupcontainsone\n",
      "values are more uniform compared to Table 6.\n",
      "attributes. For scenarios with 4 attributes, we put the two\n",
      "attributeswithrankingsinthemiddletomedium. Nevertheless, we see that StanoNet and DynoNet\n",
      "stilllearnedtostartfromrelativelyhigh-frequency\n",
      "entities. They also appear more cooperative and\n",
      "mentions relevant entities in the dialogue context\n",
      "comparedtoRule.\n",
      "J HistogramsofRatingsfromHuman\n",
      "Evaluations\n",
      "The histograms of ratings from partner and third-\n",
      "party evaluations is shown in Figure 5 and Fig-\n",
      "ure 6 respectively. As these figures show, there\n",
      "are some obvious discrepancies between the rat-\n",
      "ingsmadebyagentswhochattedwiththebotand\n",
      "those made by an ‘objective’ third party. These\n",
      "ratingsprovidesomeinterestinginsightsintohow\n",
      "dialogue participants in this task setting perceive\n",
      "theirpartners,andwhatconstitutesa‘human-like’\n",
      "ora‘fluent’partner.\n",
      "K ExampleCommentsfromPartnerand\n",
      "Third-partyEvaluations\n",
      "In Table 9, we show several pairs of ratings and\n",
      "comments on human-likeness for the same dia-\n",
      "logue from both the partner evaluation and the\n",
      "third-party evaluation. As a conversation partic-\n",
      "ipant, the dialogue partner often judges from the\n",
      "cooperation and strategy perspective, whereas the\n",
      "third-partyevaluatorreliesmoreonlinguisticfea-\n",
      "tures(e.g.,length,spelling,formality).\n",
      "60\n",
      "50\n",
      "40\n",
      "30\n",
      "20\n",
      "10\n",
      "0\n",
      "1 2 3 4 5\n",
      "egatnecreP\n",
      "Fluency Correctness\n",
      "Human\n",
      "Rule\n",
      "StanoNet\n",
      "DynoNet\n",
      "1 2 3 4 5\n",
      "60\n",
      "50\n",
      "40\n",
      "30\n",
      "20\n",
      "10\n",
      "0\n",
      "1 2 3 4 5\n",
      "egatnecreP\n",
      "Cooperation Human-likeness\n",
      "1 2 3 4 5\n",
      "Figure5: Histogramofratings(higherisbetter)fromdialoguepartners. DynoNetisbetterthanallother\n",
      "systems,especiallyincooperation.\n",
      "FriendsofB\n",
      "FriendsofA ID Major Company Hobby\n",
      "ID Major Company Hobby ForeignLanguage\n",
      "1 GannettCompany Roadbiking\n",
      "TeacherEducation\n",
      "Metallurgical\n",
      "1 GannettCompany Candlemaking Mathematics\n",
      "Engineering 2 ElectronicArts Astronomy\n",
      "Education\n",
      "2 BusinessEducation ElectronicArts Gunsmithing\n",
      "Petroleum WesternSugar\n",
      "Parks 3 Candlemaking\n",
      "3 Kenworth Watersports Engineering Cooperative\n",
      "Administration\n",
      "American\n",
      "Mathematics Mathematics\n",
      "4 ElectronicArts Astronomy 4 Broadcasting Roadbiking\n",
      "Education Education\n",
      "Company\n",
      "Agricultural\n",
      "5 AVST Fieldhockey Petroleum WesternSugar\n",
      "Mechanization 5 Roadbiking\n",
      "Engineering Cooperative\n",
      "Mathematics\n",
      "6 AVST Shopping Petroleum\n",
      "Education 6 A&WRestaurants Golfing\n",
      "Engineering\n",
      "Parks Foreignlanguage\n",
      "7 AdobeSystems American\n",
      "Administration learning Petroleum\n",
      "7 Broadcasting Origami\n",
      "Agricultural BroncoWine Engineering\n",
      "8 Shopping Company\n",
      "Mechanization Company\n",
      "TheWaltDisney\n",
      "Metallurgical Foreignlanguage 8 Russian Astronomy\n",
      "9 ElectronicArts Company\n",
      "Engineering learning\n",
      "Petroleum TheWaltDisney\n",
      "Mathematics 9 Origami\n",
      "10 ElectronicArts Poi Engineering Company\n",
      "Education\n",
      "Protestant\n",
      "10 AcmeBrick Astronomy\n",
      "Affiliation\n",
      "A:Human B:Human A:Human B:DynoNet\n",
      "A:hi B:hi\n",
      "B:hi A:hey\n",
      "A:AnyfriendsworkatAVST? B:ihavethreefriendswholikeroadbiking\n",
      "B:petroleumengi??||no A:ihavet<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   2933,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['L. Li', 'M. Galley', 'C. Brockett', 'J. Gao', 'B. Dolan', 'A. Sordoni', 'R. Lowe', 'L. Charlin', 'J. Pineau']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk:  WesternSugar\n",
      "Mechanization 5 Roadbiking\n",
      "Engineering Cooperative\n",
      "Mathematics\n",
      "6 AVST Shopping Petroleum\n",
      "Education 6 A&WRestaurants Golfing\n",
      "Engineering\n",
      "Parks Foreignlanguage\n",
      "7 AdobeSystems American\n",
      "Administration learning Petroleum\n",
      "7 Broadcasting Origami\n",
      "Agricultural BroncoWine Engineering\n",
      "8 Shopping Company\n",
      "Mechanization Company\n",
      "TheWaltDisney\n",
      "Metallurgical Foreignlanguage 8 Russian Astronomy\n",
      "9 ElectronicArts Company\n",
      "Engineering learning\n",
      "Petroleum TheWaltDisney\n",
      "Mathematics 9 Origami\n",
      "10 ElectronicArts Poi Engineering Company\n",
      "Education\n",
      "Protestant\n",
      "10 AcmeBrick Astronomy\n",
      "Affiliation\n",
      "A:Human B:Human A:Human B:DynoNet\n",
      "A:hi B:hi\n",
      "B:hi A:hey\n",
      "A:AnyfriendsworkatAVST? B:ihavethreefriendswholikeroadbiking\n",
      "B:petroleumengi??||no A:ihavetwofriendswholikeforeignlanguage\n",
      "A:nopetroleum learning\n",
      "B:russianasmajor? B:noforeignlanguagelearninghere\n",
      "A:Nope A:Ihavenofriendswholikeroadbiking\n",
      "B:protestantafiil?||math? B:dotheymajorinforeignlanguage?\n",
      "A:Ihavetwomath A:No||no\n",
      "B:SELECT2 B:petroleumengineering?\n",
      "A:oneworksatElectronicArts||SELECT10 A:NobutIhavefourfriendswhoworkforthe\n",
      "B:SELECT1 ElectronicArtscompany\n",
      "A:SELECT4 B:SELECT2 B:SELECT2 A:SELECT4\n",
      "A:StanoNet B:Human A:Human B:Rule\n",
      "B:hiya||ihaveoneforeignlanguageandgannett\n",
      "A:ihavetwoavst\n",
      "B:doyouhaveanyacmebrickandastronomy?\n",
      "A:hi||doyouhaveanyfriendsthatworkat A:manymathspeople||twoareforeignlanguage\n",
      "electronicarts? B:doyouhaveanypetroleumengineeringand\n",
      "B:Oneandtheylikeastronomy americanbroadcastingcompany?\n",
      "A:SELECT10 A:no||electronicarts\n",
      "B:SELECT2 A:SELECT4 B:SELECT1\n",
      "A:avst\n",
      "B:doyouhaveanydisneyorrestaurant?\n",
      "...\n",
      "Table8: Examplehuman-botchats. ThemutualfriendishighlightedinblueineachKB.Bots’utterances\n",
      "areinboldandselecteditemsarerepresentedbyitemIDs. Onlythefirsthalfofthehuman-Rulechatis\n",
      "shownduetospacelimit. Multipleutterancesofoneagentisseparatedby||.\n",
      "70\n",
      "60\n",
      "50\n",
      "40\n",
      "30\n",
      "20\n",
      "10\n",
      "0\n",
      "1 2 3 4 5\n",
      "egatnecreP\n",
      "Fluency Correctness\n",
      "Human\n",
      "Rule\n",
      "StanoNet\n",
      "DynoNet\n",
      "1 2 3 4 5\n",
      "70\n",
      "60\n",
      "50\n",
      "40\n",
      "30\n",
      "20\n",
      "10\n",
      "0\n",
      "1 2 3 4 5\n",
      "egatnecreP\n",
      "Cooperation Human-likeness\n",
      "1 2 3 4 5\n",
      "Figure 6: Histogram of ratings (higher is better) from third-party evaluators. Differences between\n",
      "systemsarelesssignificant.\n",
      "Partnerevaluation(1perdialogue) Third-partyevaluation(5perdialogue)\n",
      "System\n",
      "Human Comments Human Justifications\n",
      "-youhaveanyfriendswhowenttomonmouth?\n",
      "-Theflowwasniceandtheywereabletodiscernthecorrect\n",
      "answers.\n",
      "Goodpartner.Easyto\n",
      "Human 4 4.6 -humanlikebecauseofinteractiontalking\n",
      "workwith\n",
      "-Answersarehumanlike,notrobotic.Uses”hiya”tobegin\n",
      "conversation,moreofawarmtone.\n",
      "-morehumanthancomputerAgent2:hiyaAgent1:Hey\n",
      "-agent2lookedhumantome\n",
      "-definitelyhuman\n",
      "-A2couldbereplacedwitharobotwithoutnoticeable\n",
      "difference.\n",
      "Rule 2 Didn’tlistentome 4 -TheyspokeandbehavedasIoranyhumanwouldinthis\n",
      "situation.\n",
      "-Theagentjustseemstobegoingthroughthemotions,which\n",
      "givesmetheideathattheagentdoesn’texbithumanlike\n",
      "characteristics.\n",
      "-Nodjarum–Thisdoesn’tmakesenseinthiscontext,so\n",
      "doesn’tseemtobewrittenbyahuman.\n",
      "Tookforeveranddidn’t -humanlikebecauseofslightmispellingss\n",
      "StanoNet 5 reallyrespondcorrectly 3.5 -Cantelltheyarelikelyhumanbutjustnotveryverbose\n",
      "toquestions. -Theirterseconversionleanstothinkingtheywereeithernot\n",
      "payingattentionornothuman.\n",
      "-Theshortvaguesentencesareveryhumanlikemistakes.\n",
      "-Agent1isveryhumanlikebasedonthewaytheytypedand\n",
      "thefactthattheywerebeingdeceiving.\n",
      "IrepliedtwicethatI -Prettyresponsiveandlogicalprogression,butit’sverystilted\n",
      "DynoNet 4 onlyhadindoorfriends 3.8 sounding\n",
      "andwasignored. -idonothaveajose\n",
      "-Agentgivesnormalhumanresponses,“noangelaidon’t”\n",
      "-agent1waslookinglikeahumanlike\n",
      "Table 9: Comparison of ratings and comments on human-likeness from partners and third-party eval-\n",
      "uators. Each row contains results for the same dialogue. For the partner evaluation, we ask the human\n",
      "partnertoprovideasingle,optionalcommentattheendoftheconversation. Forthethird-partyevalua-\n",
      "tion,weaskfiveTurkerstorateeachdialogueandreportthemeanscore;theymustprovidejustification\n",
      "forratingsineachaspect. Fromthecomments,weseethatdialoguepartnersfocusmoreoncooperation\n",
      "andeffectiveness,whereasthird-partyevaluatorsfocusmoreonlinguisticfeaturessuchasverbosityand\n",
      "informality.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,    271,   1318, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Knowledge-Based Distant Regularization in Learning Probabilistic Models\n",
      "NaoyaTakeishi§,∗ and KosukeAkimoto†,∗\n",
      "§RIKENCenterforAdvancedIntelligenceProject\n",
      "†SecurityResearchLaboratories,NECCorporation\n",
      "naoya.takeishi@riken.jp k-akimoto@ab.jp.nec.com\n",
      "Abstract knowledge graph probabilistic modeling\n",
      "𝑦\n",
      "Exploitingtheappropriateinductivebiasbasedontheknowl- 1(Sensor 1)\n",
      "is-attached-to\n",
      "edge of data is essential for achieving good performance in is-part-of A\n",
      "statisticalmachinelearning.Inpractice,however,thedomain\n",
      "knowledgeofinterestoftenprovidesinformationontherela- 𝑦\n",
      "tionshipofdataattributesonlydistantly,whichhindersdirect 2(Sensor 2)\n",
      "utilizationofsuchdomainknowledgeinpopularregulariza- B\n",
      "tionmethods.Inthispaper,weproposetheknowledge-based is-connected-to\n",
      "distantregularizationframework,inwhichweutilizethedis- 𝑦 3(Sensor 3)\n",
      "tantinformationencodedinaknowledgegraphforregular-\n",
      "C\n",
      "ization of probabilistic model estimation. In particular, we\n",
      "propose to impose prior distributions on model parameters\n",
      "specifiedbyknowledgegraphembeddings.Asaninstanceof Figure1:Schematicdiagramofourmotivatingexample,the\n",
      "theproposedframework,wepresentthefactoranalysismodel analysisofsensordataofaplant.Insuchapplications,even\n",
      "withtheknowledge-baseddistantregularization.Weshowthe if the physical and/or statistical relations (dotted lines) be-\n",
      "resultsofpreliminaryexperimentsontheimprovementofthe\n",
      "tweenattributes,i.e.,sensorreadings,cannotbespecifieddi-\n",
      "generalizationcapabilityofsuchmodel.\n",
      "rectly, somehow distant information on plant’s instruments\n",
      "(blue arrows) are often available beforehand. We suppose\n",
      "Introduction such information can be encoded in a form of knowledge\n",
      "Thedata-drivennatureofstatisticalmachinelearningisone graphs.Wewouldliketoleveragesuchdistantinformation\n",
      "oftheprincipalreasonsforitssuccess,whereasimposingan toregularizelearningofprobabilisticmodels.\n",
      "appropriateinductivebiasisindispensableforgaininggen-\n",
      "eralizationcapability.Regularizationisaprevailingmethod-\n",
      "ologyforimposingtheinductivebias.Severaltypesofpopu- Ourmotivatingexample,depictedinFigure1,isintheanal-\n",
      "larregularizationmethodsareknown,suchasTikhonovreg- ysisofsensordataobtainedfromaplant.Althoughtherela-\n",
      "ularization and sparsity regularization, and they have been tionsbetweenplant’sinstrumentsandsubsystems(e.g.,how\n",
      "commonlyutilizedinawiderangeoftaskswithstrongtheo- they are connected each other) can often be written down\n",
      "reticalsupports.However,itisnotalwayspossibleforthose easily,therelationsbetweentheattributes(e.g.,sensorIDs)\n",
      "generalregularizationstrategiestodirectlyleveragethedo- canhardly bederiveddirectlyfrom thembecausetodo so,\n",
      "mainknowledgeavailableonagivenapplicationsincethey weneedtoidentifythemechanismandthephysicsofthein-\n",
      "areoftenbuiltuponthestatistical“meta-knowledge,”rather strumentsandthesensors,whichiscostlyinmanypractices.\n",
      "thanthedomainknowledge.\n",
      "Inthispaper,wesuggestamethodtoutilizesuchdistant\n",
      "The usability of domain knowledge is one of the pri-\n",
      "domainknowledgetoregularizetheestimationofstatistical\n",
      "malconcernsinpracticesofmachinelearning.Ifonecould\n",
      "models.Specifically,wepresentaframeworkofknowledge-\n",
      "clearly specify the relationship of attributes and objects in\n",
      "based distant regularization in learning probabilistic mod-\n",
      "data,itcouldthenbeutilizedbythemethodslikestructured\n",
      "els for continuous-valued data such as sensor readings. We\n",
      "sparsity regularization (Huang, Zhang, and Mataxas 2011)\n",
      "aim to build a regularization principle using a knowledge\n",
      "andgraphLaplacianregularization(see,e.g.,(Shahidetal.\n",
      "graph, which is a multi-relational directed graph in which\n",
      "2016) and references therein). Unfortunately, however, the\n",
      "relations between entities are enumerated. This is because\n",
      "domain knowledge of interest does not always provide di-\n",
      "a knowledge graph is often useful for summarizing the do-\n",
      "rectinformationontherelationshipoftheattributesandthe\n",
      "main knowledge of interest. Let us revisit the example of\n",
      "objects;instead,itoftengivesdistant informationonthem.\n",
      "sensor data analysis in Figure 1; engineers of a plant com-\n",
      "∗A substantial portion of this work was performed while the prehend the relations between the instruments, such as is-\n",
      "authorswereattheUniversityofTokyo. part-of andis-connected-to,aswellastherelationsbetween\n",
      "8102\n",
      "nuJ\n",
      "92\n",
      "]GL.sc[\n",
      "1v23311.6081:viXra\n",
      "theinstrumentsandthesensors,i.e.,is-attached-to,accord- However, this method is not always applicable to utilizing\n",
      "ing to which one can build a knowledge graph. Note that thepriordomainknowledgebecauseinthismethod,allthe\n",
      "what is specified on the data attributes is only the simple dataattributesmustbeidentifiedinthegraphandextension\n",
      "is-attached-to information, and no direct relations between tomulti-relationalcasesisnotstraightforward.\n",
      "themareprovidedingeneral. Another example of graph-based regularization is the\n",
      "Thecentralideaoftheproposedmethodistodefineprior methods using the graph Laplacian (see, e.g., (Shang, Jiao,\n",
      "distributions of model parameters using knowledge graph andWang2012;Pei,Chakraborty,andSycara2015;Shahid\n",
      "embeddings(seetheexcellentsurveypaperssuchas(Nickel et al. 2016)), in which the Laplacian matrix of graphs that\n",
      "etal.2015;Wangetal.2017;Cai,Zheng,andChang2018)), encode relations between objects and/or attributes is used\n",
      "whicharecontinuous-valuedrepresentationsofentities(and forspecifyingtheregularizationterm.However,suchmeth-\n",
      "relations) in knowledge graphs. The proposed method is ods are also not always applicable in practice because we\n",
      "flexible enough to utilize many types of knowledge graph maynotbeabletocomputeareliableLaplacianmatrixdue\n",
      "embeddings,whichformanactivefieldofresearchrecently. tothelackofpreciseknowledgeoftherelationsbetweenthe\n",
      "Hence,thedevelopmentinthefieldcanbeimmediatelyim- dataattributesandobjects.\n",
      "portedtotheproposedframework.\n",
      "Incorporating Knowledge as Constraints / Rules Re-\n",
      "The chief advantage of the proposed method, the distant\n",
      "searchershavebeenstudyingonmethodstoincorporatethe\n",
      "regularizationbyknowledgegraphs,isthatitcanbeapplied\n",
      "knowledgedescribedinaformofconstraintsorlogicalrules\n",
      "to a broad range of probabilistic models. We introduce the\n",
      "to well-known machine learning methods. To name a few,\n",
      "knowledge-based factor analysis model in this paper as an\n",
      "Towell and Shavlik (1994) proposed the knowledge-based\n",
      "instance, but similar modification of existing probabilistic\n",
      "neural networks, Fung, Mangasarian, and Shavlik (2003)\n",
      "models is possible whenever the domain knowledge of in-\n",
      "suggested the knowledge-based support vector machines,\n",
      "terestisdescribedinaformofknowledgegraphs.\n",
      "and Varol et al. (2012) formulated the constrained latent\n",
      "variable model. Moreover, there are studies on incorporat-\n",
      "RelatedWork\n",
      "inglogicalrulestoexistingstatisticalmodels.Schiegg,Neu-\n",
      "Theinductivebiasistheessentialelementofstatisticalma- mann,andKersting(2012)proposedthemixturesofGaus-\n",
      "chinelearningandusuallyintroducedbydesigningfeatures, sian process regressors controlled by logical rules formu-\n",
      "hypothesis spaces, loss functions, regularization terms, and latedbytheMarkovlogicnetwork(RichardsonandDomin-\n",
      "soon.Forexample,themodelfamilyisusuallyselectedac- gos2006).And,severalresearchersproposedthetopicmod-\n",
      "cordingtouser’sknowledgeofthedatatobeanalyzed.Ifthe elswithlogic(Andrzejewskietal.2011;Mei,Zhu,andZhu\n",
      "user is sure about the relationship of attributes and objects 2014; Foulds, Kumar, and Getoor 2015), which are useful\n",
      "inadataset,amodeltailoredforthedataset(e.g.,Bayesian for modeling document corpora with prior knowledge of\n",
      "networks)canbedeveloped.Thisisdifficultorimpossible, words.\n",
      "however,whentheknowledgeisgiveninadistantform.In\n",
      "StatisticalRelationalLearning Moregenerally,learning\n",
      "fact,therearealotofresearchesonutilizingknowledgein\n",
      "onrelationaldata,whichisoftentermedstatisticalrelational\n",
      "variousforms.\n",
      "learning (SRL), is an active area of research with a broad\n",
      "In what follows, we introduce only a part of the rich lit-\n",
      "rangeofmethodsandapplications(see,e.g.,(DeRaedt,Ker-\n",
      "erature on knowledge utilization for machine learning and\n",
      "sting, and Natarajan 2016)). We do not describe the details\n",
      "someotherrelatedtopicsthatwouldbeusefulforconsider-\n",
      "herebecauseofthevastamountoftheliteratureinthefield,\n",
      "ingthefuturedirectionofthiswork.\n",
      "but the methodology of SRL would be a promising tool in\n",
      "Incorporating Knowledge as Graphs Actually, it has thefuturedirectionofthiswork.\n",
      "beenactivelystudiedhowtoutilizethepriorknowledgethat\n",
      "Distant / Weak Supervision by Knowledge Particularly\n",
      "canbeencodedinaknowledgegraph.Forexample,Yaoet\n",
      "inthecontextofsupervisedlearning,thereisalineofstudies\n",
      "al. (2017) proposed an extension of latent Dirichlet alloca-\n",
      "onutilizinguser’sheuristicsorknowledgebasesasweaksu-\n",
      "tionthatmodelswordtokensaswellasentitiesindocument\n",
      "pervisionsources(see(Ratneretal.2016;Varmaetal.2017;\n",
      "corpora, where the entities are corroborated by a knowl-\n",
      "Bachetal.2017)andreferencestherein)withgrowinginter-\n",
      "edge graph and have the corresponding embeddings. Also\n",
      "ests. In the settings of those studies, the knowledge should\n",
      "in the context of topic models, Yao et al. (2015) and Hu\n",
      "provideinformationonlabelstobepredicted.\n",
      "et al. (2016) suggested to utilize a taxonomy of entities in\n",
      "documents; a taxonomy, i.e., a set of hierarchical relations Posterior Regularization Zhu, Chen, and Xing (2014)\n",
      "between entities, can be described as a (single-relational) proposed an elegant and flexible framework, termed Reg-\n",
      "graph. Note that these studies mainly treat discrete-valued Bayes,forregularizingtheposteriordistributionofBayesian\n",
      "datasuchasdocumentcorpora. inference. It can be utilized for efficiently incorporating\n",
      "Regularization based on the graph information is a ma- one’s knowledge in Bayesian inference, and in fact, Mei,\n",
      "jor field of research also in other contexts. For example, Zhu, and Zhu (2014) utilized it for incorporating logical\n",
      "thegraphsparsityregularization(Jacob,Obozinski,andVert rules to a topic model. Note that, while RegBayes tries to\n",
      "2009; Huang, Zhang, and Mataxas 2011; Mairal and Yu changetheposterior,themethodproposedinthisworktries\n",
      "2013) imposes structured sparsity on parameters according tochangetheprior,whichmayremindoneoftheempirical\n",
      "to a graph that encodes prior knowledge on data attributes. Bayesmethod.\n",
      "Background Representation learning for knowledge graphs is an ac-\n",
      "In this section, we first introduce the notations used in this tive area of research recently. We do not enumerate the\n",
      "paper to express the concepts regarding the probabilistic works and just refer to the excellent survey papers on the\n",
      "models. Afterward, we give a brief explanation on knowl- field(Nickeletal.2015;Wangetal.2017;Cai,Zheng,and\n",
      "edgegraphembeddingsforcompleteness,whichreadersfa- Chang2018).Here,weintroducetheverybasicconceptsin\n",
      "miliarwiththemmayskip. this field, which are needed when describing the proposed\n",
      "method.\n",
      "ProbabilisticModelsforContinuous-ValuedData In a typical setting, given a set of tuples that exist in a\n",
      "knowledgegraph(i.e.,positivetuples),welearnreal-valued\n",
      "Themainsubjectofthisstudyistheregularizationinthees-\n",
      "timationofprobabilisticmodelsforcontinuous-valueddata.\n",
      "vectorse ∈ Rde correspondingtoentitiesintheknowledge\n",
      "graph. Such vectors are called embeddings of entities. The\n",
      "Formally, we denote a continuous-valued dataset by a set\n",
      "{y ∈Rm |j =1,...,n},wherey istheobservationwith learningisdonesothatsomescorefunction\n",
      "j j\n",
      "mattributes(i.e.,features)correspondingtothej-thobject. ψ(e, e ; M ):Rde ×Rde →R (1)\n",
      "h t r\n",
      "And, n denotes the number of objects (individuals, times-\n",
      "becomeslargeforthepositivetuplesandsmallfortheneg-\n",
      "tamps,etc.)inadataset.Forinstance,wemayhavereadings\n",
      "ativetuples.Here,e ande denotetheembeddingsofen-\n",
      "of m types of sensors at n timestamps in the sensor data. h t\n",
      "titieshandt,respectively,And,ψisthescorefunctionthat\n",
      "Notethateachy mayormaynotbeindependentandiden-\n",
      "j hastherelation-specificparameter,M,withregardtorela-\n",
      "ticallydistributed.Theprobabilisticmodelsforsuchdatade- r\n",
      "tion r. In the latter part of this paper, as an instance of the\n",
      "fine probability density p(y,...,y | θ), where θ is a set\n",
      "1 n knowledgegraphembedding,weuseasimplemethodcalled\n",
      "ofmodelparameters.Moreover,somemodelsmayhavean\n",
      "additionalsetofrandomvariables,denotedby{x\n",
      "j\n",
      "∈Rdx}, D asis ft oM llou wlt s( :Yangetal.2015),whosescorefunctionisdefined\n",
      "whichisoftentermedlatentvariables.\n",
      "We further distinguish the model parameters as θ = ψ(e, e ; m )=e(cid:62)diag(m )e, (2)\n",
      "h t r h r t\n",
      "l{ ow ca1 l,. p. a. r, aw mm ete, rπ 1} c; oo rn reso pn oe nh da inn gd, tole tt hw eii -t∈ haR ttd riw bud te en.o Ote ntt hh ee where m\n",
      "r\n",
      "∈ Rde is the relation-specific parameter. There\n",
      "are several manners of optimization for learning e and M,\n",
      "other hand, let π be the vector of global parameters that\n",
      "suchasenergy-basedoptimizationandrankoptimization.In\n",
      "are independent of the difference of attributes. Let us give\n",
      "thispaper,asdescribedbelow,wesimplytreatψasapartof\n",
      "anexample;theobservationmodelofprobabilisticprincipal\n",
      "a logistic regressor and maximize the binary classification\n",
      "componentanalysis(TippingandBishop1999)is\n",
      "likelihoodvianegativetuplesampling.\n",
      "p(y |x,θ)=N\n",
      "(cid:0)\n",
      "y |Wx +µ,\n",
      "σ2I(cid:1)\n",
      ", Moststudiesontheknowledgegraphembeddingconsider\n",
      "j j j j\n",
      "only the sparse structure of a multi-relational graph. Re-\n",
      "where W ∈ Rm×dx is the factor loading matrix and d\n",
      "x\n",
      "is\n",
      "cently,thereareseveralstudiesonfusingadditionalinforma-\n",
      "thedimensionalityoflatentvariablex.Inthismodel,thei-th\n",
      "tionassociatedwithknowledgegraphssuchascontinuous-\n",
      "localparameter,w,correspondstothei-throwofW (and\n",
      "i valued attributes of entities (Zhang et al. 2016), literals\n",
      "the i-th element of µ), and σ2 is an element of the global\n",
      "(Kristiadi et al. 2018), text descriptions (Xie et al. 2016;\n",
      "parameters,π.\n",
      "Fan et al. 2017; Xiao et al. 2017), images (On˜oro-Rubio\n",
      "Theregularizationintheestimationofprobabilisticmod-\n",
      "etal.2017),andthemultimodalinformation(Pezeshkpour,\n",
      "els is often executed by setting prior distributions on the\n",
      "Chen, and Singh 2017; Thoma, Rettinger, and Both 2017).\n",
      "parameters,andmanyotherpopularregularizationmethods\n",
      "Theirprimaryconcernistheimprovementoftheknowledge\n",
      "can also be interpreted as imposing prior distributions. In\n",
      "graphembeddingsandtheirapplicationssuchaslinkpredic-\n",
      "thispaper,wefollowthisstrategytoachievetheknowledge-\n",
      "tion.Thetechniquesutilizedtherewouldbeofgreatinterest\n",
      "baseddistantregularization;weplacethepriordistributions\n",
      "also for our purpose, i.e., regularization in learning proba-\n",
      "that are parameterized by the embeddings of knowledge\n",
      "bilisticmodelsforcontinuousdata.\n",
      "graphs.Thedetailsaredescribedinthenextsection.\n",
      "ProposedMethod\n",
      "KnowledgeGraphEmbedding\n",
      "In this section, we describe the detail on the proposed\n",
      "A knowledge graph is a multi-relational directed graph\n",
      "method for distantly regularizing the estimation of proba-\n",
      "whoseverticescorrespondtoentitiesandedgescorrespond\n",
      "bilistic models using knowledge graph embeddings. First,\n",
      "to relations between the entities. Such graph can be de-\n",
      "thegeneralframework,inwhichwedonotpremisespecial\n",
      "scribed as a set of tuples {(h,r,t)}, where h and t denote\n",
      "typesofprobabilisticmodelsandknowledgegraphembed-\n",
      "the head entity and the tail entity respectively, and r de-\n",
      "dings,ispresented.Afterward,asaninstanceoftheframe-\n",
      "notes the relation between them. There are various large-\n",
      "work,weintroducethefactoranalysisdistantlyregularized\n",
      "scaleknowledgebaseswhichcanberegardedasknowledge\n",
      "byaknowledgegraph.\n",
      "graphs,suchasFreebaseandGeneOntology,whicharecol-\n",
      "lectionsofknowledgeongeneralentitiesoftheworldoron\n",
      "GeneralFramework\n",
      "domain-specificconcepts.\n",
      "Ourpurposeistobuildaregularizationprincipleforthees-\n",
      "1Thedimensionalityofw maydependoni,butforsimplicity, timation of the parameters of probabilistic models accord-\n",
      "i\n",
      "weassumethesamedimensionalityforalli. ing to a knowledge graph that encodes (possibly distant)\n",
      "priorknowledgeonthedata-generatingsystem.Tothisend, FactorAnalysisRegularizedbyKnowledge\n",
      "we propose to specify the prior distributions of the model\n",
      "Amongtheprobabilisticmodelsforcontinuousdata,thefac-\n",
      "parameters using knowledge graph embeddings. Formally,\n",
      "tor analysis (FA) and its special version, principal compo-\n",
      "consideraprobabilisticmodel\n",
      "nent analysis (PCA), have been favored in many applica-\n",
      "p(y,...,y |θ), (3) tions.Asaninstanceoftheproposedframework,wepresent\n",
      "1 n\n",
      "asimplemethodforincorporatingknowledgegraphstoreg-\n",
      "where θ = {w,...,w,π}. In the proposed framework,\n",
      "1 m ularizationintheparameterestimationofFA.\n",
      "wespecifythepriordistributionsonthe(partof)localmodel\n",
      "The observation probabilistic model of FA is often ex-\n",
      "parameters,w,...,w,asfollows.\n",
      "1 m pressedas\n",
      "First, we assume that we have a knowledge graph, in\n",
      "which entities corresponding to the (part of) data attributes\n",
      "p(y,...,y |x,...,x,θ)\n",
      "exist,andthatsuchentitiesappearinasetofpositivetuples 1 n 1 n\n",
      "m n\n",
      "atleastonce.Inthesensordataexample(Figure1),thisas- (cid:89)(cid:89)\n",
      "= N(y |w(cid:62)x +µ, σ2), (6)\n",
      "sumption means that there is an entity like “Sensor i,” and i,j i j i i\n",
      "theknowledgegraphhasatuplelike i=1j=1\n",
      "(h=Sensori, r =is-attached-to, t=InstrumentA) wherew(cid:62) ∈R1×dx isthei-throwofthefactorloadingma-\n",
      "i\n",
      "foratleastapartofattributesi=1,...,m(cid:48)(1≤m(cid:48) ≤m). trixofFA,µ i istheobservationmean,andσ i2 ∈ R+ isthe\n",
      "observation noise variance of the i-th attribute. Moreover,\n",
      "Inthefollowing,wedenotetheembeddingoftheentitythat\n",
      "corresponds to the i-th attribute of data by e. Note that\n",
      "x\n",
      "j\n",
      "∈ Rdx is the latent factor corresponding to the j-th ob-\n",
      "i\n",
      "ject, whose prior distribution is usually set by the standard\n",
      "in the knowledge graph of our interest, there would also\n",
      "normaldistribution,i.e.,N(0,I).\n",
      "be many entities that do not directly correspond to the at-\n",
      "Letusimposetheknowledge-basedpriordistributionson\n",
      "tributes, and not all the attributes appear in the knowledge\n",
      "(partof)therowsofthefactorloadingmatrix,w,...,w\n",
      "graph. Also note that, while here we consider only the en- 1 m(cid:48)\n",
      "(m(cid:48) ≤ m). Here, instead of the normal distribution like\n",
      "tities corresponding to the attributes, similar discussion is\n",
      "Eq. (4), we specify the prior distribution of w given em-\n",
      "straightforwardevenwhenweconsiderentitiescorrespond- i\n",
      "beddinge bythepointmass,i.e.,\n",
      "ingtotheobjectsindata. i\n",
      "Giventheknowledgegraphofthiskind,wespecifyaprior p(w |e )=δ(w −c (e )), for i=1,...,m(cid:48), (7)\n",
      "distribution on w parameterized by e, for i = 1,...,m(cid:48), i i i ξ i\n",
      "i i\n",
      "asaregularizerofthemodeltraining.Forexample,forreal- whereδdenotestheDiracdeltafunction.Thissimplification\n",
      "valued parameters, a canonical choice is the normal distri- enables us to maximize the (regularized) objective just by\n",
      "bution,i.e., replacingw withc (e ).Weobservednosignificantdegra-\n",
      "i ξ i\n",
      "dationofperformanceduetothissimplification.\n",
      "p(w |e )=N(w |c (e ), V (e )), (4)\n",
      "i i i ξ i ξ i\n",
      "In summary, an optimization problem to be solved for\n",
      "where c ξ : Rde → Rdw, V ξ : Rde → Rdw×dw, and ξ learning FA distantly regularized based on the knowledge\n",
      "denotes the set of parameters in c and V. With the priors graphisthemaximizationof\n",
      "onw,...,w specifiedinthismanner,wecomputetheir\n",
      "1 m(cid:48)\n",
      "maximum a posteriori (MAP) estimations. For the remain- f(e 1:E,M 1:R,w 1:m,µ 1:m,σ 12 :m,ξ)\n",
      "ingattributesw m(cid:48)+1,...,w m,ifany,wecomputethemaxi-\n",
      "1\n",
      "(cid:88)n\n",
      "mumlikelihoodestimationsorMAPestimationswithpriors = logN(y |µ, Σ)\n",
      "n j\n",
      "definedotherwise.\n",
      "j=1\n",
      "For the above framework, the embeddings of knowledge\n",
      "(cid:96)\n",
      "graph’sentitiesmustbepreparedinsomeways.Apromis-\n",
      "1(cid:88)\n",
      "+ logΦ(ψ(e, e ; M )) (8)\n",
      "ingwaytopreparegoodembeddingsistoutilizethemodel (cid:96) hk tk rk\n",
      "k=1\n",
      "pretrainedonalarge-scaleknowledgegraph.However,such\n",
      "(cid:96)(cid:48)\n",
      "pretrained embeddings are usually not available for user- 1 (cid:88) (cid:0) (cid:1)\n",
      "+ log 1−Φ(ψ(e, e ; M )),\n",
      "definedknowledgegraphsthatdescribethedomainknowl- (cid:96)(cid:48) h k(cid:48) t k(cid:48) r k(cid:48)\n",
      "edge.Hence,inmanypractices,theembeddingshavetobe k(cid:48)=1\n",
      "learned simultaneously with the parameters of the proba- s.t. w i =c ξ(e i) for i=1,...,m(cid:48),\n",
      "bilistic model. Now suppose we define a score function ψ\n",
      "for the embedding learning. In the framework, we propose where µ = [µ 1 ··· µ m](cid:62), Σ = WW(cid:62) +\n",
      "tomaximizebinaryclassificationlikelihood: diag{σ 12,...,σ m2 }, and W = [w 1 ··· w m](cid:62). In Eq. (8),\n",
      "E denotes the total number of entities (including the ones\n",
      "p((h,r,t)=true|e, e, M )=Φ(ψ(e, e ; M )), (5)\n",
      "h t r h t r corresponding to the data attributes), R denotes the num-\n",
      "whereΦisthesigmoidfunction.Notethatanyψcanbeused berofrelations,and(cid:96)and(cid:96)(cid:48) denotethenumbersofpositive\n",
      "heresolongasitsrangeisthereal.Bysettingtheobjective and negative tuples, respectively. The first term of Eq. (8)\n",
      "of the embedding learning in this way, the final objective isobtainedbymarginalizingoutxinEq.(6)withtheprior\n",
      "function simply becomes the addition of the logarithms of N(0,I).\n",
      "datalikelihoodEq.(3),parameterpriorsEq.(4),andknowl- The negative tuples, needed for computing Eq. (8), are\n",
      "edgegraphlikelihoodEq.(5). usually not available originally. They are often created by\n",
      "−660\n",
      "−670\n",
      "−680\n",
      "−690\n",
      "0 50 100\n",
      "proportionofKGtuples[%]\n",
      "AFfoLLNtset.evA −660\n",
      "−680\n",
      "20 40 60 80\n",
      "proportionoftrainingdata[%]\n",
      "AFfoLLNtset.evA\n",
      "w/KG\n",
      "w/oKG\n",
      "(a)\n",
      "−650\n",
      "−660\n",
      "−670\n",
      "0 50 100\n",
      "proportionofKGtuples[%]\n",
      "AFfoLLNtset.evA −550\n",
      "−600\n",
      "−650\n",
      "−700\n",
      "20 40 60 80\n",
      "proportionoftrainingdata[%]\n",
      "AFfoLLNtset.evA\n",
      "w/KG\n",
      "w/oKG\n",
      "(b)\n",
      "Figure2:Resultsunder(a)theRandomdata-partitionscenarioand(b)theShiftdata-partitionscenario.Ineachpanel,left\n",
      "plot shows average test NLLs along different proportions of knowledge graph tuples used by the proposed method, and the\n",
      "rightplotshowsaveragetestNLLsalongdifferentamountsofdatausedforthetraining.Intherightplots,thetestNLLsoftwo\n",
      "casesareshown:100%oftheknowledgegraphtupleswasusedbytheproposedmethod(w/KG)ornoknowledgegraphtuples\n",
      "wereprovided(w/oKG).Ineveryplot,themeansandthestandarddeviationsfor10randomtrialsareshown.\n",
      "randomly permuting the entities and/or the relations in the Therefore, n = 1,380 and m = 227. The dataset was cre-\n",
      "positive tuples (see, e.g., (Nickel et al. 2015; Wang et al. ated using the resource available online at Climate Change\n",
      "2017;Cai,Zheng,andChang2018)).Inthiswork,wecre- KnowledgePortal.2\n",
      "ated some negative tuples per a positive tuple by replacing\n",
      "Knowledge Graph In order to precisely model the rela-\n",
      "eithertheheadorthetailbyanyotherentitythatexistsinthe\n",
      "tionship of precipitation among countries and regions, we\n",
      "knowledge graph. When permuting the entity, we chose an\n",
      "needmeteorologicalknowledgetosomeextent.However,in\n",
      "attribute-correspondingentityforanattribute-corresponding\n",
      "ourscenario,suchknowledgethatdirectlygivestherelations\n",
      "entity,andanon-attributeentityforanon-attributeentity,re-\n",
      "betweendataattributescanhardlybeobtained.Tosimulate\n",
      "spectively.\n",
      "this situation, in the experiments, we utilize a knowledge\n",
      "InEq.(8),ψ isanarbitraryscorefunctionofknowledge\n",
      "graphthatencodessimplegeographicrelationsbetweenthe\n",
      "graph embedding learning. In the experiments introduced\n",
      "countriesandregions.\n",
      "below,weusedthesimplescorefunctionofDistMult(Yang\n",
      "etal.2015),definedinEq.(2).Moreover,asc (·),weused Webuiltaknowledgegraphusingtheinformationavail-\n",
      "ξ able online,3 which includes the geographic relationship\n",
      "anaffinemodel\n",
      "c (e )=Ae +b (9) between countries and regions in the world, following\n",
      "ξ i i\n",
      "the scheme presented by Bouchard, Singh, and Trouillon\n",
      "withparametersA ∈ Rdx×de andb ∈ Rdx usedcommonly\n",
      "(2015).Theknowledgegraphweusedhas278entities(in-\n",
      "fori=1,...,m(cid:48),i.e.,ξ ={A,b}.\n",
      "cluding ones that do not correspond to the attributes of the\n",
      "data) and two types of relations, is-inside and is-neighbor-\n",
      "PreliminaryExperiments\n",
      "of; positive tuples include (Sweden, is-inside, Europe) and\n",
      "Inthissection,weprovidetheresultsofpreliminaryexper- (Norway, is-neighbor-of, Sweden), for example. And, the\n",
      "iments to investigate how the performance of the distantly knowledge graph has 1,167 positive tuples comprising the\n",
      "regularizedmodelsisimproved. entitiesandtherelationsabove.\n",
      "Dataset We used real-world data of rainfall in the world. 2sdwebx.worldbank.org/climateportal/\n",
      "The dataset we prepared comprises the monthly means of (retrieved29April2018)\n",
      "historicalmeasurementsofprecipitationfromJanuary1901 3github.com/mledoze/countries/\n",
      "toDecember2015in227countriesandregionsintheworld. (retrieved29April2018)\n",
      "Notethat,inthisknowledgegraph,someentitiesofcoun- In Figures 2a and 2b, we show the results under the\n",
      "tries like Japan have no is-neighbor-of relations with any Random and Shift data-partition scenarios, respectively.\n",
      "other entities because the is-neighbor-of relation does not In the left plots, the test NLLs for different numbers of\n",
      "imply the cross-sea adjacency. Therefore, if we build a knowledge graph tuples used by the proposed method are\n",
      "single-relationalgraphonlywiththeis-neighbor-of relation, shown.Basically,thebettertestNLLsareachievedwiththe\n",
      "which might be utilized for methods like graph Laplacian larger amount of knowledge graph’s information provided.\n",
      "regularization,itdoesnotprovidesufficientinformationon In the right plots, the test NLLs with different amounts of\n",
      "thecountriesthathavenoneighboringcountries. trainingdataareshown.WhilethetestNLLsarealwaysim-\n",
      "provedbytheproposedmethodundertheRandomscenario,\n",
      "Setups Asforthefactoranalysismodeldistantlyregular-\n",
      "noimprovementisobservedwithsomewhatsmallamountof\n",
      "izedbyknowledge,thehyperparameterstobesetmanually\n",
      "trainingdataundertheShiftscenario.\n",
      "are the dimensionality of latent factors, d, as well as the\n",
      "x\n",
      "dimensionalityofentityembeddings,d.Forafaircompar-\n",
      "e\n",
      "Conclusion\n",
      "ison, we set the same values for them, i.e., d = d. And,\n",
      "x e\n",
      "in what follows, we show the results with d = 5, which\n",
      "x In this paper, we have proposed the knowledge-based dis-\n",
      "was decided empirically without any intensive search be-\n",
      "tantregularizationframework,ageneralmethodologytoin-\n",
      "cause the purpose of this experiment was just to compare\n",
      "corporate the possibly distant domain knowledge encoded\n",
      "theperformancesofafactoranalysismodelwithandwith-\n",
      "inaformofknowledgegraphs.Inparticular,wesuggested\n",
      "outtheknowledge-basedregularization.\n",
      "thefactoranalysiswiththeknowledge-basedregularization.\n",
      "Wepartitionedthedatasetintothetraining/validationset\n",
      "Weprovidedthepreliminaryexperimentalresultsontheim-\n",
      "andthetestsetattheratesspecifiedbelow.Withinthetrain-\n",
      "provementofthegeneralizationcapabilityoftheregularized\n",
      "ing/validationset,wealwaysusedrandomlychosen80%for\n",
      "factoranalysis.\n",
      "trainingandtheremaining20%forvalidation.Whenparti-\n",
      "Indeed, the proposed framework is in its infancy. There\n",
      "tioning the original dataset into the training/validation set\n",
      "are plenty of challenges to be tackled. For example, we\n",
      "and<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  72828,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Climate Change Knowledge Portal', 'Countries']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: regularizationframework,ageneralmethodologytoin-\n",
      "cause the purpose of this experiment was just to compare\n",
      "corporate the possibly distant domain knowledge encoded\n",
      "theperformancesofafactoranalysismodelwithandwith-\n",
      "inaformofknowledgegraphs.Inparticular,wesuggested\n",
      "outtheknowledge-basedregularization.\n",
      "thefactoranalysiswiththeknowledge-basedregularization.\n",
      "Wepartitionedthedatasetintothetraining/validationset\n",
      "Weprovidedthepreliminaryexperimentalresultsontheim-\n",
      "andthetestsetattheratesspecifiedbelow.Withinthetrain-\n",
      "provementofthegeneralizationcapabilityoftheregularized\n",
      "ing/validationset,wealwaysusedrandomlychosen80%for\n",
      "factoranalysis.\n",
      "trainingandtheremaining20%forvalidation.Whenparti-\n",
      "Indeed, the proposed framework is in its infancy. There\n",
      "tioning the original dataset into the training/validation set\n",
      "are plenty of challenges to be tackled. For example, we\n",
      "andthetestset,wefollowedtwodifferentscenarios.Oneis\n",
      "need to develop a unified method for efficiently learning\n",
      "theRandomdata-partition,inwhichrandompermutationof\n",
      "themodelsthatareregularizedbyknowledgegraphs.Also,\n",
      "the 1,380 objects in each dataset was performed before the\n",
      "how the generalization capability of the regularized mod-\n",
      "partition.AnotheristheShiftdata-partition,inwhichno\n",
      "elsisimprovedshouldbeanalyzedtheoreticallyandempir-\n",
      "randompermutationisdonebeforethepartition,andconse-\n",
      "ically. Moreover, it would be interesting to investigate how\n",
      "quently,somethinglikethecovariateshiftmayoccur,which\n",
      "the knowledge graph embeddings are modified due to the\n",
      "makesgeneralizationdifficult.\n",
      "datafedintotheprobabilisticmodels.\n",
      "We also controlled the amount of information possibly\n",
      "provided by the knowledge graph by changing the propor-\n",
      "References\n",
      "tionofknowledgegraph’s(positive)tuplesconsideredinthe\n",
      "optimizationofEq.(8).Forexample,whenweused80%of\n",
      "Andrzejewski,D.;Zhu,X.;Craven,M.;andRecht,B. 2011.\n",
      "the tuples, we randomly disposed 20% of the 1,167 posi-\n",
      "A framework for incorporating general domain knowledge\n",
      "tivetuplesandperformedtheoptimizationwiththenegative\n",
      "into latent Dirichlet allocation using first-order logic. In\n",
      "sampling. As the “no knowledge graph” limit, we ran the\n",
      "Proceedings of the 22nd International Joint Conference on\n",
      "optimizationwith0%ofthetuples.\n",
      "ArtificialIntelligence,1171–1177.\n",
      "The optimization was performed using gradient descent\n",
      "Bach, S. H.; He, B.; Ratner, A.; and Re´, C. 2017. Learn-\n",
      "whose learning rate was adjusted by Adam. For the opti-\n",
      "ingthestructureofgenerativemodelswithoutlabeleddata.\n",
      "mization, we randomly created two negative tuples per a\n",
      "InProceedingsofthe34thInternationalConferenceonMa-\n",
      "positivetuple.Thevalidationsetwasutilizedforearlystop-\n",
      "chineLearning,273–282.\n",
      "ping of optimization, in which the optimization was termi-\n",
      "natedifnoimprovementofthelosswithregardtoFA(i.e., Bouchard, G.; Singh, S.; and Trouillon, T. 2015. On ap-\n",
      "the negative of the first term of Eq. (8)) on the validation proximatereasoningcapabilitiesoflow-rankvectorspaces.\n",
      "setwasobservedfor50epochs,andthemodelthatachieved InProceedingsoftheAAAISpringSyposiumonKnowledge\n",
      "the best loss was saved. We randomly initialized the factor RepresentationandReasoning.\n",
      "loading matrix of FA, the embeddings of entities, and the Cai,H.;Zheng,V.W.;andChang,K. 2018. Acomprehen-\n",
      "relationparameters.Nopretrainingfortheembeddingswas sive survey of graph embedding: Problems, techniques and\n",
      "performed. applications. IEEE Transactions on Knowledge and Data\n",
      "Engineering. inpress,arXiv:1709.07604.\n",
      "Results In what follows, we report the average negative\n",
      "loglikelihood(NLL)ofFAforthetestdataset,i.e., De Raedt, L.; Kersting, K.; and Natarajan, S. 2016. Sta-\n",
      "tisticalRelationalArtificialIntelligence:Logic,Probability,\n",
      "1\n",
      "(cid:88)ntest\n",
      "andComputation. Morgan&ClaypoolPublishers.\n",
      "− logN(y |µ, Σ),\n",
      "n j Fan, M.; Zhou, Q.; Zheng, T. F.; and Grishman, R. 2017.\n",
      "test\n",
      "j=1\n",
      "Distributed representation learning for knowledge graphs\n",
      "wheren denotesthenumberofdatapointsinthetestset. withentitydescriptions. PatternRecognitionLetters93:31–\n",
      "test\n",
      "NotethatlowervaluesofNLLarepreferred. 37.\n",
      "Foulds,J.;Kumar,S.H.;andGetoor,L. 2015. Latenttopic Shang, F.; Jiao, L. C.; and Wang, F. 2012. Graph\n",
      "networks:Aversatileprobabilisticprogrammingframework dualregularizationnon-negativematrixfactorizationforco-\n",
      "fortopicmodels. InProceedingsofthe32ndInternational clustering. PatternRecognition45(6):2237–2250.\n",
      "ConferenceonMachineLearning,777–786. Thoma,S.;Rettinger,A.;andBoth,F. 2017. Towardsholis-\n",
      "Fung,G.M.;Mangasarian,O.L.;andShavlik,J.W. 2003. tic concept representations: Embedding relational knowl-\n",
      "Knowledge-based support vector machine classifiers. In edge, visual attributes, and distributional word semantics.\n",
      "Advances in Neural Information Processing Systems, vol- InLectureNotesinComputerScience,volume10587,694–\n",
      "ume15,537–544. 710.\n",
      "Hu, Z.; Luo, G.; Sachan, M.; Xing, E.; and Nie, Z. 2016. Tipping,M.E.,andBishop,C.M. 1999. Probabilisticprin-\n",
      "Groundingtopicmodelswithknowledgebases. InProceed- cipal component analysis. Journal of the Royal Statistical\n",
      "ingsofthe25thInternationalJointConferenceonArtificial Society:SeriesB61(3):611–622.\n",
      "Intelligence,1578–1584.\n",
      "Towell, G. G., and Shavlik, J. W. 1994. Knowledge-\n",
      "Huang,J.;Zhang,T.;andMataxas,D. 2011. Learningwith basedartificialneuralnetworks. ArtificialIntelligence70(1-\n",
      "structuredsparsity. JournalofMachineLearningResearch 2):119–165.\n",
      "12:3371–3412.\n",
      "Varma,P.;He,B.;Bajaj,P.;Khandwala,N.;Banerjee,I.;Ru-\n",
      "Jacob,L.;Obozinski,G.;andVert,J.-P. 2009. Grouplasso bin,D.;andRe´,C. 2017. Generativemodelstructurewith\n",
      "with overlap and graph lasso. In Proceedings of the 26th staticanalysis. InAdvancesinNeuralInformationProcess-\n",
      "InternationalConferenceonMachineLearning,433–440. ingSystems,volume30,239–249.\n",
      "Kristiadi, A.; Khan, M. A.; Lukovnikov, D.; Lehmann, J.; Varol, A.; Salzmann, M.; Fua, P.; and Urtasun, R. 2012.\n",
      "andFischer,A. 2018. Incorporatingliteralsintoknowledge A constrained latent variable model. In Proceedings of\n",
      "graphembeddings. arXiv:1802.00934. the2012IEEEConferenceonComputerVisionandPattern\n",
      "Mairal,J.,andYu,B. 2013. Supervisedfeatureselectionin Recognition,2248–2255.\n",
      "graphswithpathcodingpenaltiesandnetworkflows. Jour- Wang, Q.; Mao, Z.; Wang, B.; and Guo, L. 2017. Knowl-\n",
      "nalofMachineLearningResearch14(1):2449–2485. edge graph embedding: A survey of approaches and appli-\n",
      "Mei,S.;Zhu,J.;andZhu,X.2014.RobustRegBayes:Selec- cations. IEEETransactionsonKnowledgeandDataEngi-\n",
      "tivelyincorporatingfirst-orderlogicdomainknowledgeinto neering29(12):2724–2743.\n",
      "Bayesian models. In Proceedings of the 31st International Xiao, H.; Huang, M.; Meng, L.; and Zhu, X. 2017. SSP:\n",
      "ConferenceonMachineLearning,number1,253–261. Semanticspaceprojectionforknowledgegraphembedding\n",
      "Nickel, M.; Murphy, K.; Tresp, V.; and Gabrilovich, E. withtextdescriptions.InProceedingsofthe31stAAAICon-\n",
      "2015. A review of relational machine learning for knowl- ferenceonArtificialIntelligence,3104–3110.\n",
      "edgegraphs. ProceedingsoftheIEEE104(1):11–33. Xie,R.;Liu,Z.;Jia,J.;Luan,H.;andSun,M. 2016. Repre-\n",
      "On˜oro-Rubio,D.;Niepert,M.;Garc´ıa-Dura´n,A.;Gonza´lez, sentationlearningofknowledgegraphswithentitydescrip-\n",
      "R.; and Lo´pez-Sastre, R. J. 2017. Representation learning tions. InProceedingsofthe30thAAAIConferenceonArti-\n",
      "forvisual-relationalknowledgegraphs. arXiv:1709.02314. ficialIntelligence,2659–2665.\n",
      "Pei, Y.; Chakraborty, N.; and Sycara, K. 2015. Nonneg- Yang,B.;Yih,W.;He,X.;Gao,J.;andDeng,L. 2015. Em-\n",
      "ative matrix tri-factorization with graph regularization for bedding entities and relations for learning and inference in\n",
      "communitydetectioninsocialnetworks. InProceedingsof knowledge bases. In Proceedings of the 3rd International\n",
      "the24thInternationalJointConferenceonArtificialIntelli- ConferenceonLearningRepresentations.\n",
      "gence,2083–2089.\n",
      "Yao,L.;Zhang,Y.;Wei,B.;Qian,H.;andWang,Y. 2015.\n",
      "Pezeshkpour,P.;Chen,L.;andSingh,S. 2017. Embedding Incorporatingprobabilisticknowledgeintotopicmodels. In\n",
      "multimodal relational data. Presented in the 6th Workshop Advances in Knowledge Discovery and Data Mining, vol-\n",
      "onAutomatedKnowledgeBaseConstruction. ume9078ofLectureNotesinComputerScience,586–597.\n",
      "Ratner,A.;Sa,C.D.;Wu,S.;Selsam,D.;andRe´,C. 2016. Yao, L.; Zhang, Y.; Wei, B.; Jin, Z.; Zhang, R.; Zhang, Y.;\n",
      "Dataprogramming:Creatinglargetrainingsets,quickly. In andChen,Q. 2017. Incorporatingknowledgegraphembed-\n",
      "Advances in Neural Information Processing Systems, vol- dingsintotopicmodeling. InProceedingsofthe31stAAAI\n",
      "ume29,3567–3575. ConferenceonArtificialIntelligence,3119–3126.\n",
      "Richardson,M.,andDomingos,P. 2006. Markovlogicnet- Zhang, F.; Yuan, N. J.; Lian, D.; Xie, X.; and Ma, W.-\n",
      "works. MachineLearning62(1-2):107–136. Y. 2016. Collaborative knowledge base embedding for\n",
      "Schiegg,M.;Neumann,M.;andKersting,K. 2012. Markov recommender systems. In Proceedings of the 22nd ACM\n",
      "logic mixtures of Gaussian processes: Towards machines SIGKDD International Conference on Knowledge Discov-\n",
      "reading regression data. In Proceedings of the 15th Inter- eryandDataMining,353–362.\n",
      "nationalConferenceonArtificialIntelligenceandStatistics, Zhu, J.; Chen, N.; and Xing, E. P. 2014. Bayesian infer-\n",
      "1002–1011. ence with posterior regularization and applications to infi-\n",
      "Shahid,N.;Perraudin,N.;Kalofolias,V.;Puy,G.;andVan- nite latent SVMs. Journal of Machine Learning Research\n",
      "dergheynst, P. 2016. Fast robust PCA on graphs. IEEE 15:1799–1847.\n",
      "JournalofSelectedTopicsinSignalProcessing10(4):740–\n",
      "756.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   6302,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['1,380 objects']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Knowledge-Based Distant Regularization in Learning Probabilistic Models\n",
      "NaoyaTakeishi§,∗ and KosukeAkimoto†,∗\n",
      "§RIKENCenterforAdvancedIntelligenceProject\n",
      "†SecurityResearchLaboratories,NECCorporation\n",
      "naoya.takeishi@riken.jp k-akimoto@ab.jp.nec.com\n",
      "Abstract knowledge graph probabilistic modeling\n",
      "𝑦\n",
      "Exploitingtheappropriateinductivebiasbasedontheknowl- 1(Sensor 1)\n",
      "is-attached-to\n",
      "edge of data is essential for achieving good performance in is-part-of A\n",
      "statisticalmachinelearning.Inpractice,however,thedomain\n",
      "knowledgeofinterestoftenprovidesinformationontherela- 𝑦\n",
      "tionshipofdataattributesonlydistantly,whichhindersdirect 2(Sensor 2)\n",
      "utilizationofsuchdomainknowledgeinpopularregulariza- B\n",
      "tionmethods.Inthispaper,weproposetheknowledge-based is-connected-to\n",
      "distantregularizationframework,inwhichweutilizethedis- 𝑦 3(Sensor 3)\n",
      "tantinformationencodedinaknowledgegraphforregular-\n",
      "C\n",
      "ization of probabilistic model estimation. In particular, we\n",
      "propose to impose prior distributions on model parameters\n",
      "specifiedbyknowledgegraphembeddings.Asaninstanceof Figure1:Schematicdiagramofourmotivatingexample,the\n",
      "theproposedframework,wepresentthefactoranalysismodel analysisofsensordataofaplant.Insuchapplications,even\n",
      "withtheknowledge-baseddistantregularization.Weshowthe if the physical and/or statistical relations (dotted lines) be-\n",
      "resultsofpreliminaryexperimentsontheimprovementofthe\n",
      "tweenattributes,i.e.,sensorreadings,cannotbespecifieddi-\n",
      "generalizationcapabilityofsuchmodel.\n",
      "rectly, somehow distant information on plant’s instruments\n",
      "(blue arrows) are often available beforehand. We suppose\n",
      "Introduction such information can be encoded in a form of knowledge\n",
      "Thedata-drivennatureofstatisticalmachinelearningisone graphs.Wewouldliketoleveragesuchdistantinformation\n",
      "oftheprincipalreasonsforitssuccess,whereasimposingan toregularizelearningofprobabilisticmodels.\n",
      "appropriateinductivebiasisindispensableforgaininggen-\n",
      "eralizationcapability.Regularizationisaprevailingmethod-\n",
      "ologyforimposingtheinductivebias.Severaltypesofpopu- Ourmotivatingexample,depictedinFigure1,isintheanal-\n",
      "larregularizationmethodsareknown,suchasTikhonovreg- ysisofsensordataobtainedfromaplant.Althoughtherela-\n",
      "ularization and sparsity regularization, and they have been tionsbetweenplant’sinstrumentsandsubsystems(e.g.,how\n",
      "commonlyutilizedinawiderangeoftaskswithstrongtheo- they are connected each other) can often be written down\n",
      "reticalsupports.However,itisnotalwayspossibleforthose easily,therelationsbetweentheattributes(e.g.,sensorIDs)\n",
      "generalregularizationstrategiestodirectlyleveragethedo- canhardly bederiveddirectlyfrom thembecausetodo so,\n",
      "mainknowledgeavailableonagivenapplicationsincethey weneedtoidentifythemechanismandthephysicsofthein-\n",
      "areoftenbuiltuponthestatistical“meta-knowledge,”rather strumentsandthesensors,whichiscostlyinmanypractices.\n",
      "thanthedomainknowledge.\n",
      "Inthispaper,wesuggestamethodtoutilizesuchdistant\n",
      "The usability of domain knowledge is one of the pri-\n",
      "domainknowledgetoregularizetheestimationofstatistical\n",
      "malconcernsinpracticesofmachinelearning.Ifonecould\n",
      "models.Specifically,wepresentaframeworkofknowledge-\n",
      "clearly specify the relationship of attributes and objects in\n",
      "based distant regularization in learning probabilistic mod-\n",
      "data,itcouldthenbeutilizedbythemethodslikestructured\n",
      "els for continuous-valued data such as sensor readings. We\n",
      "sparsity regularization (Huang, Zhang, and Mataxas 2011)\n",
      "aim to build a regularization principle using a knowledge\n",
      "andgraphLaplacianregularization(see,e.g.,(Shahidetal.\n",
      "graph, which is a multi-relational directed graph in which\n",
      "2016) and references therein). Unfortunately, however, the\n",
      "relations between entities are enumerated. This is because\n",
      "domain knowledge of interest does not always provide di-\n",
      "a knowledge graph is often useful for summarizing the do-\n",
      "rectinformationontherelationshipoftheattributesandthe\n",
      "main knowledge of interest. Let us revisit the example of\n",
      "objects;instead,itoftengivesdistant informationonthem.\n",
      "sensor data analysis in Figure 1; engineers of a plant com-\n",
      "∗A substantial portion of this work was performed while the prehend the relations between the instruments, such as is-\n",
      "authorswereattheUniversityofTokyo. part-of andis-connected-to,aswellastherelationsbetween\n",
      "8102\n",
      "nuJ\n",
      "92\n",
      "]GL.sc[\n",
      "1v23311.6081:viXra\n",
      "theinstrumentsandthesensors,i.e.,is-attached-to,accord- However, this method is not always applicable to utilizing\n",
      "ing to which one can build a knowledge graph. Note that thepriordomainknowledgebecauseinthismethod,allthe\n",
      "what is specified on the data attributes is only the simple dataattributesmustbeidentifiedinthegraphandextension\n",
      "is-attached-to information, and no direct relations between tomulti-relationalcasesisnotstraightforward.\n",
      "themareprovidedingeneral. Another example of graph-based regularization is the\n",
      "Thecentralideaoftheproposedmethodistodefineprior methods using the graph Laplacian (see, e.g., (Shang, Jiao,\n",
      "distributions of model parameters using knowledge graph andWang2012;Pei,Chakraborty,andSycara2015;Shahid\n",
      "embeddings(seetheexcellentsurveypaperssuchas(Nickel et al. 2016)), in which the Laplacian matrix of graphs that\n",
      "etal.2015;Wangetal.2017;Cai,Zheng,andChang2018)), encode relations between objects and/or attributes is used\n",
      "whicharecontinuous-valuedrepresentationsofentities(and forspecifyingtheregularizationterm.However,suchmeth-\n",
      "relations) in knowledge graphs. The proposed method is ods are also not always applicable in practice because we\n",
      "flexible enough to utilize many types of knowledge graph maynotbeabletocomputeareliableLaplacianmatrixdue\n",
      "embeddings,whichformanactivefieldofresearchrecently. tothelackofpreciseknowledgeoftherelationsbetweenthe\n",
      "Hence,thedevelopmentinthefieldcanbeimmediatelyim- dataattributesandobjects.\n",
      "portedtotheproposedframework.\n",
      "Incorporating Knowledge as Constraints / Rules Re-\n",
      "The chief advantage of the proposed method, the distant\n",
      "searchershavebeenstudyingonmethodstoincorporatethe\n",
      "regularizationbyknowledgegraphs,isthatitcanbeapplied\n",
      "knowledgedescribedinaformofconstraintsorlogicalrules\n",
      "to a broad range of probabilistic models. We introduce the\n",
      "to well-known machine learning methods. To name a few,\n",
      "knowledge-based factor analysis model in this paper as an\n",
      "Towell and Shavlik (1994) proposed the knowledge-based\n",
      "instance, but similar modification of existing probabilistic\n",
      "neural networks, Fung, Mangasarian, and Shavlik (2003)\n",
      "models is possible whenever the domain knowledge of in-\n",
      "suggested the knowledge-based support vector machines,\n",
      "terestisdescribedinaformofknowledgegraphs.\n",
      "and Varol et al. (2012) formulated the constrained latent\n",
      "variable model. Moreover, there are studies on incorporat-\n",
      "RelatedWork\n",
      "inglogicalrulestoexistingstatisticalmodels.Schiegg,Neu-\n",
      "Theinductivebiasistheessentialelementofstatisticalma- mann,andKersting(2012)proposedthemixturesofGaus-\n",
      "chinelearningandusuallyintroducedbydesigningfeatures, sian process regressors controlled by logical rules formu-\n",
      "hypothesis spaces, loss functions, regularization terms, and latedbytheMarkovlogicnetwork(RichardsonandDomin-\n",
      "soon.Forexample,themodelfamilyisusuallyselectedac- gos2006).And,severalresearchersproposedthetopicmod-\n",
      "cordingtouser’sknowledgeofthedatatobeanalyzed.Ifthe elswithlogic(Andrzejewskietal.2011;Mei,Zhu,andZhu\n",
      "user is sure about the relationship of attributes and objects 2014; Foulds, Kumar, and Getoor 2015), which are useful\n",
      "inadataset,amodeltailoredforthedataset(e.g.,Bayesian for modeling document corpora with prior knowledge of\n",
      "networks)canbedeveloped.Thisisdifficultorimpossible, words.\n",
      "however,whentheknowledgeisgiveninadistantform.In\n",
      "StatisticalRelationalLearning Moregenerally,learning\n",
      "fact,therearealotofresearchesonutilizingknowledgein\n",
      "onrelationaldata,whichisoftentermedstatisticalrelational\n",
      "variousforms.\n",
      "learning (SRL), is an active area of research with a broad\n",
      "In what follows, we introduce only a part of the rich lit-\n",
      "rangeofmethodsandapplications(see,e.g.,(DeRaedt,Ker-\n",
      "erature on knowledge utilization for machine learning and\n",
      "sting, and Natarajan 2016)). We do not describe the details\n",
      "someotherrelatedtopicsthatwouldbeusefulforconsider-\n",
      "herebecauseofthevastamountoftheliteratureinthefield,\n",
      "ingthefuturedirectionofthiswork.\n",
      "but the methodology of SRL would be a promising tool in\n",
      "Incorporating Knowledge as Graphs Actually, it has thefuturedirectionofthiswork.\n",
      "beenactivelystudiedhowtoutilizethepriorknowledgethat\n",
      "Distant / Weak Supervision by Knowledge Particularly\n",
      "canbeencodedinaknowledgegraph.Forexample,Yaoet\n",
      "inthecontextofsupervisedlearning,thereisalineofstudies\n",
      "al. (2017) proposed an extension of latent Dirichlet alloca-\n",
      "onutilizinguser’sheuristicsorknowledgebasesasweaksu-\n",
      "tionthatmodelswordtokensaswellasentitiesindocument\n",
      "pervisionsources(see(Ratneretal.2016;Varmaetal.2017;\n",
      "corpora, where the entities are corroborated by a knowl-\n",
      "Bachetal.2017)andreferencestherein)withgrowinginter-\n",
      "edge graph and have the corresponding embeddings. Also\n",
      "ests. In the settings of those studies, the knowledge should\n",
      "in the context of topic models, Yao et al. (2015) and Hu\n",
      "provideinformationonlabelstobepredicted.\n",
      "et al. (2016) suggested to utilize a taxonomy of entities in\n",
      "documents; a taxonomy, i.e., a set of hierarchical relations Posterior Regularization Zhu, Chen, and Xing (2014)\n",
      "between entities, can be described as a (single-relational) proposed an elegant and flexible framework, termed Reg-\n",
      "graph. Note that these studies mainly treat discrete-valued Bayes,forregularizingtheposteriordistributionofBayesian\n",
      "datasuchasdocumentcorpora. inference. It can be utilized for efficiently incorporating\n",
      "Regularization based on the graph information is a ma- one’s knowledge in Bayesian inference, and in fact, Mei,\n",
      "jor field of research also in other contexts. For example, Zhu, and Zhu (2014) utilized it for incorporating logical\n",
      "thegraphsparsityregularization(Jacob,Obozinski,andVert rules to a topic model. Note that, while RegBayes tries to\n",
      "2009; Huang, Zhang, and Mataxas 2011; Mairal and Yu changetheposterior,themethodproposedinthisworktries\n",
      "2013) imposes structured sparsity on parameters according tochangetheprior,whichmayremindoneoftheempirical\n",
      "to a graph that encodes prior knowledge on data attributes. Bayesmethod.\n",
      "Background Representation learning for knowledge graphs is an ac-\n",
      "In this section, we first introduce the notations used in this tive area of research recently. We do not enumerate the\n",
      "paper to express the concepts regarding the probabilistic works and just refer to the excellent survey papers on the\n",
      "models. Afterward, we give a brief explanation on knowl- field(Nickeletal.2015;Wangetal.2017;Cai,Zheng,and\n",
      "edgegraphembeddingsforcompleteness,whichreadersfa- Chang2018).Here,weintroducetheverybasicconceptsin\n",
      "miliarwiththemmayskip. this field, which are needed when describing the proposed\n",
      "method.\n",
      "ProbabilisticModelsforContinuous-ValuedData In a typical setting, given a set of tuples that exist in a\n",
      "knowledgegraph(i.e.,positivetuples),welearnreal-valued\n",
      "Themainsubjectofthisstudyistheregularizationinthees-\n",
      "timationofprobabilisticmodelsforcontinuous-valueddata.\n",
      "vectorse ∈ Rde correspondingtoentitiesintheknowledge\n",
      "graph. Such vectors are called embeddings of entities. The\n",
      "Formally, we denote a continuous-valued dataset by a set\n",
      "{y ∈Rm |j =1,...,n},wherey istheobservationwith learningisdonesothatsomescorefunction\n",
      "j j\n",
      "mattributes(i.e.,features)correspondingtothej-thobject. ψ(e, e ; M ):Rde ×Rde →R (1)\n",
      "h t r\n",
      "And, n denotes the number of objects (individuals, times-\n",
      "becomeslargeforthepositivetuplesandsmallfortheneg-\n",
      "tamps,etc.)inadataset.Forinstance,wemayhavereadings\n",
      "ativetuples.Here,e ande denotetheembeddingsofen-\n",
      "of m types of sensors at n timestamps in the sensor data. h t\n",
      "titieshandt,respectively,And,ψisthescorefunctionthat\n",
      "Notethateachy mayormaynotbeindependentandiden-\n",
      "j hastherelation-specificparameter,M,withregardtorela-\n",
      "ticallydistributed.Theprobabilisticmodelsforsuchdatade- r\n",
      "tion r. In the latter part of this paper, as an instance of the\n",
      "fine probability density p(y,...,y | θ), where θ is a set\n",
      "1 n knowledgegraphembedding,weuseasimplemethodcalled\n",
      "ofmodelparameters.Moreover,somemodelsmayhavean\n",
      "additionalsetofrandomvariables,denotedby{x\n",
      "j\n",
      "∈Rdx}, D asis ft oM llou wlt s( :Yangetal.2015),whosescorefunctionisdefined\n",
      "whichisoftentermedlatentvariables.\n",
      "We further distinguish the model parameters as θ = ψ(e, e ; m )=e(cid:62)diag(m )e, (2)\n",
      "h t r h r t\n",
      "l{ ow ca1 l,. p. a. r, aw mm ete, rπ 1} c; oo rn reso pn oe nh da inn gd, tole tt hw eii -t∈ haR ttd riw bud te en.o Ote ntt hh ee where m\n",
      "r\n",
      "∈ Rde is the relation-specific parameter. There\n",
      "are several manners of optimization for learning e and M,\n",
      "other hand, let π be the vector of global parameters that\n",
      "suchasenergy-basedoptimizationandrankoptimization.In\n",
      "are independent of the difference of attributes. Let us give\n",
      "thispaper,asdescribedbelow,wesimplytreatψasapartof\n",
      "anexample;theobservationmodelofprobabilisticprincipal\n",
      "a logistic regressor and maximize the binary classification\n",
      "componentanalysis(TippingandBishop1999)is\n",
      "likelihoodvianegativetuplesampling.\n",
      "p(y |x,θ)=N\n",
      "(cid:0)\n",
      "y |Wx +µ,\n",
      "σ2I(cid:1)\n",
      ", Moststudiesontheknowledgegraphembeddingconsider\n",
      "j j j j\n",
      "only the sparse structure of a multi-relational graph. Re-\n",
      "where W ∈ Rm×dx is the factor loading matrix and d\n",
      "x\n",
      "is\n",
      "cently,thereareseveralstudiesonfusingadditionalinforma-\n",
      "thedimensionalityoflatentvariablex.Inthismodel,thei-th\n",
      "tionassociatedwithknowledgegraphssuchascontinuous-\n",
      "localparameter,w,correspondstothei-throwofW (and\n",
      "i valued attributes of entities (Zhang et al. 2016), literals\n",
      "the i-th element of µ), and σ2 is an element of the global\n",
      "(Kristiadi et al. 2018), text descriptions (Xie et al. 2016;\n",
      "parameters,π.\n",
      "Fan et al. 2017; Xiao et al. 2017), images (On˜oro-Rubio\n",
      "Theregularizationintheestimationofprobabilisticmod-\n",
      "etal.2017),andthemultimodalinformation(Pezeshkpour,\n",
      "els is often executed by setting prior distributions on the\n",
      "Chen, and Singh 2017; Thoma, Rettinger, and Both 2017).\n",
      "parameters,andmanyotherpopularregularizationmethods\n",
      "Theirprimaryconcernistheimprovementoftheknowledge\n",
      "can also be interpreted as imposing prior distributions. In\n",
      "graphembeddingsandtheirapplicationssuchaslinkpredic-\n",
      "thispaper,wefollowthisstrategytoachievetheknowledge-\n",
      "tion.Thetechniquesutilizedtherewouldbeofgreatinterest\n",
      "baseddistantregularization;weplacethepriordistributions\n",
      "also for our purpose, i.e., regularization in learning proba-\n",
      "that are parameterized by the embeddings of knowledge\n",
      "bilisticmodelsforcontinuousdata.\n",
      "graphs.Thedetailsaredescribedinthenextsection.\n",
      "ProposedMethod\n",
      "KnowledgeGraphEmbedding\n",
      "In this section, we describe the detail on the proposed\n",
      "A knowledge graph is a multi-relational directed graph\n",
      "method for distantly regularizing the estimation of proba-\n",
      "whoseverticescorrespondtoentitiesandedgescorrespond\n",
      "bilistic models using knowledge graph embeddings. First,\n",
      "to relations between the entities. Such graph can be de-\n",
      "thegeneralframework,inwhichwedonotpremisespecial\n",
      "scribed as a set of tuples {(h,r,t)}, where h and t denote\n",
      "typesofprobabilisticmodelsandknowledgegraphembed-\n",
      "the head entity and the tail entity respectively, and r de-\n",
      "dings,ispresented.Afterward,asaninstanceoftheframe-\n",
      "notes the relation between them. There are various large-\n",
      "work,weintroducethefactoranalysisdistantlyregularized\n",
      "scaleknowledgebaseswhichcanberegardedasknowledge\n",
      "byaknowledgegraph.\n",
      "graphs,suchasFreebaseandGeneOntology,whicharecol-\n",
      "lectionsofknowledgeongeneralentitiesoftheworldoron\n",
      "GeneralFramework\n",
      "domain-specificconcepts.\n",
      "Ourpurposeistobuildaregularizationprincipleforthees-\n",
      "1Thedimensionalityofw maydependoni,butforsimplicity, timation of the parameters of probabilistic models accord-\n",
      "i\n",
      "weassumethesamedimensionalityforalli. ing to a knowledge graph that encodes (possibly distant)\n",
      "priorknowledgeonthedata-generatingsystem.Tothisend, FactorAnalysisRegularizedbyKnowledge\n",
      "we propose to specify the prior distributions of the model\n",
      "Amongtheprobabilisticmodelsforcontinuousdata,thefac-\n",
      "parameters using knowledge graph embeddings. Formally,\n",
      "tor analysis (FA) and its special version, principal compo-\n",
      "consideraprobabilisticmodel\n",
      "nent analysis (PCA), have been favored in many applica-\n",
      "p(y,...,y |θ), (3) tions.Asaninstanceoftheproposedframework,wepresent\n",
      "1 n\n",
      "asimplemethodforincorporatingknowledgegraphstoreg-\n",
      "where θ = {w,...,w,π}. In the proposed framework,\n",
      "1 m ularizationintheparameterestimationofFA.\n",
      "wespecifythepriordistributionsonthe(partof)localmodel\n",
      "The observation probabilistic model of FA is often ex-\n",
      "parameters,w,...,w,asfollows.\n",
      "1 m pressedas\n",
      "First, we assume that we have a knowledge graph, in\n",
      "which entities corresponding to the (part of) data attributes\n",
      "p(y,...,y |x,...,x,θ)\n",
      "exist,andthatsuchentitiesappearinasetofpositivetuples 1 n 1 n\n",
      "m n\n",
      "atleastonce.Inthesensordataexample(Figure1),thisas- (cid:89)(cid:89)\n",
      "= N(y |w(cid:62)x +µ, σ2), (6)\n",
      "sumption means that there is an entity like “Sensor i,” and i,j i j i i\n",
      "theknowledgegraphhasatuplelike i=1j=1\n",
      "(h=Sensori, r =is-attached-to, t=InstrumentA) wherew(cid:62) ∈R1×dx isthei-throwofthefactorloadingma-\n",
      "i\n",
      "foratleastapartofattributesi=1,...,m(cid:48)(1≤m(cid:48) ≤m). trixofFA,µ i istheobservationmean,andσ i2 ∈ R+ isthe\n",
      "observation noise variance of the i-th attribute. Moreover,\n",
      "Inthefollowing,wedenotetheembeddingoftheentitythat\n",
      "corresponds to the i-th attribute of data by e. Note that\n",
      "x\n",
      "j\n",
      "∈ Rdx is the latent factor corresponding to the j-th ob-\n",
      "i\n",
      "ject, whose prior distribution is usually set by the standard\n",
      "in the knowledge graph of our interest, there would also\n",
      "normaldistribution,i.e.,N(0,I).\n",
      "be many entities that do not directly correspond to the at-\n",
      "Letusimposetheknowledge-basedpriordistributionson\n",
      "tributes, and not all the attributes appear in the knowledge\n",
      "(partof)therowsofthefactorloadingmatrix,w,...,w\n",
      "graph. Also note that, while here we consider only the en- 1 m(cid:48)\n",
      "(m(cid:48) ≤ m). Here, instead of the normal distribution like\n",
      "tities corresponding to the attributes, similar discussion is\n",
      "Eq. (4), we specify the prior distribution of w given em-\n",
      "straightforwardevenwhenweconsiderentitiescorrespond- i\n",
      "beddinge bythepointmass,i.e.,\n",
      "ingtotheobjectsindata. i\n",
      "Giventheknowledgegraphofthiskind,wespecifyaprior p(w |e )=δ(w −c (e )), for i=1,...,m(cid:48), (7)\n",
      "distribution on w parameterized by e, for i = 1,...,m(cid:48), i i i ξ i\n",
      "i i\n",
      "asaregularizerofthemodeltraining.Forexample,forreal- whereδdenotestheDiracdeltafunction.Thissimplification\n",
      "valued parameters, a canonical choice is the normal distri- enables us to maximize the (regularized) objective just by\n",
      "bution,i.e., replacingw withc (e ).Weobservednosignificantdegra-\n",
      "i ξ i\n",
      "dationofperformanceduetothissimplification.\n",
      "p(w |e )=N(w |c (e ), V (e )), (4)\n",
      "i i i ξ i ξ i\n",
      "In summary, an optimization problem to be solved for\n",
      "where c ξ : Rde → Rdw, V ξ : Rde → Rdw×dw, and ξ learning FA distantly regularized based on the knowledge\n",
      "denotes the set of parameters in c and V. With the priors graphisthemaximizationof\n",
      "onw,...,w specifiedinthismanner,wecomputetheir\n",
      "1 m(cid:48)\n",
      "maximum a posteriori (MAP) estimations. For the remain- f(e 1:E,M 1:R,w 1:m,µ 1:m,σ 12 :m,ξ)\n",
      "ingattributesw m(cid:48)+1,...,w m,ifany,wecomputethemaxi-\n",
      "1\n",
      "(cid:88)n\n",
      "mumlikelihoodestimationsorMAPestimationswithpriors = logN(y |µ, Σ)\n",
      "n j\n",
      "definedotherwise.\n",
      "j=1\n",
      "For the above framework, the embeddings of knowledge\n",
      "(cid:96)\n",
      "graph’sentitiesmustbepreparedinsomeways.Apromis-\n",
      "1(cid:88)\n",
      "+ logΦ(ψ(e, e ; M )) (8)\n",
      "ingwaytopreparegoodembeddingsistoutilizethemodel (cid:96) hk tk rk\n",
      "k=1\n",
      "pretrainedonalarge-scaleknowledgegraph.However,such\n",
      "(cid:96)(cid:48)\n",
      "pretrained embeddings are usually not available for user- 1 (cid:88) (cid:0) (cid:1)\n",
      "+ log 1−Φ(ψ(e, e ; M )),\n",
      "definedknowledgegraphsthatdescribethedomainknowl- (cid:96)(cid:48) h k(cid:48) t k(cid:48) r k(cid:48)\n",
      "edge.Hence,inmanypractices,theembeddingshavetobe k(cid:48)=1\n",
      "learned simultaneously with the parameters of the proba- s.t. w i =c ξ(e i) for i=1,...,m(cid:48),\n",
      "bilistic model. Now suppose we define a score function ψ\n",
      "for the embedding learning. In the framework, we propose where µ = [µ 1 ··· µ m](cid:62), Σ = WW(cid:62) +\n",
      "tomaximizebinaryclassificationlikelihood: diag{σ 12,...,σ m2 }, and W = [w 1 ··· w m](cid:62). In Eq. (8),\n",
      "E denotes the total number of entities (including the ones\n",
      "p((h,r,t)=true|e, e, M )=Φ(ψ(e, e ; M )), (5)\n",
      "h t r h t r corresponding to the data attributes), R denotes the num-\n",
      "whereΦisthesigmoidfunction.Notethatanyψcanbeused berofrelations,and(cid:96)and(cid:96)(cid:48) denotethenumbersofpositive\n",
      "heresolongasitsrangeisthereal.Bysettingtheobjective and negative tuples, respectively. The first term of Eq. (8)\n",
      "of the embedding learning in this way, the final objective isobtainedbymarginalizingoutxinEq.(6)withtheprior\n",
      "function simply becomes the addition of the logarithms of N(0,I).\n",
      "datalikelihoodEq.(3),parameterpriorsEq.(4),andknowl- The negative tuples, needed for computing Eq. (8), are\n",
      "edgegraphlikelihoodEq.(5). usually not available originally. They are often created by\n",
      "−660\n",
      "−670\n",
      "−680\n",
      "−690\n",
      "0 50 100\n",
      "proportionofKGtuples[%]\n",
      "AFfoLLNtset.evA −660\n",
      "−680\n",
      "20 40 60 80\n",
      "proportionoftrainingdata[%]\n",
      "AFfoLLNtset.evA\n",
      "w/KG\n",
      "w/oKG\n",
      "(a)\n",
      "−650\n",
      "−660\n",
      "−670\n",
      "0 50 100\n",
      "proportionofKGtuples[%]\n",
      "AFfoLLNtset.evA −550\n",
      "−600\n",
      "−650\n",
      "−700\n",
      "20 40 60 80\n",
      "proportionoftrainingdata[%]\n",
      "AFfoLLNtset.evA\n",
      "w/KG\n",
      "w/oKG\n",
      "(b)\n",
      "Figure2:Resultsunder(a)theRandomdata-partitionscenarioand(b)theShiftdata-partitionscenario.Ineachpanel,left\n",
      "plot shows average test NLLs along different proportions of knowledge graph tuples used by the proposed method, and the\n",
      "rightplotshowsaveragetestNLLsalongdifferentamountsofdatausedforthetraining.Intherightplots,thetestNLLsoftwo\n",
      "casesareshown:100%oftheknowledgegraphtupleswasusedbytheproposedmethod(w/KG)ornoknowledgegraphtuples\n",
      "wereprovided(w/oKG).Ineveryplot,themeansandthestandarddeviationsfor10randomtrialsareshown.\n",
      "randomly permuting the entities and/or the relations in the Therefore, n = 1,380 and m = 227. The dataset was cre-\n",
      "positive tuples (see, e.g., (Nickel et al. 2015; Wang et al. ated using the resource available online at Climate Change\n",
      "2017;Cai,Zheng,andChang2018)).Inthiswork,wecre- KnowledgePortal.2\n",
      "ated some negative tuples per a positive tuple by replacing\n",
      "Knowledge Graph In order to precisely model the rela-\n",
      "eithertheheadorthetailbyanyotherentitythatexistsinthe\n",
      "tionship of precipitation among countries and regions, we\n",
      "knowledge graph. When permuting the entity, we chose an\n",
      "needmeteorologicalknowledgetosomeextent.However,in\n",
      "attribute-correspondingentityforanattribute-corresponding\n",
      "ourscenario,suchknowledgethatdirectlygivestherelations\n",
      "entity,andanon-attributeentityforanon-attributeentity,re-\n",
      "betweendataattributescanhardlybeobtained.Tosimulate\n",
      "spectively.\n",
      "this situation, in the experiments, we utilize a knowledge\n",
      "InEq.(8),ψ isanarbitraryscorefunctionofknowledge\n",
      "graphthatencodessimplegeographicrelationsbetweenthe\n",
      "graph embedding learning. In the experiments introduced\n",
      "countriesandregions.\n",
      "below,weusedthesimplescorefunctionofDistMult(Yang\n",
      "etal.2015),definedinEq.(2).Moreover,asc (·),weused Webuiltaknowledgegraphusingtheinformationavail-\n",
      "ξ able online,3 which includes the geographic relationship\n",
      "anaffinemodel\n",
      "c (e )=Ae +b (9) between countries and regions in the world, following\n",
      "ξ i i\n",
      "the scheme presented by Bouchard, Singh, and Trouillon\n",
      "withparametersA ∈ Rdx×de andb ∈ Rdx usedcommonly\n",
      "(2015).Theknowledgegraphweusedhas278entities(in-\n",
      "fori=1,...,m(cid:48),i.e.,ξ ={A,b}.\n",
      "cluding ones that do not correspond to the attributes of the\n",
      "data) and two types of relations, is-inside and is-neighbor-\n",
      "PreliminaryExperiments\n",
      "of; positive tuples include (Sweden, is-inside, Europe) and\n",
      "Inthissection,weprovidetheresultsofpreliminaryexper- (Norway, is-neighbor-of, Sweden), for example. And, the\n",
      "iments to investigate how the performance of the distantly knowledge graph has 1,167 positive tuples comprising the\n",
      "regularizedmodelsisimproved. entitiesandtherelationsabove.\n",
      "Dataset We used real-world data of rainfall in the world. 2sdwebx.worldbank.org/climateportal/\n",
      "The dataset we prepared comprises the monthly means of (retrieved29April2018)\n",
      "historicalmeasurementsofprecipitationfromJanuary1901 3github.com/mledoze/countries/\n",
      "toDecember2015in227countriesandregionsintheworld. (retrieved29April2018)\n",
      "Notethat,inthisknowledgegraph,someentitiesofcoun- In Figures 2a and 2b, we show the results under the\n",
      "tries like Japan have no is-neighbor-of relations with any Random and Shift data-partition scenarios, respectively.\n",
      "other entities because the is-neighbor-of relation does not In the left plots, the test NLLs for different numbers of\n",
      "imply the cross-sea adjacency. Therefore, if we build a knowledge graph tuples used by the proposed method are\n",
      "single-relationalgraphonlywiththeis-neighbor-of relation, shown.Basically,thebettertestNLLsareachievedwiththe\n",
      "which might be utilized for methods like graph Laplacian larger amount of knowledge graph’s information provided.\n",
      "regularization,itdoesnotprovidesufficientinformationon In the right plots, the test NLLs with different amounts of\n",
      "thecountriesthathavenoneighboringcountries. trainingdataareshown.WhilethetestNLLsarealwaysim-\n",
      "provedbytheproposedmethodundertheRandomscenario,\n",
      "Setups Asforthefactoranalysismodeldistantlyregular-\n",
      "noimprovementisobservedwithsomewhatsmallamountof\n",
      "izedbyknowledge,thehyperparameterstobesetmanually\n",
      "trainingdataundertheShiftscenario.\n",
      "are the dimensionality of latent factors, d, as well as the\n",
      "x\n",
      "dimensionalityofentityembeddings,d.Forafaircompar-\n",
      "e\n",
      "Conclusion\n",
      "ison, we set the same values for them, i.e., d = d. And,\n",
      "x e\n",
      "in what follows, we show the results with d = 5, which\n",
      "x In this paper, we have proposed the knowledge-based dis-\n",
      "was decided empirically without any intensive search be-\n",
      "tantregularizationframework,ageneralmethodologytoin-\n",
      "cause the purpose of this experiment was just to compare\n",
      "corporate the possibly distant domain knowledge encoded\n",
      "theperformancesofafactoranalysismodelwithandwith-\n",
      "inaformofknowledgegraphs.Inparticular,wesuggested\n",
      "outtheknowledge-basedregularization.\n",
      "thefactoranalysiswiththeknowledge-basedregularization.\n",
      "Wepartitionedthedatasetintothetraining/validationset\n",
      "Weprovidedthepreliminaryexperimentalresultsontheim-\n",
      "andthetestsetattheratesspecifiedbelow.Withinthetrain-\n",
      "provementofthegeneralizationcapabilityoftheregularized\n",
      "ing/validationset,wealwaysusedrandomlychosen80%for\n",
      "factoranalysis.\n",
      "trainingandtheremaining20%forvalidation.Whenparti-\n",
      "Indeed, the proposed framework is in its infancy. There\n",
      "tioning the original dataset into the training/validation set\n",
      "are plenty of challenges to be tackled. For example, we\n",
      "and<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   2065,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Factor Analysis', 'Regularization']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: regularizationframework,ageneralmethodologytoin-\n",
      "cause the purpose of this experiment was just to compare\n",
      "corporate the possibly distant domain knowledge encoded\n",
      "theperformancesofafactoranalysismodelwithandwith-\n",
      "inaformofknowledgegraphs.Inparticular,wesuggested\n",
      "outtheknowledge-basedregularization.\n",
      "thefactoranalysiswiththeknowledge-basedregularization.\n",
      "Wepartitionedthedatasetintothetraining/validationset\n",
      "Weprovidedthepreliminaryexperimentalresultsontheim-\n",
      "andthetestsetattheratesspecifiedbelow.Withinthetrain-\n",
      "provementofthegeneralizationcapabilityoftheregularized\n",
      "ing/validationset,wealwaysusedrandomlychosen80%for\n",
      "factoranalysis.\n",
      "trainingandtheremaining20%forvalidation.Whenparti-\n",
      "Indeed, the proposed framework is in its infancy. There\n",
      "tioning the original dataset into the training/validation set\n",
      "are plenty of challenges to be tackled. For example, we\n",
      "andthetestset,wefollowedtwodifferentscenarios.Oneis\n",
      "need to develop a unified method for efficiently learning\n",
      "theRandomdata-partition,inwhichrandompermutationof\n",
      "themodelsthatareregularizedbyknowledgegraphs.Also,\n",
      "the 1,380 objects in each dataset was performed before the\n",
      "how the generalization capability of the regularized mod-\n",
      "partition.AnotheristheShiftdata-partition,inwhichno\n",
      "elsisimprovedshouldbeanalyzedtheoreticallyandempir-\n",
      "randompermutationisdonebeforethepartition,andconse-\n",
      "ically. Moreover, it would be interesting to investigate how\n",
      "quently,somethinglikethecovariateshiftmayoccur,which\n",
      "the knowledge graph embeddings are modified due to the\n",
      "makesgeneralizationdifficult.\n",
      "datafedintotheprobabilisticmodels.\n",
      "We also controlled the amount of information possibly\n",
      "provided by the knowledge graph by changing the propor-\n",
      "References\n",
      "tionofknowledgegraph’s(positive)tuplesconsideredinthe\n",
      "optimizationofEq.(8).Forexample,whenweused80%of\n",
      "Andrzejewski,D.;Zhu,X.;Craven,M.;andRecht,B. 2011.\n",
      "the tuples, we randomly disposed 20% of the 1,167 posi-\n",
      "A framework for incorporating general domain knowledge\n",
      "tivetuplesandperformedtheoptimizationwiththenegative\n",
      "into latent Dirichlet allocation using first-order logic. In\n",
      "sampling. As the “no knowledge graph” limit, we ran the\n",
      "Proceedings of the 22nd International Joint Conference on\n",
      "optimizationwith0%ofthetuples.\n",
      "ArtificialIntelligence,1171–1177.\n",
      "The optimization was performed using gradient descent\n",
      "Bach, S. H.; He, B.; Ratner, A.; and Re´, C. 2017. Learn-\n",
      "whose learning rate was adjusted by Adam. For the opti-\n",
      "ingthestructureofgenerativemodelswithoutlabeleddata.\n",
      "mization, we randomly created two negative tuples per a\n",
      "InProceedingsofthe34thInternationalConferenceonMa-\n",
      "positivetuple.Thevalidationsetwasutilizedforearlystop-\n",
      "chineLearning,273–282.\n",
      "ping of optimization, in which the optimization was termi-\n",
      "natedifnoimprovementofthelosswithregardtoFA(i.e., Bouchard, G.; Singh, S.; and Trouillon, T. 2015. On ap-\n",
      "the negative of the first term of Eq. (8)) on the validation proximatereasoningcapabilitiesoflow-rankvectorspaces.\n",
      "setwasobservedfor50epochs,andthemodelthatachieved InProceedingsoftheAAAISpringSyposiumonKnowledge\n",
      "the best loss was saved. We randomly initialized the factor RepresentationandReasoning.\n",
      "loading matrix of FA, the embeddings of entities, and the Cai,H.;Zheng,V.W.;andChang,K. 2018. Acomprehen-\n",
      "relationparameters.Nopretrainingfortheembeddingswas sive survey of graph embedding: Problems, techniques and\n",
      "performed. applications. IEEE Transactions on Knowledge and Data\n",
      "Engineering. inpress,arXiv:1709.07604.\n",
      "Results In what follows, we report the average negative\n",
      "loglikelihood(NLL)ofFAforthetestdataset,i.e., De Raedt, L.; Kersting, K.; and Natarajan, S. 2016. Sta-\n",
      "tisticalRelationalArtificialIntelligence:Logic,Probability,\n",
      "1\n",
      "(cid:88)ntest\n",
      "andComputation. Morgan&ClaypoolPublishers.\n",
      "− logN(y |µ, Σ),\n",
      "n j Fan, M.; Zhou, Q.; Zheng, T. F.; and Grishman, R. 2017.\n",
      "test\n",
      "j=1\n",
      "Distributed representation learning for knowledge graphs\n",
      "wheren denotesthenumberofdatapointsinthetestset. withentitydescriptions. PatternRecognitionLetters93:31–\n",
      "test\n",
      "NotethatlowervaluesofNLLarepreferred. 37.\n",
      "Foulds,J.;Kumar,S.H.;andGetoor,L. 2015. Latenttopic Shang, F.; Jiao, L. C.; and Wang, F. 2012. Graph\n",
      "networks:Aversatileprobabilisticprogrammingframework dualregularizationnon-negativematrixfactorizationforco-\n",
      "fortopicmodels. InProceedingsofthe32ndInternational clustering. PatternRecognition45(6):2237–2250.\n",
      "ConferenceonMachineLearning,777–786. Thoma,S.;Rettinger,A.;andBoth,F. 2017. Towardsholis-\n",
      "Fung,G.M.;Mangasarian,O.L.;andShavlik,J.W. 2003. tic concept representations: Embedding relational knowl-\n",
      "Knowledge-based support vector machine classifiers. In edge, visual attributes, and distributional word semantics.\n",
      "Advances in Neural Information Processing Systems, vol- InLectureNotesinComputerScience,volume10587,694–\n",
      "ume15,537–544. 710.\n",
      "Hu, Z.; Luo, G.; Sachan, M.; Xing, E.; and Nie, Z. 2016. Tipping,M.E.,andBishop,C.M. 1999. Probabilisticprin-\n",
      "Groundingtopicmodelswithknowledgebases. InProceed- cipal component analysis. Journal of the Royal Statistical\n",
      "ingsofthe25thInternationalJointConferenceonArtificial Society:SeriesB61(3):611–622.\n",
      "Intelligence,1578–1584.\n",
      "Towell, G. G., and Shavlik, J. W. 1994. Knowledge-\n",
      "Huang,J.;Zhang,T.;andMataxas,D. 2011. Learningwith basedartificialneuralnetworks. ArtificialIntelligence70(1-\n",
      "structuredsparsity. JournalofMachineLearningResearch 2):119–165.\n",
      "12:3371–3412.\n",
      "Varma,P.;He,B.;Bajaj,P.;Khandwala,N.;Banerjee,I.;Ru-\n",
      "Jacob,L.;Obozinski,G.;andVert,J.-P. 2009. Grouplasso bin,D.;andRe´,C. 2017. Generativemodelstructurewith\n",
      "with overlap and graph lasso. In Proceedings of the 26th staticanalysis. InAdvancesinNeuralInformationProcess-\n",
      "InternationalConferenceonMachineLearning,433–440. ingSystems,volume30,239–249.\n",
      "Kristiadi, A.; Khan, M. A.; Lukovnikov, D.; Lehmann, J.; Varol, A.; Salzmann, M.; Fua, P.; and Urtasun, R. 2012.\n",
      "andFischer,A. 2018. Incorporatingliteralsintoknowledge A constrained latent variable model. In Proceedings of\n",
      "graphembeddings. arXiv:1802.00934. the2012IEEEConferenceonComputerVisionandPattern\n",
      "Mairal,J.,andYu,B. 2013. Supervisedfeatureselectionin Recognition,2248–2255.\n",
      "graphswithpathcodingpenaltiesandnetworkflows. Jour- Wang, Q.; Mao, Z.; Wang, B.; and Guo, L. 2017. Knowl-\n",
      "nalofMachineLearningResearch14(1):2449–2485. edge graph embedding: A survey of approaches and appli-\n",
      "Mei,S.;Zhu,J.;andZhu,X.2014.RobustRegBayes:Selec- cations. IEEETransactionsonKnowledgeandDataEngi-\n",
      "tivelyincorporatingfirst-orderlogicdomainknowledgeinto neering29(12):2724–2743.\n",
      "Bayesian models. In Proceedings of the 31st International Xiao, H.; Huang, M.; Meng, L.; and Zhu, X. 2017. SSP:\n",
      "ConferenceonMachineLearning,number1,253–261. Semanticspaceprojectionforknowledgegraphembedding\n",
      "Nickel, M.; Murphy, K.; Tresp, V.; and Gabrilovich, E. withtextdescriptions.InProceedingsofthe31stAAAICon-\n",
      "2015. A review of relational machine learning for knowl- ferenceonArtificialIntelligence,3104–3110.\n",
      "edgegraphs. ProceedingsoftheIEEE104(1):11–33. Xie,R.;Liu,Z.;Jia,J.;Luan,H.;andSun,M. 2016. Repre-\n",
      "On˜oro-Rubio,D.;Niepert,M.;Garc´ıa-Dura´n,A.;Gonza´lez, sentationlearningofknowledgegraphswithentitydescrip-\n",
      "R.; and Lo´pez-Sastre, R. J. 2017. Representation learning tions. InProceedingsofthe30thAAAIConferenceonArti-\n",
      "forvisual-relationalknowledgegraphs. arXiv:1709.02314. ficialIntelligence,2659–2665.\n",
      "Pei, Y.; Chakraborty, N.; and Sycara, K. 2015. Nonneg- Yang,B.;Yih,W.;He,X.;Gao,J.;andDeng,L. 2015. Em-\n",
      "ative matrix tri-factorization with graph regularization for bedding entities and relations for learning and inference in\n",
      "communitydetectioninsocialnetworks. InProceedingsof knowledge bases. In Proceedings of the 3rd International\n",
      "the24thInternationalJointConferenceonArtificialIntelli- ConferenceonLearningRepresentations.\n",
      "gence,2083–2089.\n",
      "Yao,L.;Zhang,Y.;Wei,B.;Qian,H.;andWang,Y. 2015.\n",
      "Pezeshkpour,P.;Chen,L.;andSingh,S. 2017. Embedding Incorporatingprobabilisticknowledgeintotopicmodels. In\n",
      "multimodal relational data. Presented in the 6th Workshop Advances in Knowledge Discovery and Data Mining, vol-\n",
      "onAutomatedKnowledgeBaseConstruction. ume9078ofLectureNotesinComputerScience,586–597.\n",
      "Ratner,A.;Sa,C.D.;Wu,S.;Selsam,D.;andRe´,C. 2016. Yao, L.; Zhang, Y.; Wei, B.; Jin, Z.; Zhang, R.; Zhang, Y.;\n",
      "Dataprogramming:Creatinglargetrainingsets,quickly. In andChen,Q. 2017. Incorporatingknowledgegraphembed-\n",
      "Advances in Neural Information Processing Systems, vol- dingsintotopicmodeling. InProceedingsofthe31stAAAI\n",
      "ume29,3567–3575. ConferenceonArtificialIntelligence,3119–3126.\n",
      "Richardson,M.,andDomingos,P. 2006. Markovlogicnet- Zhang, F.; Yuan, N. J.; Lian, D.; Xie, X.; and Ma, W.-\n",
      "works. MachineLearning62(1-2):107–136. Y. 2016. Collaborative knowledge base embedding for\n",
      "Schiegg,M.;Neumann,M.;andKersting,K. 2012. Markov recommender systems. In Proceedings of the 22nd ACM\n",
      "logic mixtures of Gaussian processes: Towards machines SIGKDD International Conference on Knowledge Discov-\n",
      "reading regression data. In Proceedings of the 15th Inter- eryandDataMining,353–362.\n",
      "nationalConferenceonArtificialIntelligenceandStatistics, Zhu, J.; Chen, N.; and Xing, E. P. 2014. Bayesian infer-\n",
      "1002–1011. ence with posterior regularization and applications to infi-\n",
      "Shahid,N.;Perraudin,N.;Kalofolias,V.;Puy,G.;andVan- nite latent SVMs. Journal of Machine Learning Research\n",
      "dergheynst, P. 2016. Fast robust PCA on graphs. IEEE 15:1799–1847.\n",
      "JournalofSelectedTopicsinSignalProcessing10(4):740–\n",
      "756.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  29696,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Factor Analysis', 'Regularized Factor Analysis', 'Knowledge Graph Embeddings', 'Latent Dirichlet Allocation', 'Generative Model Structure']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Knowledge-Based Distant Regularization in Learning Probabilistic Models\n",
      "NaoyaTakeishi§,∗ and KosukeAkimoto†,∗\n",
      "§RIKENCenterforAdvancedIntelligenceProject\n",
      "†SecurityResearchLaboratories,NECCorporation\n",
      "naoya.takeishi@riken.jp k-akimoto@ab.jp.nec.com\n",
      "Abstract knowledge graph probabilistic modeling\n",
      "𝑦\n",
      "Exploitingtheappropriateinductivebiasbasedontheknowl- 1(Sensor 1)\n",
      "is-attached-to\n",
      "edge of data is essential for achieving good performance in is-part-of A\n",
      "statisticalmachinelearning.Inpractice,however,thedomain\n",
      "knowledgeofinterestoftenprovidesinformationontherela- 𝑦\n",
      "tionshipofdataattributesonlydistantly,whichhindersdirect 2(Sensor 2)\n",
      "utilizationofsuchdomainknowledgeinpopularregulariza- B\n",
      "tionmethods.Inthispaper,weproposetheknowledge-based is-connected-to\n",
      "distantregularizationframework,inwhichweutilizethedis- 𝑦 3(Sensor 3)\n",
      "tantinformationencodedinaknowledgegraphforregular-\n",
      "C\n",
      "ization of probabilistic model estimation. In particular, we\n",
      "propose to impose prior distributions on model parameters\n",
      "specifiedbyknowledgegraphembeddings.Asaninstanceof Figure1:Schematicdiagramofourmotivatingexample,the\n",
      "theproposedframework,wepresentthefactoranalysismodel analysisofsensordataofaplant.Insuchapplications,even\n",
      "withtheknowledge-baseddistantregularization.Weshowthe if the physical and/or statistical relations (dotted lines) be-\n",
      "resultsofpreliminaryexperimentsontheimprovementofthe\n",
      "tweenattributes,i.e.,sensorreadings,cannotbespecifieddi-\n",
      "generalizationcapabilityofsuchmodel.\n",
      "rectly, somehow distant information on plant’s instruments\n",
      "(blue arrows) are often available beforehand. We suppose\n",
      "Introduction such information can be encoded in a form of knowledge\n",
      "Thedata-drivennatureofstatisticalmachinelearningisone graphs.Wewouldliketoleveragesuchdistantinformation\n",
      "oftheprincipalreasonsforitssuccess,whereasimposingan toregularizelearningofprobabilisticmodels.\n",
      "appropriateinductivebiasisindispensableforgaininggen-\n",
      "eralizationcapability.Regularizationisaprevailingmethod-\n",
      "ologyforimposingtheinductivebias.Severaltypesofpopu- Ourmotivatingexample,depictedinFigure1,isintheanal-\n",
      "larregularizationmethodsareknown,suchasTikhonovreg- ysisofsensordataobtainedfromaplant.Althoughtherela-\n",
      "ularization and sparsity regularization, and they have been tionsbetweenplant’sinstrumentsandsubsystems(e.g.,how\n",
      "commonlyutilizedinawiderangeoftaskswithstrongtheo- they are connected each other) can often be written down\n",
      "reticalsupports.However,itisnotalwayspossibleforthose easily,therelationsbetweentheattributes(e.g.,sensorIDs)\n",
      "generalregularizationstrategiestodirectlyleveragethedo- canhardly bederiveddirectlyfrom thembecausetodo so,\n",
      "mainknowledgeavailableonagivenapplicationsincethey weneedtoidentifythemechanismandthephysicsofthein-\n",
      "areoftenbuiltuponthestatistical“meta-knowledge,”rather strumentsandthesensors,whichiscostlyinmanypractices.\n",
      "thanthedomainknowledge.\n",
      "Inthispaper,wesuggestamethodtoutilizesuchdistant\n",
      "The usability of domain knowledge is one of the pri-\n",
      "domainknowledgetoregularizetheestimationofstatistical\n",
      "malconcernsinpracticesofmachinelearning.Ifonecould\n",
      "models.Specifically,wepresentaframeworkofknowledge-\n",
      "clearly specify the relationship of attributes and objects in\n",
      "based distant regularization in learning probabilistic mod-\n",
      "data,itcouldthenbeutilizedbythemethodslikestructured\n",
      "els for continuous-valued data such as sensor readings. We\n",
      "sparsity regularization (Huang, Zhang, and Mataxas 2011)\n",
      "aim to build a regularization principle using a knowledge\n",
      "andgraphLaplacianregularization(see,e.g.,(Shahidetal.\n",
      "graph, which is a multi-relational directed graph in which\n",
      "2016) and references therein). Unfortunately, however, the\n",
      "relations between entities are enumerated. This is because\n",
      "domain knowledge of interest does not always provide di-\n",
      "a knowledge graph is often useful for summarizing the do-\n",
      "rectinformationontherelationshipoftheattributesandthe\n",
      "main knowledge of interest. Let us revisit the example of\n",
      "objects;instead,itoftengivesdistant informationonthem.\n",
      "sensor data analysis in Figure 1; engineers of a plant com-\n",
      "∗A substantial portion of this work was performed while the prehend the relations between the instruments, such as is-\n",
      "authorswereattheUniversityofTokyo. part-of andis-connected-to,aswellastherelationsbetween\n",
      "8102\n",
      "nuJ\n",
      "92\n",
      "]GL.sc[\n",
      "1v23311.6081:viXra\n",
      "theinstrumentsandthesensors,i.e.,is-attached-to,accord- However, this method is not always applicable to utilizing\n",
      "ing to which one can build a knowledge graph. Note that thepriordomainknowledgebecauseinthismethod,allthe\n",
      "what is specified on the data attributes is only the simple dataattributesmustbeidentifiedinthegraphandextension\n",
      "is-attached-to information, and no direct relations between tomulti-relationalcasesisnotstraightforward.\n",
      "themareprovidedingeneral. Another example of graph-based regularization is the\n",
      "Thecentralideaoftheproposedmethodistodefineprior methods using the graph Laplacian (see, e.g., (Shang, Jiao,\n",
      "distributions of model parameters using knowledge graph andWang2012;Pei,Chakraborty,andSycara2015;Shahid\n",
      "embeddings(seetheexcellentsurveypaperssuchas(Nickel et al. 2016)), in which the Laplacian matrix of graphs that\n",
      "etal.2015;Wangetal.2017;Cai,Zheng,andChang2018)), encode relations between objects and/or attributes is used\n",
      "whicharecontinuous-valuedrepresentationsofentities(and forspecifyingtheregularizationterm.However,suchmeth-\n",
      "relations) in knowledge graphs. The proposed method is ods are also not always applicable in practice because we\n",
      "flexible enough to utilize many types of knowledge graph maynotbeabletocomputeareliableLaplacianmatrixdue\n",
      "embeddings,whichformanactivefieldofresearchrecently. tothelackofpreciseknowledgeoftherelationsbetweenthe\n",
      "Hence,thedevelopmentinthefieldcanbeimmediatelyim- dataattributesandobjects.\n",
      "portedtotheproposedframework.\n",
      "Incorporating Knowledge as Constraints / Rules Re-\n",
      "The chief advantage of the proposed method, the distant\n",
      "searchershavebeenstudyingonmethodstoincorporatethe\n",
      "regularizationbyknowledgegraphs,isthatitcanbeapplied\n",
      "knowledgedescribedinaformofconstraintsorlogicalrules\n",
      "to a broad range of probabilistic models. We introduce the\n",
      "to well-known machine learning methods. To name a few,\n",
      "knowledge-based factor analysis model in this paper as an\n",
      "Towell and Shavlik (1994) proposed the knowledge-based\n",
      "instance, but similar modification of existing probabilistic\n",
      "neural networks, Fung, Mangasarian, and Shavlik (2003)\n",
      "models is possible whenever the domain knowledge of in-\n",
      "suggested the knowledge-based support vector machines,\n",
      "terestisdescribedinaformofknowledgegraphs.\n",
      "and Varol et al. (2012) formulated the constrained latent\n",
      "variable model. Moreover, there are studies on incorporat-\n",
      "RelatedWork\n",
      "inglogicalrulestoexistingstatisticalmodels.Schiegg,Neu-\n",
      "Theinductivebiasistheessentialelementofstatisticalma- mann,andKersting(2012)proposedthemixturesofGaus-\n",
      "chinelearningandusuallyintroducedbydesigningfeatures, sian process regressors controlled by logical rules formu-\n",
      "hypothesis spaces, loss functions, regularization terms, and latedbytheMarkovlogicnetwork(RichardsonandDomin-\n",
      "soon.Forexample,themodelfamilyisusuallyselectedac- gos2006).And,severalresearchersproposedthetopicmod-\n",
      "cordingtouser’sknowledgeofthedatatobeanalyzed.Ifthe elswithlogic(Andrzejewskietal.2011;Mei,Zhu,andZhu\n",
      "user is sure about the relationship of attributes and objects 2014; Foulds, Kumar, and Getoor 2015), which are useful\n",
      "inadataset,amodeltailoredforthedataset(e.g.,Bayesian for modeling document corpora with prior knowledge of\n",
      "networks)canbedeveloped.Thisisdifficultorimpossible, words.\n",
      "however,whentheknowledgeisgiveninadistantform.In\n",
      "StatisticalRelationalLearning Moregenerally,learning\n",
      "fact,therearealotofresearchesonutilizingknowledgein\n",
      "onrelationaldata,whichisoftentermedstatisticalrelational\n",
      "variousforms.\n",
      "learning (SRL), is an active area of research with a broad\n",
      "In what follows, we introduce only a part of the rich lit-\n",
      "rangeofmethodsandapplications(see,e.g.,(DeRaedt,Ker-\n",
      "erature on knowledge utilization for machine learning and\n",
      "sting, and Natarajan 2016)). We do not describe the details\n",
      "someotherrelatedtopicsthatwouldbeusefulforconsider-\n",
      "herebecauseofthevastamountoftheliteratureinthefield,\n",
      "ingthefuturedirectionofthiswork.\n",
      "but the methodology of SRL would be a promising tool in\n",
      "Incorporating Knowledge as Graphs Actually, it has thefuturedirectionofthiswork.\n",
      "beenactivelystudiedhowtoutilizethepriorknowledgethat\n",
      "Distant / Weak Supervision by Knowledge Particularly\n",
      "canbeencodedinaknowledgegraph.Forexample,Yaoet\n",
      "inthecontextofsupervisedlearning,thereisalineofstudies\n",
      "al. (2017) proposed an extension of latent Dirichlet alloca-\n",
      "onutilizinguser’sheuristicsorknowledgebasesasweaksu-\n",
      "tionthatmodelswordtokensaswellasentitiesindocument\n",
      "pervisionsources(see(Ratneretal.2016;Varmaetal.2017;\n",
      "corpora, where the entities are corroborated by a knowl-\n",
      "Bachetal.2017)andreferencestherein)withgrowinginter-\n",
      "edge graph and have the corresponding embeddings. Also\n",
      "ests. In the settings of those studies, the knowledge should\n",
      "in the context of topic models, Yao et al. (2015) and Hu\n",
      "provideinformationonlabelstobepredicted.\n",
      "et al. (2016) suggested to utilize a taxonomy of entities in\n",
      "documents; a taxonomy, i.e., a set of hierarchical relations Posterior Regularization Zhu, Chen, and Xing (2014)\n",
      "between entities, can be described as a (single-relational) proposed an elegant and flexible framework, termed Reg-\n",
      "graph. Note that these studies mainly treat discrete-valued Bayes,forregularizingtheposteriordistributionofBayesian\n",
      "datasuchasdocumentcorpora. inference. It can be utilized for efficiently incorporating\n",
      "Regularization based on the graph information is a ma- one’s knowledge in Bayesian inference, and in fact, Mei,\n",
      "jor field of research also in other contexts. For example, Zhu, and Zhu (2014) utilized it for incorporating logical\n",
      "thegraphsparsityregularization(Jacob,Obozinski,andVert rules to a topic model. Note that, while RegBayes tries to\n",
      "2009; Huang, Zhang, and Mataxas 2011; Mairal and Yu changetheposterior,themethodproposedinthisworktries\n",
      "2013) imposes structured sparsity on parameters according tochangetheprior,whichmayremindoneoftheempirical\n",
      "to a graph that encodes prior knowledge on data attributes. Bayesmethod.\n",
      "Background Representation learning for knowledge graphs is an ac-\n",
      "In this section, we first introduce the notations used in this tive area of research recently. We do not enumerate the\n",
      "paper to express the concepts regarding the probabilistic works and just refer to the excellent survey papers on the\n",
      "models. Afterward, we give a brief explanation on knowl- field(Nickeletal.2015;Wangetal.2017;Cai,Zheng,and\n",
      "edgegraphembeddingsforcompleteness,whichreadersfa- Chang2018).Here,weintroducetheverybasicconceptsin\n",
      "miliarwiththemmayskip. this field, which are needed when describing the proposed\n",
      "method.\n",
      "ProbabilisticModelsforContinuous-ValuedData In a typical setting, given a set of tuples that exist in a\n",
      "knowledgegraph(i.e.,positivetuples),welearnreal-valued\n",
      "Themainsubjectofthisstudyistheregularizationinthees-\n",
      "timationofprobabilisticmodelsforcontinuous-valueddata.\n",
      "vectorse ∈ Rde correspondingtoentitiesintheknowledge\n",
      "graph. Such vectors are called embeddings of entities. The\n",
      "Formally, we denote a continuous-valued dataset by a set\n",
      "{y ∈Rm |j =1,...,n},wherey istheobservationwith learningisdonesothatsomescorefunction\n",
      "j j\n",
      "mattributes(i.e.,features)correspondingtothej-thobject. ψ(e, e ; M ):Rde ×Rde →R (1)\n",
      "h t r\n",
      "And, n denotes the number of objects (individuals, times-\n",
      "becomeslargeforthepositivetuplesandsmallfortheneg-\n",
      "tamps,etc.)inadataset.Forinstance,wemayhavereadings\n",
      "ativetuples.Here,e ande denotetheembeddingsofen-\n",
      "of m types of sensors at n timestamps in the sensor data. h t\n",
      "titieshandt,respectively,And,ψisthescorefunctionthat\n",
      "Notethateachy mayormaynotbeindependentandiden-\n",
      "j hastherelation-specificparameter,M,withregardtorela-\n",
      "ticallydistributed.Theprobabilisticmodelsforsuchdatade- r\n",
      "tion r. In the latter part of this paper, as an instance of the\n",
      "fine probability density p(y,...,y | θ), where θ is a set\n",
      "1 n knowledgegraphembedding,weuseasimplemethodcalled\n",
      "ofmodelparameters.Moreover,somemodelsmayhavean\n",
      "additionalsetofrandomvariables,denotedby{x\n",
      "j\n",
      "∈Rdx}, D asis ft oM llou wlt s( :Yangetal.2015),whosescorefunctionisdefined\n",
      "whichisoftentermedlatentvariables.\n",
      "We further distinguish the model parameters as θ = ψ(e, e ; m )=e(cid:62)diag(m )e, (2)\n",
      "h t r h r t\n",
      "l{ ow ca1 l,. p. a. r, aw mm ete, rπ 1} c; oo rn reso pn oe nh da inn gd, tole tt hw eii -t∈ haR ttd riw bud te en.o Ote ntt hh ee where m\n",
      "r\n",
      "∈ Rde is the relation-specific parameter. There\n",
      "are several manners of optimization for learning e and M,\n",
      "other hand, let π be the vector of global parameters that\n",
      "suchasenergy-basedoptimizationandrankoptimization.In\n",
      "are independent of the difference of attributes. Let us give\n",
      "thispaper,asdescribedbelow,wesimplytreatψasapartof\n",
      "anexample;theobservationmodelofprobabilisticprincipal\n",
      "a logistic regressor and maximize the binary classification\n",
      "componentanalysis(TippingandBishop1999)is\n",
      "likelihoodvianegativetuplesampling.\n",
      "p(y |x,θ)=N\n",
      "(cid:0)\n",
      "y |Wx +µ,\n",
      "σ2I(cid:1)\n",
      ", Moststudiesontheknowledgegraphembeddingconsider\n",
      "j j j j\n",
      "only the sparse structure of a multi-relational graph. Re-\n",
      "where W ∈ Rm×dx is the factor loading matrix and d\n",
      "x\n",
      "is\n",
      "cently,thereareseveralstudiesonfusingadditionalinforma-\n",
      "thedimensionalityoflatentvariablex.Inthismodel,thei-th\n",
      "tionassociatedwithknowledgegraphssuchascontinuous-\n",
      "localparameter,w,correspondstothei-throwofW (and\n",
      "i valued attributes of entities (Zhang et al. 2016), literals\n",
      "the i-th element of µ), and σ2 is an element of the global\n",
      "(Kristiadi et al. 2018), text descriptions (Xie et al. 2016;\n",
      "parameters,π.\n",
      "Fan et al. 2017; Xiao et al. 2017), images (On˜oro-Rubio\n",
      "Theregularizationintheestimationofprobabilisticmod-\n",
      "etal.2017),andthemultimodalinformation(Pezeshkpour,\n",
      "els is often executed by setting prior distributions on the\n",
      "Chen, and Singh 2017; Thoma, Rettinger, and Both 2017).\n",
      "parameters,andmanyotherpopularregularizationmethods\n",
      "Theirprimaryconcernistheimprovementoftheknowledge\n",
      "can also be interpreted as imposing prior distributions. In\n",
      "graphembeddingsandtheirapplicationssuchaslinkpredic-\n",
      "thispaper,wefollowthisstrategytoachievetheknowledge-\n",
      "tion.Thetechniquesutilizedtherewouldbeofgreatinterest\n",
      "baseddistantregularization;weplacethepriordistributions\n",
      "also for our purpose, i.e., regularization in learning proba-\n",
      "that are parameterized by the embeddings of knowledge\n",
      "bilisticmodelsforcontinuousdata.\n",
      "graphs.Thedetailsaredescribedinthenextsection.\n",
      "ProposedMethod\n",
      "KnowledgeGraphEmbedding\n",
      "In this section, we describe the detail on the proposed\n",
      "A knowledge graph is a multi-relational directed graph\n",
      "method for distantly regularizing the estimation of proba-\n",
      "whoseverticescorrespondtoentitiesandedgescorrespond\n",
      "bilistic models using knowledge graph embeddings. First,\n",
      "to relations between the entities. Such graph can be de-\n",
      "thegeneralframework,inwhichwedonotpremisespecial\n",
      "scribed as a set of tuples {(h,r,t)}, where h and t denote\n",
      "typesofprobabilisticmodelsandknowledgegraphembed-\n",
      "the head entity and the tail entity respectively, and r de-\n",
      "dings,ispresented.Afterward,asaninstanceoftheframe-\n",
      "notes the relation between them. There are various large-\n",
      "work,weintroducethefactoranalysisdistantlyregularized\n",
      "scaleknowledgebaseswhichcanberegardedasknowledge\n",
      "byaknowledgegraph.\n",
      "graphs,suchasFreebaseandGeneOntology,whicharecol-\n",
      "lectionsofknowledgeongeneralentitiesoftheworldoron\n",
      "GeneralFramework\n",
      "domain-specificconcepts.\n",
      "Ourpurposeistobuildaregularizationprincipleforthees-\n",
      "1Thedimensionalityofw maydependoni,butforsimplicity, timation of the parameters of probabilistic models accord-\n",
      "i\n",
      "weassumethesamedimensionalityforalli. ing to a knowledge graph that encodes (possibly distant)\n",
      "priorknowledgeonthedata-generatingsystem.Tothisend, FactorAnalysisRegularizedbyKnowledge\n",
      "we propose to specify the prior distributions of the model\n",
      "Amongtheprobabilisticmodelsforcontinuousdata,thefac-\n",
      "parameters using knowledge graph embeddings. Formally,\n",
      "tor analysis (FA) and its special version, principal compo-\n",
      "consideraprobabilisticmodel\n",
      "nent analysis (PCA), have been favored in many applica-\n",
      "p(y,...,y |θ), (3) tions.Asaninstanceoftheproposedframework,wepresent\n",
      "1 n\n",
      "asimplemethodforincorporatingknowledgegraphstoreg-\n",
      "where θ = {w,...,w,π}. In the proposed framework,\n",
      "1 m ularizationintheparameterestimationofFA.\n",
      "wespecifythepriordistributionsonthe(partof)localmodel\n",
      "The observation probabilistic model of FA is often ex-\n",
      "parameters,w,...,w,asfollows.\n",
      "1 m pressedas\n",
      "First, we assume that we have a knowledge graph, in\n",
      "which entities corresponding to the (part of) data attributes\n",
      "p(y,...,y |x,...,x,θ)\n",
      "exist,andthatsuchentitiesappearinasetofpositivetuples 1 n 1 n\n",
      "m n\n",
      "atleastonce.Inthesensordataexample(Figure1),thisas- (cid:89)(cid:89)\n",
      "= N(y |w(cid:62)x +µ, σ2), (6)\n",
      "sumption means that there is an entity like “Sensor i,” and i,j i j i i\n",
      "theknowledgegraphhasatuplelike i=1j=1\n",
      "(h=Sensori, r =is-attached-to, t=InstrumentA) wherew(cid:62) ∈R1×dx isthei-throwofthefactorloadingma-\n",
      "i\n",
      "foratleastapartofattributesi=1,...,m(cid:48)(1≤m(cid:48) ≤m). trixofFA,µ i istheobservationmean,andσ i2 ∈ R+ isthe\n",
      "observation noise variance of the i-th attribute. Moreover,\n",
      "Inthefollowing,wedenotetheembeddingoftheentitythat\n",
      "corresponds to the i-th attribute of data by e. Note that\n",
      "x\n",
      "j\n",
      "∈ Rdx is the latent factor corresponding to the j-th ob-\n",
      "i\n",
      "ject, whose prior distribution is usually set by the standard\n",
      "in the knowledge graph of our interest, there would also\n",
      "normaldistribution,i.e.,N(0,I).\n",
      "be many entities that do not directly correspond to the at-\n",
      "Letusimposetheknowledge-basedpriordistributionson\n",
      "tributes, and not all the attributes appear in the knowledge\n",
      "(partof)therowsofthefactorloadingmatrix,w,...,w\n",
      "graph. Also note that, while here we consider only the en- 1 m(cid:48)\n",
      "(m(cid:48) ≤ m). Here, instead of the normal distribution like\n",
      "tities corresponding to the attributes, similar discussion is\n",
      "Eq. (4), we specify the prior distribution of w given em-\n",
      "straightforwardevenwhenweconsiderentitiescorrespond- i\n",
      "beddinge bythepointmass,i.e.,\n",
      "ingtotheobjectsindata. i\n",
      "Giventheknowledgegraphofthiskind,wespecifyaprior p(w |e )=δ(w −c (e )), for i=1,...,m(cid:48), (7)\n",
      "distribution on w parameterized by e, for i = 1,...,m(cid:48), i i i ξ i\n",
      "i i\n",
      "asaregularizerofthemodeltraining.Forexample,forreal- whereδdenotestheDiracdeltafunction.Thissimplification\n",
      "valued parameters, a canonical choice is the normal distri- enables us to maximize the (regularized) objective just by\n",
      "bution,i.e., replacingw withc (e ).Weobservednosignificantdegra-\n",
      "i ξ i\n",
      "dationofperformanceduetothissimplification.\n",
      "p(w |e )=N(w |c (e ), V (e )), (4)\n",
      "i i i ξ i ξ i\n",
      "In summary, an optimization problem to be solved for\n",
      "where c ξ : Rde → Rdw, V ξ : Rde → Rdw×dw, and ξ learning FA distantly regularized based on the knowledge\n",
      "denotes the set of parameters in c and V. With the priors graphisthemaximizationof\n",
      "onw,...,w specifiedinthismanner,wecomputetheir\n",
      "1 m(cid:48)\n",
      "maximum a posteriori (MAP) estimations. For the remain- f(e 1:E,M 1:R,w 1:m,µ 1:m,σ 12 :m,ξ)\n",
      "ingattributesw m(cid:48)+1,...,w m,ifany,wecomputethemaxi-\n",
      "1\n",
      "(cid:88)n\n",
      "mumlikelihoodestimationsorMAPestimationswithpriors = logN(y |µ, Σ)\n",
      "n j\n",
      "definedotherwise.\n",
      "j=1\n",
      "For the above framework, the embeddings of knowledge\n",
      "(cid:96)\n",
      "graph’sentitiesmustbepreparedinsomeways.Apromis-\n",
      "1(cid:88)\n",
      "+ logΦ(ψ(e, e ; M )) (8)\n",
      "ingwaytopreparegoodembeddingsistoutilizethemodel (cid:96) hk tk rk\n",
      "k=1\n",
      "pretrainedonalarge-scaleknowledgegraph.However,such\n",
      "(cid:96)(cid:48)\n",
      "pretrained embeddings are usually not available for user- 1 (cid:88) (cid:0) (cid:1)\n",
      "+ log 1−Φ(ψ(e, e ; M )),\n",
      "definedknowledgegraphsthatdescribethedomainknowl- (cid:96)(cid:48) h k(cid:48) t k(cid:48) r k(cid:48)\n",
      "edge.Hence,inmanypractices,theembeddingshavetobe k(cid:48)=1\n",
      "learned simultaneously with the parameters of the proba- s.t. w i =c ξ(e i) for i=1,...,m(cid:48),\n",
      "bilistic model. Now suppose we define a score function ψ\n",
      "for the embedding learning. In the framework, we propose where µ = [µ 1 ··· µ m](cid:62), Σ = WW(cid:62) +\n",
      "tomaximizebinaryclassificationlikelihood: diag{σ 12,...,σ m2 }, and W = [w 1 ··· w m](cid:62). In Eq. (8),\n",
      "E denotes the total number of entities (including the ones\n",
      "p((h,r,t)=true|e, e, M )=Φ(ψ(e, e ; M )), (5)\n",
      "h t r h t r corresponding to the data attributes), R denotes the num-\n",
      "whereΦisthesigmoidfunction.Notethatanyψcanbeused berofrelations,and(cid:96)and(cid:96)(cid:48) denotethenumbersofpositive\n",
      "heresolongasitsrangeisthereal.Bysettingtheobjective and negative tuples, respectively. The first term of Eq. (8)\n",
      "of the embedding learning in this way, the final objective isobtainedbymarginalizingoutxinEq.(6)withtheprior\n",
      "function simply becomes the addition of the logarithms of N(0,I).\n",
      "datalikelihoodEq.(3),parameterpriorsEq.(4),andknowl- The negative tuples, needed for computing Eq. (8), are\n",
      "edgegraphlikelihoodEq.(5). usually not available originally. They are often created by\n",
      "−660\n",
      "−670\n",
      "−680\n",
      "−690\n",
      "0 50 100\n",
      "proportionofKGtuples[%]\n",
      "AFfoLLNtset.evA −660\n",
      "−680\n",
      "20 40 60 80\n",
      "proportionoftrainingdata[%]\n",
      "AFfoLLNtset.evA\n",
      "w/KG\n",
      "w/oKG\n",
      "(a)\n",
      "−650\n",
      "−660\n",
      "−670\n",
      "0 50 100\n",
      "proportionofKGtuples[%]\n",
      "AFfoLLNtset.evA −550\n",
      "−600\n",
      "−650\n",
      "−700\n",
      "20 40 60 80\n",
      "proportionoftrainingdata[%]\n",
      "AFfoLLNtset.evA\n",
      "w/KG\n",
      "w/oKG\n",
      "(b)\n",
      "Figure2:Resultsunder(a)theRandomdata-partitionscenarioand(b)theShiftdata-partitionscenario.Ineachpanel,left\n",
      "plot shows average test NLLs along different proportions of knowledge graph tuples used by the proposed method, and the\n",
      "rightplotshowsaveragetestNLLsalongdifferentamountsofdatausedforthetraining.Intherightplots,thetestNLLsoftwo\n",
      "casesareshown:100%oftheknowledgegraphtupleswasusedbytheproposedmethod(w/KG)ornoknowledgegraphtuples\n",
      "wereprovided(w/oKG).Ineveryplot,themeansandthestandarddeviationsfor10randomtrialsareshown.\n",
      "randomly permuting the entities and/or the relations in the Therefore, n = 1,380 and m = 227. The dataset was cre-\n",
      "positive tuples (see, e.g., (Nickel et al. 2015; Wang et al. ated using the resource available online at Climate Change\n",
      "2017;Cai,Zheng,andChang2018)).Inthiswork,wecre- KnowledgePortal.2\n",
      "ated some negative tuples per a positive tuple by replacing\n",
      "Knowledge Graph In order to precisely model the rela-\n",
      "eithertheheadorthetailbyanyotherentitythatexistsinthe\n",
      "tionship of precipitation among countries and regions, we\n",
      "knowledge graph. When permuting the entity, we chose an\n",
      "needmeteorologicalknowledgetosomeextent.However,in\n",
      "attribute-correspondingentityforanattribute-corresponding\n",
      "ourscenario,suchknowledgethatdirectlygivestherelations\n",
      "entity,andanon-attributeentityforanon-attributeentity,re-\n",
      "betweendataattributescanhardlybeobtained.Tosimulate\n",
      "spectively.\n",
      "this situation, in the experiments, we utilize a knowledge\n",
      "InEq.(8),ψ isanarbitraryscorefunctionofknowledge\n",
      "graphthatencodessimplegeographicrelationsbetweenthe\n",
      "graph embedding learning. In the experiments introduced\n",
      "countriesandregions.\n",
      "below,weusedthesimplescorefunctionofDistMult(Yang\n",
      "etal.2015),definedinEq.(2).Moreover,asc (·),weused Webuiltaknowledgegraphusingtheinformationavail-\n",
      "ξ able online,3 which includes the geographic relationship\n",
      "anaffinemodel\n",
      "c (e )=Ae +b (9) between countries and regions in the world, following\n",
      "ξ i i\n",
      "the scheme presented by Bouchard, Singh, and Trouillon\n",
      "withparametersA ∈ Rdx×de andb ∈ Rdx usedcommonly\n",
      "(2015).Theknowledgegraphweusedhas278entities(in-\n",
      "fori=1,...,m(cid:48),i.e.,ξ ={A,b}.\n",
      "cluding ones that do not correspond to the attributes of the\n",
      "data) and two types of relations, is-inside and is-neighbor-\n",
      "PreliminaryExperiments\n",
      "of; positive tuples include (Sweden, is-inside, Europe) and\n",
      "Inthissection,weprovidetheresultsofpreliminaryexper- (Norway, is-neighbor-of, Sweden), for example. And, the\n",
      "iments to investigate how the performance of the distantly knowledge graph has 1,167 positive tuples comprising the\n",
      "regularizedmodelsisimproved. entitiesandtherelationsabove.\n",
      "Dataset We used real-world data of rainfall in the world. 2sdwebx.worldbank.org/climateportal/\n",
      "The dataset we prepared comprises the monthly means of (retrieved29April2018)\n",
      "historicalmeasurementsofprecipitationfromJanuary1901 3github.com/mledoze/countries/\n",
      "toDecember2015in227countriesandregionsintheworld. (retrieved29April2018)\n",
      "Notethat,inthisknowledgegraph,someentitiesofcoun- In Figures 2a and 2b, we show the results under the\n",
      "tries like Japan have no is-neighbor-of relations with any Random and Shift data-partition scenarios, respectively.\n",
      "other entities because the is-neighbor-of relation does not In the left plots, the test NLLs for different numbers of\n",
      "imply the cross-sea adjacency. Therefore, if we build a knowledge graph tuples used by the proposed method are\n",
      "single-relationalgraphonlywiththeis-neighbor-of relation, shown.Basically,thebettertestNLLsareachievedwiththe\n",
      "which might be utilized for methods like graph Laplacian larger amount of knowledge graph’s information provided.\n",
      "regularization,itdoesnotprovidesufficientinformationon In the right plots, the test NLLs with different amounts of\n",
      "thecountriesthathavenoneighboringcountries. trainingdataareshown.WhilethetestNLLsarealwaysim-\n",
      "provedbytheproposedmethodundertheRandomscenario,\n",
      "Setups Asforthefactoranalysismodeldistantlyregular-\n",
      "noimprovementisobservedwithsomewhatsmallamountof\n",
      "izedbyknowledge,thehyperparameterstobesetmanually\n",
      "trainingdataundertheShiftscenario.\n",
      "are the dimensionality of latent factors, d, as well as the\n",
      "x\n",
      "dimensionalityofentityembeddings,d.Forafaircompar-\n",
      "e\n",
      "Conclusion\n",
      "ison, we set the same values for them, i.e., d = d. And,\n",
      "x e\n",
      "in what follows, we show the results with d = 5, which\n",
      "x In this paper, we have proposed the knowledge-based dis-\n",
      "was decided empirically without any intensive search be-\n",
      "tantregularizationframework,ageneralmethodologytoin-\n",
      "cause the purpose of this experiment was just to compare\n",
      "corporate the possibly distant domain knowledge encoded\n",
      "theperformancesofafactoranalysismodelwithandwith-\n",
      "inaformofknowledgegraphs.Inparticular,wesuggested\n",
      "outtheknowledge-basedregularization.\n",
      "thefactoranalysiswiththeknowledge-basedregularization.\n",
      "Wepartitionedthedatasetintothetraining/validationset\n",
      "Weprovidedthepreliminaryexperimentalresultsontheim-\n",
      "andthetestsetattheratesspecifiedbelow.Withinthetrain-\n",
      "provementofthegeneralizationcapabilityoftheregularized\n",
      "ing/validationset,wealwaysusedrandomlychosen80%for\n",
      "factoranalysis.\n",
      "trainingandtheremaining20%forvalidation.Whenparti-\n",
      "Indeed, the proposed framework is in its infancy. There\n",
      "tioning the original dataset into the training/validation set\n",
      "are plenty of challenges to be tackled. For example, we\n",
      "and<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  97258,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Naoya Takeishi', 'Kosuke Akimoto']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: regularizationframework,ageneralmethodologytoin-\n",
      "cause the purpose of this experiment was just to compare\n",
      "corporate the possibly distant domain knowledge encoded\n",
      "theperformancesofafactoranalysismodelwithandwith-\n",
      "inaformofknowledgegraphs.Inparticular,wesuggested\n",
      "outtheknowledge-basedregularization.\n",
      "thefactoranalysiswiththeknowledge-basedregularization.\n",
      "Wepartitionedthedatasetintothetraining/validationset\n",
      "Weprovidedthepreliminaryexperimentalresultsontheim-\n",
      "andthetestsetattheratesspecifiedbelow.Withinthetrain-\n",
      "provementofthegeneralizationcapabilityoftheregularized\n",
      "ing/validationset,wealwaysusedrandomlychosen80%for\n",
      "factoranalysis.\n",
      "trainingandtheremaining20%forvalidation.Whenparti-\n",
      "Indeed, the proposed framework is in its infancy. There\n",
      "tioning the original dataset into the training/validation set\n",
      "are plenty of challenges to be tackled. For example, we\n",
      "andthetestset,wefollowedtwodifferentscenarios.Oneis\n",
      "need to develop a unified method for efficiently learning\n",
      "theRandomdata-partition,inwhichrandompermutationof\n",
      "themodelsthatareregularizedbyknowledgegraphs.Also,\n",
      "the 1,380 objects in each dataset was performed before the\n",
      "how the generalization capability of the regularized mod-\n",
      "partition.AnotheristheShiftdata-partition,inwhichno\n",
      "elsisimprovedshouldbeanalyzedtheoreticallyandempir-\n",
      "randompermutationisdonebeforethepartition,andconse-\n",
      "ically. Moreover, it would be interesting to investigate how\n",
      "quently,somethinglikethecovariateshiftmayoccur,which\n",
      "the knowledge graph embeddings are modified due to the\n",
      "makesgeneralizationdifficult.\n",
      "datafedintotheprobabilisticmodels.\n",
      "We also controlled the amount of information possibly\n",
      "provided by the knowledge graph by changing the propor-\n",
      "References\n",
      "tionofknowledgegraph’s(positive)tuplesconsideredinthe\n",
      "optimizationofEq.(8).Forexample,whenweused80%of\n",
      "Andrzejewski,D.;Zhu,X.;Craven,M.;andRecht,B. 2011.\n",
      "the tuples, we randomly disposed 20% of the 1,167 posi-\n",
      "A framework for incorporating general domain knowledge\n",
      "tivetuplesandperformedtheoptimizationwiththenegative\n",
      "into latent Dirichlet allocation using first-order logic. In\n",
      "sampling. As the “no knowledge graph” limit, we ran the\n",
      "Proceedings of the 22nd International Joint Conference on\n",
      "optimizationwith0%ofthetuples.\n",
      "ArtificialIntelligence,1171–1177.\n",
      "The optimization was performed using gradient descent\n",
      "Bach, S. H.; He, B.; Ratner, A.; and Re´, C. 2017. Learn-\n",
      "whose learning rate was adjusted by Adam. For the opti-\n",
      "ingthestructureofgenerativemodelswithoutlabeleddata.\n",
      "mization, we randomly created two negative tuples per a\n",
      "InProceedingsofthe34thInternationalConferenceonMa-\n",
      "positivetuple.Thevalidationsetwasutilizedforearlystop-\n",
      "chineLearning,273–282.\n",
      "ping of optimization, in which the optimization was termi-\n",
      "natedifnoimprovementofthelosswithregardtoFA(i.e., Bouchard, G.; Singh, S.; and Trouillon, T. 2015. On ap-\n",
      "the negative of the first term of Eq. (8)) on the validation proximatereasoningcapabilitiesoflow-rankvectorspaces.\n",
      "setwasobservedfor50epochs,andthemodelthatachieved InProceedingsoftheAAAISpringSyposiumonKnowledge\n",
      "the best loss was saved. We randomly initialized the factor RepresentationandReasoning.\n",
      "loading matrix of FA, the embeddings of entities, and the Cai,H.;Zheng,V.W.;andChang,K. 2018. Acomprehen-\n",
      "relationparameters.Nopretrainingfortheembeddingswas sive survey of graph embedding: Problems, techniques and\n",
      "performed. applications. IEEE Transactions on Knowledge and Data\n",
      "Engineering. inpress,arXiv:1709.07604.\n",
      "Results In what follows, we report the average negative\n",
      "loglikelihood(NLL)ofFAforthetestdataset,i.e., De Raedt, L.; Kersting, K.; and Natarajan, S. 2016. Sta-\n",
      "tisticalRelationalArtificialIntelligence:Logic,Probability,\n",
      "1\n",
      "(cid:88)ntest\n",
      "andComputation. Morgan&ClaypoolPublishers.\n",
      "− logN(y |µ, Σ),\n",
      "n j Fan, M.; Zhou, Q.; Zheng, T. F.; and Grishman, R. 2017.\n",
      "test\n",
      "j=1\n",
      "Distributed representation learning for knowledge graphs\n",
      "wheren denotesthenumberofdatapointsinthetestset. withentitydescriptions. PatternRecognitionLetters93:31–\n",
      "test\n",
      "NotethatlowervaluesofNLLarepreferred. 37.\n",
      "Foulds,J.;Kumar,S.H.;andGetoor,L. 2015. Latenttopic Shang, F.; Jiao, L. C.; and Wang, F. 2012. Graph\n",
      "networks:Aversatileprobabilisticprogrammingframework dualregularizationnon-negativematrixfactorizationforco-\n",
      "fortopicmodels. InProceedingsofthe32ndInternational clustering. PatternRecognition45(6):2237–2250.\n",
      "ConferenceonMachineLearning,777–786. Thoma,S.;Rettinger,A.;andBoth,F. 2017. Towardsholis-\n",
      "Fung,G.M.;Mangasarian,O.L.;andShavlik,J.W. 2003. tic concept representations: Embedding relational knowl-\n",
      "Knowledge-based support vector machine classifiers. In edge, visual attributes, and distributional word semantics.\n",
      "Advances in Neural Information Processing Systems, vol- InLectureNotesinComputerScience,volume10587,694–\n",
      "ume15,537–544. 710.\n",
      "Hu, Z.; Luo, G.; Sachan, M.; Xing, E.; and Nie, Z. 2016. Tipping,M.E.,andBishop,C.M. 1999. Probabilisticprin-\n",
      "Groundingtopicmodelswithknowledgebases. InProceed- cipal component analysis. Journal of the Royal Statistical\n",
      "ingsofthe25thInternationalJointConferenceonArtificial Society:SeriesB61(3):611–622.\n",
      "Intelligence,1578–1584.\n",
      "Towell, G. G., and Shavlik, J. W. 1994. Knowledge-\n",
      "Huang,J.;Zhang,T.;andMataxas,D. 2011. Learningwith basedartificialneuralnetworks. ArtificialIntelligence70(1-\n",
      "structuredsparsity. JournalofMachineLearningResearch 2):119–165.\n",
      "12:3371–3412.\n",
      "Varma,P.;He,B.;Bajaj,P.;Khandwala,N.;Banerjee,I.;Ru-\n",
      "Jacob,L.;Obozinski,G.;andVert,J.-P. 2009. Grouplasso bin,D.;andRe´,C. 2017. Generativemodelstructurewith\n",
      "with overlap and graph lasso. In Proceedings of the 26th staticanalysis. InAdvancesinNeuralInformationProcess-\n",
      "InternationalConferenceonMachineLearning,433–440. ingSystems,volume30,239–249.\n",
      "Kristiadi, A.; Khan, M. A.; Lukovnikov, D.; Lehmann, J.; Varol, A.; Salzmann, M.; Fua, P.; and Urtasun, R. 2012.\n",
      "andFischer,A. 2018. Incorporatingliteralsintoknowledge A constrained latent variable model. In Proceedings of\n",
      "graphembeddings. arXiv:1802.00934. the2012IEEEConferenceonComputerVisionandPattern\n",
      "Mairal,J.,andYu,B. 2013. Supervisedfeatureselectionin Recognition,2248–2255.\n",
      "graphswithpathcodingpenaltiesandnetworkflows. Jour- Wang, Q.; Mao, Z.; Wang, B.; and Guo, L. 2017. Knowl-\n",
      "nalofMachineLearningResearch14(1):2449–2485. edge graph embedding: A survey of approaches and appli-\n",
      "Mei,S.;Zhu,J.;andZhu,X.2014.RobustRegBayes:Selec- cations. IEEETransactionsonKnowledgeandDataEngi-\n",
      "tivelyincorporatingfirst-orderlogicdomainknowledgeinto neering29(12):2724–2743.\n",
      "Bayesian models. In Proceedings of the 31st International Xiao, H.; Huang, M.; Meng, L.; and Zhu, X. 2017. SSP:\n",
      "ConferenceonMachineLearning,number1,253–261. Semanticspaceprojectionforknowledgegraphembedding\n",
      "Nickel, M.; Murphy, K.; Tresp, V.; and Gabrilovich, E. withtextdescriptions.InProceedingsofthe31stAAAICon-\n",
      "2015. A review of relational machine learning for knowl- ferenceonArtificialIntelligence,3104–3110.\n",
      "edgegraphs. ProceedingsoftheIEEE104(1):11–33. Xie,R.;Liu,Z.;Jia,J.;Luan,H.;andSun,M. 2016. Repre-\n",
      "On˜oro-Rubio,D.;Niepert,M.;Garc´ıa-Dura´n,A.;Gonza´lez, sentationlearningofknowledgegraphswithentitydescrip-\n",
      "R.; and Lo´pez-Sastre, R. J. 2017. Representation learning tions. InProceedingsofthe30thAAAIConferenceonArti-\n",
      "forvisual-relationalknowledgegraphs. arXiv:1709.02314. ficialIntelligence,2659–2665.\n",
      "Pei, Y.; Chakraborty, N.; and Sycara, K. 2015. Nonneg- Yang,B.;Yih,W.;He,X.;Gao,J.;andDeng,L. 2015. Em-\n",
      "ative matrix tri-factorization with graph regularization for bedding entities and relations for learning and inference in\n",
      "communitydetectioninsocialnetworks. InProceedingsof knowledge bases. In Proceedings of the 3rd International\n",
      "the24thInternationalJointConferenceonArtificialIntelli- ConferenceonLearningRepresentations.\n",
      "gence,2083–2089.\n",
      "Yao,L.;Zhang,Y.;Wei,B.;Qian,H.;andWang,Y. 2015.\n",
      "Pezeshkpour,P.;Chen,L.;andSingh,S. 2017. Embedding Incorporatingprobabilisticknowledgeintotopicmodels. In\n",
      "multimodal relational data. Presented in the 6th Workshop Advances in Knowledge Discovery and Data Mining, vol-\n",
      "onAutomatedKnowledgeBaseConstruction. ume9078ofLectureNotesinComputerScience,586–597.\n",
      "Ratner,A.;Sa,C.D.;Wu,S.;Selsam,D.;andRe´,C. 2016. Yao, L.; Zhang, Y.; Wei, B.; Jin, Z.; Zhang, R.; Zhang, Y.;\n",
      "Dataprogramming:Creatinglargetrainingsets,quickly. In andChen,Q. 2017. Incorporatingknowledgegraphembed-\n",
      "Advances in Neural Information Processing Systems, vol- dingsintotopicmodeling. InProceedingsofthe31stAAAI\n",
      "ume29,3567–3575. ConferenceonArtificialIntelligence,3119–3126.\n",
      "Richardson,M.,andDomingos,P. 2006. Markovlogicnet- Zhang, F.; Yuan, N. J.; Lian, D.; Xie, X.; and Ma, W.-\n",
      "works. MachineLearning62(1-2):107–136. Y. 2016. Collaborative knowledge base embedding for\n",
      "Schiegg,M.;Neumann,M.;andKersting,K. 2012. Markov recommender systems. In Proceedings of the 22nd ACM\n",
      "logic mixtures of Gaussian processes: Towards machines SIGKDD International Conference on Knowledge Discov-\n",
      "reading regression data. In Proceedings of the 15th Inter- eryandDataMining,353–362.\n",
      "nationalConferenceonArtificialIntelligenceandStatistics, Zhu, J.; Chen, N.; and Xing, E. P. 2014. Bayesian infer-\n",
      "1002–1011. ence with posterior regularization and applications to infi-\n",
      "Shahid,N.;Perraudin,N.;Kalofolias,V.;Puy,G.;andVan- nite latent SVMs. Journal of Machine Learning Research\n",
      "dergheynst, P. 2016. Fast robust PCA on graphs. IEEE 15:1799–1847.\n",
      "JournalofSelectedTopicsinSignalProcessing10(4):740–\n",
      "756.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,    364,     50,   2053]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Andrzejewski, D.', 'Zhu, X.', 'Craven, M.', 'Recht, B.', 'Bach, S. H.', 'He, B.', 'Ratner, A.', 'Re´, C.', 'Bouchard, G.', 'Singh, S.', 'Trouillon, T.', 'Cai, H.', 'Zheng, V. W.', 'Chang, K.', 'De Raedt, L.', 'Kersting, K.', 'Natarajan, S.', 'Fan, M.', 'Zhou, Q.', 'Zheng, T. F.', 'Grishman, R.', 'Foulds, J.', 'Kumar, S. H.', 'Getoor, L.', 'Shang, F.', 'Jiao, L. C.', 'Wang, F.', 'Thoma, S.', 'Rettinger, A.', 'Both, F.', 'Fung, G. M.', 'Mangasarian, O. L.', 'Shavlik, J. W.', 'Hu, Z.', 'Luo, G.', 'Sachan, M.', 'Xing, E.', 'Nie, Z.', 'Tipping, M. E.', 'Bishop, C. M.', 'Towell, G. G.', 'Shavlik, J. W.', 'Jacob, L.', 'Obozinski, G.', 'Vert, J.-P.', 'Kristiadi, A.', 'Khan, M. A.', 'Lukovnikov, D.', 'Lehmann, J.', 'Varol, A.', 'Salzmann, M.', 'Fua, P.', 'Urtasun, R.', 'Mairal, J.', 'Yu, B.', 'Mei, S.', 'Zhu, J.', 'Zhu, X.', 'On˜oro-Rubio, D.', 'Niepert, M.', 'Garc´ıa-Dura´n, A.', 'Gonzalez, R.', 'Lo´pez-Sastre, R. J.', 'Pei, Y.', 'Chakraborty, N.', 'Sycara, K.', 'Pezeshkpour, P.', 'Chen, L.', 'Singh, S.', 'Ratner, A.', 'Sa, C. D.', 'Wu, S.', 'Sels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Embedding Models for Episodic Knowledge Graphs\n",
      "Yunpu Maa,b, Volker Trespa,b, Erik A. Daxberger 1c\n",
      "aSiemens AG, Corporate Technology, Munich, Germany\n",
      "bLudwig Maximilian University of Munich, Munich, Germany\n",
      "cETH Zurich\n",
      "Abstract\n",
      "In recent years a number of large-scale triple-oriented knowledge graphs have\n",
      "been generated and various models have been proposed to perform learning in\n",
      "those graphs. Most knowledge graphs are static and reflect the world in its\n",
      "current state. In reality, of course, the state of the world is changing: a healthy\n",
      "personbecomesdiagnosedwithadiseaseandanewpresidentisinaugurated. In\n",
      "thispaper,weextendmodelsforstaticknowledgegraphstotemporalknowledge\n",
      "graphs. This enables us to store episodic data and to generalize to new facts\n",
      "(inductive learning). We generalize leading learning models for static knowl-\n",
      "edge graphs (i.e., Tucker, RESCAL, HolE, ComplEx, DistMult) to temporal\n",
      "knowledge graphs. In particular, we introduce a new tensor model, ConT,\n",
      "with superior generalization performance. The performances of all proposed\n",
      "models are analyzed on two different datasets: the Global Database of Events,\n",
      "Language, and Tone (GDELT) and the database for Integrated Conflict Early\n",
      "Warning System (ICEWS). We argue that temporal knowledge graph embed-\n",
      "dings might be models also for cognitive episodic memory (facts we remember\n",
      "and can recollect) and that a semantic memory (current facts we know) can be\n",
      "generated from episodic memory by a marginalization operation. We validate\n",
      "this episodic-to-semantic projection hypothesis with the ICEWS dataset.\n",
      "Keywords: knowledge graph, temporal knowledge graph, semantic memory,\n",
      "episodic memory, tensor models\n",
      "1WorkdonewhileatSiemensAG.\n",
      "Preprint submitted to Journal of Web Semantics December 5, 2018\n",
      "8102\n",
      "ceD\n",
      "3\n",
      "]IA.sc[\n",
      "2v82200.7081:viXra\n",
      "1. Introduction\n",
      "In recent years a number of sizable Knowledge Graphs (KGs) have been\n",
      "developed, the largest ones containing more than 100 billion facts. Well known\n",
      "examples are DBpedia [1],YAGO [2], Freebase [3], Wikidata [4] and the Google\n",
      "KG [5]. Practical issues with completeness, quality and maintenance have been\n",
      "solved to a degree that some of these knowledge graphs support search, text\n",
      "understandingandquestionansweringinlarge-scalecommercialsystems[5]. In\n",
      "addition, statistical embedding models have been developed that can be used\n",
      "tocompressaknowledgegraph, toderiveimplicitfacts, todetecterrors, andto\n",
      "support the above mentioned applications. A recent survey on KG models can\n",
      "be found in [6].\n",
      "Mostknowledgegraphsarestaticandreflecttheworldatitscurrentstate. In\n",
      "reality, of course, the state of the world is changing: a healthy person becomes\n",
      "diagnosed with a disease and a new president is inaugurated. In this paper,\n",
      "we extend semantic knowledge graph embedding models to episodic/temporal\n",
      "knowledge graphs as an efficient way to store episodic data and to be able to\n",
      "generalize to new facts (inductive learning). In particular, we generalize lead-\n",
      "ing approaches for static knowledge graphs (i.e., constrained Tucker, DistMult,\n",
      "RESCAL, HolE, ComplEx) to temporal knowledge graphs. We test these mod-\n",
      "elsusingtwotemporalKGs. ThefirstoneisderivedfromtheIntegratedConflict\n",
      "Early Warning System (ICEWS) data set which describes interactions between\n",
      "nationsoverseveralyears. ThesecondoneisderivedfromtheGlobalDatabase\n",
      "ofEvents,LanguageandTone(GDELT)that,formorethan30years,monitors\n",
      "news media from all over the world. In the experiments, we analyze the gener-\n",
      "alization abilities to new facts that might be missing in the temporal KGs and\n",
      "also analyze to what degree a factorized KG can serve as an explicit memory.\n",
      "Weproposethatourtechnicalmodelsmightberelatedtothebrain’sexplicit\n",
      "memorysystems,i.e.,itsepisodicanditssemanticmemory. Bothareconsidered\n",
      "long-term memories and store information potentially over the life-time of an\n",
      "individual [7, 8, 9, 7]. The semantic memory stores general factual knowledge,\n",
      "2\n",
      "i.e., information we know, independent of the context where this knowledge\n",
      "was acquired and would be related to a static KG. Episodic memory concerns\n",
      "informationweremember andincludesthespatiotemporalcontextofevents[10]\n",
      "and would correspond to a temporal KG.\n",
      "primeMinisterOf\n",
      "10.1995 - 04.2002\n",
      "memberOf\n",
      "secretaryGeneralOf\n",
      "1955-m noem wberOf secretar 0y 1.G 0e 1.n 2er 0a 1l\n",
      "7-Of\n",
      "now\n",
      "memberOf secretaryGeneralOf mem 1b 9er 91O -f now 01.01.20se 0c 7-re 3t 1a.1ry\n",
      "2G.2e 0n 1e 6ralOf\n",
      "foreignMinisterOf\n",
      "01.2004 - 11.2006\n",
      "Figure1: Illustrationsof(left)asemanticknowledgegraphand(right)anepisodicknowledge\n",
      "graph. (Left)Everyarrowrepresentsa(subject,predicate,object)triple,withtheannotation\n",
      "of the arrow denoting the respective predicate. The triple (Ban Ki-moon, SecretaryOf, UN)\n",
      "is deleted, since the knowledge graph has been updated with the triple (Ant´onio Guterres,\n",
      "SecretaryOf, UN). (Right) Every arrow represents a (subject, predicate, object, timestamp)\n",
      "quadruple, where the arrow is both annotated with the respective predicate and timestamp.\n",
      "Here the quadruple involving is not deleted, since the attached timestamp reveals that the\n",
      "relationshipisnotvalidatpresent.\n",
      "An interesting question is how episodic and semantic memories are related.\n",
      "There is evidence that these main cognitive categories are partially dissociated\n",
      "from one another in the brain, as expressed in their differential sensitivity to\n",
      "brain damage. However, there is also evidence indicating that the different\n",
      "memory functions are not mutually independent and support one another [11].\n",
      "We propose that semantic memory can be derived from episodic memory by\n",
      "marginalization. Hereby we also consider that many episodes describe starting\n",
      "and endpoints of state changes. For example, an individual might become sick\n",
      "3\n",
      "with a disease, which eventually is cured. Similarly, a president’s tenure even-\n",
      "tuallyends. WestudyourhypothesisontheIntegratedConflictEarlyWarning\n",
      "System(ICEWS)dataset,whichcontainsmanyeventswithstartandenddates.\n",
      "Figure 1 compares semantic and episodic knowledge graphs. Furthermore, Fig-\n",
      "ure 2 illustrates the main ideas of building and modeling semantic and episodic\n",
      "knowledge graphs.\n",
      "Modeling\n",
      "Tensorization\n",
      "Graph Construction\n",
      "assigns entries as nodes and\n",
      "predicates as labeled edges\n",
      "for each timestamp\n",
      "Data Accumulation (e.g. GDELT)\n",
      "through knowledge extraction\n",
      "Websites Newspapers Social Media\n",
      "Figure 2: Illustration of the main idea behind the models presented in this paper. Step\n",
      "1: Knowledge is extracted from unstructured data, such as websites, newspapers or social\n",
      "media. Step 2: The knowledge graph is constructed, where entities are assigned as nodes,\n",
      "and predicates as labeled edges; note that there is a labeled edge for each timestamp. Step\n",
      "3: The knowledge graph is represented as a tensor; for semantic KGs, we obtain a 3-way\n",
      "tensor, storing (subject, predicate, object) triples, and for episodic KGs, we obtain a 4-way\n",
      "tensor, storing (subject, predicate, object, timestamp) quadruples. Step 4: The semantic\n",
      "and episodic tensors are decomposed and modeled via compositional or tensor models (see\n",
      "Section2).\n",
      "The paper is organized as follows. Section 2 introduces knowledge graphs,\n",
      "4\n",
      "the mapping of a knowledge graph to an adjacency tensor, and the statistical\n",
      "embedding models for knowledge graphs. We also describe how popular em-\n",
      "bedding models for KGs can be extended to episodic KGs. Section 3 shows\n",
      "experimental results on modelling episodic KGs. Finally, we present experi-\n",
      "ments on the possible relationships between episodic and semantic memory in\n",
      "Section 4.\n",
      "2. Model Descriptions\n",
      "A static or semantic knowledge graph (KG) is a triple-oriented knowledge\n",
      "representation. Here we consider a slight extension to the subject-predicate-\n",
      "object triple form by adding the value in the form (e,e,e ; Value), where\n",
      "s p o\n",
      "Value is a function of e,e,e and, e.g., can be a Boolean variable (True for 1,\n",
      "s p o\n",
      "False for 0) or a real number. Thus (Jack, likes, Mary; True) states that Jack\n",
      "(the subject or head entity) likes Mary (the object or tail entity). Note that e\n",
      "s\n",
      "ande representtheentitiesforsubjectindexsandobjectindexo. Tosimplify\n",
      "o\n",
      "notationwealsoconsidere tobeageneralizedentityassociatedwithpredicate\n",
      "p\n",
      "typewithindex p. For the episodic KGswe introduce e, whichisageneralized\n",
      "t\n",
      "entity for time t.\n",
      "TomodelastaticKG,weintroducethethree-waysemanticadjacencytensor\n",
      "χwherethetensorelementx istheassociatedValue ofthetriple(e,e,e ).\n",
      "s,p,o s p o\n",
      "One can also define a companion tensor Θ with the same dimensions as χ and\n",
      "χ\n",
      "with entries θ. Thus, the probabilistic model for the semantic tensor χ is\n",
      "s,p,o\n",
      "defined as P(x θ )=σ(θ ), where σ(x)=1/(1+exp( x)). Similarly,\n",
      "s,p,o s,p,o s,p,o\n",
      "| −\n",
      "the four-way temporal or episodic tensor has elements x which are the\n",
      "t,s,p,o\n",
      "E\n",
      "associated values of the quadruples (e,e,e,e ), with t=1,...,T. Therefore,\n",
      "t s p o\n",
      "the probabilistic model for episodic tensor is defined with the corresponding\n",
      "companion tensor Θ as\n",
      "E\n",
      "P(x θ )=σ(θ ). (1)\n",
      "t,s,p,o t,s,p,o t,s,p,o\n",
      "|\n",
      "We assume that each entity e has a unique latent representation a. In particu-\n",
      "lar,theembeddingapproachusedformodelingsemanticandepisodicknowledge\n",
      "5\n",
      "graphsassumesthatθsem =fsem(a,a,a ),andθepi =fepi(a,a,a,a ),\n",
      "s,p,o es ep eo t,s,p,o et es ep eo\n",
      "respectively. Here,theindicatorfunctionfsem/epi()isafunctiontobelearned.\n",
      "·\n",
      "Given a labeled dataset = (x,y ) m, latent representations and other\n",
      "D { i i }i=1\n",
      "parameters (denoted as ) are learned by minimizing the regularized logistic\n",
      "P\n",
      "loss\n",
      "m\n",
      "(cid:88)\n",
      "min log(1+exp( y θsem/epi))+λ 2. (2)\n",
      "P − i i ||P||2\n",
      "i=1\n",
      "In general, most KGs only contain positive triples; non-existing triples are nor-\n",
      "mally used as negative examples sampled with local closed- world assumption.\n",
      "Alternatively, we can minimize a margin-based ranking loss over the dataset\n",
      "such as\n",
      "(cid:88) (cid:88)\n",
      "min max(0,γ+σ(θsem/epi) σ(θsem/epi)), (3)\n",
      "P j − i\n",
      "i∈D+j∈D−\n",
      "where γ is the margin parameter, and and denote the set of positive\n",
      "+ −\n",
      "D D\n",
      "and negative samples, respectively.\n",
      "Therearedifferentwaysformodelingtheindicatorfunctionfepi()orfsem().\n",
      "· ·\n",
      "In this paper, we will only investigate multilinear models derived from tensor\n",
      "decompositions and compositional operations. We now describe the models in\n",
      "detail. Graphical illustrations of the described models are shown in Figure 3.\n",
      "p p\n",
      "p\n",
      "diag\n",
      "Re\n",
      "Im p\n",
      "Gp\n",
      "s o s o\n",
      "G G1 G2\n",
      "Gp Gt\n",
      "t t s p o s t o t s o t\n",
      "(a) (b) (c) (d) (e)\n",
      "Figure 3: Illustrations of (a) episodic Tucker, (b) episodic ComplEx (where • denotes con-\n",
      "traction),(c)RESCAL,(d)ConTand(e)Tree. Eachentityinthefigureisrepresentedasa\n",
      "circlewithtwoedges,sincetherepresentationforanentityeisae,i. Inaddition,Grepresents\n",
      "thecoretensorinTucker,Gp representsthematrixlatentrepresentationofpredicatepinthe\n",
      "RESCAL and Tree models, Gt represents the three-dimensional tensor latent representation\n",
      "oftimestamptintheConTmodel.\n",
      "Table 1 and Table 2 summarize notations used throughout this paper for\n",
      "6\n",
      "easyreference,whileTable3summarizesthenumberofparametersrequiredfor\n",
      "each model.2\n",
      "Table1: Summaryofthegeneralnotations.\n",
      "General\n",
      "Symbol Meaning\n",
      "e Entity for subject index s\n",
      "s\n",
      "e Entity for object index o\n",
      "o\n",
      "e Generalized entity for predicate index p\n",
      "p\n",
      "e Generalized entity for time index t\n",
      "t\n",
      "a Latent representation of entity e\n",
      "ei i\n",
      "a(e ) Latent representation of starting timestamp\n",
      "tstart\n",
      "a r -th element of a\n",
      "ei,ri i ei\n",
      "r˜ Rank/Dimensionality of a for i s,p,o\n",
      "ei\n",
      "∈{ }\n",
      "r˜ Rank/Dimensionality of a\n",
      "t et\n",
      "N Number of entities / predicates / timestamps\n",
      "e/p/t\n",
      "Tucker. First, we consider the Tucker model for semantic tensor decom-\n",
      "position of the form θsem = (cid:80)r˜ a a a gsem(r,r,r ). Here,\n",
      "s,p,o r1,r2,r3=1 es,r1 ep,r2 eo,r3 1 2 3\n",
      "gsem(r,r,r ) R are elements of the core tensor sem Rr˜×r˜×r˜. Similarly,\n",
      "1 2 3\n",
      "∈ G ∈\n",
      "the indicator function of a four-way Tucker model for episodic tensor decompo-\n",
      "sition is of the form\n",
      "(cid:88)r˜t (cid:88)r˜\n",
      "θepi =\n",
      "t,s,p,o\n",
      "r1=1r2,r3,r4=1\n",
      "a a a a gepi(r,r,r,r ), (4)\n",
      "et,r1 es,r2 ep,r3 eo,r4 1 2 3 4\n",
      "with a four dimensional core tensor epi Rr˜t×r˜×r˜×r˜. Note that this is a con-\n",
      "G ∈\n",
      "2For DistMult, ComplEx, and HolE it is required that r˜= r˜t. In our experiments (see\n",
      "Sections 3 and 4), in order to enable a fair comparison between the different models, we\n",
      "assume that the latent representations of entities, predicates, and time indices all have the\n",
      "samerank/dimensionality.\n",
      "7\n",
      "Table2: Summaryofthenotationsforsemanticandepisodicknowledgegraphs.\n",
      "Semantic knowledge graphs Episodic knowledge graphs\n",
      "Symbol Meaning Symbol Meaning\n",
      "χ Sem. adjacency tensor Epi. adjacency tensor\n",
      "E\n",
      "Θ Companion tensor of χ Θ Companion tensor of\n",
      "χ E\n",
      "E\n",
      "x Value of (e, e, e ) x Value of (e, e, e, e )\n",
      "s,p,o s p o t,s,p,o t s p o\n",
      "θsem Logit of (e, e, e ) θepi Logit of (e, e, e, e )\n",
      "s,p,o s p o t,s,p,o t s p o\n",
      "fsem() Sem. indicator function fepi() Epi. indicator function\n",
      "· ·\n",
      "sem Sem. core tensor epi Epi. core tensor\n",
      "G G\n",
      "gsem() Element of sem gepi() Element of epi\n",
      "· G · G\n",
      "straintTuckermodel,since,asinRESCAL,entitieshaveuniquerepresentations,\n",
      "independent of the roles as subject or object.\n",
      "RESCAL. Another model closely related to the semantic Tucker tensor\n",
      "decomposition is the RESCAL model, which has shown excellent performance\n",
      "in modelling KGs [12]. In RESCAL, subjects and objects have vector latent\n",
      "representations, while predicates have matrix latent representations. The indi-\n",
      "cator function of RESCAL for modeling semantic KGs takes the form θsem =\n",
      "s,p,o\n",
      "(cid:80)r˜\n",
      "a g (r,r )a,whereg (r,r )representsthematrixlatentrep-\n",
      "r1,r2=1 es,r1 p 1 2 eo,r2 p 1 2\n",
      "resentation for the predicate e. Then next two models, Tree and ConT, are\n",
      "p\n",
      "novel generalizations of RESCAL to episodic tensors.\n",
      "Tree. Fromapracticalperspective,traininganepisodicTuckertensormodel\n",
      "is very expensive since the computational complexity is approximately r˜4. Ten-\n",
      "sor networks provide a general and flexible framework to design nonstandard\n",
      "tensor decompositions [13, 14]. One of the simplest tensor networks is a tree\n",
      "tensordecomposition( )oftheepisodicindicatorfunction,whichisillustrated\n",
      "T\n",
      "in compositional operations. We now describe the models in detail. Graphical\n",
      "illustrations of the described models are shown in Figure 3(e). Therefore, we\n",
      "proposeatreetensordecomposition( )oftheepisodicindicatorfunction. The\n",
      "T\n",
      "tree is partitioned into two subtrees and, wherein subject e and time\n",
      "1 2 s\n",
      "T T T\n",
      "8\n",
      "e reside in, while object e and an auxiliary time e reside in. and\n",
      "t 1 o t 2 1 2\n",
      "T T T T\n",
      "are connected with e through two core tensors and. Thus, the indicator\n",
      "p 1 2\n",
      "G G\n",
      "function can be written as\n",
      "(cid:88)r˜t (cid:88)r˜\n",
      "θepi =\n",
      "t,s,p,o\n",
      "r1,r6=1r2,r3,r4,r5=1\n",
      "a a g (r,r,r )g (r,r )g (r,r,r )a a. (5)\n",
      "et,r1 es,r2 1 1 2 3 p 3 4 2 4 5 6 eo,r5 et,r6\n",
      "Within,wereducethefour-waycoretensorinTuckerintotwothree-dimensional\n",
      "T\n",
      "tensors and, so that the computational complexity of is approximately\n",
      "1 2\n",
      "G G T\n",
      "r˜3.\n",
      "ConT. ConT is another generalization of the RESCAL model to episodic\n",
      "tensors with reduced computational complexity of approximately r˜3. The idea\n",
      "is that another way of reducing the complexity is by contracting indices of the\n",
      "core tensor. Therefore, we contract the from Tucker with the time index\n",
      "G\n",
      "givingathree-waycoretensor foreachtimeinstance. Theindicatorfunction\n",
      "t\n",
      "G\n",
      "takes the form\n",
      "r˜\n",
      "(cid:88)\n",
      "θepi = a a a g (r,r,r ). (6)\n",
      "t,s,p,o es,r1 ep,r2 eo,r3 t 1 2 3\n",
      "r1,r2,r3=1\n",
      "In this model, the tensor resembles the relation-specific matrix from\n",
      "t p\n",
      "G G\n",
      "RESCAL.Later,wewillseethatConTisasuperiormodelformodelingepisodic\n",
      "knowledge graphs due to the representational flexibility of its high-dimensional\n",
      "tensor for the time index.\n",
      "t\n",
      "G\n",
      "Even though the complexity of Tree and ConT is reduced as compared to\n",
      "episodicTucker,thethree-dimensionalcoretensormightcauserapidoverfitting\n",
      "during training. Therefore, we next propose episodic generalization of compo-\n",
      "sitional models, such as DistMult [15], HolE [16] and ComplEx [17]. For those\n",
      "models, the number of parameters only increases linearly with the rank.\n",
      "DistMult. DistMult[15]isasimplegeneralizationoftheCPmodel,byen-\n",
      "forcingtheconstraintthatentitiesshouldhaveuniquerepresentations. Episodic\n",
      "DistMult takes the form θepi = (cid:80)r˜ λ a a a a. Here, we require\n",
      "t,s,p,o i=1 i et,i es,i ep,i eo,i\n",
      "that vector latent representations of entities, predicates, and timestamps have\n",
      "9\n",
      "the same rank. DistMult is a special case of Tucker having a core tensor with\n",
      "only diagonal elements λ.\n",
      "i\n",
      "HolE. Holographic embedding (HolE) [16] is a state-of-art link prediction\n",
      "andknowledgegraphcompletionmethod,whichisinspiredbyholographicmod-\n",
      "els of associative memory.\n",
      "HolE uses circular correlation to generate a compositional representation\n",
      "frominputse ande. TheindicatorofHolEreadsθsem =a (a (cid:63)a ),where\n",
      "s o s,p,o ep· es eo\n",
      "(cid:63):Rd Rd Rd denotesthecircularcorrelation[a(cid:63)b] =(cid:80)d−1a b.\n",
      "× → k i=0 i (k+i)modd\n",
      "We define the episodic extension of HolE as\n",
      "θepi =a (cid:0) a (cid:63)(a (cid:63)a )(cid:1). (7)\n",
      "t,s,p,o et · ep es eo\n",
      "As argued by [16], HolE employs a holographic reduced representation [18]\n",
      "to store and retrieve the predicates from e and e. Analogously, episodic HolE\n",
      "s o\n",
      "should be able to retrieve the stored timestamps from e, e and e. In the se-\n",
      "p s o\n",
      "manticcase,e canberetrievedifexistingtriplerelationsarestoredviacircular\n",
      "p\n",
      "(cid:80)\n",
      "convolution,andsuperpositionintherepresentationa = a a,\n",
      "∗ eo (s,p)∈So ep∗ es\n",
      "where is the set of all true triples given e. This is based on the fact that\n",
      "o o\n",
      "S\n",
      "a(cid:63)a δ [16]. Analogously, the stored timestamp e for an event can be re-\n",
      "t\n",
      "≈\n",
      "trieved if all existing episodic events are stored via, and superposition in the\n",
      "∗\n",
      "(cid:80)\n",
      "representation of e, a = a (a a ), where is the set of\n",
      "o eo (t,s,p)∈So et ∗ ep ∗ es So\n",
      "all true quadruples (t,s,p,o) given e. However, high order circular correla-\n",
      "o\n",
      "tion/convolution will increase the inaccuracy of retrieval. Another motivation\n",
      "forourepisodicextension(7)isthatacompositionaloperatoroftheforma f˜\n",
      "et·\n",
      "allows a projection from episodic memory to semantic memory, to be detailed\n",
      "later.\n",
      "ComplEx. Complex embedding (ComplEx) [17] is another state-of-art\n",
      "method closely related to HolE. It can accurately describe both symmetric and\n",
      "antisymmetric relations. HolE is a special case of ComplEx with imposed con-\n",
      "jugatesymmetryonembeddings[19]. Thus,ComplExhasmoredegreesoffree-\n",
      "dom, if compared to HolE. For the semantic complex embedding, the indicator\n",
      "(cid:16) (cid:17)\n",
      "function is θsem = Re (cid:80)r˜a a,a¯ with complex valued a and where\n",
      "s,p,o i es,i ep,i eo,i\n",
      "10\n",
      "the bar indicates the complex conjugate. To be consistent with the episodic\n",
      "HolE, the episodic complex embedding is defined as3\n",
      "(cid:32) (cid:33)\n",
      "r˜\n",
      "(cid:88)\n",
      "θepi =Re a a a,a¯. (8)\n",
      "t,s,p,o et,i es,i ep,i eo,i\n",
      "i\n",
      "3. Experiments on Episodic Models\n",
      "We investigate the proposed tensor and compositional models with experi-\n",
      "ments which are evaluated on two datasets:\n",
      "ICEWS. The Integrated Conflict Early Warning System (ICEWS) dataset\n",
      "[20]isanaturalepisodicdatasetrecordingdyadiceventsbetweendifferentcoun-\n",
      "tries. An example entry could be (Turkey, Syria, Fight, 12/25/2014). These\n",
      "dyadiceventsareaggregatedintoafour-waytensor with258entities, 20rela-\n",
      "E\n",
      "tiontypes,and72timestamps,whichhasintotal320,118positive(e,e,e,e )\n",
      "t s p o\n",
      "quadruples4. Thisdatasetwasfirstcreatedandusedin[21]. FromthisICEWS\n",
      "dataset, a semantic tensor is generated by extracting consecutive events that\n",
      "last until the last timestamp, constituting the current 5 semantic facts of the\n",
      "world.\n",
      "GDELT. The Global Database of Events, Language and Tone (GDELT)\n",
      "[20] monitors the world’s news media in broadcast, print and web formats from\n",
      "all over the world, daily since January 1, 1979 6. We use GDELT as a large\n",
      "episodic dataset. For our experiments, GDELT data is collected from January\n",
      "1, 2012 to December 31, 2012 (with a temporal granularity of 24 hrs). These\n",
      "events are aggregated into an episodic tensor with 1100 entities, 180 relation\n",
      "E\n",
      "3One can show that Eq. (7) is equivalent to Eq. (8) by converting it to the frequency\n",
      "domain[19]. Then,θ te,p si\n",
      ",p,o\n",
      "∝ωT et(ω¯ep(cid:12)ω¯es(cid:12)ωeo),whereω=F(a)∈Cr˜ arethediscrete\n",
      "Fouriertransformsofembeddingsa,andusingthefactthatωisconjugatesymmetricforreal\n",
      "vectora.\n",
      "4Notethatforanepisodiceventthedatasetcontainsallthequadruples(eti,es,ep,e0)for\n",
      "ti∈{tstart,tstart+1,···,t end−1,t end}.\n",
      "5Current alwaysindicatesthelasttimestamp/timestampsoftheappliedepisodicKGs.\n",
      "6https://www.gdeltproject.org/about.html\n",
      "11\n",
      "Table3: Numberofparametersfordifferentmodelsandtheruntimeofonetrainingepochon\n",
      "theGDELTdataset.\n",
      "Runtime\n",
      "Model Semantic Episodic Complexity rank40 rank60 rank150\n",
      "DistMult (Ne+Np+1)r˜ (Ne+Np+Nt+1)r˜ O(r˜) 35.2s 36.4s 53.7s\n",
      "HolE (Ne+Np)r˜ (Ne+Np)r˜ O(r˜logr˜) 42.8s 43.2s 59.0s\n",
      "ComplEx 2(Ne+Np)r˜ 2(Ne+Np+Nt)r˜ O(r˜) 40.1s 42.4s 57.5s\n",
      "Tree − Ner˜+Npr˜2+(Nt+2r˜2)r˜t O(r˜3) 133.6s 160.2s −\n",
      "ConT − (Ne+Np)r˜+Ntr˜3 O(r˜3) 95.4s 226.1s −\n",
      "Tucker (Ne+Np)r˜+r˜3 (Ne+Np)r˜+(Nt+r˜3)r˜t O(r˜4) 144.2s 387.9s −\n",
      "types, and 366 timestamps, which has in total 2,563,561 positive (e,e,e,e )\n",
      "t s p o\n",
      "quadruples.\n",
      "We assess the quality of episodic information retrieval on both datasets for\n",
      "the proposed tensor and compositional models. Since both episodic datasets\n",
      "only consist of positive quadruples, we generated negative episodic instances\n",
      "followingtheprotocolofcorruptingsemantictriplesgivenbyBordes[22]: nega-\n",
      "tiveinstancesofanepisodicquadruple(e,e,e,e )aredrawnbycorruptingthe\n",
      "s p o t\n",
      "objecte o toe o(cid:48) orthetimestampe t toe t(cid:48),meaningthat(e s,e p,e o(cid:48),e t)servesas\n",
      "a negative evidence of the episodic event at time instance e t, and (e s,e p,e o,e t(cid:48))\n",
      "is a true fact which cannot be correctly recalled at time instance e t(cid:48). During\n",
      "training, for each positive sample in a batch we assigned two negative samples\n",
      "with corrupted object or corrupted subject.\n",
      "The model performance is evaluated using the following scores. To retrieve\n",
      "the occurrence time, for each true quadruple, we replace the time index e with\n",
      "t\n",
      "every other possible time index e t(cid:48), compute the value of the indicator function\n",
      "θepi, and rank them in a decreasing order. We filter the ranking as in [22]\n",
      "t(cid:48),s,p,o\n",
      "by removing all quadruples where x t(cid:48),s,p,o =1 and t=t(cid:48), in order to eliminate\n",
      "(cid:54)\n",
      "ambiguity during episodic information retrieval. Similarly, we evaluated the\n",
      "retrieval of the predicate between a given subject and object at a certain time\n",
      "instancebycomputingandrankingtheindicatorθepi. Wealsoevaluatedthe\n",
      "t,s,p(cid:48),o\n",
      "12\n",
      "retrieval of entities by ranking and averaging the filtered indicators θ t,s(cid:48),p,o and\n",
      "θ t,s,p,o(cid:48). Tomeasurethegeneralizationabilityofthemodels,wereportdifferent\n",
      "measures of the ranking: mean reciprocal rank (MRR), and Hits@n on the test\n",
      "dataset.\n",
      "The datasets were split into train, validation, and test sets that contain the\n",
      "most frequently appearing entities in the episodic knowledge graphs. Training\n",
      "was performed by minimizing the logistic loss (2), and was terminated using\n",
      "early stopping on the validation dataset by monitoring the filtered MRR recall\n",
      "scores every 50,100 epochs depending on the models, where the maximum\n",
      "{ }\n",
      "training duration was 500 epochs. This ensures that the generalization ability\n",
      "ofuniquelatentrepresentationsofentitiesdoesn’tsufferfromoverfitting. Before\n",
      "training,allmodelparametersareinitializedusingXavierinitialization[23]. We\n",
      "alsoapplyanl2normpenaltyonallparametersforregularizationpurposes(see\n",
      "Eq. (2)).\n",
      "In Table 3 we summarize the runtime for one training epoch on the GDELT\n",
      "dataset for different models at ranks r˜ = r˜ 40,60,150. All experiments\n",
      "t\n",
      "∈ { }\n",
      "were performed on a single Tesla K80 GPU. In the following experiments, for\n",
      "compositional models we search rank in 100,150, while for tensor models we\n",
      "{ }\n",
      "search optimal rank in 40,50,60 since larger ranks could lead to overfitting\n",
      "{ }\n",
      "rapidly. Loss function is minimized with Adam method [24] with the learning\n",
      "rate selected from 0.001,1e 4,5e 5.\n",
      "{ − − }\n",
      "We first assess the filtered MRR, Hits@1, Hits@3, and Hits@10 scores of\n",
      "inferring missing entities and predicates on the GDELT test dataset. Table 4\n",
      "summarizes the results. Generalizations on the test dataset indicate the induc-\n",
      "tive reasoning capability of the proposed models. This generalization can be\n",
      "useful for the completion of evolving KGs with missing records, such as clinical\n",
      "datasets. Itcanbeseenthattensormodelsareabletooutperformcompositional\n",
      "modelsconsistentlyonbothentityandpredicatepredictiontasks. ConThasthe\n",
      "best inference results on the entity-related tasks, while Tucker performs better\n",
      "on the predicate-related tasks. The superior Hits@1 result of ConT on the en-\n",
      "titypredictionindicatesthatthereareeasilytobefittedentitiesintheGDELT\n",
      "13\n",
      "Table 4: Filtered results of inferring missing entities and predicates of episodic quadruples\n",
      "evaluatedontheGDELTdataset.\n",
      "Entity Predicate\n",
      "Method MRR @1 @3 @10 MRR @1 @3 @10\n",
      "DistMult 0.182 6.55 19.77 43.70 0.269 12.65 30.29 59.40\n",
      "HolE 0.177 6.67 18.95 41.84 0.256 11.81 28.35 57.73\n",
      "ComplEx 0.172 6.54 17.52 41.56 0.255 12.05 27.75 56.60\n",
      "Tree 0.196 8.17 21.00 44.65 0.274 13.30 30.66 60.05\n",
      "Tucker<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  28504,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['ICEWS', 'GDELT']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: intheGDELT\n",
      "13\n",
      "Table 4: Filtered results of inferring missing entities and predicates of episodic quadruples\n",
      "evaluatedontheGDELTdataset.\n",
      "Entity Predicate\n",
      "Method MRR @1 @3 @10 MRR @1 @3 @10\n",
      "DistMult 0.182 6.55 19.77 43.70 0.269 12.65 30.29 59.40\n",
      "HolE 0.177 6.67 18.95 41.84 0.256 11.81 28.35 57.73\n",
      "ComplEx 0.172 6.54 17.52 41.56 0.255 12.05 27.75 56.60\n",
      "Tree 0.196 8.17 21.00 44.65 0.274 13.30 30.66 60.05\n",
      "Tucker 0.204 8.93 21.85 46.35 0.275 12.69 31.35 60.70\n",
      "ConT 0.233 13.85 24.65 42.96 0.263 12.83 29.27 57.30\n",
      "Table 5: Filtered results for entities and predicates recollection/prediction evaluated on the\n",
      "ICEWSdataset.\n",
      "Entity Predicate\n",
      "Method MRR @1 @3 @10 MRR @1 @3 @10\n",
      "DistMult 0.222 9.72 22.48 52.32 0.520 33.73 62.25 91.13\n",
      "HolE 0.229 9.85 23.49 54.21 0.517 31.55 65.47 93.59\n",
      "ComplEx 0.229 8.94 23.53 57.72 0.506 30.99 61.46 93.44\n",
      "Tree 0.205 10.48 19.84 42.81 0.554 36.62 67.25 94.70\n",
      "Tucker 0.257 12.88 27.10 54.43 0.563 36.96 69.55 95.43\n",
      "ConT 0.264 15.71 29.60 46.67 0.557 38.12 67.76 87.71\n",
      "dataset along the timestamps. In fact, the GDELT dataset is unbalanced, and\n",
      "episodic quadruples related to certain entities dominate in the episodic Knowl-\n",
      "edgegraph,suchasquadruplescontainingtheentitiesUSA,orUN.Experiment\n",
      "results on balanced and extremely sparse episodic dataset will be reported in\n",
      "the following.\n",
      "Next,Table5showstheMRR,Hits@1,Hits@3,andHits@10scoresofinfer-\n",
      "ring missing entities and predicates on the ICEWS test dataset. Similarly, we\n",
      "can read that tensor models outperform compositional models on both missing\n",
      "entityandpredicateinferencetasks. ThesuperiorHits@1resultofConTforthe\n",
      "14\n",
      "missingentitypredictionindicatesagainthattheICEWSdatasetisunbalanced,\n",
      "and episodic quadruples related to certain entities dominate.\n",
      "Table 6: Filtered recall scores for entities and timestamps recollection on the ICEWS (rare)\n",
      "trainingdataset.\n",
      "Timestamp Entity\n",
      "Method Rank MRR @3 MRR @3\n",
      "DistMult 200 0.257 27.0 0.211 21.9\n",
      "HolE 200 0.216 20.8 0.179 16.3\n",
      "ComplEx 200 0.354 40.3 0.301 33.2\n",
      "Tree 40 0.421 55.3 0.314 35.7\n",
      "Tucker 40 0.923 98.9 0.893 97.1\n",
      "ConT 40 0.982 99.7 0.950 97.9\n",
      "The recollection of the exact occurrence time of a significant past event\n",
      "(e.g. unusual, novel, attached with emotion) is also an important capability\n",
      "of episodic cognitive memory function. In order to manifest this perspective\n",
      "of proposed models, Table 6 shows the filtered MRR, and Hits@3 scores for\n",
      "the timestamps and entities recollection on the episodic ICEWS (rare) training\n",
      "dataset, where rank column registers the optimal and minimum rank r˜ = r˜\n",
      "t\n",
      "havingtheoutstandingrecallscores. Figure4furtherdisplaysthefilteredMRR\n",
      "score as a function of rank. Unlike the original ICEWS, which contains many\n",
      "consecutive events that last from the first to the last timestamp leading to\n",
      "unreasonably high filtered timestamp recall scores, this ICEWS (rare) dataset\n",
      "consists of rare temporal events that happen less than three times throughout\n",
      "the whole time and starting points of events.\n",
      "The outstanding performance of ConT compared with other compositional\n",
      "models indicates the importance of large dimensionality of time latent repre-\n",
      "sentation for the episodic tensor reconstruction / episodic memory recollection.\n",
      "Recall that for ConT the real dimension of the latent representation of time is\n",
      "actuallyr˜3 afterflattening. Thisflexiblelatentrepresentationfortimecould\n",
      "t\n",
      "G\n",
      "15\n",
      "compress almost all the semantic triples that occur at a certain instance 7.\n",
      "Figure 4: Filtered MRR scores vs. rank for the entities (left) and timestamps (right) recol-\n",
      "lectionontheICEWS(rare)trainingdataset.\n",
      "4. Semantic Memory from Episodic Memory with Marginalization\n",
      "We already discussed that a semantic KG might be related to a human\n",
      "semanticmemoryandthatanepisodicKGmightberelatedtoahumanepisodic\n",
      "memory. It has been speculated that episodic and semantic memory must be\n",
      "closely related, and that semantic memory is generated from episodic memory\n",
      "by some training process [28, 29]. As a very simple implementation of that\n",
      "idea, we propose that a semantic memory could be generated from episodic\n",
      "memory by marginalizing time. Thus, both types of memories would rely on\n",
      "identical representations and the marginalization step can be easily performed:\n",
      "Since probabilistic tensor models belong to the classes of sum-product nets, a\n",
      "marginalization simply means an integration over all time representations.\n",
      "Thus,inthesecondsetofexperiments,wetestthehypothesisthatsemantic\n",
      "7Thisobservationhasitsbiologicalcounterpart. Infact,theentorhinalcortex,whichplays\n",
      "an important role in the formation of episodic memory, is the main part of the adult hip-\n",
      "pocampusthatshowsneurogenesis[25]. Inanadulthuman,approximately700newneurons\n",
      "areaddedperdaythroughhippocampalneurogenesis,whicharebelievedtoperformsensory\n",
      "andspatialinformationencoding,aswellastemporalseparationofevents[26,27].\n",
      "16\n",
      "memory can be derived from episodic memory by projection. In other words,\n",
      "a semantic knowledge graph containing current semantic facts can be approx-\n",
      "imately constructed after modeling a corresponding episodic knowledge graph\n",
      "via marginalization. A marginalization can be performed by activating all time\n",
      "index neurons, i.e., summing over all a, since, e.g., Tucker decompositions are\n",
      "et\n",
      "an instance of a so-called sum-product network [30]. However, events having\n",
      "startaswellasendtimestampscannotsimplybeintegratedintoourcurrent se-\n",
      "mantic knowledge describing what we know now. For example, (Ban Ki-moon,\n",
      "SecretaryOf,UN)isnotconsistentwithwhatweknow currently. Toresolvethis\n",
      "problem, we introduce two types of time indices, e and e, having the\n",
      "tstart tend\n",
      "latent representations a(e ) and a(e ), respectively. Those time indices\n",
      "tstart tend\n",
      "can be used to construct the episodic tensor aggregating the start times-\n",
      "start\n",
      "E\n",
      "tamps of consecutive events, as well as the episodic tensor aggregating the\n",
      "end\n",
      "E\n",
      "end timestamps8.\n",
      "For the projection, instead of only summing over a(e ), we also subtract\n",
      "tstart\n",
      "the sum over a(e ). In this way, we can achieve the effect that events that\n",
      "tend\n",
      "have terminated already (i.e., have an end time index smaller than the current\n",
      "time index) are not integrated into the current semantic facts. Now, to test our\n",
      "hypothesis that this extended projection allows us to derive semantic memory\n",
      "fromepisodicmemory,wetrainedHolE,DistMult,ComplEx,ConT,andTucker\n",
      "on the episodic tensors and as well as on the semantic tensor χ\n",
      "start end\n",
      "E E\n",
      "derived from ICEWS. Note that only these models allow projection, since their\n",
      "indicator functions can be written in the form θepi =a f˜, where f˜can be\n",
      "t,s,p,o et ·\n",
      "arbitrary function of a, a, and a depending on the model choice9. The\n",
      "es ep eo\n",
      "8E.g., if the duration of a triple event (es,ep,eo) lasts from tstart to t end, the quadruple\n",
      "(es,ep,eo,etstart) is stored in Estart, while (es,ep,eo,etend) is stored E\n",
      "end\n",
      "only if t\n",
      "end\n",
      "<T\n",
      "(whereT isthelasttimestamp). Inotherwords,eventsthatlastuntilthelasttimestampdo\n",
      "notpossesse.\n",
      "end\n",
      "9For ConT, θ te,p si\n",
      ",p,o\n",
      "=flatten(gt)·(aes ⊗aep ⊗aeo), where ⊗ denotes the outer product.\n",
      "ForComplEx,θ te,p si,p,o=Re(aet)·Re(aes(cid:12)aep(cid:12)a¯eo)−Im(aet)·Im(aes(cid:12)aep(cid:12)a¯eo),where\n",
      "(cid:12) denotes the Hadamard product. The Tree model cannot be written in this form since et\n",
      "17\n",
      "model parameters are optimized using the margin-based ranking loss (3)10.\n",
      "Training was first performed on the episodic tensor, and then on\n",
      "start end\n",
      "E E\n",
      "with fixed a, a, and a obtained from the training on, since we\n",
      "es ep eo Estart\n",
      "assume that latent representations for subject, object, and predicate of a con-\n",
      "secutive event do not change during the event. Note that after training in this\n",
      "way, we could recall the starting and terminal point of a consecutive event (see\n",
      "the episodic tensor reconstruction experiments in Section 3), or infer a cur-\n",
      "rent semantic fact solely from the latent representations instead of rule-based\n",
      "reasoning.\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0 5 10 15 20 25 30\n",
      "Rank\n",
      "llaceR\n",
      "ConT Icews Train Dataset\n",
      "0.9\n",
      "0.8\n",
      "0.7\n",
      "0.6\n",
      "Semantic 0.5\n",
      "Start\n",
      "Start-End 0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "0.00 5 10 15 20 25\n",
      "Rank\n",
      "llaceR\n",
      "Tucker Icews Train Dataset\n",
      "Semantic\n",
      "Start\n",
      "Start_End\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.00 10 20 30 40 50 60 70\n",
      "Rank\n",
      "llaceR\n",
      "ComplEx Icews Train Dataset\n",
      "1.0\n",
      "0.9\n",
      "0.8\n",
      "0.7\n",
      "Semantic 0.6\n",
      "Start\n",
      "Start_End 0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1 10 20 30 40 50 60 70\n",
      "Rank\n",
      "llaceR\n",
      "HolE Icews Train Dataset\n",
      "Semantic\n",
      "Start\n",
      "Start_End\n",
      "Figure5: Recallscoresvs. rankfortheepisodic-to-semanticprojectionontheICEWS\n",
      "dataset with two different projection methods.\n",
      "To evaluate the projection, we compute the recall and area under precision-\n",
      "recall-curve(AUPRC)scoresfortheprojectionatdifferentranksontheICEWS\n",
      "residesinbothsubtreesT1 andT2.\n",
      "10Fortheprojectionexperiment,weomitthesigmoidfunctioninEq.(3),trainandinterpret\n",
      "themultilinearindicatorθ te,p si,p,o=aet·f˜(aes,aep,aeo)directlyastheprobabilityofepisodic\n",
      "quadruple. Onlyinthiswayoftraining,aprojectionismathematicallylegitimate.\n",
      "18\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "5 10 15 20 25 30\n",
      "Rank\n",
      "CRPUA\n",
      "ConT Icews Train Dataset\n",
      "0.55\n",
      "0.50\n",
      "0.45\n",
      "Semantic 0.40\n",
      "Start 0.35\n",
      "Start-End\n",
      "0.30\n",
      "0.25\n",
      "0.20\n",
      "0.150 5 10 15 20 25\n",
      "Rank\n",
      "CRPUA\n",
      "Tucker Icews Train Dataset\n",
      "Semantic\n",
      "Start\n",
      "Start_End\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0 10 20 30 40 50 60 70\n",
      "Rank\n",
      "CRPUA\n",
      "ComplEx Icews Train Dataset\n",
      "0.60\n",
      "0.55\n",
      "0.50\n",
      "Semantic 0.45\n",
      "Start 0.40\n",
      "Start_End\n",
      "0.35\n",
      "0.30\n",
      "0.25\n",
      "0.20 10 20 30 40 50 60 70\n",
      "Rank\n",
      "CRPUA\n",
      "HolE Icews Train Dataset\n",
      "Semantic\n",
      "Start\n",
      "Start_End\n",
      "Figure6: AUPRCscoresvs. rankfortheepisodic-to-semanticprojectionontheICEWS\n",
      "dataset with two different projection methods.\n",
      "training dataset, and compare them with the scores obtained from training\n",
      "the semantic tensor separately. The semantic dataset contains positive triples,\n",
      "which are episodic events that continue until the last (current) timestamp,\n",
      "e.g. (Ant´onio Guterres, SecretaryOf, UN, True), along with negative triples\n",
      "extracted from already terminated episodic events, e.g. (Ban Ki-moon, Secre-\n",
      "taryOf,UN,False). Duringthetestphaseofprojection,atriplefromtheseman-\n",
      "tic dataset is given with non-specified time index, e.g. (e,e,e,True/False,t).\n",
      "s p o\n",
      "Then, for the first method considering only the starting point of an episodic\n",
      "event, the projection to semantic space is computed as\n",
      "T\n",
      "θproj =[ (cid:88) a(e )] f˜, (9)\n",
      "s,p,o tstart ·\n",
      "tstart=1\n",
      "while for the second method considering both starting and terminal points, the\n",
      "projection is computed as\n",
      "(cid:34) (cid:35)\n",
      "T T\n",
      "θproj = (cid:88) a(e ) (cid:88) a(e ) f˜. (10)\n",
      "s,p,o tstart − tend ·\n",
      "tstart=1 tend=1\n",
      "19\n",
      "Then, the scores are evaluated by taking the label of the given semantic triple\n",
      "asthetarget,andtakingθproj astheprediction. Thegoalofthistestistocheck\n",
      "s,p,o\n",
      "howwellthealgorithmscanprojectagivenconsecutiveevent(e,e,e,t t )\n",
      "s p o start end\n",
      "···\n",
      "to semantic knowledge space using only the marginalized latent representation\n",
      "of time. All other experimental settings are similar to those in Section 3, and\n",
      "theexperimentswererepeatedfourtimesondifferentsampledtrainingdatasets.\n",
      "Figure 5 shows the recall scores for the two different projection methods on\n",
      "the training dataset in comparison to the separately trained semantic dataset.\n",
      "Due to limited space, we only show four models: ConT, Tucker, ComplEx, and\n",
      "HolE. As we can see, only the marginalization considering both starting and\n",
      "terminal time indices allows a reasonable projection from episodic memory to\n",
      "the current semantic memory. Again, ConT11 exhibits the best performance,\n",
      "with its recall score saturating after r˜ 15. In contrast, HolE shows insuffi-\n",
      "≈\n",
      "cient projection quality with sizable errors, especially at small ranks, which is\n",
      "due to its higher-order encoding noise. To show that the two types of latent\n",
      "representationsoftimedonotsimplyeliminateeachotherforacorrectepisodic\n",
      "projection,Figure6showstheAUPRCscoresevaluatedonthetrainingdataset.\n",
      "Overall,thisexperimentsupportstheideathatsemanticmemoryisalong-term\n",
      "storage for episodic memory, where the exact timing information is lost.\n",
      "For a fair comparison, in the last experiment we report the recall scores of\n",
      "the semantic models obtained by projecting the episodic models with respect\n",
      "to the temporal dimension. We compare two projection methods, the Start\n",
      "projection which only considers the staring point of episodic events (see Eq. 9),\n",
      "andtheStart-Endprojectionwhichtakesboththestartingandterminalpoints\n",
      "of episodic events into consideration. In addition, we report the recall scores on\n",
      "two semantic datasets. The first one contains genuine semantic facts, while the\n",
      "seconddatasetcontainsfalsesemantictripleswhichshouldalreadyberuledout\n",
      "11Note that since ConT doesn’t have a direct semantic counterpart, we instead use the\n",
      "semanticresultsobtainedusingRESCAL.ThisisreasonablesinceConTcanbeviewedasa\n",
      "high-dimensional(i.e.,episodic)generalizationofRESCAL.\n",
      "20\n",
      "Table 7: Filtered and raw Hits@10 scores for the episodic-to-semantic projection. Two pro-\n",
      "jection methods, Start (Eq. 9), Start-End (Eq. 10), are compared. Furthermore, semantic\n",
      "ICEWSdatasetwithgenuinesemantictriples,andsemanticICEWSdatasetwithfalsetriples\n",
      "are used for the projection experiments. Various projection scores are compared with the\n",
      "scores which are obtained by directly modeling the semantic ICEWS dataset with genuine\n",
      "semantictriples.\n",
      "Start Start-End Start(false) Start-End(false) Semantic\n",
      "Method Filter Raw Filter Raw Filter Raw Filter Raw Filter Raw\n",
      "DistMult 3.8 3.6 5.6 5.0 4.0 3.8 3.8 3.6 59.3 32.4\n",
      "HolE 5.8 5.4 5.5 5.1 4.7 4.5 5.6 5.2 56.1 31.3\n",
      "ComplEx 4.1 3.7 4.9 4.4 3.9 3.7 3.8 3.6 60.1 29.4\n",
      "Tucker 14.8 13.1 15.1 13.4 11.3 10.3 11.8 10.9 46.5 23.7\n",
      "ConT 30.9 24.6 40.8 30.3 23.0 19.9 22.6 19.3 43.8 20.4\n",
      "through the projection.\n",
      "Two different projections are performed on two semantic datasets, the gen-\n",
      "uine one and the false one. Theoretically, the recall scores on the genuine\n",
      "semantic dataset should be higher than those on the false dataset. Thus, the\n",
      "model hyper-parameters are chosen by monitoring the difference between the\n",
      "recall scores Hits@10 on the genuine and false semantic datasets.\n",
      "Table. 7 reports the filtered and raw Hits@10 metrics for different models,\n",
      "projection methods, and datasets. Moreover, we also compare the projection\n",
      "withtherecallscoresobtainedbydirectlymodelingthegenuinesemanticdataset\n",
      "usingthecorrespondingsemanticmodels12. TheConTmodelhasthebestpro-\n",
      "jection performance, since its projected recall scores on the genuine dataset are\n",
      "much higher than those obtained on the false semantic dataset. Moreover, the\n",
      "Start-EndprojectionmethodbasedontheConTmodelistheonlycombination\n",
      "which achieves similar results compared to the corresponding semantic model.\n",
      "One can also notice that all the projected compositional models are only able\n",
      "to tell whether a semantic triple is already ruled out or not before the last\n",
      "12NotethatweusetheRESCALmodelasthecorrespondingsemanticmodelfortheConT.\n",
      "21\n",
      "timestamp, however they can not provide good inference results on the genuine\n",
      "semantic dataset.\n",
      "5. Conclusion\n",
      "Thispaperdescribedthefirstmathematicalmodelsforthedeclarativemem-\n",
      "ories: the semantic and episodic memory functions. To model these cogni-\n",
      "tive functions, we generalized leading approaches for static knowledge graphs\n",
      "(i.e., Tucker, RESCAL, HolE, ComplEx, DistMult) to 4-dimensional tempo-\n",
      "ral/episodic knowledge graphs. In addition, we developed two novel generaliza-\n",
      "tions of RESCAL to episodic tensors, i.e., Tree and ConT. In particular, ConT\n",
      "has superior performance overall, which indicates the importance of introduced\n",
      "high-dimensional latent representation of time for both sparse episodic tensor\n",
      "reconstruction and generalization.\n",
      "Our hypothesis is that perception includes an active semantic decoding pro-\n",
      "cess, which relies on latent representations of entities and predicates, and that\n",
      "episodic and semantic memories depend on the same decoding process. We ar-\n",
      "gue that temporal knowledge graph embeddings might be models for human\n",
      "cognitive episodic memory and that semantic memory (facts we know) can be\n",
      "generated from episodic memory by a marginalization operation. We also test\n",
      "this hypothesis on the ICEWS dataset, the experiments show that the current\n",
      "semantic facts can only be derived from the episodic tensor by a proper projec-\n",
      "tion considering both starting and terminal points of consecutive events.\n",
      "Acknowledgements. This work is funded by the Cognitive Deep Learning\n",
      "research project in Siemens AG.\n",
      "22\n",
      "References\n",
      "References\n",
      "[1] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, Z. Ives, Dbpe-\n",
      "dia: A nucleus for a web of open data, The semantic web (2007) 722–735.\n",
      "[2] F. M. Suchanek, G. Kasneci, G. Weikum, Yago: a core of semantic knowl-\n",
      "edge, in: Proceedings of the 16th international conference on World Wide\n",
      "Web, ACM, 2007, pp. 697–706.\n",
      "[3] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, J. Taylor, Freebase: a\n",
      "collaboratively created graph database for structuring human knowledge,\n",
      "in: Proceedings of the 2008 ACM SIGMOD international conference on\n",
      "Management of data, AcM, 2008, pp. 1247–1250.\n",
      "[4] D. Vrandeˇci´c, M. Kr¨otzsch, Wikidata: a free collaborative knowledgebase,\n",
      "Communications of the ACM 57 (10) (2014) 78–85.\n",
      "[5] A. Singhal, Introducing the knowledge graph: things, not strings, Official\n",
      "google blog.\n",
      "[6] M. Nickel, K. Murphy, V. Tresp, E. Gabrilovich, A review of relational\n",
      "machine learning for knowledge graphs, Proceedings of the IEEE.\n",
      "[7] H. Ebbinghaus, U¨ber das ged¨achtnis: untersuchungen zur experimentellen\n",
      "psychologie, Duncker & Humblot, 1885.\n",
      "[8] R.C.Atkinson,R.M.Shiffrin,Humanmemory: Aproposedsystemandits\n",
      "control processes, Psychology of learning and motivation 2 (1968) 89–195.\n",
      "[9] L. R. Squire, Memory and brain.\n",
      "[10] E. Tulving, Episodic and semantic memory: Where should we go from\n",
      "here?, Behavioral and Brain Sciences 9 (03) (1986) 573–577.\n",
      "23\n",
      "[11] D. L. Greenberg, M. Verfaellie, Interdependence of episodic and seman-\n",
      "tic memory: evidence from neuropsychology, Journal of the International\n",
      "Neuropsychological society 16 (05) (2010) 748–753.\n",
      "[12] M.Nickel,V.Tresp,H.-P.Kriegel,Athree-waymodelforcollectivelearning\n",
      "on multi-relational data, in: Proceedings of the 28th international confer-\n",
      "ence on machine learning (ICML-11), 2011, pp. 809–816.\n",
      "[13] A. Cichocki, Era of big data processing: A new approach via tensor net-\n",
      "works and tensor decompositions, in: International Workshop on Smart\n",
      "Info-Media Systems in Asia (SISA-2013), 2013.\n",
      "[14] A. Cichocki, Tensor networks for big data analytic and large-scale opti-\n",
      "mization problems, in: Second Int. Conference on Engineering and Com-\n",
      "putational Schematics (ECM2013), 2013.\n",
      "[15] B. Yang, W.-t. Yih, X. He, J. Gao, L. Deng, Embedding entities and rela-\n",
      "tions for learning and inference in knowledge bases, International Confer-\n",
      "ence on Learning Representations (ICLR).\n",
      "[16] M. Nickel, L. Rosasco, T. Poggio, Holographic embeddings of knowledge\n",
      "graphs, in: Thirtieth AAAI Conference on Artificial Intelligence, 2016.\n",
      "[17] T. Trouillon, J. Welbl, S. Riedel, E´. Gaussier, G. Bouchard, Complex em-\n",
      "beddings for simple link prediction, in: International Conference on Ma-\n",
      "chine Learning, 2016, pp. 2071–2080.\n",
      "[18] T. A. Plate, Holographic reduced representations, IEEE Transactions on\n",
      "Neural Networks 6 (3) (1995) 623–641.\n",
      "[19] K. Hayashi, M. Shimbo, On the equivalence of holographic and complex\n",
      "embeddings for link prediction, CoRR abs/1702.05563.\n",
      "URL http://arxiv.org/abs/1702.05563\n",
      "[20] M. D. Ward, A. Beger, J. Cutler, M. Dickenson, C. Dorff, B. Radford,\n",
      "Comparing gdelt and icews event data, Analysis 21 (2013) 267–297.\n",
      "24\n",
      "[21] A. Schein, J. Paisley, D. M. Blei, H. Wallach, Bayesian poisson tensor\n",
      "factorization for inferring multilateral relations from sparse dyadic event\n",
      "counts, in: Proceedings of the 21th ACM SIGKDD International Confer-\n",
      "enceonKnowledgeDiscoveryandDataMining,ACM,2015,pp.1045–1054.\n",
      "[22] A.Bordes,N.Usunier,A.Garcia-Duran,J.Weston,O.Yakhnenko,Trans-\n",
      "latingembeddingsformodelingmulti-relationaldata,in: Advancesinneu-\n",
      "ral information processing systems, 2013, pp. 2787–2795.\n",
      "[23] X. Glorot, Y. Bengio, Understanding the difficulty of training deep feed-\n",
      "forward neural networks., in: Aistats, Vol. 9, 2010, pp. 249–256.\n",
      "[24] D. Kingma, J. Ba, Adam: A method for stochastic optimization, Pro-\n",
      "ceedings of the 3rd International Conference on Learning Representations\n",
      "(ICLR).\n",
      "[25] W.Deng,J.B.Aimone,F.H.Gage,Newneuronsandnewmemories: how\n",
      "doesadulthippocampalneurogenesisaffectlearningandmemory?, Nature\n",
      "reviews. Neuroscience 11 (5) (2010) 339.\n",
      "[26] O.Lazarov,C.Hollands,Hippocampalneurogenesis: learningtoremember,\n",
      "Progress in neurobiology 138 (2016) 1–18.\n",
      "[27] K. L. Spalding, O. Bergmann, K. Alkass, S. Bernard, M. Salehpour, H. B.\n",
      "Huttner, E. Bostr¨om, I. Westerlund, C. Vial, B. A. Buchholz, et al., Dy-\n",
      "namics of hippocampal neurogenesis in adult humans, Cell 153 (6) (2013)\n",
      "1219–1227.\n",
      "[28] J.L.McClelland,B.L.McNaughton,R.C.O’reilly,Whytherearecomple-\n",
      "mentarylearningsystemsinthehippocampusandneocortex: insightsfrom\n",
      "thesuccessesandfailuresofconnectionistmodelsoflearningandmemory.,\n",
      "Psychological review 102 (3) (1995) 419.\n",
      "[29] L. Nadel, A. Samsonovich, L. Ryan, M. Moscovitch, Multiple trace theory\n",
      "of human memory: computational, neuroimaging, and neuropsychological\n",
      "results, Hippocampus 10 (4) (2000) 352–368.\n",
      "25\n",
      "[30] H.Poon,P.Domingos,Sum-productnetworks: Anewdeeparchitecture,in:\n",
      "ComputerVisionWorkshops(ICCVWorkshops), 2011IEEEInternational\n",
      "Conference on, IEEE, 2011, pp. 689–690.\n",
      "26<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   7585,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['GDELT', 'ICEWS']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Embedding Models for Episodic Knowledge Graphs\n",
      "Yunpu Maa,b, Volker Trespa,b, Erik A. Daxberger 1c\n",
      "aSiemens AG, Corporate Technology, Munich, Germany\n",
      "bLudwig Maximilian University of Munich, Munich, Germany\n",
      "cETH Zurich\n",
      "Abstract\n",
      "In recent years a number of large-scale triple-oriented knowledge graphs have\n",
      "been generated and various models have been proposed to perform learning in\n",
      "those graphs. Most knowledge graphs are static and reflect the world in its\n",
      "current state. In reality, of course, the state of the world is changing: a healthy\n",
      "personbecomesdiagnosedwithadiseaseandanewpresidentisinaugurated. In\n",
      "thispaper,weextendmodelsforstaticknowledgegraphstotemporalknowledge\n",
      "graphs. This enables us to store episodic data and to generalize to new facts\n",
      "(inductive learning). We generalize leading learning models for static knowl-\n",
      "edge graphs (i.e., Tucker, RESCAL, HolE, ComplEx, DistMult) to temporal\n",
      "knowledge graphs. In particular, we introduce a new tensor model, ConT,\n",
      "with superior generalization performance. The performances of all proposed\n",
      "models are analyzed on two different datasets: the Global Database of Events,\n",
      "Language, and Tone (GDELT) and the database for Integrated Conflict Early\n",
      "Warning System (ICEWS). We argue that temporal knowledge graph embed-\n",
      "dings might be models also for cognitive episodic memory (facts we remember\n",
      "and can recollect) and that a semantic memory (current facts we know) can be\n",
      "generated from episodic memory by a marginalization operation. We validate\n",
      "this episodic-to-semantic projection hypothesis with the ICEWS dataset.\n",
      "Keywords: knowledge graph, temporal knowledge graph, semantic memory,\n",
      "episodic memory, tensor models\n",
      "1WorkdonewhileatSiemensAG.\n",
      "Preprint submitted to Journal of Web Semantics December 5, 2018\n",
      "8102\n",
      "ceD\n",
      "3\n",
      "]IA.sc[\n",
      "2v82200.7081:viXra\n",
      "1. Introduction\n",
      "In recent years a number of sizable Knowledge Graphs (KGs) have been\n",
      "developed, the largest ones containing more than 100 billion facts. Well known\n",
      "examples are DBpedia [1],YAGO [2], Freebase [3], Wikidata [4] and the Google\n",
      "KG [5]. Practical issues with completeness, quality and maintenance have been\n",
      "solved to a degree that some of these knowledge graphs support search, text\n",
      "understandingandquestionansweringinlarge-scalecommercialsystems[5]. In\n",
      "addition, statistical embedding models have been developed that can be used\n",
      "tocompressaknowledgegraph, toderiveimplicitfacts, todetecterrors, andto\n",
      "support the above mentioned applications. A recent survey on KG models can\n",
      "be found in [6].\n",
      "Mostknowledgegraphsarestaticandreflecttheworldatitscurrentstate. In\n",
      "reality, of course, the state of the world is changing: a healthy person becomes\n",
      "diagnosed with a disease and a new president is inaugurated. In this paper,\n",
      "we extend semantic knowledge graph embedding models to episodic/temporal\n",
      "knowledge graphs as an efficient way to store episodic data and to be able to\n",
      "generalize to new facts (inductive learning). In particular, we generalize lead-\n",
      "ing approaches for static knowledge graphs (i.e., constrained Tucker, DistMult,\n",
      "RESCAL, HolE, ComplEx) to temporal knowledge graphs. We test these mod-\n",
      "elsusingtwotemporalKGs. ThefirstoneisderivedfromtheIntegratedConflict\n",
      "Early Warning System (ICEWS) data set which describes interactions between\n",
      "nationsoverseveralyears. ThesecondoneisderivedfromtheGlobalDatabase\n",
      "ofEvents,LanguageandTone(GDELT)that,formorethan30years,monitors\n",
      "news media from all over the world. In the experiments, we analyze the gener-\n",
      "alization abilities to new facts that might be missing in the temporal KGs and\n",
      "also analyze to what degree a factorized KG can serve as an explicit memory.\n",
      "Weproposethatourtechnicalmodelsmightberelatedtothebrain’sexplicit\n",
      "memorysystems,i.e.,itsepisodicanditssemanticmemory. Bothareconsidered\n",
      "long-term memories and store information potentially over the life-time of an\n",
      "individual [7, 8, 9, 7]. The semantic memory stores general factual knowledge,\n",
      "2\n",
      "i.e., information we know, independent of the context where this knowledge\n",
      "was acquired and would be related to a static KG. Episodic memory concerns\n",
      "informationweremember andincludesthespatiotemporalcontextofevents[10]\n",
      "and would correspond to a temporal KG.\n",
      "primeMinisterOf\n",
      "10.1995 - 04.2002\n",
      "memberOf\n",
      "secretaryGeneralOf\n",
      "1955-m noem wberOf secretar 0y 1.G 0e 1.n 2er 0a 1l\n",
      "7-Of\n",
      "now\n",
      "memberOf secretaryGeneralOf mem 1b 9er 91O -f now 01.01.20se 0c 7-re 3t 1a.1ry\n",
      "2G.2e 0n 1e 6ralOf\n",
      "foreignMinisterOf\n",
      "01.2004 - 11.2006\n",
      "Figure1: Illustrationsof(left)asemanticknowledgegraphand(right)anepisodicknowledge\n",
      "graph. (Left)Everyarrowrepresentsa(subject,predicate,object)triple,withtheannotation\n",
      "of the arrow denoting the respective predicate. The triple (Ban Ki-moon, SecretaryOf, UN)\n",
      "is deleted, since the knowledge graph has been updated with the triple (Ant´onio Guterres,\n",
      "SecretaryOf, UN). (Right) Every arrow represents a (subject, predicate, object, timestamp)\n",
      "quadruple, where the arrow is both annotated with the respective predicate and timestamp.\n",
      "Here the quadruple involving is not deleted, since the attached timestamp reveals that the\n",
      "relationshipisnotvalidatpresent.\n",
      "An interesting question is how episodic and semantic memories are related.\n",
      "There is evidence that these main cognitive categories are partially dissociated\n",
      "from one another in the brain, as expressed in their differential sensitivity to\n",
      "brain damage. However, there is also evidence indicating that the different\n",
      "memory functions are not mutually independent and support one another [11].\n",
      "We propose that semantic memory can be derived from episodic memory by\n",
      "marginalization. Hereby we also consider that many episodes describe starting\n",
      "and endpoints of state changes. For example, an individual might become sick\n",
      "3\n",
      "with a disease, which eventually is cured. Similarly, a president’s tenure even-\n",
      "tuallyends. WestudyourhypothesisontheIntegratedConflictEarlyWarning\n",
      "System(ICEWS)dataset,whichcontainsmanyeventswithstartandenddates.\n",
      "Figure 1 compares semantic and episodic knowledge graphs. Furthermore, Fig-\n",
      "ure 2 illustrates the main ideas of building and modeling semantic and episodic\n",
      "knowledge graphs.\n",
      "Modeling\n",
      "Tensorization\n",
      "Graph Construction\n",
      "assigns entries as nodes and\n",
      "predicates as labeled edges\n",
      "for each timestamp\n",
      "Data Accumulation (e.g. GDELT)\n",
      "through knowledge extraction\n",
      "Websites Newspapers Social Media\n",
      "Figure 2: Illustration of the main idea behind the models presented in this paper. Step\n",
      "1: Knowledge is extracted from unstructured data, such as websites, newspapers or social\n",
      "media. Step 2: The knowledge graph is constructed, where entities are assigned as nodes,\n",
      "and predicates as labeled edges; note that there is a labeled edge for each timestamp. Step\n",
      "3: The knowledge graph is represented as a tensor; for semantic KGs, we obtain a 3-way\n",
      "tensor, storing (subject, predicate, object) triples, and for episodic KGs, we obtain a 4-way\n",
      "tensor, storing (subject, predicate, object, timestamp) quadruples. Step 4: The semantic\n",
      "and episodic tensors are decomposed and modeled via compositional or tensor models (see\n",
      "Section2).\n",
      "The paper is organized as follows. Section 2 introduces knowledge graphs,\n",
      "4\n",
      "the mapping of a knowledge graph to an adjacency tensor, and the statistical\n",
      "embedding models for knowledge graphs. We also describe how popular em-\n",
      "bedding models for KGs can be extended to episodic KGs. Section 3 shows\n",
      "experimental results on modelling episodic KGs. Finally, we present experi-\n",
      "ments on the possible relationships between episodic and semantic memory in\n",
      "Section 4.\n",
      "2. Model Descriptions\n",
      "A static or semantic knowledge graph (KG) is a triple-oriented knowledge\n",
      "representation. Here we consider a slight extension to the subject-predicate-\n",
      "object triple form by adding the value in the form (e,e,e ; Value), where\n",
      "s p o\n",
      "Value is a function of e,e,e and, e.g., can be a Boolean variable (True for 1,\n",
      "s p o\n",
      "False for 0) or a real number. Thus (Jack, likes, Mary; True) states that Jack\n",
      "(the subject or head entity) likes Mary (the object or tail entity). Note that e\n",
      "s\n",
      "ande representtheentitiesforsubjectindexsandobjectindexo. Tosimplify\n",
      "o\n",
      "notationwealsoconsidere tobeageneralizedentityassociatedwithpredicate\n",
      "p\n",
      "typewithindex p. For the episodic KGswe introduce e, whichisageneralized\n",
      "t\n",
      "entity for time t.\n",
      "TomodelastaticKG,weintroducethethree-waysemanticadjacencytensor\n",
      "χwherethetensorelementx istheassociatedValue ofthetriple(e,e,e ).\n",
      "s,p,o s p o\n",
      "One can also define a companion tensor Θ with the same dimensions as χ and\n",
      "χ\n",
      "with entries θ. Thus, the probabilistic model for the semantic tensor χ is\n",
      "s,p,o\n",
      "defined as P(x θ )=σ(θ ), where σ(x)=1/(1+exp( x)). Similarly,\n",
      "s,p,o s,p,o s,p,o\n",
      "| −\n",
      "the four-way temporal or episodic tensor has elements x which are the\n",
      "t,s,p,o\n",
      "E\n",
      "associated values of the quadruples (e,e,e,e ), with t=1,...,T. Therefore,\n",
      "t s p o\n",
      "the probabilistic model for episodic tensor is defined with the corresponding\n",
      "companion tensor Θ as\n",
      "E\n",
      "P(x θ )=σ(θ ). (1)\n",
      "t,s,p,o t,s,p,o t,s,p,o\n",
      "|\n",
      "We assume that each entity e has a unique latent representation a. In particu-\n",
      "lar,theembeddingapproachusedformodelingsemanticandepisodicknowledge\n",
      "5\n",
      "graphsassumesthatθsem =fsem(a,a,a ),andθepi =fepi(a,a,a,a ),\n",
      "s,p,o es ep eo t,s,p,o et es ep eo\n",
      "respectively. Here,theindicatorfunctionfsem/epi()isafunctiontobelearned.\n",
      "·\n",
      "Given a labeled dataset = (x,y ) m, latent representations and other\n",
      "D { i i }i=1\n",
      "parameters (denoted as ) are learned by minimizing the regularized logistic\n",
      "P\n",
      "loss\n",
      "m\n",
      "(cid:88)\n",
      "min log(1+exp( y θsem/epi))+λ 2. (2)\n",
      "P − i i ||P||2\n",
      "i=1\n",
      "In general, most KGs only contain positive triples; non-existing triples are nor-\n",
      "mally used as negative examples sampled with local closed- world assumption.\n",
      "Alternatively, we can minimize a margin-based ranking loss over the dataset\n",
      "such as\n",
      "(cid:88) (cid:88)\n",
      "min max(0,γ+σ(θsem/epi) σ(θsem/epi)), (3)\n",
      "P j − i\n",
      "i∈D+j∈D−\n",
      "where γ is the margin parameter, and and denote the set of positive\n",
      "+ −\n",
      "D D\n",
      "and negative samples, respectively.\n",
      "Therearedifferentwaysformodelingtheindicatorfunctionfepi()orfsem().\n",
      "· ·\n",
      "In this paper, we will only investigate multilinear models derived from tensor\n",
      "decompositions and compositional operations. We now describe the models in\n",
      "detail. Graphical illustrations of the described models are shown in Figure 3.\n",
      "p p\n",
      "p\n",
      "diag\n",
      "Re\n",
      "Im p\n",
      "Gp\n",
      "s o s o\n",
      "G G1 G2\n",
      "Gp Gt\n",
      "t t s p o s t o t s o t\n",
      "(a) (b) (c) (d) (e)\n",
      "Figure 3: Illustrations of (a) episodic Tucker, (b) episodic ComplEx (where • denotes con-\n",
      "traction),(c)RESCAL,(d)ConTand(e)Tree. Eachentityinthefigureisrepresentedasa\n",
      "circlewithtwoedges,sincetherepresentationforanentityeisae,i. Inaddition,Grepresents\n",
      "thecoretensorinTucker,Gp representsthematrixlatentrepresentationofpredicatepinthe\n",
      "RESCAL and Tree models, Gt represents the three-dimensional tensor latent representation\n",
      "oftimestamptintheConTmodel.\n",
      "Table 1 and Table 2 summarize notations used throughout this paper for\n",
      "6\n",
      "easyreference,whileTable3summarizesthenumberofparametersrequiredfor\n",
      "each model.2\n",
      "Table1: Summaryofthegeneralnotations.\n",
      "General\n",
      "Symbol Meaning\n",
      "e Entity for subject index s\n",
      "s\n",
      "e Entity for object index o\n",
      "o\n",
      "e Generalized entity for predicate index p\n",
      "p\n",
      "e Generalized entity for time index t\n",
      "t\n",
      "a Latent representation of entity e\n",
      "ei i\n",
      "a(e ) Latent representation of starting timestamp\n",
      "tstart\n",
      "a r -th element of a\n",
      "ei,ri i ei\n",
      "r˜ Rank/Dimensionality of a for i s,p,o\n",
      "ei\n",
      "∈{ }\n",
      "r˜ Rank/Dimensionality of a\n",
      "t et\n",
      "N Number of entities / predicates / timestamps\n",
      "e/p/t\n",
      "Tucker. First, we consider the Tucker model for semantic tensor decom-\n",
      "position of the form θsem = (cid:80)r˜ a a a gsem(r,r,r ). Here,\n",
      "s,p,o r1,r2,r3=1 es,r1 ep,r2 eo,r3 1 2 3\n",
      "gsem(r,r,r ) R are elements of the core tensor sem Rr˜×r˜×r˜. Similarly,\n",
      "1 2 3\n",
      "∈ G ∈\n",
      "the indicator function of a four-way Tucker model for episodic tensor decompo-\n",
      "sition is of the form\n",
      "(cid:88)r˜t (cid:88)r˜\n",
      "θepi =\n",
      "t,s,p,o\n",
      "r1=1r2,r3,r4=1\n",
      "a a a a gepi(r,r,r,r ), (4)\n",
      "et,r1 es,r2 ep,r3 eo,r4 1 2 3 4\n",
      "with a four dimensional core tensor epi Rr˜t×r˜×r˜×r˜. Note that this is a con-\n",
      "G ∈\n",
      "2For DistMult, ComplEx, and HolE it is required that r˜= r˜t. In our experiments (see\n",
      "Sections 3 and 4), in order to enable a fair comparison between the different models, we\n",
      "assume that the latent representations of entities, predicates, and time indices all have the\n",
      "samerank/dimensionality.\n",
      "7\n",
      "Table2: Summaryofthenotationsforsemanticandepisodicknowledgegraphs.\n",
      "Semantic knowledge graphs Episodic knowledge graphs\n",
      "Symbol Meaning Symbol Meaning\n",
      "χ Sem. adjacency tensor Epi. adjacency tensor\n",
      "E\n",
      "Θ Companion tensor of χ Θ Companion tensor of\n",
      "χ E\n",
      "E\n",
      "x Value of (e, e, e ) x Value of (e, e, e, e )\n",
      "s,p,o s p o t,s,p,o t s p o\n",
      "θsem Logit of (e, e, e ) θepi Logit of (e, e, e, e )\n",
      "s,p,o s p o t,s,p,o t s p o\n",
      "fsem() Sem. indicator function fepi() Epi. indicator function\n",
      "· ·\n",
      "sem Sem. core tensor epi Epi. core tensor\n",
      "G G\n",
      "gsem() Element of sem gepi() Element of epi\n",
      "· G · G\n",
      "straintTuckermodel,since,asinRESCAL,entitieshaveuniquerepresentations,\n",
      "independent of the roles as subject or object.\n",
      "RESCAL. Another model closely related to the semantic Tucker tensor\n",
      "decomposition is the RESCAL model, which has shown excellent performance\n",
      "in modelling KGs [12]. In RESCAL, subjects and objects have vector latent\n",
      "representations, while predicates have matrix latent representations. The indi-\n",
      "cator function of RESCAL for modeling semantic KGs takes the form θsem =\n",
      "s,p,o\n",
      "(cid:80)r˜\n",
      "a g (r,r )a,whereg (r,r )representsthematrixlatentrep-\n",
      "r1,r2=1 es,r1 p 1 2 eo,r2 p 1 2\n",
      "resentation for the predicate e. Then next two models, Tree and ConT, are\n",
      "p\n",
      "novel generalizations of RESCAL to episodic tensors.\n",
      "Tree. Fromapracticalperspective,traininganepisodicTuckertensormodel\n",
      "is very expensive since the computational complexity is approximately r˜4. Ten-\n",
      "sor networks provide a general and flexible framework to design nonstandard\n",
      "tensor decompositions [13, 14]. One of the simplest tensor networks is a tree\n",
      "tensordecomposition( )oftheepisodicindicatorfunction,whichisillustrated\n",
      "T\n",
      "in compositional operations. We now describe the models in detail. Graphical\n",
      "illustrations of the described models are shown in Figure 3(e). Therefore, we\n",
      "proposeatreetensordecomposition( )oftheepisodicindicatorfunction. The\n",
      "T\n",
      "tree is partitioned into two subtrees and, wherein subject e and time\n",
      "1 2 s\n",
      "T T T\n",
      "8\n",
      "e reside in, while object e and an auxiliary time e reside in. and\n",
      "t 1 o t 2 1 2\n",
      "T T T T\n",
      "are connected with e through two core tensors and. Thus, the indicator\n",
      "p 1 2\n",
      "G G\n",
      "function can be written as\n",
      "(cid:88)r˜t (cid:88)r˜\n",
      "θepi =\n",
      "t,s,p,o\n",
      "r1,r6=1r2,r3,r4,r5=1\n",
      "a a g (r,r,r )g (r,r )g (r,r,r )a a. (5)\n",
      "et,r1 es,r2 1 1 2 3 p 3 4 2 4 5 6 eo,r5 et,r6\n",
      "Within,wereducethefour-waycoretensorinTuckerintotwothree-dimensional\n",
      "T\n",
      "tensors and, so that the computational complexity of is approximately\n",
      "1 2\n",
      "G G T\n",
      "r˜3.\n",
      "ConT. ConT is another generalization of the RESCAL model to episodic\n",
      "tensors with reduced computational complexity of approximately r˜3. The idea\n",
      "is that another way of reducing the complexity is by contracting indices of the\n",
      "core tensor. Therefore, we contract the from Tucker with the time index\n",
      "G\n",
      "givingathree-waycoretensor foreachtimeinstance. Theindicatorfunction\n",
      "t\n",
      "G\n",
      "takes the form\n",
      "r˜\n",
      "(cid:88)\n",
      "θepi = a a a g (r,r,r ). (6)\n",
      "t,s,p,o es,r1 ep,r2 eo,r3 t 1 2 3\n",
      "r1,r2,r3=1\n",
      "In this model, the tensor resembles the relation-specific matrix from\n",
      "t p\n",
      "G G\n",
      "RESCAL.Later,wewillseethatConTisasuperiormodelformodelingepisodic\n",
      "knowledge graphs due to the representational flexibility of its high-dimensional\n",
      "tensor for the time index.\n",
      "t\n",
      "G\n",
      "Even though the complexity of Tree and ConT is reduced as compared to\n",
      "episodicTucker,thethree-dimensionalcoretensormightcauserapidoverfitting\n",
      "during training. Therefore, we next propose episodic generalization of compo-\n",
      "sitional models, such as DistMult [15], HolE [16] and ComplEx [17]. For those\n",
      "models, the number of parameters only increases linearly with the rank.\n",
      "DistMult. DistMult[15]isasimplegeneralizationoftheCPmodel,byen-\n",
      "forcingtheconstraintthatentitiesshouldhaveuniquerepresentations. Episodic\n",
      "DistMult takes the form θepi = (cid:80)r˜ λ a a a a. Here, we require\n",
      "t,s,p,o i=1 i et,i es,i ep,i eo,i\n",
      "that vector latent representations of entities, predicates, and timestamps have\n",
      "9\n",
      "the same rank. DistMult is a special case of Tucker having a core tensor with\n",
      "only diagonal elements λ.\n",
      "i\n",
      "HolE. Holographic embedding (HolE) [16] is a state-of-art link prediction\n",
      "andknowledgegraphcompletionmethod,whichisinspiredbyholographicmod-\n",
      "els of associative memory.\n",
      "HolE uses circular correlation to generate a compositional representation\n",
      "frominputse ande. TheindicatorofHolEreadsθsem =a (a (cid:63)a ),where\n",
      "s o s,p,o ep· es eo\n",
      "(cid:63):Rd Rd Rd denotesthecircularcorrelation[a(cid:63)b] =(cid:80)d−1a b.\n",
      "× → k i=0 i (k+i)modd\n",
      "We define the episodic extension of HolE as\n",
      "θepi =a (cid:0) a (cid:63)(a (cid:63)a )(cid:1). (7)\n",
      "t,s,p,o et · ep es eo\n",
      "As argued by [16], HolE employs a holographic reduced representation [18]\n",
      "to store and retrieve the predicates from e and e. Analogously, episodic HolE\n",
      "s o\n",
      "should be able to retrieve the stored timestamps from e, e and e. In the se-\n",
      "p s o\n",
      "manticcase,e canberetrievedifexistingtriplerelationsarestoredviacircular\n",
      "p\n",
      "(cid:80)\n",
      "convolution,andsuperpositionintherepresentationa = a a,\n",
      "∗ eo (s,p)∈So ep∗ es\n",
      "where is the set of all true triples given e. This is based on the fact that\n",
      "o o\n",
      "S\n",
      "a(cid:63)a δ [16]. Analogously, the stored timestamp e for an event can be re-\n",
      "t\n",
      "≈\n",
      "trieved if all existing episodic events are stored via, and superposition in the\n",
      "∗\n",
      "(cid:80)\n",
      "representation of e, a = a (a a ), where is the set of\n",
      "o eo (t,s,p)∈So et ∗ ep ∗ es So\n",
      "all true quadruples (t,s,p,o) given e. However, high order circular correla-\n",
      "o\n",
      "tion/convolution will increase the inaccuracy of retrieval. Another motivation\n",
      "forourepisodicextension(7)isthatacompositionaloperatoroftheforma f˜\n",
      "et·\n",
      "allows a projection from episodic memory to semantic memory, to be detailed\n",
      "later.\n",
      "ComplEx. Complex embedding (ComplEx) [17] is another state-of-art\n",
      "method closely related to HolE. It can accurately describe both symmetric and\n",
      "antisymmetric relations. HolE is a special case of ComplEx with imposed con-\n",
      "jugatesymmetryonembeddings[19]. Thus,ComplExhasmoredegreesoffree-\n",
      "dom, if compared to HolE. For the semantic complex embedding, the indicator\n",
      "(cid:16) (cid:17)\n",
      "function is θsem = Re (cid:80)r˜a a,a¯ with complex valued a and where\n",
      "s,p,o i es,i ep,i eo,i\n",
      "10\n",
      "the bar indicates the complex conjugate. To be consistent with the episodic\n",
      "HolE, the episodic complex embedding is defined as3\n",
      "(cid:32) (cid:33)\n",
      "r˜\n",
      "(cid:88)\n",
      "θepi =Re a a a,a¯. (8)\n",
      "t,s,p,o et,i es,i ep,i eo,i\n",
      "i\n",
      "3. Experiments on Episodic Models\n",
      "We investigate the proposed tensor and compositional models with experi-\n",
      "ments which are evaluated on two datasets:\n",
      "ICEWS. The Integrated Conflict Early Warning System (ICEWS) dataset\n",
      "[20]isanaturalepisodicdatasetrecordingdyadiceventsbetweendifferentcoun-\n",
      "tries. An example entry could be (Turkey, Syria, Fight, 12/25/2014). These\n",
      "dyadiceventsareaggregatedintoafour-waytensor with258entities, 20rela-\n",
      "E\n",
      "tiontypes,and72timestamps,whichhasintotal320,118positive(e,e,e,e )\n",
      "t s p o\n",
      "quadruples4. Thisdatasetwasfirstcreatedandusedin[21]. FromthisICEWS\n",
      "dataset, a semantic tensor is generated by extracting consecutive events that\n",
      "last until the last timestamp, constituting the current 5 semantic facts of the\n",
      "world.\n",
      "GDELT. The Global Database of Events, Language and Tone (GDELT)\n",
      "[20] monitors the world’s news media in broadcast, print and web formats from\n",
      "all over the world, daily since January 1, 1979 6. We use GDELT as a large\n",
      "episodic dataset. For our experiments, GDELT data is collected from January\n",
      "1, 2012 to December 31, 2012 (with a temporal granularity of 24 hrs). These\n",
      "events are aggregated into an episodic tensor with 1100 entities, 180 relation\n",
      "E\n",
      "3One can show that Eq. (7) is equivalent to Eq. (8) by converting it to the frequency\n",
      "domain[19]. Then,θ te,p si\n",
      ",p,o\n",
      "∝ωT et(ω¯ep(cid:12)ω¯es(cid:12)ωeo),whereω=F(a)∈Cr˜ arethediscrete\n",
      "Fouriertransformsofembeddingsa,andusingthefactthatωisconjugatesymmetricforreal\n",
      "vectora.\n",
      "4Notethatforanepisodiceventthedatasetcontainsallthequadruples(eti,es,ep,e0)for\n",
      "ti∈{tstart,tstart+1,···,t end−1,t end}.\n",
      "5Current alwaysindicatesthelasttimestamp/timestampsoftheappliedepisodicKGs.\n",
      "6https://www.gdeltproject.org/about.html\n",
      "11\n",
      "Table3: Numberofparametersfordifferentmodelsandtheruntimeofonetrainingepochon\n",
      "theGDELTdataset.\n",
      "Runtime\n",
      "Model Semantic Episodic Complexity rank40 rank60 rank150\n",
      "DistMult (Ne+Np+1)r˜ (Ne+Np+Nt+1)r˜ O(r˜) 35.2s 36.4s 53.7s\n",
      "HolE (Ne+Np)r˜ (Ne+Np)r˜ O(r˜logr˜) 42.8s 43.2s 59.0s\n",
      "ComplEx 2(Ne+Np)r˜ 2(Ne+Np+Nt)r˜ O(r˜) 40.1s 42.4s 57.5s\n",
      "Tree − Ner˜+Npr˜2+(Nt+2r˜2)r˜t O(r˜3) 133.6s 160.2s −\n",
      "ConT − (Ne+Np)r˜+Ntr˜3 O(r˜3) 95.4s 226.1s −\n",
      "Tucker (Ne+Np)r˜+r˜3 (Ne+Np)r˜+(Nt+r˜3)r˜t O(r˜4) 144.2s 387.9s −\n",
      "types, and 366 timestamps, which has in total 2,563,561 positive (e,e,e,e )\n",
      "t s p o\n",
      "quadruples.\n",
      "We assess the quality of episodic information retrieval on both datasets for\n",
      "the proposed tensor and compositional models. Since both episodic datasets\n",
      "only consist of positive quadruples, we generated negative episodic instances\n",
      "followingtheprotocolofcorruptingsemantictriplesgivenbyBordes[22]: nega-\n",
      "tiveinstancesofanepisodicquadruple(e,e,e,e )aredrawnbycorruptingthe\n",
      "s p o t\n",
      "objecte o toe o(cid:48) orthetimestampe t toe t(cid:48),meaningthat(e s,e p,e o(cid:48),e t)servesas\n",
      "a negative evidence of the episodic event at time instance e t, and (e s,e p,e o,e t(cid:48))\n",
      "is a true fact which cannot be correctly recalled at time instance e t(cid:48). During\n",
      "training, for each positive sample in a batch we assigned two negative samples\n",
      "with corrupted object or corrupted subject.\n",
      "The model performance is evaluated using the following scores. To retrieve\n",
      "the occurrence time, for each true quadruple, we replace the time index e with\n",
      "t\n",
      "every other possible time index e t(cid:48), compute the value of the indicator function\n",
      "θepi, and rank them in a decreasing order. We filter the ranking as in [22]\n",
      "t(cid:48),s,p,o\n",
      "by removing all quadruples where x t(cid:48),s,p,o =1 and t=t(cid:48), in order to eliminate\n",
      "(cid:54)\n",
      "ambiguity during episodic information retrieval. Similarly, we evaluated the\n",
      "retrieval of the predicate between a given subject and object at a certain time\n",
      "instancebycomputingandrankingtheindicatorθepi. Wealsoevaluatedthe\n",
      "t,s,p(cid:48),o\n",
      "12\n",
      "retrieval of entities by ranking and averaging the filtered indicators θ t,s(cid:48),p,o and\n",
      "θ t,s,p,o(cid:48). Tomeasurethegeneralizationabilityofthemodels,wereportdifferent\n",
      "measures of the ranking: mean reciprocal rank (MRR), and Hits@n on the test\n",
      "dataset.\n",
      "The datasets were split into train, validation, and test sets that contain the\n",
      "most frequently appearing entities in the episodic knowledge graphs. Training\n",
      "was performed by minimizing the logistic loss (2), and was terminated using\n",
      "early stopping on the validation dataset by monitoring the filtered MRR recall\n",
      "scores every 50,100 epochs depending on the models, where the maximum\n",
      "{ }\n",
      "training duration was 500 epochs. This ensures that the generalization ability\n",
      "ofuniquelatentrepresentationsofentitiesdoesn’tsufferfromoverfitting. Before\n",
      "training,allmodelparametersareinitializedusingXavierinitialization[23]. We\n",
      "alsoapplyanl2normpenaltyonallparametersforregularizationpurposes(see\n",
      "Eq. (2)).\n",
      "In Table 3 we summarize the runtime for one training epoch on the GDELT\n",
      "dataset for different models at ranks r˜ = r˜ 40,60,150. All experiments\n",
      "t\n",
      "∈ { }\n",
      "were performed on a single Tesla K80 GPU. In the following experiments, for\n",
      "compositional models we search rank in 100,150, while for tensor models we\n",
      "{ }\n",
      "search optimal rank in 40,50,60 since larger ranks could lead to overfitting\n",
      "{ }\n",
      "rapidly. Loss function is minimized with Adam method [24] with the learning\n",
      "rate selected from 0.001,1e 4,5e 5.\n",
      "{ − − }\n",
      "We first assess the filtered MRR, Hits@1, Hits@3, and Hits@10 scores of\n",
      "inferring missing entities and predicates on the GDELT test dataset. Table 4\n",
      "summarizes the results. Generalizations on the test dataset indicate the induc-\n",
      "tive reasoning capability of the proposed models. This generalization can be\n",
      "useful for the completion of evolving KGs with missing records, such as clinical\n",
      "datasets. Itcanbeseenthattensormodelsareabletooutperformcompositional\n",
      "modelsconsistentlyonbothentityandpredicatepredictiontasks. ConThasthe\n",
      "best inference results on the entity-related tasks, while Tucker performs better\n",
      "on the predicate-related tasks. The superior Hits@1 result of ConT on the en-\n",
      "titypredictionindicatesthatthereareeasilytobefittedentitiesintheGDELT\n",
      "13\n",
      "Table 4: Filtered results of inferring missing entities and predicates of episodic quadruples\n",
      "evaluatedontheGDELTdataset.\n",
      "Entity Predicate\n",
      "Method MRR @1 @3 @10 MRR @1 @3 @10\n",
      "DistMult 0.182 6.55 19.77 43.70 0.269 12.65 30.29 59.40\n",
      "HolE 0.177 6.67 18.95 41.84 0.256 11.81 28.35 57.73\n",
      "ComplEx 0.172 6.54 17.52 41.56 0.255 12.05 27.75 56.60\n",
      "Tree 0.196 8.17 21.00 44.65 0.274 13.30 30.66 60.05\n",
      "Tucker<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  20212,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Entity prediction', 'Predicate prediction']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: intheGDELT\n",
      "13\n",
      "Table 4: Filtered results of inferring missing entities and predicates of episodic quadruples\n",
      "evaluatedontheGDELTdataset.\n",
      "Entity Predicate\n",
      "Method MRR @1 @3 @10 MRR @1 @3 @10\n",
      "DistMult 0.182 6.55 19.77 43.70 0.269 12.65 30.29 59.40\n",
      "HolE 0.177 6.67 18.95 41.84 0.256 11.81 28.35 57.73\n",
      "ComplEx 0.172 6.54 17.52 41.56 0.255 12.05 27.75 56.60\n",
      "Tree 0.196 8.17 21.00 44.65 0.274 13.30 30.66 60.05\n",
      "Tucker 0.204 8.93 21.85 46.35 0.275 12.69 31.35 60.70\n",
      "ConT 0.233 13.85 24.65 42.96 0.263 12.83 29.27 57.30\n",
      "Table 5: Filtered results for entities and predicates recollection/prediction evaluated on the\n",
      "ICEWSdataset.\n",
      "Entity Predicate\n",
      "Method MRR @1 @3 @10 MRR @1 @3 @10\n",
      "DistMult 0.222 9.72 22.48 52.32 0.520 33.73 62.25 91.13\n",
      "HolE 0.229 9.85 23.49 54.21 0.517 31.55 65.47 93.59\n",
      "ComplEx 0.229 8.94 23.53 57.72 0.506 30.99 61.46 93.44\n",
      "Tree 0.205 10.48 19.84 42.81 0.554 36.62 67.25 94.70\n",
      "Tucker 0.257 12.88 27.10 54.43 0.563 36.96 69.55 95.43\n",
      "ConT 0.264 15.71 29.60 46.67 0.557 38.12 67.76 87.71\n",
      "dataset along the timestamps. In fact, the GDELT dataset is unbalanced, and\n",
      "episodic quadruples related to certain entities dominate in the episodic Knowl-\n",
      "edgegraph,suchasquadruplescontainingtheentitiesUSA,orUN.Experiment\n",
      "results on balanced and extremely sparse episodic dataset will be reported in\n",
      "the following.\n",
      "Next,Table5showstheMRR,Hits@1,Hits@3,andHits@10scoresofinfer-\n",
      "ring missing entities and predicates on the ICEWS test dataset. Similarly, we\n",
      "can read that tensor models outperform compositional models on both missing\n",
      "entityandpredicateinferencetasks. ThesuperiorHits@1resultofConTforthe\n",
      "14\n",
      "missingentitypredictionindicatesagainthattheICEWSdatasetisunbalanced,\n",
      "and episodic quadruples related to certain entities dominate.\n",
      "Table 6: Filtered recall scores for entities and timestamps recollection on the ICEWS (rare)\n",
      "trainingdataset.\n",
      "Timestamp Entity\n",
      "Method Rank MRR @3 MRR @3\n",
      "DistMult 200 0.257 27.0 0.211 21.9\n",
      "HolE 200 0.216 20.8 0.179 16.3\n",
      "ComplEx 200 0.354 40.3 0.301 33.2\n",
      "Tree 40 0.421 55.3 0.314 35.7\n",
      "Tucker 40 0.923 98.9 0.893 97.1\n",
      "ConT 40 0.982 99.7 0.950 97.9\n",
      "The recollection of the exact occurrence time of a significant past event\n",
      "(e.g. unusual, novel, attached with emotion) is also an important capability\n",
      "of episodic cognitive memory function. In order to manifest this perspective\n",
      "of proposed models, Table 6 shows the filtered MRR, and Hits@3 scores for\n",
      "the timestamps and entities recollection on the episodic ICEWS (rare) training\n",
      "dataset, where rank column registers the optimal and minimum rank r˜ = r˜\n",
      "t\n",
      "havingtheoutstandingrecallscores. Figure4furtherdisplaysthefilteredMRR\n",
      "score as a function of rank. Unlike the original ICEWS, which contains many\n",
      "consecutive events that last from the first to the last timestamp leading to\n",
      "unreasonably high filtered timestamp recall scores, this ICEWS (rare) dataset\n",
      "consists of rare temporal events that happen less than three times throughout\n",
      "the whole time and starting points of events.\n",
      "The outstanding performance of ConT compared with other compositional\n",
      "models indicates the importance of large dimensionality of time latent repre-\n",
      "sentation for the episodic tensor reconstruction / episodic memory recollection.\n",
      "Recall that for ConT the real dimension of the latent representation of time is\n",
      "actuallyr˜3 afterflattening. Thisflexiblelatentrepresentationfortimecould\n",
      "t\n",
      "G\n",
      "15\n",
      "compress almost all the semantic triples that occur at a certain instance 7.\n",
      "Figure 4: Filtered MRR scores vs. rank for the entities (left) and timestamps (right) recol-\n",
      "lectionontheICEWS(rare)trainingdataset.\n",
      "4. Semantic Memory from Episodic Memory with Marginalization\n",
      "We already discussed that a semantic KG might be related to a human\n",
      "semanticmemoryandthatanepisodicKGmightberelatedtoahumanepisodic\n",
      "memory. It has been speculated that episodic and semantic memory must be\n",
      "closely related, and that semantic memory is generated from episodic memory\n",
      "by some training process [28, 29]. As a very simple implementation of that\n",
      "idea, we propose that a semantic memory could be generated from episodic\n",
      "memory by marginalizing time. Thus, both types of memories would rely on\n",
      "identical representations and the marginalization step can be easily performed:\n",
      "Since probabilistic tensor models belong to the classes of sum-product nets, a\n",
      "marginalization simply means an integration over all time representations.\n",
      "Thus,inthesecondsetofexperiments,wetestthehypothesisthatsemantic\n",
      "7Thisobservationhasitsbiologicalcounterpart. Infact,theentorhinalcortex,whichplays\n",
      "an important role in the formation of episodic memory, is the main part of the adult hip-\n",
      "pocampusthatshowsneurogenesis[25]. Inanadulthuman,approximately700newneurons\n",
      "areaddedperdaythroughhippocampalneurogenesis,whicharebelievedtoperformsensory\n",
      "andspatialinformationencoding,aswellastemporalseparationofevents[26,27].\n",
      "16\n",
      "memory can be derived from episodic memory by projection. In other words,\n",
      "a semantic knowledge graph containing current semantic facts can be approx-\n",
      "imately constructed after modeling a corresponding episodic knowledge graph\n",
      "via marginalization. A marginalization can be performed by activating all time\n",
      "index neurons, i.e., summing over all a, since, e.g., Tucker decompositions are\n",
      "et\n",
      "an instance of a so-called sum-product network [30]. However, events having\n",
      "startaswellasendtimestampscannotsimplybeintegratedintoourcurrent se-\n",
      "mantic knowledge describing what we know now. For example, (Ban Ki-moon,\n",
      "SecretaryOf,UN)isnotconsistentwithwhatweknow currently. Toresolvethis\n",
      "problem, we introduce two types of time indices, e and e, having the\n",
      "tstart tend\n",
      "latent representations a(e ) and a(e ), respectively. Those time indices\n",
      "tstart tend\n",
      "can be used to construct the episodic tensor aggregating the start times-\n",
      "start\n",
      "E\n",
      "tamps of consecutive events, as well as the episodic tensor aggregating the\n",
      "end\n",
      "E\n",
      "end timestamps8.\n",
      "For the projection, instead of only summing over a(e ), we also subtract\n",
      "tstart\n",
      "the sum over a(e ). In this way, we can achieve the effect that events that\n",
      "tend\n",
      "have terminated already (i.e., have an end time index smaller than the current\n",
      "time index) are not integrated into the current semantic facts. Now, to test our\n",
      "hypothesis that this extended projection allows us to derive semantic memory\n",
      "fromepisodicmemory,wetrainedHolE,DistMult,ComplEx,ConT,andTucker\n",
      "on the episodic tensors and as well as on the semantic tensor χ\n",
      "start end\n",
      "E E\n",
      "derived from ICEWS. Note that only these models allow projection, since their\n",
      "indicator functions can be written in the form θepi =a f˜, where f˜can be\n",
      "t,s,p,o et ·\n",
      "arbitrary function of a, a, and a depending on the model choice9. The\n",
      "es ep eo\n",
      "8E.g., if the duration of a triple event (es,ep,eo) lasts from tstart to t end, the quadruple\n",
      "(es,ep,eo,etstart) is stored in Estart, while (es,ep,eo,etend) is stored E\n",
      "end\n",
      "only if t\n",
      "end\n",
      "<T\n",
      "(whereT isthelasttimestamp). Inotherwords,eventsthatlastuntilthelasttimestampdo\n",
      "notpossesse.\n",
      "end\n",
      "9For ConT, θ te,p si\n",
      ",p,o\n",
      "=flatten(gt)·(aes ⊗aep ⊗aeo), where ⊗ denotes the outer product.\n",
      "ForComplEx,θ te,p si,p,o=Re(aet)·Re(aes(cid:12)aep(cid:12)a¯eo)−Im(aet)·Im(aes(cid:12)aep(cid:12)a¯eo),where\n",
      "(cid:12) denotes the Hadamard product. The Tree model cannot be written in this form since et\n",
      "17\n",
      "model parameters are optimized using the margin-based ranking loss (3)10.\n",
      "Training was first performed on the episodic tensor, and then on\n",
      "start end\n",
      "E E\n",
      "with fixed a, a, and a obtained from the training on, since we\n",
      "es ep eo Estart\n",
      "assume that latent representations for subject, object, and predicate of a con-\n",
      "secutive event do not change during the event. Note that after training in this\n",
      "way, we could recall the starting and terminal point of a consecutive event (see\n",
      "the episodic tensor reconstruction experiments in Section 3), or infer a cur-\n",
      "rent semantic fact solely from the latent representations instead of rule-based\n",
      "reasoning.\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0 5 10 15 20 25 30\n",
      "Rank\n",
      "llaceR\n",
      "ConT Icews Train Dataset\n",
      "0.9\n",
      "0.8\n",
      "0.7\n",
      "0.6\n",
      "Semantic 0.5\n",
      "Start\n",
      "Start-End 0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "0.00 5 10 15 20 25\n",
      "Rank\n",
      "llaceR\n",
      "Tucker Icews Train Dataset\n",
      "Semantic\n",
      "Start\n",
      "Start_End\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.00 10 20 30 40 50 60 70\n",
      "Rank\n",
      "llaceR\n",
      "ComplEx Icews Train Dataset\n",
      "1.0\n",
      "0.9\n",
      "0.8\n",
      "0.7\n",
      "Semantic 0.6\n",
      "Start\n",
      "Start_End 0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1 10 20 30 40 50 60 70\n",
      "Rank\n",
      "llaceR\n",
      "HolE Icews Train Dataset\n",
      "Semantic\n",
      "Start\n",
      "Start_End\n",
      "Figure5: Recallscoresvs. rankfortheepisodic-to-semanticprojectionontheICEWS\n",
      "dataset with two different projection methods.\n",
      "To evaluate the projection, we compute the recall and area under precision-\n",
      "recall-curve(AUPRC)scoresfortheprojectionatdifferentranksontheICEWS\n",
      "residesinbothsubtreesT1 andT2.\n",
      "10Fortheprojectionexperiment,weomitthesigmoidfunctioninEq.(3),trainandinterpret\n",
      "themultilinearindicatorθ te,p si,p,o=aet·f˜(aes,aep,aeo)directlyastheprobabilityofepisodic\n",
      "quadruple. Onlyinthiswayoftraining,aprojectionismathematicallylegitimate.\n",
      "18\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "5 10 15 20 25 30\n",
      "Rank\n",
      "CRPUA\n",
      "ConT Icews Train Dataset\n",
      "0.55\n",
      "0.50\n",
      "0.45\n",
      "Semantic 0.40\n",
      "Start 0.35\n",
      "Start-End\n",
      "0.30\n",
      "0.25\n",
      "0.20\n",
      "0.150 5 10 15 20 25\n",
      "Rank\n",
      "CRPUA\n",
      "Tucker Icews Train Dataset\n",
      "Semantic\n",
      "Start\n",
      "Start_End\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0 10 20 30 40 50 60 70\n",
      "Rank\n",
      "CRPUA\n",
      "ComplEx Icews Train Dataset\n",
      "0.60\n",
      "0.55\n",
      "0.50\n",
      "Semantic 0.45\n",
      "Start 0.40\n",
      "Start_End\n",
      "0.35\n",
      "0.30\n",
      "0.25\n",
      "0.20 10 20 30 40 50 60 70\n",
      "Rank\n",
      "CRPUA\n",
      "HolE Icews Train Dataset\n",
      "Semantic\n",
      "Start\n",
      "Start_End\n",
      "Figure6: AUPRCscoresvs. rankfortheepisodic-to-semanticprojectionontheICEWS\n",
      "dataset with two different projection methods.\n",
      "training dataset, and compare them with the scores obtained from training\n",
      "the semantic tensor separately. The semantic dataset contains positive triples,\n",
      "which are episodic events that continue until the last (current) timestamp,\n",
      "e.g. (Ant´onio Guterres, SecretaryOf, UN, True), along with negative triples\n",
      "extracted from already terminated episodic events, e.g. (Ban Ki-moon, Secre-\n",
      "taryOf,UN,False). Duringthetestphaseofprojection,atriplefromtheseman-\n",
      "tic dataset is given with non-specified time index, e.g. (e,e,e,True/False,t).\n",
      "s p o\n",
      "Then, for the first method considering only the starting point of an episodic\n",
      "event, the projection to semantic space is computed as\n",
      "T\n",
      "θproj =[ (cid:88) a(e )] f˜, (9)\n",
      "s,p,o tstart ·\n",
      "tstart=1\n",
      "while for the second method considering both starting and terminal points, the\n",
      "projection is computed as\n",
      "(cid:34) (cid:35)\n",
      "T T\n",
      "θproj = (cid:88) a(e ) (cid:88) a(e ) f˜. (10)\n",
      "s,p,o tstart − tend ·\n",
      "tstart=1 tend=1\n",
      "19\n",
      "Then, the scores are evaluated by taking the label of the given semantic triple\n",
      "asthetarget,andtakingθproj astheprediction. Thegoalofthistestistocheck\n",
      "s,p,o\n",
      "howwellthealgorithmscanprojectagivenconsecutiveevent(e,e,e,t t )\n",
      "s p o start end\n",
      "···\n",
      "to semantic knowledge space using only the marginalized latent representation\n",
      "of time. All other experimental settings are similar to those in Section 3, and\n",
      "theexperimentswererepeatedfourtimesondifferentsampledtrainingdatasets.\n",
      "Figure 5 shows the recall scores for the two different projection methods on\n",
      "the training dataset in comparison to the separately trained semantic dataset.\n",
      "Due to limited space, we only show four models: ConT, Tucker, ComplEx, and\n",
      "HolE. As we can see, only the marginalization considering both starting and\n",
      "terminal time indices allows a reasonable projection from episodic memory to\n",
      "the current semantic memory. Again, ConT11 exhibits the best performance,\n",
      "with its recall score saturating after r˜ 15. In contrast, HolE shows insuffi-\n",
      "≈\n",
      "cient projection quality with sizable errors, especially at small ranks, which is\n",
      "due to its higher-order encoding noise. To show that the two types of latent\n",
      "representationsoftimedonotsimplyeliminateeachotherforacorrectepisodic\n",
      "projection,Figure6showstheAUPRCscoresevaluatedonthetrainingdataset.\n",
      "Overall,thisexperimentsupportstheideathatsemanticmemoryisalong-term\n",
      "storage for episodic memory, where the exact timing information is lost.\n",
      "For a fair comparison, in the last experiment we report the recall scores of\n",
      "the semantic models obtained by projecting the episodic models with respect\n",
      "to the temporal dimension. We compare two projection methods, the Start\n",
      "projection which only considers the staring point of episodic events (see Eq. 9),\n",
      "andtheStart-Endprojectionwhichtakesboththestartingandterminalpoints\n",
      "of episodic events into consideration. In addition, we report the recall scores on\n",
      "two semantic datasets. The first one contains genuine semantic facts, while the\n",
      "seconddatasetcontainsfalsesemantictripleswhichshouldalreadyberuledout\n",
      "11Note that since ConT doesn’t have a direct semantic counterpart, we instead use the\n",
      "semanticresultsobtainedusingRESCAL.ThisisreasonablesinceConTcanbeviewedasa\n",
      "high-dimensional(i.e.,episodic)generalizationofRESCAL.\n",
      "20\n",
      "Table 7: Filtered and raw Hits@10 scores for the episodic-to-semantic projection. Two pro-\n",
      "jection methods, Start (Eq. 9), Start-End (Eq. 10), are compared. Furthermore, semantic\n",
      "ICEWSdatasetwithgenuinesemantictriples,andsemanticICEWSdatasetwithfalsetriples\n",
      "are used for the projection experiments. Various projection scores are compared with the\n",
      "scores which are obtained by directly modeling the semantic ICEWS dataset with genuine\n",
      "semantictriples.\n",
      "Start Start-End Start(false) Start-End(false) Semantic\n",
      "Method Filter Raw Filter Raw Filter Raw Filter Raw Filter Raw\n",
      "DistMult 3.8 3.6 5.6 5.0 4.0 3.8 3.8 3.6 59.3 32.4\n",
      "HolE 5.8 5.4 5.5 5.1 4.7 4.5 5.6 5.2 56.1 31.3\n",
      "ComplEx 4.1 3.7 4.9 4.4 3.9 3.7 3.8 3.6 60.1 29.4\n",
      "Tucker 14.8 13.1 15.1 13.4 11.3 10.3 11.8 10.9 46.5 23.7\n",
      "ConT 30.9 24.6 40.8 30.3 23.0 19.9 22.6 19.3 43.8 20.4\n",
      "through the projection.\n",
      "Two different projections are performed on two semantic datasets, the gen-\n",
      "uine one and the false one. Theoretically, the recall scores on the genuine\n",
      "semantic dataset should be higher than those on the false dataset. Thus, the\n",
      "model hyper-parameters are chosen by monitoring the difference between the\n",
      "recall scores Hits@10 on the genuine and false semantic datasets.\n",
      "Table. 7 reports the filtered and raw Hits@10 metrics for different models,\n",
      "projection methods, and datasets. Moreover, we also compare the projection\n",
      "withtherecallscoresobtainedbydirectlymodelingthegenuinesemanticdataset\n",
      "usingthecorrespondingsemanticmodels12. TheConTmodelhasthebestpro-\n",
      "jection performance, since its projected recall scores on the genuine dataset are\n",
      "much higher than those obtained on the false semantic dataset. Moreover, the\n",
      "Start-EndprojectionmethodbasedontheConTmodelistheonlycombination\n",
      "which achieves similar results compared to the corresponding semantic model.\n",
      "One can also notice that all the projected compositional models are only able\n",
      "to tell whether a semantic triple is already ruled out or not before the last\n",
      "12NotethatweusetheRESCALmodelasthecorrespondingsemanticmodelfortheConT.\n",
      "21\n",
      "timestamp, however they can not provide good inference results on the genuine\n",
      "semantic dataset.\n",
      "5. Conclusion\n",
      "Thispaperdescribedthefirstmathematicalmodelsforthedeclarativemem-\n",
      "ories: the semantic and episodic memory functions. To model these cogni-\n",
      "tive functions, we generalized leading approaches for static knowledge graphs\n",
      "(i.e., Tucker, RESCAL, HolE, ComplEx, DistMult) to 4-dimensional tempo-\n",
      "ral/episodic knowledge graphs. In addition, we developed two novel generaliza-\n",
      "tions of RESCAL to episodic tensors, i.e., Tree and ConT. In particular, ConT\n",
      "has superior performance overall, which indicates the importance of introduced\n",
      "high-dimensional latent representation of time for both sparse episodic tensor\n",
      "reconstruction and generalization.\n",
      "Our hypothesis is that perception includes an active semantic decoding pro-\n",
      "cess, which relies on latent representations of entities and predicates, and that\n",
      "episodic and semantic memories depend on the same decoding process. We ar-\n",
      "gue that temporal knowledge graph embeddings might be models for human\n",
      "cognitive episodic memory and that semantic memory (facts we know) can be\n",
      "generated from episodic memory by a marginalization operation. We also test\n",
      "this hypothesis on the ICEWS dataset, the experiments show that the current\n",
      "semantic facts can only be derived from the episodic tensor by a proper projec-\n",
      "tion considering both starting and terminal points of consecutive events.\n",
      "Acknowledgements. This work is funded by the Cognitive Deep Learning\n",
      "research project in Siemens AG.\n",
      "22\n",
      "References\n",
      "References\n",
      "[1] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, Z. Ives, Dbpe-\n",
      "dia: A nucleus for a web of open data, The semantic web (2007) 722–735.\n",
      "[2] F. M. Suchanek, G. Kasneci, G. Weikum, Yago: a core of semantic knowl-\n",
      "edge, in: Proceedings of the 16th international conference on World Wide\n",
      "Web, ACM, 2007, pp. 697–706.\n",
      "[3] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, J. Taylor, Freebase: a\n",
      "collaboratively created graph database for structuring human knowledge,\n",
      "in: Proceedings of the 2008 ACM SIGMOD international conference on\n",
      "Management of data, AcM, 2008, pp. 1247–1250.\n",
      "[4] D. Vrandeˇci´c, M. Kr¨otzsch, Wikidata: a free collaborative knowledgebase,\n",
      "Communications of the ACM 57 (10) (2014) 78–85.\n",
      "[5] A. Singhal, Introducing the knowledge graph: things, not strings, Official\n",
      "google blog.\n",
      "[6] M. Nickel, K. Murphy, V. Tresp, E. Gabrilovich, A review of relational\n",
      "machine learning for knowledge graphs, Proceedings of the IEEE.\n",
      "[7] H. Ebbinghaus, U¨ber das ged¨achtnis: untersuchungen zur experimentellen\n",
      "psychologie, Duncker & Humblot, 1885.\n",
      "[8] R.C.Atkinson,R.M.Shiffrin,Humanmemory: Aproposedsystemandits\n",
      "control processes, Psychology of learning and motivation 2 (1968) 89–195.\n",
      "[9] L. R. Squire, Memory and brain.\n",
      "[10] E. Tulving, Episodic and semantic memory: Where should we go from\n",
      "here?, Behavioral and Brain Sciences 9 (03) (1986) 573–577.\n",
      "23\n",
      "[11] D. L. Greenberg, M. Verfaellie, Interdependence of episodic and seman-\n",
      "tic memory: evidence from neuropsychology, Journal of the International\n",
      "Neuropsychological society 16 (05) (2010) 748–753.\n",
      "[12] M.Nickel,V.Tresp,H.-P.Kriegel,Athree-waymodelforcollectivelearning\n",
      "on multi-relational data, in: Proceedings of the 28th international confer-\n",
      "ence on machine learning (ICML-11), 2011, pp. 809–816.\n",
      "[13] A. Cichocki, Era of big data processing: A new approach via tensor net-\n",
      "works and tensor decompositions, in: International Workshop on Smart\n",
      "Info-Media Systems in Asia (SISA-2013), 2013.\n",
      "[14] A. Cichocki, Tensor networks for big data analytic and large-scale opti-\n",
      "mization problems, in: Second Int. Conference on Engineering and Com-\n",
      "putational Schematics (ECM2013), 2013.\n",
      "[15] B. Yang, W.-t. Yih, X. He, J. Gao, L. Deng, Embedding entities and rela-\n",
      "tions for learning and inference in knowledge bases, International Confer-\n",
      "ence on Learning Representations (ICLR).\n",
      "[16] M. Nickel, L. Rosasco, T. Poggio, Holographic embeddings of knowledge\n",
      "graphs, in: Thirtieth AAAI Conference on Artificial Intelligence, 2016.\n",
      "[17] T. Trouillon, J. Welbl, S. Riedel, E´. Gaussier, G. Bouchard, Complex em-\n",
      "beddings for simple link prediction, in: International Conference on Ma-\n",
      "chine Learning, 2016, pp. 2071–2080.\n",
      "[18] T. A. Plate, Holographic reduced representations, IEEE Transactions on\n",
      "Neural Networks 6 (3) (1995) 623–641.\n",
      "[19] K. Hayashi, M. Shimbo, On the equivalence of holographic and complex\n",
      "embeddings for link prediction, CoRR abs/1702.05563.\n",
      "URL http://arxiv.org/abs/1702.05563\n",
      "[20] M. D. Ward, A. Beger, J. Cutler, M. Dickenson, C. Dorff, B. Radford,\n",
      "Comparing gdelt and icews event data, Analysis 21 (2013) 267–297.\n",
      "24\n",
      "[21] A. Schein, J. Paisley, D. M. Blei, H. Wallach, Bayesian poisson tensor\n",
      "factorization for inferring multilateral relations from sparse dyadic event\n",
      "counts, in: Proceedings of the 21th ACM SIGKDD International Confer-\n",
      "enceonKnowledgeDiscoveryandDataMining,ACM,2015,pp.1045–1054.\n",
      "[22] A.Bordes,N.Usunier,A.Garcia-Duran,J.Weston,O.Yakhnenko,Trans-\n",
      "latingembeddingsformodelingmulti-relationaldata,in: Advancesinneu-\n",
      "ral information processing systems, 2013, pp. 2787–2795.\n",
      "[23] X. Glorot, Y. Bengio, Understanding the difficulty of training deep feed-\n",
      "forward neural networks., in: Aistats, Vol. 9, 2010, pp. 249–256.\n",
      "[24] D. Kingma, J. Ba, Adam: A method for stochastic optimization, Pro-\n",
      "ceedings of the 3rd International Conference on Learning Representations\n",
      "(ICLR).\n",
      "[25] W.Deng,J.B.Aimone,F.H.Gage,Newneuronsandnewmemories: how\n",
      "doesadulthippocampalneurogenesisaffectlearningandmemory?, Nature\n",
      "reviews. Neuroscience 11 (5) (2010) 339.\n",
      "[26] O.Lazarov,C.Hollands,Hippocampalneurogenesis: learningtoremember,\n",
      "Progress in neurobiology 138 (2016) 1–18.\n",
      "[27] K. L. Spalding, O. Bergmann, K. Alkass, S. Bernard, M. Salehpour, H. B.\n",
      "Huttner, E. Bostr¨om, I. Westerlund, C. Vial, B. A. Buchholz, et al., Dy-\n",
      "namics of hippocampal neurogenesis in adult humans, Cell 153 (6) (2013)\n",
      "1219–1227.\n",
      "[28] J.L.McClelland,B.L.McNaughton,R.C.O’reilly,Whytherearecomple-\n",
      "mentarylearningsystemsinthehippocampusandneocortex: insightsfrom\n",
      "thesuccessesandfailuresofconnectionistmodelsoflearningandmemory.,\n",
      "Psychological review 102 (3) (1995) 419.\n",
      "[29] L. Nadel, A. Samsonovich, L. Ryan, M. Moscovitch, Multiple trace theory\n",
      "of human memory: computational, neuroimaging, and neuropsychological\n",
      "results, Hippocampus 10 (4) (2000) 352–368.\n",
      "25\n",
      "[30] H.Poon,P.Domingos,Sum-productnetworks: Anewdeeparchitecture,in:\n",
      "ComputerVisionWorkshops(ICCVWorkshops), 2011IEEEInternational\n",
      "Conference on, IEEE, 2011, pp. 689–690.\n",
      "26<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  22343,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Episodic quadruple inference', 'Episodic tensor reconstruction', 'Semantic memory generation', 'Episodic-to-semantic projection']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Embedding Models for Episodic Knowledge Graphs\n",
      "Yunpu Maa,b, Volker Trespa,b, Erik A. Daxberger 1c\n",
      "aSiemens AG, Corporate Technology, Munich, Germany\n",
      "bLudwig Maximilian University of Munich, Munich, Germany\n",
      "cETH Zurich\n",
      "Abstract\n",
      "In recent years a number of large-scale triple-oriented knowledge graphs have\n",
      "been generated and various models have been proposed to perform learning in\n",
      "those graphs. Most knowledge graphs are static and reflect the world in its\n",
      "current state. In reality, of course, the state of the world is changing: a healthy\n",
      "personbecomesdiagnosedwithadiseaseandanewpresidentisinaugurated. In\n",
      "thispaper,weextendmodelsforstaticknowledgegraphstotemporalknowledge\n",
      "graphs. This enables us to store episodic data and to generalize to new facts\n",
      "(inductive learning). We generalize leading learning models for static knowl-\n",
      "edge graphs (i.e., Tucker, RESCAL, HolE, ComplEx, DistMult) to temporal\n",
      "knowledge graphs. In particular, we introduce a new tensor model, ConT,\n",
      "with superior generalization performance. The performances of all proposed\n",
      "models are analyzed on two different datasets: the Global Database of Events,\n",
      "Language, and Tone (GDELT) and the database for Integrated Conflict Early\n",
      "Warning System (ICEWS). We argue that temporal knowledge graph embed-\n",
      "dings might be models also for cognitive episodic memory (facts we remember\n",
      "and can recollect) and that a semantic memory (current facts we know) can be\n",
      "generated from episodic memory by a marginalization operation. We validate\n",
      "this episodic-to-semantic projection hypothesis with the ICEWS dataset.\n",
      "Keywords: knowledge graph, temporal knowledge graph, semantic memory,\n",
      "episodic memory, tensor models\n",
      "1WorkdonewhileatSiemensAG.\n",
      "Preprint submitted to Journal of Web Semantics December 5, 2018\n",
      "8102\n",
      "ceD\n",
      "3\n",
      "]IA.sc[\n",
      "2v82200.7081:viXra\n",
      "1. Introduction\n",
      "In recent years a number of sizable Knowledge Graphs (KGs) have been\n",
      "developed, the largest ones containing more than 100 billion facts. Well known\n",
      "examples are DBpedia [1],YAGO [2], Freebase [3], Wikidata [4] and the Google\n",
      "KG [5]. Practical issues with completeness, quality and maintenance have been\n",
      "solved to a degree that some of these knowledge graphs support search, text\n",
      "understandingandquestionansweringinlarge-scalecommercialsystems[5]. In\n",
      "addition, statistical embedding models have been developed that can be used\n",
      "tocompressaknowledgegraph, toderiveimplicitfacts, todetecterrors, andto\n",
      "support the above mentioned applications. A recent survey on KG models can\n",
      "be found in [6].\n",
      "Mostknowledgegraphsarestaticandreflecttheworldatitscurrentstate. In\n",
      "reality, of course, the state of the world is changing: a healthy person becomes\n",
      "diagnosed with a disease and a new president is inaugurated. In this paper,\n",
      "we extend semantic knowledge graph embedding models to episodic/temporal\n",
      "knowledge graphs as an efficient way to store episodic data and to be able to\n",
      "generalize to new facts (inductive learning). In particular, we generalize lead-\n",
      "ing approaches for static knowledge graphs (i.e., constrained Tucker, DistMult,\n",
      "RESCAL, HolE, ComplEx) to temporal knowledge graphs. We test these mod-\n",
      "elsusingtwotemporalKGs. ThefirstoneisderivedfromtheIntegratedConflict\n",
      "Early Warning System (ICEWS) data set which describes interactions between\n",
      "nationsoverseveralyears. ThesecondoneisderivedfromtheGlobalDatabase\n",
      "ofEvents,LanguageandTone(GDELT)that,formorethan30years,monitors\n",
      "news media from all over the world. In the experiments, we analyze the gener-\n",
      "alization abilities to new facts that might be missing in the temporal KGs and\n",
      "also analyze to what degree a factorized KG can serve as an explicit memory.\n",
      "Weproposethatourtechnicalmodelsmightberelatedtothebrain’sexplicit\n",
      "memorysystems,i.e.,itsepisodicanditssemanticmemory. Bothareconsidered\n",
      "long-term memories and store information potentially over the life-time of an\n",
      "individual [7, 8, 9, 7]. The semantic memory stores general factual knowledge,\n",
      "2\n",
      "i.e., information we know, independent of the context where this knowledge\n",
      "was acquired and would be related to a static KG. Episodic memory concerns\n",
      "informationweremember andincludesthespatiotemporalcontextofevents[10]\n",
      "and would correspond to a temporal KG.\n",
      "primeMinisterOf\n",
      "10.1995 - 04.2002\n",
      "memberOf\n",
      "secretaryGeneralOf\n",
      "1955-m noem wberOf secretar 0y 1.G 0e 1.n 2er 0a 1l\n",
      "7-Of\n",
      "now\n",
      "memberOf secretaryGeneralOf mem 1b 9er 91O -f now 01.01.20se 0c 7-re 3t 1a.1ry\n",
      "2G.2e 0n 1e 6ralOf\n",
      "foreignMinisterOf\n",
      "01.2004 - 11.2006\n",
      "Figure1: Illustrationsof(left)asemanticknowledgegraphand(right)anepisodicknowledge\n",
      "graph. (Left)Everyarrowrepresentsa(subject,predicate,object)triple,withtheannotation\n",
      "of the arrow denoting the respective predicate. The triple (Ban Ki-moon, SecretaryOf, UN)\n",
      "is deleted, since the knowledge graph has been updated with the triple (Ant´onio Guterres,\n",
      "SecretaryOf, UN). (Right) Every arrow represents a (subject, predicate, object, timestamp)\n",
      "quadruple, where the arrow is both annotated with the respective predicate and timestamp.\n",
      "Here the quadruple involving is not deleted, since the attached timestamp reveals that the\n",
      "relationshipisnotvalidatpresent.\n",
      "An interesting question is how episodic and semantic memories are related.\n",
      "There is evidence that these main cognitive categories are partially dissociated\n",
      "from one another in the brain, as expressed in their differential sensitivity to\n",
      "brain damage. However, there is also evidence indicating that the different\n",
      "memory functions are not mutually independent and support one another [11].\n",
      "We propose that semantic memory can be derived from episodic memory by\n",
      "marginalization. Hereby we also consider that many episodes describe starting\n",
      "and endpoints of state changes. For example, an individual might become sick\n",
      "3\n",
      "with a disease, which eventually is cured. Similarly, a president’s tenure even-\n",
      "tuallyends. WestudyourhypothesisontheIntegratedConflictEarlyWarning\n",
      "System(ICEWS)dataset,whichcontainsmanyeventswithstartandenddates.\n",
      "Figure 1 compares semantic and episodic knowledge graphs. Furthermore, Fig-\n",
      "ure 2 illustrates the main ideas of building and modeling semantic and episodic\n",
      "knowledge graphs.\n",
      "Modeling\n",
      "Tensorization\n",
      "Graph Construction\n",
      "assigns entries as nodes and\n",
      "predicates as labeled edges\n",
      "for each timestamp\n",
      "Data Accumulation (e.g. GDELT)\n",
      "through knowledge extraction\n",
      "Websites Newspapers Social Media\n",
      "Figure 2: Illustration of the main idea behind the models presented in this paper. Step\n",
      "1: Knowledge is extracted from unstructured data, such as websites, newspapers or social\n",
      "media. Step 2: The knowledge graph is constructed, where entities are assigned as nodes,\n",
      "and predicates as labeled edges; note that there is a labeled edge for each timestamp. Step\n",
      "3: The knowledge graph is represented as a tensor; for semantic KGs, we obtain a 3-way\n",
      "tensor, storing (subject, predicate, object) triples, and for episodic KGs, we obtain a 4-way\n",
      "tensor, storing (subject, predicate, object, timestamp) quadruples. Step 4: The semantic\n",
      "and episodic tensors are decomposed and modeled via compositional or tensor models (see\n",
      "Section2).\n",
      "The paper is organized as follows. Section 2 introduces knowledge graphs,\n",
      "4\n",
      "the mapping of a knowledge graph to an adjacency tensor, and the statistical\n",
      "embedding models for knowledge graphs. We also describe how popular em-\n",
      "bedding models for KGs can be extended to episodic KGs. Section 3 shows\n",
      "experimental results on modelling episodic KGs. Finally, we present experi-\n",
      "ments on the possible relationships between episodic and semantic memory in\n",
      "Section 4.\n",
      "2. Model Descriptions\n",
      "A static or semantic knowledge graph (KG) is a triple-oriented knowledge\n",
      "representation. Here we consider a slight extension to the subject-predicate-\n",
      "object triple form by adding the value in the form (e,e,e ; Value), where\n",
      "s p o\n",
      "Value is a function of e,e,e and, e.g., can be a Boolean variable (True for 1,\n",
      "s p o\n",
      "False for 0) or a real number. Thus (Jack, likes, Mary; True) states that Jack\n",
      "(the subject or head entity) likes Mary (the object or tail entity). Note that e\n",
      "s\n",
      "ande representtheentitiesforsubjectindexsandobjectindexo. Tosimplify\n",
      "o\n",
      "notationwealsoconsidere tobeageneralizedentityassociatedwithpredicate\n",
      "p\n",
      "typewithindex p. For the episodic KGswe introduce e, whichisageneralized\n",
      "t\n",
      "entity for time t.\n",
      "TomodelastaticKG,weintroducethethree-waysemanticadjacencytensor\n",
      "χwherethetensorelementx istheassociatedValue ofthetriple(e,e,e ).\n",
      "s,p,o s p o\n",
      "One can also define a companion tensor Θ with the same dimensions as χ and\n",
      "χ\n",
      "with entries θ. Thus, the probabilistic model for the semantic tensor χ is\n",
      "s,p,o\n",
      "defined as P(x θ )=σ(θ ), where σ(x)=1/(1+exp( x)). Similarly,\n",
      "s,p,o s,p,o s,p,o\n",
      "| −\n",
      "the four-way temporal or episodic tensor has elements x which are the\n",
      "t,s,p,o\n",
      "E\n",
      "associated values of the quadruples (e,e,e,e ), with t=1,...,T. Therefore,\n",
      "t s p o\n",
      "the probabilistic model for episodic tensor is defined with the corresponding\n",
      "companion tensor Θ as\n",
      "E\n",
      "P(x θ )=σ(θ ). (1)\n",
      "t,s,p,o t,s,p,o t,s,p,o\n",
      "|\n",
      "We assume that each entity e has a unique latent representation a. In particu-\n",
      "lar,theembeddingapproachusedformodelingsemanticandepisodicknowledge\n",
      "5\n",
      "graphsassumesthatθsem =fsem(a,a,a ),andθepi =fepi(a,a,a,a ),\n",
      "s,p,o es ep eo t,s,p,o et es ep eo\n",
      "respectively. Here,theindicatorfunctionfsem/epi()isafunctiontobelearned.\n",
      "·\n",
      "Given a labeled dataset = (x,y ) m, latent representations and other\n",
      "D { i i }i=1\n",
      "parameters (denoted as ) are learned by minimizing the regularized logistic\n",
      "P\n",
      "loss\n",
      "m\n",
      "(cid:88)\n",
      "min log(1+exp( y θsem/epi))+λ 2. (2)\n",
      "P − i i ||P||2\n",
      "i=1\n",
      "In general, most KGs only contain positive triples; non-existing triples are nor-\n",
      "mally used as negative examples sampled with local closed- world assumption.\n",
      "Alternatively, we can minimize a margin-based ranking loss over the dataset\n",
      "such as\n",
      "(cid:88) (cid:88)\n",
      "min max(0,γ+σ(θsem/epi) σ(θsem/epi)), (3)\n",
      "P j − i\n",
      "i∈D+j∈D−\n",
      "where γ is the margin parameter, and and denote the set of positive\n",
      "+ −\n",
      "D D\n",
      "and negative samples, respectively.\n",
      "Therearedifferentwaysformodelingtheindicatorfunctionfepi()orfsem().\n",
      "· ·\n",
      "In this paper, we will only investigate multilinear models derived from tensor\n",
      "decompositions and compositional operations. We now describe the models in\n",
      "detail. Graphical illustrations of the described models are shown in Figure 3.\n",
      "p p\n",
      "p\n",
      "diag\n",
      "Re\n",
      "Im p\n",
      "Gp\n",
      "s o s o\n",
      "G G1 G2\n",
      "Gp Gt\n",
      "t t s p o s t o t s o t\n",
      "(a) (b) (c) (d) (e)\n",
      "Figure 3: Illustrations of (a) episodic Tucker, (b) episodic ComplEx (where • denotes con-\n",
      "traction),(c)RESCAL,(d)ConTand(e)Tree. Eachentityinthefigureisrepresentedasa\n",
      "circlewithtwoedges,sincetherepresentationforanentityeisae,i. Inaddition,Grepresents\n",
      "thecoretensorinTucker,Gp representsthematrixlatentrepresentationofpredicatepinthe\n",
      "RESCAL and Tree models, Gt represents the three-dimensional tensor latent representation\n",
      "oftimestamptintheConTmodel.\n",
      "Table 1 and Table 2 summarize notations used throughout this paper for\n",
      "6\n",
      "easyreference,whileTable3summarizesthenumberofparametersrequiredfor\n",
      "each model.2\n",
      "Table1: Summaryofthegeneralnotations.\n",
      "General\n",
      "Symbol Meaning\n",
      "e Entity for subject index s\n",
      "s\n",
      "e Entity for object index o\n",
      "o\n",
      "e Generalized entity for predicate index p\n",
      "p\n",
      "e Generalized entity for time index t\n",
      "t\n",
      "a Latent representation of entity e\n",
      "ei i\n",
      "a(e ) Latent representation of starting timestamp\n",
      "tstart\n",
      "a r -th element of a\n",
      "ei,ri i ei\n",
      "r˜ Rank/Dimensionality of a for i s,p,o\n",
      "ei\n",
      "∈{ }\n",
      "r˜ Rank/Dimensionality of a\n",
      "t et\n",
      "N Number of entities / predicates / timestamps\n",
      "e/p/t\n",
      "Tucker. First, we consider the Tucker model for semantic tensor decom-\n",
      "position of the form θsem = (cid:80)r˜ a a a gsem(r,r,r ). Here,\n",
      "s,p,o r1,r2,r3=1 es,r1 ep,r2 eo,r3 1 2 3\n",
      "gsem(r,r,r ) R are elements of the core tensor sem Rr˜×r˜×r˜. Similarly,\n",
      "1 2 3\n",
      "∈ G ∈\n",
      "the indicator function of a four-way Tucker model for episodic tensor decompo-\n",
      "sition is of the form\n",
      "(cid:88)r˜t (cid:88)r˜\n",
      "θepi =\n",
      "t,s,p,o\n",
      "r1=1r2,r3,r4=1\n",
      "a a a a gepi(r,r,r,r ), (4)\n",
      "et,r1 es,r2 ep,r3 eo,r4 1 2 3 4\n",
      "with a four dimensional core tensor epi Rr˜t×r˜×r˜×r˜. Note that this is a con-\n",
      "G ∈\n",
      "2For DistMult, ComplEx, and HolE it is required that r˜= r˜t. In our experiments (see\n",
      "Sections 3 and 4), in order to enable a fair comparison between the different models, we\n",
      "assume that the latent representations of entities, predicates, and time indices all have the\n",
      "samerank/dimensionality.\n",
      "7\n",
      "Table2: Summaryofthenotationsforsemanticandepisodicknowledgegraphs.\n",
      "Semantic knowledge graphs Episodic knowledge graphs\n",
      "Symbol Meaning Symbol Meaning\n",
      "χ Sem. adjacency tensor Epi. adjacency tensor\n",
      "E\n",
      "Θ Companion tensor of χ Θ Companion tensor of\n",
      "χ E\n",
      "E\n",
      "x Value of (e, e, e ) x Value of (e, e, e, e )\n",
      "s,p,o s p o t,s,p,o t s p o\n",
      "θsem Logit of (e, e, e ) θepi Logit of (e, e, e, e )\n",
      "s,p,o s p o t,s,p,o t s p o\n",
      "fsem() Sem. indicator function fepi() Epi. indicator function\n",
      "· ·\n",
      "sem Sem. core tensor epi Epi. core tensor\n",
      "G G\n",
      "gsem() Element of sem gepi() Element of epi\n",
      "· G · G\n",
      "straintTuckermodel,since,asinRESCAL,entitieshaveuniquerepresentations,\n",
      "independent of the roles as subject or object.\n",
      "RESCAL. Another model closely related to the semantic Tucker tensor\n",
      "decomposition is the RESCAL model, which has shown excellent performance\n",
      "in modelling KGs [12]. In RESCAL, subjects and objects have vector latent\n",
      "representations, while predicates have matrix latent representations. The indi-\n",
      "cator function of RESCAL for modeling semantic KGs takes the form θsem =\n",
      "s,p,o\n",
      "(cid:80)r˜\n",
      "a g (r,r )a,whereg (r,r )representsthematrixlatentrep-\n",
      "r1,r2=1 es,r1 p 1 2 eo,r2 p 1 2\n",
      "resentation for the predicate e. Then next two models, Tree and ConT, are\n",
      "p\n",
      "novel generalizations of RESCAL to episodic tensors.\n",
      "Tree. Fromapracticalperspective,traininganepisodicTuckertensormodel\n",
      "is very expensive since the computational complexity is approximately r˜4. Ten-\n",
      "sor networks provide a general and flexible framework to design nonstandard\n",
      "tensor decompositions [13, 14]. One of the simplest tensor networks is a tree\n",
      "tensordecomposition( )oftheepisodicindicatorfunction,whichisillustrated\n",
      "T\n",
      "in compositional operations. We now describe the models in detail. Graphical\n",
      "illustrations of the described models are shown in Figure 3(e). Therefore, we\n",
      "proposeatreetensordecomposition( )oftheepisodicindicatorfunction. The\n",
      "T\n",
      "tree is partitioned into two subtrees and, wherein subject e and time\n",
      "1 2 s\n",
      "T T T\n",
      "8\n",
      "e reside in, while object e and an auxiliary time e reside in. and\n",
      "t 1 o t 2 1 2\n",
      "T T T T\n",
      "are connected with e through two core tensors and. Thus, the indicator\n",
      "p 1 2\n",
      "G G\n",
      "function can be written as\n",
      "(cid:88)r˜t (cid:88)r˜\n",
      "θepi =\n",
      "t,s,p,o\n",
      "r1,r6=1r2,r3,r4,r5=1\n",
      "a a g (r,r,r )g (r,r )g (r,r,r )a a. (5)\n",
      "et,r1 es,r2 1 1 2 3 p 3 4 2 4 5 6 eo,r5 et,r6\n",
      "Within,wereducethefour-waycoretensorinTuckerintotwothree-dimensional\n",
      "T\n",
      "tensors and, so that the computational complexity of is approximately\n",
      "1 2\n",
      "G G T\n",
      "r˜3.\n",
      "ConT. ConT is another generalization of the RESCAL model to episodic\n",
      "tensors with reduced computational complexity of approximately r˜3. The idea\n",
      "is that another way of reducing the complexity is by contracting indices of the\n",
      "core tensor. Therefore, we contract the from Tucker with the time index\n",
      "G\n",
      "givingathree-waycoretensor foreachtimeinstance. Theindicatorfunction\n",
      "t\n",
      "G\n",
      "takes the form\n",
      "r˜\n",
      "(cid:88)\n",
      "θepi = a a a g (r,r,r ). (6)\n",
      "t,s,p,o es,r1 ep,r2 eo,r3 t 1 2 3\n",
      "r1,r2,r3=1\n",
      "In this model, the tensor resembles the relation-specific matrix from\n",
      "t p\n",
      "G G\n",
      "RESCAL.Later,wewillseethatConTisasuperiormodelformodelingepisodic\n",
      "knowledge graphs due to the representational flexibility of its high-dimensional\n",
      "tensor for the time index.\n",
      "t\n",
      "G\n",
      "Even though the complexity of Tree and ConT is reduced as compared to\n",
      "episodicTucker,thethree-dimensionalcoretensormightcauserapidoverfitting\n",
      "during training. Therefore, we next propose episodic generalization of compo-\n",
      "sitional models, such as DistMult [15], HolE [16] and ComplEx [17]. For those\n",
      "models, the number of parameters only increases linearly with the rank.\n",
      "DistMult. DistMult[15]isasimplegeneralizationoftheCPmodel,byen-\n",
      "forcingtheconstraintthatentitiesshouldhaveuniquerepresentations. Episodic\n",
      "DistMult takes the form θepi = (cid:80)r˜ λ a a a a. Here, we require\n",
      "t,s,p,o i=1 i et,i es,i ep,i eo,i\n",
      "that vector latent representations of entities, predicates, and timestamps have\n",
      "9\n",
      "the same rank. DistMult is a special case of Tucker having a core tensor with\n",
      "only diagonal elements λ.\n",
      "i\n",
      "HolE. Holographic embedding (HolE) [16] is a state-of-art link prediction\n",
      "andknowledgegraphcompletionmethod,whichisinspiredbyholographicmod-\n",
      "els of associative memory.\n",
      "HolE uses circular correlation to generate a compositional representation\n",
      "frominputse ande. TheindicatorofHolEreadsθsem =a (a (cid:63)a ),where\n",
      "s o s,p,o ep· es eo\n",
      "(cid:63):Rd Rd Rd denotesthecircularcorrelation[a(cid:63)b] =(cid:80)d−1a b.\n",
      "× → k i=0 i (k+i)modd\n",
      "We define the episodic extension of HolE as\n",
      "θepi =a (cid:0) a (cid:63)(a (cid:63)a )(cid:1). (7)\n",
      "t,s,p,o et · ep es eo\n",
      "As argued by [16], HolE employs a holographic reduced representation [18]\n",
      "to store and retrieve the predicates from e and e. Analogously, episodic HolE\n",
      "s o\n",
      "should be able to retrieve the stored timestamps from e, e and e. In the se-\n",
      "p s o\n",
      "manticcase,e canberetrievedifexistingtriplerelationsarestoredviacircular\n",
      "p\n",
      "(cid:80)\n",
      "convolution,andsuperpositionintherepresentationa = a a,\n",
      "∗ eo (s,p)∈So ep∗ es\n",
      "where is the set of all true triples given e. This is based on the fact that\n",
      "o o\n",
      "S\n",
      "a(cid:63)a δ [16]. Analogously, the stored timestamp e for an event can be re-\n",
      "t\n",
      "≈\n",
      "trieved if all existing episodic events are stored via, and superposition in the\n",
      "∗\n",
      "(cid:80)\n",
      "representation of e, a = a (a a ), where is the set of\n",
      "o eo (t,s,p)∈So et ∗ ep ∗ es So\n",
      "all true quadruples (t,s,p,o) given e. However, high order circular correla-\n",
      "o\n",
      "tion/convolution will increase the inaccuracy of retrieval. Another motivation\n",
      "forourepisodicextension(7)isthatacompositionaloperatoroftheforma f˜\n",
      "et·\n",
      "allows a projection from episodic memory to semantic memory, to be detailed\n",
      "later.\n",
      "ComplEx. Complex embedding (ComplEx) [17] is another state-of-art\n",
      "method closely related to HolE. It can accurately describe both symmetric and\n",
      "antisymmetric relations. HolE is a special case of ComplEx with imposed con-\n",
      "jugatesymmetryonembeddings[19]. Thus,ComplExhasmoredegreesoffree-\n",
      "dom, if compared to HolE. For the semantic complex embedding, the indicator\n",
      "(cid:16) (cid:17)\n",
      "function is θsem = Re (cid:80)r˜a a,a¯ with complex valued a and where\n",
      "s,p,o i es,i ep,i eo,i\n",
      "10\n",
      "the bar indicates the complex conjugate. To be consistent with the episodic\n",
      "HolE, the episodic complex embedding is defined as3\n",
      "(cid:32) (cid:33)\n",
      "r˜\n",
      "(cid:88)\n",
      "θepi =Re a a a,a¯. (8)\n",
      "t,s,p,o et,i es,i ep,i eo,i\n",
      "i\n",
      "3. Experiments on Episodic Models\n",
      "We investigate the proposed tensor and compositional models with experi-\n",
      "ments which are evaluated on two datasets:\n",
      "ICEWS. The Integrated Conflict Early Warning System (ICEWS) dataset\n",
      "[20]isanaturalepisodicdatasetrecordingdyadiceventsbetweendifferentcoun-\n",
      "tries. An example entry could be (Turkey, Syria, Fight, 12/25/2014). These\n",
      "dyadiceventsareaggregatedintoafour-waytensor with258entities, 20rela-\n",
      "E\n",
      "tiontypes,and72timestamps,whichhasintotal320,118positive(e,e,e,e )\n",
      "t s p o\n",
      "quadruples4. Thisdatasetwasfirstcreatedandusedin[21]. FromthisICEWS\n",
      "dataset, a semantic tensor is generated by extracting consecutive events that\n",
      "last until the last timestamp, constituting the current 5 semantic facts of the\n",
      "world.\n",
      "GDELT. The Global Database of Events, Language and Tone (GDELT)\n",
      "[20] monitors the world’s news media in broadcast, print and web formats from\n",
      "all over the world, daily since January 1, 1979 6. We use GDELT as a large\n",
      "episodic dataset. For our experiments, GDELT data is collected from January\n",
      "1, 2012 to December 31, 2012 (with a temporal granularity of 24 hrs). These\n",
      "events are aggregated into an episodic tensor with 1100 entities, 180 relation\n",
      "E\n",
      "3One can show that Eq. (7) is equivalent to Eq. (8) by converting it to the frequency\n",
      "domain[19]. Then,θ te,p si\n",
      ",p,o\n",
      "∝ωT et(ω¯ep(cid:12)ω¯es(cid:12)ωeo),whereω=F(a)∈Cr˜ arethediscrete\n",
      "Fouriertransformsofembeddingsa,andusingthefactthatωisconjugatesymmetricforreal\n",
      "vectora.\n",
      "4Notethatforanepisodiceventthedatasetcontainsallthequadruples(eti,es,ep,e0)for\n",
      "ti∈{tstart,tstart+1,···,t end−1,t end}.\n",
      "5Current alwaysindicatesthelasttimestamp/timestampsoftheappliedepisodicKGs.\n",
      "6https://www.gdeltproject.org/about.html\n",
      "11\n",
      "Table3: Numberofparametersfordifferentmodelsandtheruntimeofonetrainingepochon\n",
      "theGDELTdataset.\n",
      "Runtime\n",
      "Model Semantic Episodic Complexity rank40 rank60 rank150\n",
      "DistMult (Ne+Np+1)r˜ (Ne+Np+Nt+1)r˜ O(r˜) 35.2s 36.4s 53.7s\n",
      "HolE (Ne+Np)r˜ (Ne+Np)r˜ O(r˜logr˜) 42.8s 43.2s 59.0s\n",
      "ComplEx 2(Ne+Np)r˜ 2(Ne+Np+Nt)r˜ O(r˜) 40.1s 42.4s 57.5s\n",
      "Tree − Ner˜+Npr˜2+(Nt+2r˜2)r˜t O(r˜3) 133.6s 160.2s −\n",
      "ConT − (Ne+Np)r˜+Ntr˜3 O(r˜3) 95.4s 226.1s −\n",
      "Tucker (Ne+Np)r˜+r˜3 (Ne+Np)r˜+(Nt+r˜3)r˜t O(r˜4) 144.2s 387.9s −\n",
      "types, and 366 timestamps, which has in total 2,563,561 positive (e,e,e,e )\n",
      "t s p o\n",
      "quadruples.\n",
      "We assess the quality of episodic information retrieval on both datasets for\n",
      "the proposed tensor and compositional models. Since both episodic datasets\n",
      "only consist of positive quadruples, we generated negative episodic instances\n",
      "followingtheprotocolofcorruptingsemantictriplesgivenbyBordes[22]: nega-\n",
      "tiveinstancesofanepisodicquadruple(e,e,e,e )aredrawnbycorruptingthe\n",
      "s p o t\n",
      "objecte o toe o(cid:48) orthetimestampe t toe t(cid:48),meaningthat(e s,e p,e o(cid:48),e t)servesas\n",
      "a negative evidence of the episodic event at time instance e t, and (e s,e p,e o,e t(cid:48))\n",
      "is a true fact which cannot be correctly recalled at time instance e t(cid:48). During\n",
      "training, for each positive sample in a batch we assigned two negative samples\n",
      "with corrupted object or corrupted subject.\n",
      "The model performance is evaluated using the following scores. To retrieve\n",
      "the occurrence time, for each true quadruple, we replace the time index e with\n",
      "t\n",
      "every other possible time index e t(cid:48), compute the value of the indicator function\n",
      "θepi, and rank them in a decreasing order. We filter the ranking as in [22]\n",
      "t(cid:48),s,p,o\n",
      "by removing all quadruples where x t(cid:48),s,p,o =1 and t=t(cid:48), in order to eliminate\n",
      "(cid:54)\n",
      "ambiguity during episodic information retrieval. Similarly, we evaluated the\n",
      "retrieval of the predicate between a given subject and object at a certain time\n",
      "instancebycomputingandrankingtheindicatorθepi. Wealsoevaluatedthe\n",
      "t,s,p(cid:48),o\n",
      "12\n",
      "retrieval of entities by ranking and averaging the filtered indicators θ t,s(cid:48),p,o and\n",
      "θ t,s,p,o(cid:48). Tomeasurethegeneralizationabilityofthemodels,wereportdifferent\n",
      "measures of the ranking: mean reciprocal rank (MRR), and Hits@n on the test\n",
      "dataset.\n",
      "The datasets were split into train, validation, and test sets that contain the\n",
      "most frequently appearing entities in the episodic knowledge graphs. Training\n",
      "was performed by minimizing the logistic loss (2), and was terminated using\n",
      "early stopping on the validation dataset by monitoring the filtered MRR recall\n",
      "scores every 50,100 epochs depending on the models, where the maximum\n",
      "{ }\n",
      "training duration was 500 epochs. This ensures that the generalization ability\n",
      "ofuniquelatentrepresentationsofentitiesdoesn’tsufferfromoverfitting. Before\n",
      "training,allmodelparametersareinitializedusingXavierinitialization[23]. We\n",
      "alsoapplyanl2normpenaltyonallparametersforregularizationpurposes(see\n",
      "Eq. (2)).\n",
      "In Table 3 we summarize the runtime for one training epoch on the GDELT\n",
      "dataset for different models at ranks r˜ = r˜ 40,60,150. All experiments\n",
      "t\n",
      "∈ { }\n",
      "were performed on a single Tesla K80 GPU. In the following experiments, for\n",
      "compositional models we search rank in 100,150, while for tensor models we\n",
      "{ }\n",
      "search optimal rank in 40,50,60 since larger ranks could lead to overfitting\n",
      "{ }\n",
      "rapidly. Loss function is minimized with Adam method [24] with the learning\n",
      "rate selected from 0.001,1e 4,5e 5.\n",
      "{ − − }\n",
      "We first assess the filtered MRR, Hits@1, Hits@3, and Hits@10 scores of\n",
      "inferring missing entities and predicates on the GDELT test dataset. Table 4\n",
      "summarizes the results. Generalizations on the test dataset indicate the induc-\n",
      "tive reasoning capability of the proposed models. This generalization can be\n",
      "useful for the completion of evolving KGs with missing records, such as clinical\n",
      "datasets. Itcanbeseenthattensormodelsareabletooutperformcompositional\n",
      "modelsconsistentlyonbothentityandpredicatepredictiontasks. ConThasthe\n",
      "best inference results on the entity-related tasks, while Tucker performs better\n",
      "on the predicate-related tasks. The superior Hits@1 result of ConT on the en-\n",
      "titypredictionindicatesthatthereareeasilytobefittedentitiesintheGDELT\n",
      "13\n",
      "Table 4: Filtered results of inferring missing entities and predicates of episodic quadruples\n",
      "evaluatedontheGDELTdataset.\n",
      "Entity Predicate\n",
      "Method MRR @1 @3 @10 MRR @1 @3 @10\n",
      "DistMult 0.182 6.55 19.77 43.70 0.269 12.65 30.29 59.40\n",
      "HolE 0.177 6.67 18.95 41.84 0.256 11.81 28.35 57.73\n",
      "ComplEx 0.172 6.54 17.52 41.56 0.255 12.05 27.75 56.60\n",
      "Tree 0.196 8.17 21.00 44.65 0.274 13.30 30.66 60.05\n",
      "Tucker<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  49120,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Yunpu Ma', 'Volker Trespa', 'Erik A. Daxberger']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: intheGDELT\n",
      "13\n",
      "Table 4: Filtered results of inferring missing entities and predicates of episodic quadruples\n",
      "evaluatedontheGDELTdataset.\n",
      "Entity Predicate\n",
      "Method MRR @1 @3 @10 MRR @1 @3 @10\n",
      "DistMult 0.182 6.55 19.77 43.70 0.269 12.65 30.29 59.40\n",
      "HolE 0.177 6.67 18.95 41.84 0.256 11.81 28.35 57.73\n",
      "ComplEx 0.172 6.54 17.52 41.56 0.255 12.05 27.75 56.60\n",
      "Tree 0.196 8.17 21.00 44.65 0.274 13.30 30.66 60.05\n",
      "Tucker 0.204 8.93 21.85 46.35 0.275 12.69 31.35 60.70\n",
      "ConT 0.233 13.85 24.65 42.96 0.263 12.83 29.27 57.30\n",
      "Table 5: Filtered results for entities and predicates recollection/prediction evaluated on the\n",
      "ICEWSdataset.\n",
      "Entity Predicate\n",
      "Method MRR @1 @3 @10 MRR @1 @3 @10\n",
      "DistMult 0.222 9.72 22.48 52.32 0.520 33.73 62.25 91.13\n",
      "HolE 0.229 9.85 23.49 54.21 0.517 31.55 65.47 93.59\n",
      "ComplEx 0.229 8.94 23.53 57.72 0.506 30.99 61.46 93.44\n",
      "Tree 0.205 10.48 19.84 42.81 0.554 36.62 67.25 94.70\n",
      "Tucker 0.257 12.88 27.10 54.43 0.563 36.96 69.55 95.43\n",
      "ConT 0.264 15.71 29.60 46.67 0.557 38.12 67.76 87.71\n",
      "dataset along the timestamps. In fact, the GDELT dataset is unbalanced, and\n",
      "episodic quadruples related to certain entities dominate in the episodic Knowl-\n",
      "edgegraph,suchasquadruplescontainingtheentitiesUSA,orUN.Experiment\n",
      "results on balanced and extremely sparse episodic dataset will be reported in\n",
      "the following.\n",
      "Next,Table5showstheMRR,Hits@1,Hits@3,andHits@10scoresofinfer-\n",
      "ring missing entities and predicates on the ICEWS test dataset. Similarly, we\n",
      "can read that tensor models outperform compositional models on both missing\n",
      "entityandpredicateinferencetasks. ThesuperiorHits@1resultofConTforthe\n",
      "14\n",
      "missingentitypredictionindicatesagainthattheICEWSdatasetisunbalanced,\n",
      "and episodic quadruples related to certain entities dominate.\n",
      "Table 6: Filtered recall scores for entities and timestamps recollection on the ICEWS (rare)\n",
      "trainingdataset.\n",
      "Timestamp Entity\n",
      "Method Rank MRR @3 MRR @3\n",
      "DistMult 200 0.257 27.0 0.211 21.9\n",
      "HolE 200 0.216 20.8 0.179 16.3\n",
      "ComplEx 200 0.354 40.3 0.301 33.2\n",
      "Tree 40 0.421 55.3 0.314 35.7\n",
      "Tucker 40 0.923 98.9 0.893 97.1\n",
      "ConT 40 0.982 99.7 0.950 97.9\n",
      "The recollection of the exact occurrence time of a significant past event\n",
      "(e.g. unusual, novel, attached with emotion) is also an important capability\n",
      "of episodic cognitive memory function. In order to manifest this perspective\n",
      "of proposed models, Table 6 shows the filtered MRR, and Hits@3 scores for\n",
      "the timestamps and entities recollection on the episodic ICEWS (rare) training\n",
      "dataset, where rank column registers the optimal and minimum rank r˜ = r˜\n",
      "t\n",
      "havingtheoutstandingrecallscores. Figure4furtherdisplaysthefilteredMRR\n",
      "score as a function of rank. Unlike the original ICEWS, which contains many\n",
      "consecutive events that last from the first to the last timestamp leading to\n",
      "unreasonably high filtered timestamp recall scores, this ICEWS (rare) dataset\n",
      "consists of rare temporal events that happen less than three times throughout\n",
      "the whole time and starting points of events.\n",
      "The outstanding performance of ConT compared with other compositional\n",
      "models indicates the importance of large dimensionality of time latent repre-\n",
      "sentation for the episodic tensor reconstruction / episodic memory recollection.\n",
      "Recall that for ConT the real dimension of the latent representation of time is\n",
      "actuallyr˜3 afterflattening. Thisflexiblelatentrepresentationfortimecould\n",
      "t\n",
      "G\n",
      "15\n",
      "compress almost all the semantic triples that occur at a certain instance 7.\n",
      "Figure 4: Filtered MRR scores vs. rank for the entities (left) and timestamps (right) recol-\n",
      "lectionontheICEWS(rare)trainingdataset.\n",
      "4. Semantic Memory from Episodic Memory with Marginalization\n",
      "We already discussed that a semantic KG might be related to a human\n",
      "semanticmemoryandthatanepisodicKGmightberelatedtoahumanepisodic\n",
      "memory. It has been speculated that episodic and semantic memory must be\n",
      "closely related, and that semantic memory is generated from episodic memory\n",
      "by some training process [28, 29]. As a very simple implementation of that\n",
      "idea, we propose that a semantic memory could be generated from episodic\n",
      "memory by marginalizing time. Thus, both types of memories would rely on\n",
      "identical representations and the marginalization step can be easily performed:\n",
      "Since probabilistic tensor models belong to the classes of sum-product nets, a\n",
      "marginalization simply means an integration over all time representations.\n",
      "Thus,inthesecondsetofexperiments,wetestthehypothesisthatsemantic\n",
      "7Thisobservationhasitsbiologicalcounterpart. Infact,theentorhinalcortex,whichplays\n",
      "an important role in the formation of episodic memory, is the main part of the adult hip-\n",
      "pocampusthatshowsneurogenesis[25]. Inanadulthuman,approximately700newneurons\n",
      "areaddedperdaythroughhippocampalneurogenesis,whicharebelievedtoperformsensory\n",
      "andspatialinformationencoding,aswellastemporalseparationofevents[26,27].\n",
      "16\n",
      "memory can be derived from episodic memory by projection. In other words,\n",
      "a semantic knowledge graph containing current semantic facts can be approx-\n",
      "imately constructed after modeling a corresponding episodic knowledge graph\n",
      "via marginalization. A marginalization can be performed by activating all time\n",
      "index neurons, i.e., summing over all a, since, e.g., Tucker decompositions are\n",
      "et\n",
      "an instance of a so-called sum-product network [30]. However, events having\n",
      "startaswellasendtimestampscannotsimplybeintegratedintoourcurrent se-\n",
      "mantic knowledge describing what we know now. For example, (Ban Ki-moon,\n",
      "SecretaryOf,UN)isnotconsistentwithwhatweknow currently. Toresolvethis\n",
      "problem, we introduce two types of time indices, e and e, having the\n",
      "tstart tend\n",
      "latent representations a(e ) and a(e ), respectively. Those time indices\n",
      "tstart tend\n",
      "can be used to construct the episodic tensor aggregating the start times-\n",
      "start\n",
      "E\n",
      "tamps of consecutive events, as well as the episodic tensor aggregating the\n",
      "end\n",
      "E\n",
      "end timestamps8.\n",
      "For the projection, instead of only summing over a(e ), we also subtract\n",
      "tstart\n",
      "the sum over a(e ). In this way, we can achieve the effect that events that\n",
      "tend\n",
      "have terminated already (i.e., have an end time index smaller than the current\n",
      "time index) are not integrated into the current semantic facts. Now, to test our\n",
      "hypothesis that this extended projection allows us to derive semantic memory\n",
      "fromepisodicmemory,wetrainedHolE,DistMult,ComplEx,ConT,andTucker\n",
      "on the episodic tensors and as well as on the semantic tensor χ\n",
      "start end\n",
      "E E\n",
      "derived from ICEWS. Note that only these models allow projection, since their\n",
      "indicator functions can be written in the form θepi =a f˜, where f˜can be\n",
      "t,s,p,o et ·\n",
      "arbitrary function of a, a, and a depending on the model choice9. The\n",
      "es ep eo\n",
      "8E.g., if the duration of a triple event (es,ep,eo) lasts from tstart to t end, the quadruple\n",
      "(es,ep,eo,etstart) is stored in Estart, while (es,ep,eo,etend) is stored E\n",
      "end\n",
      "only if t\n",
      "end\n",
      "<T\n",
      "(whereT isthelasttimestamp). Inotherwords,eventsthatlastuntilthelasttimestampdo\n",
      "notpossesse.\n",
      "end\n",
      "9For ConT, θ te,p si\n",
      ",p,o\n",
      "=flatten(gt)·(aes ⊗aep ⊗aeo), where ⊗ denotes the outer product.\n",
      "ForComplEx,θ te,p si,p,o=Re(aet)·Re(aes(cid:12)aep(cid:12)a¯eo)−Im(aet)·Im(aes(cid:12)aep(cid:12)a¯eo),where\n",
      "(cid:12) denotes the Hadamard product. The Tree model cannot be written in this form since et\n",
      "17\n",
      "model parameters are optimized using the margin-based ranking loss (3)10.\n",
      "Training was first performed on the episodic tensor, and then on\n",
      "start end\n",
      "E E\n",
      "with fixed a, a, and a obtained from the training on, since we\n",
      "es ep eo Estart\n",
      "assume that latent representations for subject, object, and predicate of a con-\n",
      "secutive event do not change during the event. Note that after training in this\n",
      "way, we could recall the starting and terminal point of a consecutive event (see\n",
      "the episodic tensor reconstruction experiments in Section 3), or infer a cur-\n",
      "rent semantic fact solely from the latent representations instead of rule-based\n",
      "reasoning.\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0 5 10 15 20 25 30\n",
      "Rank\n",
      "llaceR\n",
      "ConT Icews Train Dataset\n",
      "0.9\n",
      "0.8\n",
      "0.7\n",
      "0.6\n",
      "Semantic 0.5\n",
      "Start\n",
      "Start-End 0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "0.00 5 10 15 20 25\n",
      "Rank\n",
      "llaceR\n",
      "Tucker Icews Train Dataset\n",
      "Semantic\n",
      "Start\n",
      "Start_End\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.00 10 20 30 40 50 60 70\n",
      "Rank\n",
      "llaceR\n",
      "ComplEx Icews Train Dataset\n",
      "1.0\n",
      "0.9\n",
      "0.8\n",
      "0.7\n",
      "Semantic 0.6\n",
      "Start\n",
      "Start_End 0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1 10 20 30 40 50 60 70\n",
      "Rank\n",
      "llaceR\n",
      "HolE Icews Train Dataset\n",
      "Semantic\n",
      "Start\n",
      "Start_End\n",
      "Figure5: Recallscoresvs. rankfortheepisodic-to-semanticprojectionontheICEWS\n",
      "dataset with two different projection methods.\n",
      "To evaluate the projection, we compute the recall and area under precision-\n",
      "recall-curve(AUPRC)scoresfortheprojectionatdifferentranksontheICEWS\n",
      "residesinbothsubtreesT1 andT2.\n",
      "10Fortheprojectionexperiment,weomitthesigmoidfunctioninEq.(3),trainandinterpret\n",
      "themultilinearindicatorθ te,p si,p,o=aet·f˜(aes,aep,aeo)directlyastheprobabilityofepisodic\n",
      "quadruple. Onlyinthiswayoftraining,aprojectionismathematicallylegitimate.\n",
      "18\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "5 10 15 20 25 30\n",
      "Rank\n",
      "CRPUA\n",
      "ConT Icews Train Dataset\n",
      "0.55\n",
      "0.50\n",
      "0.45\n",
      "Semantic 0.40\n",
      "Start 0.35\n",
      "Start-End\n",
      "0.30\n",
      "0.25\n",
      "0.20\n",
      "0.150 5 10 15 20 25\n",
      "Rank\n",
      "CRPUA\n",
      "Tucker Icews Train Dataset\n",
      "Semantic\n",
      "Start\n",
      "Start_End\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0 10 20 30 40 50 60 70\n",
      "Rank\n",
      "CRPUA\n",
      "ComplEx Icews Train Dataset\n",
      "0.60\n",
      "0.55\n",
      "0.50\n",
      "Semantic 0.45\n",
      "Start 0.40\n",
      "Start_End\n",
      "0.35\n",
      "0.30\n",
      "0.25\n",
      "0.20 10 20 30 40 50 60 70\n",
      "Rank\n",
      "CRPUA\n",
      "HolE Icews Train Dataset\n",
      "Semantic\n",
      "Start\n",
      "Start_End\n",
      "Figure6: AUPRCscoresvs. rankfortheepisodic-to-semanticprojectionontheICEWS\n",
      "dataset with two different projection methods.\n",
      "training dataset, and compare them with the scores obtained from training\n",
      "the semantic tensor separately. The semantic dataset contains positive triples,\n",
      "which are episodic events that continue until the last (current) timestamp,\n",
      "e.g. (Ant´onio Guterres, SecretaryOf, UN, True), along with negative triples\n",
      "extracted from already terminated episodic events, e.g. (Ban Ki-moon, Secre-\n",
      "taryOf,UN,False). Duringthetestphaseofprojection,atriplefromtheseman-\n",
      "tic dataset is given with non-specified time index, e.g. (e,e,e,True/False,t).\n",
      "s p o\n",
      "Then, for the first method considering only the starting point of an episodic\n",
      "event, the projection to semantic space is computed as\n",
      "T\n",
      "θproj =[ (cid:88) a(e )] f˜, (9)\n",
      "s,p,o tstart ·\n",
      "tstart=1\n",
      "while for the second method considering both starting and terminal points, the\n",
      "projection is computed as\n",
      "(cid:34) (cid:35)\n",
      "T T\n",
      "θproj = (cid:88) a(e ) (cid:88) a(e ) f˜. (10)\n",
      "s,p,o tstart − tend ·\n",
      "tstart=1 tend=1\n",
      "19\n",
      "Then, the scores are evaluated by taking the label of the given semantic triple\n",
      "asthetarget,andtakingθproj astheprediction. Thegoalofthistestistocheck\n",
      "s,p,o\n",
      "howwellthealgorithmscanprojectagivenconsecutiveevent(e,e,e,t t )\n",
      "s p o start end\n",
      "···\n",
      "to semantic knowledge space using only the marginalized latent representation\n",
      "of time. All other experimental settings are similar to those in Section 3, and\n",
      "theexperimentswererepeatedfourtimesondifferentsampledtrainingdatasets.\n",
      "Figure 5 shows the recall scores for the two different projection methods on\n",
      "the training dataset in comparison to the separately trained semantic dataset.\n",
      "Due to limited space, we only show four models: ConT, Tucker, ComplEx, and\n",
      "HolE. As we can see, only the marginalization considering both starting and\n",
      "terminal time indices allows a reasonable projection from episodic memory to\n",
      "the current semantic memory. Again, ConT11 exhibits the best performance,\n",
      "with its recall score saturating after r˜ 15. In contrast, HolE shows insuffi-\n",
      "≈\n",
      "cient projection quality with sizable errors, especially at small ranks, which is\n",
      "due to its higher-order encoding noise. To show that the two types of latent\n",
      "representationsoftimedonotsimplyeliminateeachotherforacorrectepisodic\n",
      "projection,Figure6showstheAUPRCscoresevaluatedonthetrainingdataset.\n",
      "Overall,thisexperimentsupportstheideathatsemanticmemoryisalong-term\n",
      "storage for episodic memory, where the exact timing information is lost.\n",
      "For a fair comparison, in the last experiment we report the recall scores of\n",
      "the semantic models obtained by projecting the episodic models with respect\n",
      "to the temporal dimension. We compare two projection methods, the Start\n",
      "projection which only considers the staring point of episodic events (see Eq. 9),\n",
      "andtheStart-Endprojectionwhichtakesboththestartingandterminalpoints\n",
      "of episodic events into consideration. In addition, we report the recall scores on\n",
      "two semantic datasets. The first one contains genuine semantic facts, while the\n",
      "seconddatasetcontainsfalsesemantictripleswhichshouldalreadyberuledout\n",
      "11Note that since ConT doesn’t have a direct semantic counterpart, we instead use the\n",
      "semanticresultsobtainedusingRESCAL.ThisisreasonablesinceConTcanbeviewedasa\n",
      "high-dimensional(i.e.,episodic)generalizationofRESCAL.\n",
      "20\n",
      "Table 7: Filtered and raw Hits@10 scores for the episodic-to-semantic projection. Two pro-\n",
      "jection methods, Start (Eq. 9), Start-End (Eq. 10), are compared. Furthermore, semantic\n",
      "ICEWSdatasetwithgenuinesemantictriples,andsemanticICEWSdatasetwithfalsetriples\n",
      "are used for the projection experiments. Various projection scores are compared with the\n",
      "scores which are obtained by directly modeling the semantic ICEWS dataset with genuine\n",
      "semantictriples.\n",
      "Start Start-End Start(false) Start-End(false) Semantic\n",
      "Method Filter Raw Filter Raw Filter Raw Filter Raw Filter Raw\n",
      "DistMult 3.8 3.6 5.6 5.0 4.0 3.8 3.8 3.6 59.3 32.4\n",
      "HolE 5.8 5.4 5.5 5.1 4.7 4.5 5.6 5.2 56.1 31.3\n",
      "ComplEx 4.1 3.7 4.9 4.4 3.9 3.7 3.8 3.6 60.1 29.4\n",
      "Tucker 14.8 13.1 15.1 13.4 11.3 10.3 11.8 10.9 46.5 23.7\n",
      "ConT 30.9 24.6 40.8 30.3 23.0 19.9 22.6 19.3 43.8 20.4\n",
      "through the projection.\n",
      "Two different projections are performed on two semantic datasets, the gen-\n",
      "uine one and the false one. Theoretically, the recall scores on the genuine\n",
      "semantic dataset should be higher than those on the false dataset. Thus, the\n",
      "model hyper-parameters are chosen by monitoring the difference between the\n",
      "recall scores Hits@10 on the genuine and false semantic datasets.\n",
      "Table. 7 reports the filtered and raw Hits@10 metrics for different models,\n",
      "projection methods, and datasets. Moreover, we also compare the projection\n",
      "withtherecallscoresobtainedbydirectlymodelingthegenuinesemanticdataset\n",
      "usingthecorrespondingsemanticmodels12. TheConTmodelhasthebestpro-\n",
      "jection performance, since its projected recall scores on the genuine dataset are\n",
      "much higher than those obtained on the false semantic dataset. Moreover, the\n",
      "Start-EndprojectionmethodbasedontheConTmodelistheonlycombination\n",
      "which achieves similar results compared to the corresponding semantic model.\n",
      "One can also notice that all the projected compositional models are only able\n",
      "to tell whether a semantic triple is already ruled out or not before the last\n",
      "12NotethatweusetheRESCALmodelasthecorrespondingsemanticmodelfortheConT.\n",
      "21\n",
      "timestamp, however they can not provide good inference results on the genuine\n",
      "semantic dataset.\n",
      "5. Conclusion\n",
      "Thispaperdescribedthefirstmathematicalmodelsforthedeclarativemem-\n",
      "ories: the semantic and episodic memory functions. To model these cogni-\n",
      "tive functions, we generalized leading approaches for static knowledge graphs\n",
      "(i.e., Tucker, RESCAL, HolE, ComplEx, DistMult) to 4-dimensional tempo-\n",
      "ral/episodic knowledge graphs. In addition, we developed two novel generaliza-\n",
      "tions of RESCAL to episodic tensors, i.e., Tree and ConT. In particular, ConT\n",
      "has superior performance overall, which indicates the importance of introduced\n",
      "high-dimensional latent representation of time for both sparse episodic tensor\n",
      "reconstruction and generalization.\n",
      "Our hypothesis is that perception includes an active semantic decoding pro-\n",
      "cess, which relies on latent representations of entities and predicates, and that\n",
      "episodic and semantic memories depend on the same decoding process. We ar-\n",
      "gue that temporal knowledge graph embeddings might be models for human\n",
      "cognitive episodic memory and that semantic memory (facts we know) can be\n",
      "generated from episodic memory by a marginalization operation. We also test\n",
      "this hypothesis on the ICEWS dataset, the experiments show that the current\n",
      "semantic facts can only be derived from the episodic tensor by a proper projec-\n",
      "tion considering both starting and terminal points of consecutive events.\n",
      "Acknowledgements. This work is funded by the Cognitive Deep Learning\n",
      "research project in Siemens AG.\n",
      "22\n",
      "References\n",
      "References\n",
      "[1] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, Z. Ives, Dbpe-\n",
      "dia: A nucleus for a web of open data, The semantic web (2007) 722–735.\n",
      "[2] F. M. Suchanek, G. Kasneci, G. Weikum, Yago: a core of semantic knowl-\n",
      "edge, in: Proceedings of the 16th international conference on World Wide\n",
      "Web, ACM, 2007, pp. 697–706.\n",
      "[3] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, J. Taylor, Freebase: a\n",
      "collaboratively created graph database for structuring human knowledge,\n",
      "in: Proceedings of the 2008 ACM SIGMOD international conference on\n",
      "Management of data, AcM, 2008, pp. 1247–1250.\n",
      "[4] D. Vrandeˇci´c, M. Kr¨otzsch, Wikidata: a free collaborative knowledgebase,\n",
      "Communications of the ACM 57 (10) (2014) 78–85.\n",
      "[5] A. Singhal, Introducing the knowledge graph: things, not strings, Official\n",
      "google blog.\n",
      "[6] M. Nickel, K. Murphy, V. Tresp, E. Gabrilovich, A review of relational\n",
      "machine learning for knowledge graphs, Proceedings of the IEEE.\n",
      "[7] H. Ebbinghaus, U¨ber das ged¨achtnis: untersuchungen zur experimentellen\n",
      "psychologie, Duncker & Humblot, 1885.\n",
      "[8] R.C.Atkinson,R.M.Shiffrin,Humanmemory: Aproposedsystemandits\n",
      "control processes, Psychology of learning and motivation 2 (1968) 89–195.\n",
      "[9] L. R. Squire, Memory and brain.\n",
      "[10] E. Tulving, Episodic and semantic memory: Where should we go from\n",
      "here?, Behavioral and Brain Sciences 9 (03) (1986) 573–577.\n",
      "23\n",
      "[11] D. L. Greenberg, M. Verfaellie, Interdependence of episodic and seman-\n",
      "tic memory: evidence from neuropsychology, Journal of the International\n",
      "Neuropsychological society 16 (05) (2010) 748–753.\n",
      "[12] M.Nickel,V.Tresp,H.-P.Kriegel,Athree-waymodelforcollectivelearning\n",
      "on multi-relational data, in: Proceedings of the 28th international confer-\n",
      "ence on machine learning (ICML-11), 2011, pp. 809–816.\n",
      "[13] A. Cichocki, Era of big data processing: A new approach via tensor net-\n",
      "works and tensor decompositions, in: International Workshop on Smart\n",
      "Info-Media Systems in Asia (SISA-2013), 2013.\n",
      "[14] A. Cichocki, Tensor networks for big data analytic and large-scale opti-\n",
      "mization problems, in: Second Int. Conference on Engineering and Com-\n",
      "putational Schematics (ECM2013), 2013.\n",
      "[15] B. Yang, W.-t. Yih, X. He, J. Gao, L. Deng, Embedding entities and rela-\n",
      "tions for learning and inference in knowledge bases, International Confer-\n",
      "ence on Learning Representations (ICLR).\n",
      "[16] M. Nickel, L. Rosasco, T. Poggio, Holographic embeddings of knowledge\n",
      "graphs, in: Thirtieth AAAI Conference on Artificial Intelligence, 2016.\n",
      "[17] T. Trouillon, J. Welbl, S. Riedel, E´. Gaussier, G. Bouchard, Complex em-\n",
      "beddings for simple link prediction, in: International Conference on Ma-\n",
      "chine Learning, 2016, pp. 2071–2080.\n",
      "[18] T. A. Plate, Holographic reduced representations, IEEE Transactions on\n",
      "Neural Networks 6 (3) (1995) 623–641.\n",
      "[19] K. Hayashi, M. Shimbo, On the equivalence of holographic and complex\n",
      "embeddings for link prediction, CoRR abs/1702.05563.\n",
      "URL http://arxiv.org/abs/1702.05563\n",
      "[20] M. D. Ward, A. Beger, J. Cutler, M. Dickenson, C. Dorff, B. Radford,\n",
      "Comparing gdelt and icews event data, Analysis 21 (2013) 267–297.\n",
      "24\n",
      "[21] A. Schein, J. Paisley, D. M. Blei, H. Wallach, Bayesian poisson tensor\n",
      "factorization for inferring multilateral relations from sparse dyadic event\n",
      "counts, in: Proceedings of the 21th ACM SIGKDD International Confer-\n",
      "enceonKnowledgeDiscoveryandDataMining,ACM,2015,pp.1045–1054.\n",
      "[22] A.Bordes,N.Usunier,A.Garcia-Duran,J.Weston,O.Yakhnenko,Trans-\n",
      "latingembeddingsformodelingmulti-relationaldata,in: Advancesinneu-\n",
      "ral information processing systems, 2013, pp. 2787–2795.\n",
      "[23] X. Glorot, Y. Bengio, Understanding the difficulty of training deep feed-\n",
      "forward neural networks., in: Aistats, Vol. 9, 2010, pp. 249–256.\n",
      "[24] D. Kingma, J. Ba, Adam: A method for stochastic optimization, Pro-\n",
      "ceedings of the 3rd International Conference on Learning Representations\n",
      "(ICLR).\n",
      "[25] W.Deng,J.B.Aimone,F.H.Gage,Newneuronsandnewmemories: how\n",
      "doesadulthippocampalneurogenesisaffectlearningandmemory?, Nature\n",
      "reviews. Neuroscience 11 (5) (2010) 339.\n",
      "[26] O.Lazarov,C.Hollands,Hippocampalneurogenesis: learningtoremember,\n",
      "Progress in neurobiology 138 (2016) 1–18.\n",
      "[27] K. L. Spalding, O. Bergmann, K. Alkass, S. Bernard, M. Salehpour, H. B.\n",
      "Huttner, E. Bostr¨om, I. Westerlund, C. Vial, B. A. Buchholz, et al., Dy-\n",
      "namics of hippocampal neurogenesis in adult humans, Cell 153 (6) (2013)\n",
      "1219–1227.\n",
      "[28] J.L.McClelland,B.L.McNaughton,R.C.O’reilly,Whytherearecomple-\n",
      "mentarylearningsystemsinthehippocampusandneocortex: insightsfrom\n",
      "thesuccessesandfailuresofconnectionistmodelsoflearningandmemory.,\n",
      "Psychological review 102 (3) (1995) 419.\n",
      "[29] L. Nadel, A. Samsonovich, L. Ryan, M. Moscovitch, Multiple trace theory\n",
      "of human memory: computational, neuroimaging, and neuropsychological\n",
      "results, Hippocampus 10 (4) (2000) 352–368.\n",
      "25\n",
      "[30] H.Poon,P.Domingos,Sum-productnetworks: Anewdeeparchitecture,in:\n",
      "ComputerVisionWorkshops(ICCVWorkshops), 2011IEEEInternational\n",
      "Conference on, IEEE, 2011, pp. 689–690.\n",
      "26<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,     47,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Seq2RDF: An end-to-end application for\n",
      "deriving Triples from Natural Language Text\n",
      "Yue Liu, Tongtao Zhang, Zhicheng Liang, Heng Ji, Deborah L. McGuinness\n",
      "Department of Computer Science, Rensselaer Polytechnic Institute\n",
      "Abstract. Wepresentanend-to-endapproachthattakesunstructured\n",
      "textual input and generates structured output compliant with a given\n",
      "vocabulary. We treat the triples within a given knowledge graph as an\n",
      "independentgraphlanguageandproposeanencoder-decoderframework\n",
      "with an attention mechanism that leverages knowledge graph embed-\n",
      "dings.Ourmodellearnsthemappingfromnaturallanguagetexttotriple\n",
      "representation in the form of subject-predicate-object using the se-\n",
      "lectedknowledgegraphvocabulary.Experimentsonthreedifferentdata\n",
      "sets show that we achieve competitive F1-Measures over the baselines\n",
      "using our simple yet effective approach. A demo video is included.\n",
      "1 Introduction\n",
      "Converting free text into usable structured knowledge for downstream applica-\n",
      "tionsusuallyrequiresexperthumancurators,orreliesontheabilityofmachines\n",
      "to accurately parse natural language based on the meanings in the knowledge\n",
      "graph (KG) vocabulary. Despite many advances in text extraction and seman-\n",
      "tic technologies, there is yet to be a simple system that generates RDF triples\n",
      "fromfreetextgivenachosenKGvocabularyinjust one step,whichweconsider\n",
      "an end-to-end system. We aim to automate the process of translating a natural\n",
      "language sentence into a structured triple representation defined in the form of\n",
      "subject-predicate-object, s-p-o for short, and build an end-to-end model\n",
      "based on an encoder-decoder architecture that learns the semantic parsing pro-\n",
      "cess from text to triple without tedious feature engineering and intermediate\n",
      "steps. We evaluate our approach on three different datasets and achieve com-\n",
      "petitive F1-measures outperforming our proposed baselines, respectively. The\n",
      "system, data set and demo are publicly available12.\n",
      "2 Our Approach\n",
      "Inspired by the sequence-to-sequence model[5] in recent Neural Machine Trans-\n",
      "lation, we attempt to use this model to bridge the gap between natural lan-\n",
      "guage and triple representation. We consider a natural language sentence X =\n",
      "[x,...,x ] as a source sequence, and we aim to map X to an RDF triple\n",
      "1 |X|\n",
      "Y = [y,y,y ] with regard to s-p-o as a target sequence that is aligned with\n",
      "1 2 3\n",
      "1 https://github.com/YueLiu/NeuralTripleTranslation\n",
      "2 https://youtu.be/ssiQEDF-HHE\n",
      "8102\n",
      "guA\n",
      "8\n",
      "]LC.sc[\n",
      "3v36710.7081:viXra\n",
      "a given KG vocabulary set or schema. Given DBpedia for example, we take a\n",
      "largeamountofexistingtriplesfromDBpediaasgroundtruthfactsfortraining.\n",
      "Our model learns how to form a compliant triple with appropriate terms in the\n",
      "existing vocabulary. Furthermore, the architecture of the decoder enables the\n",
      "model to capture the differences, dependencies and constraints when selecting\n",
      "s-p-o respectively, which makes the model a natural fit for this learning task.\n",
      "Lake George is at the southeast base of the Adirondack Mountains\n",
      "Bi-directional LSTM\n",
      "Concatenate\n",
      "Encoder\n",
      "<Start_of_Triple>\n",
      "dbr:Lake_George_(New_York) dbo:country dbr:Adirondacks\n",
      "dbr:George_Lake dbo:birthplace dbr:Adirondack_Mountains\n",
      "dbr:Lake_George_(Florida) dbo:location yago:Mountain109359803\n",
      "dbo:isPartOf\n",
      "dbr:Lake_George_(New_South_Wales) dbr:Whiteface_Mountain\n",
      "Decoder\n",
      "Fig.1: ModelOverview.Threecolors(red,yellow,blue)representtheactiveattention\n",
      "during s-p-o decoding respectively. We currently only generate a single triple per\n",
      "sentence, leaving the generation of multiple triples per sentence for future work.\n",
      "As shown in Figure 1, the model consists of an encoder taking in a natural\n",
      "language sentence as sequence input and a decoder generating the target RDF\n",
      "triple. The model pursues the maximized conditional probability\n",
      "3\n",
      "(cid:89)\n",
      "p(Y|X)= p(y|y,X), (1)\n",
      "<td\n",
      "td=1\n",
      "Bothencoderanddecoderarerecurrentneuralnetworks3 withLongShortTerm\n",
      "Memory(LSTM)cells.Weapplytheattentionmechanismthatforcesthemodel\n",
      "to learn to focus on specific parts of the input sequence when decoding, instead\n",
      "3 Weusetf.contrib.seq2seq.sequence losswhichisaweightedcross-entropyloss\n",
      "for a sequence of logits. We concatenate the last hidden output of forward and\n",
      "backward LSTM networks, the concatenated vector comes with fixed dimensions\n",
      "of relying only on the last hidden state of the encoder. Furthermore, in order\n",
      "to capture the semantics of the entities and relations within our training data,\n",
      "we apply domain specific resources[2] to obtain the word embeddings and the\n",
      "TransE model[1] to obtain KG embeddings for entities and relations in the KG.\n",
      "Weusethesepre-trainedWordembeddingsandKGembeddingsforentitiesand\n",
      "relations to initialize the encoder and decoder embedding matrix, respectively,\n",
      "and results show that this approach improves the overall performance.\n",
      "3 Experiments\n",
      "Data Sets We ran experiments on two public datasets NYT4[4], ADE5 with se-\n",
      "lected vocabularies and a Wiki-DBpedia dataset that is produced by distant\n",
      "supervision6. For data obtained by distant supervision, the test set is manually\n",
      "labeled to ensure its quality. Each data set is an annotated corpus with corre-\n",
      "spondingtriplesintheformofeithers-p-oorentity mentionsandrelation\n",
      "types at the sentence level. Details are available on our GitHub page.\n",
      "Text Berlin is the capital city of Germany.\n",
      "Triple dbr:Germany dbo:capital dbr:Berlin\n",
      "Table 1: Example annotated pair with distant supervision on Wiki-DBpedia\n",
      "Evaluation Metrics We consider pipeline-based approaches that combine En-\n",
      "tity Linking (EL) and Relation Classification (RC) as state of the art. We pro-\n",
      "pose several baselines with combined outputs from state-of-the-art EL7 and RC\n",
      "for evaluation. We use F1-measure to evaluate triple generation (an output is\n",
      "consideredcorrectonlyifs-p-oareallcorrect)incomparisonwiththebaselines.\n",
      "Baselines We implement multiple baselines including a classical supervised\n",
      "learning using simple Lexical features, a state-of-the-art recurrent neural net-\n",
      "work (RNN) approach with LSTM [3] and one with a Gate Recurrent Unit\n",
      "(GRU) variant. Then we evaluate the performance on triple generation with re-\n",
      "sultscombiningELandRC.Thehyper-parametersinourmodelaretunedwith\n",
      "10-fold cross-validation on the training set according to the best F1-scores. We\n",
      "applied the same settings to the baselines. The details regarding the parameters\n",
      "and settings are available on our GitHub page for replication purposes.\n",
      "4 Result Analysis\n",
      "We achieve the best F1 Measure of 84.3 on the triple generation from Table 2.\n",
      "Note that the baseline approaches that we implemented are pipeline-based, and\n",
      "thus they are very likely to propagate errors to downstream components. How-\n",
      "ever,ourmodelmergesthetwodifferenttasksofELandRCintooneduringthe\n",
      "4 New York Times articles: https://github.com/shanzhenren/CoType\n",
      "5 Adverse drug events: https://sites.google.com/site/adecorpus\n",
      "6 http://deepdive.stanford.edu/distant_supervision\n",
      "7 Stanford, Domain specific NER\n",
      "decoding, which composes a major advantage over pipeline-based approaches\n",
      "that usually apply separate models on EL and RC. The most common errors\n",
      "are caused by Out of vocabulary and Noise from overlapping relations\n",
      "in text. As we do not cover all rare entity names or consider multiple triple\n",
      "situations, these errors are valid in some sense.\n",
      "Tasks NYT ADE Wiki-DBpedia\n",
      "Metric F1-MeasureF1-Measure F1-Measure\n",
      "EL+Lexical 36.8 61.4 37.8\n",
      "EL+LSTM 58.7 70.3 65.5\n",
      "EL+GRU 59.8 73.2 67.0\n",
      "Seq2Seq 64.2 73.4 73.5\n",
      "S+A+W+G 71.4 79.5 84.3\n",
      "Table 2: Cross-dataset comparison on triple generations. Seq2Seq denotes the imple-\n",
      "mentation of Seq2Seq without any attention mechanism and pre-trained embeddings;\n",
      "Adenotesattentionmechanism;WandGdenotepre-trainedwordembeddingsforthe\n",
      "encoders and KG embeddings for the decoders, respectively.\n",
      "5 Conclusions and Future Work\n",
      "We present an end-end system for translating a natural language sentence to\n",
      "its triple representation. Our system performs competitively on three different\n",
      "datasets and our assumption on enhancing the model with pre-trained KG em-\n",
      "beddingsimprovesperformanceacrosstheboard.Itiseasytoreplicateourwork\n",
      "and use our system following the demonstration. In the future, we plan to re-\n",
      "design the decoder and enable the generation of multiple triples per sentence.\n",
      "Acknowledgement This work was partially supported by the NIEHS Award\n",
      "0255-0236-4609 / 1U2CES026555-01.\n",
      "References\n",
      "1. Bordes,A.,Usunier,N.,Garcia-Duran,A.,Weston,J.,Yakhnenko,O.:Translating\n",
      "embeddings for modeling multi-relational data. In: Advances in neural information\n",
      "processing systems. pp. 2787–2795 (2013)\n",
      "2. Liu, Y., Ge, T., Mathews, K., Ji, H., McGuinness, D.: Exploiting task-oriented\n",
      "resourcestolearnwordembeddingsforclinicalabbreviationexpansion.Proceedings\n",
      "of BioNLP 15 pp. 92–97 (2015)\n",
      "3. Miwa,M.,Bansal,M.:End-to-endrelationextractionusinglstmsonsequencesand\n",
      "tree structures. arXiv preprint arXiv:1601.00770 (2016)\n",
      "4. Ren, X., Wu, Z., He, W., Qu, M., Voss, C.R., Ji, H., Abdelzaher, T.F., Han, J.:\n",
      "Cotype: Joint extraction of typed entities and relations with knowledge bases. In:\n",
      "Proceedings of the 26th International Conference on World Wide Web. pp. 1015–\n",
      "1024 (2017)\n",
      "5. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural\n",
      "networks. In: Advances in neural information processing systems. pp. 3104–3112\n",
      "(2014)<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  83150,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['NYT', 'ADE', 'Wiki-DBpedia']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Seq2RDF: An end-to-end application for\n",
      "deriving Triples from Natural Language Text\n",
      "Yue Liu, Tongtao Zhang, Zhicheng Liang, Heng Ji, Deborah L. McGuinness\n",
      "Department of Computer Science, Rensselaer Polytechnic Institute\n",
      "Abstract. Wepresentanend-to-endapproachthattakesunstructured\n",
      "textual input and generates structured output compliant with a given\n",
      "vocabulary. We treat the triples within a given knowledge graph as an\n",
      "independentgraphlanguageandproposeanencoder-decoderframework\n",
      "with an attention mechanism that leverages knowledge graph embed-\n",
      "dings.Ourmodellearnsthemappingfromnaturallanguagetexttotriple\n",
      "representation in the form of subject-predicate-object using the se-\n",
      "lectedknowledgegraphvocabulary.Experimentsonthreedifferentdata\n",
      "sets show that we achieve competitive F1-Measures over the baselines\n",
      "using our simple yet effective approach. A demo video is included.\n",
      "1 Introduction\n",
      "Converting free text into usable structured knowledge for downstream applica-\n",
      "tionsusuallyrequiresexperthumancurators,orreliesontheabilityofmachines\n",
      "to accurately parse natural language based on the meanings in the knowledge\n",
      "graph (KG) vocabulary. Despite many advances in text extraction and seman-\n",
      "tic technologies, there is yet to be a simple system that generates RDF triples\n",
      "fromfreetextgivenachosenKGvocabularyinjust one step,whichweconsider\n",
      "an end-to-end system. We aim to automate the process of translating a natural\n",
      "language sentence into a structured triple representation defined in the form of\n",
      "subject-predicate-object, s-p-o for short, and build an end-to-end model\n",
      "based on an encoder-decoder architecture that learns the semantic parsing pro-\n",
      "cess from text to triple without tedious feature engineering and intermediate\n",
      "steps. We evaluate our approach on three different datasets and achieve com-\n",
      "petitive F1-measures outperforming our proposed baselines, respectively. The\n",
      "system, data set and demo are publicly available12.\n",
      "2 Our Approach\n",
      "Inspired by the sequence-to-sequence model[5] in recent Neural Machine Trans-\n",
      "lation, we attempt to use this model to bridge the gap between natural lan-\n",
      "guage and triple representation. We consider a natural language sentence X =\n",
      "[x,...,x ] as a source sequence, and we aim to map X to an RDF triple\n",
      "1 |X|\n",
      "Y = [y,y,y ] with regard to s-p-o as a target sequence that is aligned with\n",
      "1 2 3\n",
      "1 https://github.com/YueLiu/NeuralTripleTranslation\n",
      "2 https://youtu.be/ssiQEDF-HHE\n",
      "8102\n",
      "guA\n",
      "8\n",
      "]LC.sc[\n",
      "3v36710.7081:viXra\n",
      "a given KG vocabulary set or schema. Given DBpedia for example, we take a\n",
      "largeamountofexistingtriplesfromDBpediaasgroundtruthfactsfortraining.\n",
      "Our model learns how to form a compliant triple with appropriate terms in the\n",
      "existing vocabulary. Furthermore, the architecture of the decoder enables the\n",
      "model to capture the differences, dependencies and constraints when selecting\n",
      "s-p-o respectively, which makes the model a natural fit for this learning task.\n",
      "Lake George is at the southeast base of the Adirondack Mountains\n",
      "Bi-directional LSTM\n",
      "Concatenate\n",
      "Encoder\n",
      "<Start_of_Triple>\n",
      "dbr:Lake_George_(New_York) dbo:country dbr:Adirondacks\n",
      "dbr:George_Lake dbo:birthplace dbr:Adirondack_Mountains\n",
      "dbr:Lake_George_(Florida) dbo:location yago:Mountain109359803\n",
      "dbo:isPartOf\n",
      "dbr:Lake_George_(New_South_Wales) dbr:Whiteface_Mountain\n",
      "Decoder\n",
      "Fig.1: ModelOverview.Threecolors(red,yellow,blue)representtheactiveattention\n",
      "during s-p-o decoding respectively. We currently only generate a single triple per\n",
      "sentence, leaving the generation of multiple triples per sentence for future work.\n",
      "As shown in Figure 1, the model consists of an encoder taking in a natural\n",
      "language sentence as sequence input and a decoder generating the target RDF\n",
      "triple. The model pursues the maximized conditional probability\n",
      "3\n",
      "(cid:89)\n",
      "p(Y|X)= p(y|y,X), (1)\n",
      "<td\n",
      "td=1\n",
      "Bothencoderanddecoderarerecurrentneuralnetworks3 withLongShortTerm\n",
      "Memory(LSTM)cells.Weapplytheattentionmechanismthatforcesthemodel\n",
      "to learn to focus on specific parts of the input sequence when decoding, instead\n",
      "3 Weusetf.contrib.seq2seq.sequence losswhichisaweightedcross-entropyloss\n",
      "for a sequence of logits. We concatenate the last hidden output of forward and\n",
      "backward LSTM networks, the concatenated vector comes with fixed dimensions\n",
      "of relying only on the last hidden state of the encoder. Furthermore, in order\n",
      "to capture the semantics of the entities and relations within our training data,\n",
      "we apply domain specific resources[2] to obtain the word embeddings and the\n",
      "TransE model[1] to obtain KG embeddings for entities and relations in the KG.\n",
      "Weusethesepre-trainedWordembeddingsandKGembeddingsforentitiesand\n",
      "relations to initialize the encoder and decoder embedding matrix, respectively,\n",
      "and results show that this approach improves the overall performance.\n",
      "3 Experiments\n",
      "Data Sets We ran experiments on two public datasets NYT4[4], ADE5 with se-\n",
      "lected vocabularies and a Wiki-DBpedia dataset that is produced by distant\n",
      "supervision6. For data obtained by distant supervision, the test set is manually\n",
      "labeled to ensure its quality. Each data set is an annotated corpus with corre-\n",
      "spondingtriplesintheformofeithers-p-oorentity mentionsandrelation\n",
      "types at the sentence level. Details are available on our GitHub page.\n",
      "Text Berlin is the capital city of Germany.\n",
      "Triple dbr:Germany dbo:capital dbr:Berlin\n",
      "Table 1: Example annotated pair with distant supervision on Wiki-DBpedia\n",
      "Evaluation Metrics We consider pipeline-based approaches that combine En-\n",
      "tity Linking (EL) and Relation Classification (RC) as state of the art. We pro-\n",
      "pose several baselines with combined outputs from state-of-the-art EL7 and RC\n",
      "for evaluation. We use F1-measure to evaluate triple generation (an output is\n",
      "consideredcorrectonlyifs-p-oareallcorrect)incomparisonwiththebaselines.\n",
      "Baselines We implement multiple baselines including a classical supervised\n",
      "learning using simple Lexical features, a state-of-the-art recurrent neural net-\n",
      "work (RNN) approach with LSTM [3] and one with a Gate Recurrent Unit\n",
      "(GRU) variant. Then we evaluate the performance on triple generation with re-\n",
      "sultscombiningELandRC.Thehyper-parametersinourmodelaretunedwith\n",
      "10-fold cross-validation on the training set according to the best F1-scores. We\n",
      "applied the same settings to the baselines. The details regarding the parameters\n",
      "and settings are available on our GitHub page for replication purposes.\n",
      "4 Result Analysis\n",
      "We achieve the best F1 Measure of 84.3 on the triple generation from Table 2.\n",
      "Note that the baseline approaches that we implemented are pipeline-based, and\n",
      "thus they are very likely to propagate errors to downstream components. How-\n",
      "ever,ourmodelmergesthetwodifferenttasksofELandRCintooneduringthe\n",
      "4 New York Times articles: https://github.com/shanzhenren/CoType\n",
      "5 Adverse drug events: https://sites.google.com/site/adecorpus\n",
      "6 http://deepdive.stanford.edu/distant_supervision\n",
      "7 Stanford, Domain specific NER\n",
      "decoding, which composes a major advantage over pipeline-based approaches\n",
      "that usually apply separate models on EL and RC. The most common errors\n",
      "are caused by Out of vocabulary and Noise from overlapping relations\n",
      "in text. As we do not cover all rare entity names or consider multiple triple\n",
      "situations, these errors are valid in some sense.\n",
      "Tasks NYT ADE Wiki-DBpedia\n",
      "Metric F1-MeasureF1-Measure F1-Measure\n",
      "EL+Lexical 36.8 61.4 37.8\n",
      "EL+LSTM 58.7 70.3 65.5\n",
      "EL+GRU 59.8 73.2 67.0\n",
      "Seq2Seq 64.2 73.4 73.5\n",
      "S+A+W+G 71.4 79.5 84.3\n",
      "Table 2: Cross-dataset comparison on triple generations. Seq2Seq denotes the imple-\n",
      "mentation of Seq2Seq without any attention mechanism and pre-trained embeddings;\n",
      "Adenotesattentionmechanism;WandGdenotepre-trainedwordembeddingsforthe\n",
      "encoders and KG embeddings for the decoders, respectively.\n",
      "5 Conclusions and Future Work\n",
      "We present an end-end system for translating a natural language sentence to\n",
      "its triple representation. Our system performs competitively on three different\n",
      "datasets and our assumption on enhancing the model with pre-trained KG em-\n",
      "beddingsimprovesperformanceacrosstheboard.Itiseasytoreplicateourwork\n",
      "and use our system following the demonstration. In the future, we plan to re-\n",
      "design the decoder and enable the generation of multiple triples per sentence.\n",
      "Acknowledgement This work was partially supported by the NIEHS Award\n",
      "0255-0236-4609 / 1U2CES026555-01.\n",
      "References\n",
      "1. Bordes,A.,Usunier,N.,Garcia-Duran,A.,Weston,J.,Yakhnenko,O.:Translating\n",
      "embeddings for modeling multi-relational data. In: Advances in neural information\n",
      "processing systems. pp. 2787–2795 (2013)\n",
      "2. Liu, Y., Ge, T., Mathews, K., Ji, H., McGuinness, D.: Exploiting task-oriented\n",
      "resourcestolearnwordembeddingsforclinicalabbreviationexpansion.Proceedings\n",
      "of BioNLP 15 pp. 92–97 (2015)\n",
      "3. Miwa,M.,Bansal,M.:End-to-endrelationextractionusinglstmsonsequencesand\n",
      "tree structures. arXiv preprint arXiv:1601.00770 (2016)\n",
      "4. Ren, X., Wu, Z., He, W., Qu, M., Voss, C.R., Ji, H., Abdelzaher, T.F., Han, J.:\n",
      "Cotype: Joint extraction of typed entities and relations with knowledge bases. In:\n",
      "Proceedings of the 26th International Conference on World Wide Web. pp. 1015–\n",
      "1024 (2017)\n",
      "5. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural\n",
      "networks. In: Advances in neural information processing systems. pp. 3104–3112\n",
      "(2014)<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  55982,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Triple generation', 'Entity Linking', 'Relation Classification']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Seq2RDF: An end-to-end application for\n",
      "deriving Triples from Natural Language Text\n",
      "Yue Liu, Tongtao Zhang, Zhicheng Liang, Heng Ji, Deborah L. McGuinness\n",
      "Department of Computer Science, Rensselaer Polytechnic Institute\n",
      "Abstract. Wepresentanend-to-endapproachthattakesunstructured\n",
      "textual input and generates structured output compliant with a given\n",
      "vocabulary. We treat the triples within a given knowledge graph as an\n",
      "independentgraphlanguageandproposeanencoder-decoderframework\n",
      "with an attention mechanism that leverages knowledge graph embed-\n",
      "dings.Ourmodellearnsthemappingfromnaturallanguagetexttotriple\n",
      "representation in the form of subject-predicate-object using the se-\n",
      "lectedknowledgegraphvocabulary.Experimentsonthreedifferentdata\n",
      "sets show that we achieve competitive F1-Measures over the baselines\n",
      "using our simple yet effective approach. A demo video is included.\n",
      "1 Introduction\n",
      "Converting free text into usable structured knowledge for downstream applica-\n",
      "tionsusuallyrequiresexperthumancurators,orreliesontheabilityofmachines\n",
      "to accurately parse natural language based on the meanings in the knowledge\n",
      "graph (KG) vocabulary. Despite many advances in text extraction and seman-\n",
      "tic technologies, there is yet to be a simple system that generates RDF triples\n",
      "fromfreetextgivenachosenKGvocabularyinjust one step,whichweconsider\n",
      "an end-to-end system. We aim to automate the process of translating a natural\n",
      "language sentence into a structured triple representation defined in the form of\n",
      "subject-predicate-object, s-p-o for short, and build an end-to-end model\n",
      "based on an encoder-decoder architecture that learns the semantic parsing pro-\n",
      "cess from text to triple without tedious feature engineering and intermediate\n",
      "steps. We evaluate our approach on three different datasets and achieve com-\n",
      "petitive F1-measures outperforming our proposed baselines, respectively. The\n",
      "system, data set and demo are publicly available12.\n",
      "2 Our Approach\n",
      "Inspired by the sequence-to-sequence model[5] in recent Neural Machine Trans-\n",
      "lation, we attempt to use this model to bridge the gap between natural lan-\n",
      "guage and triple representation. We consider a natural language sentence X =\n",
      "[x,...,x ] as a source sequence, and we aim to map X to an RDF triple\n",
      "1 |X|\n",
      "Y = [y,y,y ] with regard to s-p-o as a target sequence that is aligned with\n",
      "1 2 3\n",
      "1 https://github.com/YueLiu/NeuralTripleTranslation\n",
      "2 https://youtu.be/ssiQEDF-HHE\n",
      "8102\n",
      "guA\n",
      "8\n",
      "]LC.sc[\n",
      "3v36710.7081:viXra\n",
      "a given KG vocabulary set or schema. Given DBpedia for example, we take a\n",
      "largeamountofexistingtriplesfromDBpediaasgroundtruthfactsfortraining.\n",
      "Our model learns how to form a compliant triple with appropriate terms in the\n",
      "existing vocabulary. Furthermore, the architecture of the decoder enables the\n",
      "model to capture the differences, dependencies and constraints when selecting\n",
      "s-p-o respectively, which makes the model a natural fit for this learning task.\n",
      "Lake George is at the southeast base of the Adirondack Mountains\n",
      "Bi-directional LSTM\n",
      "Concatenate\n",
      "Encoder\n",
      "<Start_of_Triple>\n",
      "dbr:Lake_George_(New_York) dbo:country dbr:Adirondacks\n",
      "dbr:George_Lake dbo:birthplace dbr:Adirondack_Mountains\n",
      "dbr:Lake_George_(Florida) dbo:location yago:Mountain109359803\n",
      "dbo:isPartOf\n",
      "dbr:Lake_George_(New_South_Wales) dbr:Whiteface_Mountain\n",
      "Decoder\n",
      "Fig.1: ModelOverview.Threecolors(red,yellow,blue)representtheactiveattention\n",
      "during s-p-o decoding respectively. We currently only generate a single triple per\n",
      "sentence, leaving the generation of multiple triples per sentence for future work.\n",
      "As shown in Figure 1, the model consists of an encoder taking in a natural\n",
      "language sentence as sequence input and a decoder generating the target RDF\n",
      "triple. The model pursues the maximized conditional probability\n",
      "3\n",
      "(cid:89)\n",
      "p(Y|X)= p(y|y,X), (1)\n",
      "<td\n",
      "td=1\n",
      "Bothencoderanddecoderarerecurrentneuralnetworks3 withLongShortTerm\n",
      "Memory(LSTM)cells.Weapplytheattentionmechanismthatforcesthemodel\n",
      "to learn to focus on specific parts of the input sequence when decoding, instead\n",
      "3 Weusetf.contrib.seq2seq.sequence losswhichisaweightedcross-entropyloss\n",
      "for a sequence of logits. We concatenate the last hidden output of forward and\n",
      "backward LSTM networks, the concatenated vector comes with fixed dimensions\n",
      "of relying only on the last hidden state of the encoder. Furthermore, in order\n",
      "to capture the semantics of the entities and relations within our training data,\n",
      "we apply domain specific resources[2] to obtain the word embeddings and the\n",
      "TransE model[1] to obtain KG embeddings for entities and relations in the KG.\n",
      "Weusethesepre-trainedWordembeddingsandKGembeddingsforentitiesand\n",
      "relations to initialize the encoder and decoder embedding matrix, respectively,\n",
      "and results show that this approach improves the overall performance.\n",
      "3 Experiments\n",
      "Data Sets We ran experiments on two public datasets NYT4[4], ADE5 with se-\n",
      "lected vocabularies and a Wiki-DBpedia dataset that is produced by distant\n",
      "supervision6. For data obtained by distant supervision, the test set is manually\n",
      "labeled to ensure its quality. Each data set is an annotated corpus with corre-\n",
      "spondingtriplesintheformofeithers-p-oorentity mentionsandrelation\n",
      "types at the sentence level. Details are available on our GitHub page.\n",
      "Text Berlin is the capital city of Germany.\n",
      "Triple dbr:Germany dbo:capital dbr:Berlin\n",
      "Table 1: Example annotated pair with distant supervision on Wiki-DBpedia\n",
      "Evaluation Metrics We consider pipeline-based approaches that combine En-\n",
      "tity Linking (EL) and Relation Classification (RC) as state of the art. We pro-\n",
      "pose several baselines with combined outputs from state-of-the-art EL7 and RC\n",
      "for evaluation. We use F1-measure to evaluate triple generation (an output is\n",
      "consideredcorrectonlyifs-p-oareallcorrect)incomparisonwiththebaselines.\n",
      "Baselines We implement multiple baselines including a classical supervised\n",
      "learning using simple Lexical features, a state-of-the-art recurrent neural net-\n",
      "work (RNN) approach with LSTM [3] and one with a Gate Recurrent Unit\n",
      "(GRU) variant. Then we evaluate the performance on triple generation with re-\n",
      "sultscombiningELandRC.Thehyper-parametersinourmodelaretunedwith\n",
      "10-fold cross-validation on the training set according to the best F1-scores. We\n",
      "applied the same settings to the baselines. The details regarding the parameters\n",
      "and settings are available on our GitHub page for replication purposes.\n",
      "4 Result Analysis\n",
      "We achieve the best F1 Measure of 84.3 on the triple generation from Table 2.\n",
      "Note that the baseline approaches that we implemented are pipeline-based, and\n",
      "thus they are very likely to propagate errors to downstream components. How-\n",
      "ever,ourmodelmergesthetwodifferenttasksofELandRCintooneduringthe\n",
      "4 New York Times articles: https://github.com/shanzhenren/CoType\n",
      "5 Adverse drug events: https://sites.google.com/site/adecorpus\n",
      "6 http://deepdive.stanford.edu/distant_supervision\n",
      "7 Stanford, Domain specific NER\n",
      "decoding, which composes a major advantage over pipeline-based approaches\n",
      "that usually apply separate models on EL and RC. The most common errors\n",
      "are caused by Out of vocabulary and Noise from overlapping relations\n",
      "in text. As we do not cover all rare entity names or consider multiple triple\n",
      "situations, these errors are valid in some sense.\n",
      "Tasks NYT ADE Wiki-DBpedia\n",
      "Metric F1-MeasureF1-Measure F1-Measure\n",
      "EL+Lexical 36.8 61.4 37.8\n",
      "EL+LSTM 58.7 70.3 65.5\n",
      "EL+GRU 59.8 73.2 67.0\n",
      "Seq2Seq 64.2 73.4 73.5\n",
      "S+A+W+G 71.4 79.5 84.3\n",
      "Table 2: Cross-dataset comparison on triple generations. Seq2Seq denotes the imple-\n",
      "mentation of Seq2Seq without any attention mechanism and pre-trained embeddings;\n",
      "Adenotesattentionmechanism;WandGdenotepre-trainedwordembeddingsforthe\n",
      "encoders and KG embeddings for the decoders, respectively.\n",
      "5 Conclusions and Future Work\n",
      "We present an end-end system for translating a natural language sentence to\n",
      "its triple representation. Our system performs competitively on three different\n",
      "datasets and our assumption on enhancing the model with pre-trained KG em-\n",
      "beddingsimprovesperformanceacrosstheboard.Itiseasytoreplicateourwork\n",
      "and use our system following the demonstration. In the future, we plan to re-\n",
      "design the decoder and enable the generation of multiple triples per sentence.\n",
      "Acknowledgement This work was partially supported by the NIEHS Award\n",
      "0255-0236-4609 / 1U2CES026555-01.\n",
      "References\n",
      "1. Bordes,A.,Usunier,N.,Garcia-Duran,A.,Weston,J.,Yakhnenko,O.:Translating\n",
      "embeddings for modeling multi-relational data. In: Advances in neural information\n",
      "processing systems. pp. 2787–2795 (2013)\n",
      "2. Liu, Y., Ge, T., Mathews, K., Ji, H., McGuinness, D.: Exploiting task-oriented\n",
      "resourcestolearnwordembeddingsforclinicalabbreviationexpansion.Proceedings\n",
      "of BioNLP 15 pp. 92–97 (2015)\n",
      "3. Miwa,M.,Bansal,M.:End-to-endrelationextractionusinglstmsonsequencesand\n",
      "tree structures. arXiv preprint arXiv:1601.00770 (2016)\n",
      "4. Ren, X., Wu, Z., He, W., Qu, M., Voss, C.R., Ji, H., Abdelzaher, T.F., Han, J.:\n",
      "Cotype: Joint extraction of typed entities and relations with knowledge bases. In:\n",
      "Proceedings of the 26th International Conference on World Wide Web. pp. 1015–\n",
      "1024 (2017)\n",
      "5. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural\n",
      "networks. In: Advances in neural information processing systems. pp. 3104–3112\n",
      "(2014)<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  71773,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Yue Liu', 'Tongtao Zhang', 'Zhicheng Liang', 'Heng Ji', 'Deborah L. McGuinness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: 9102\n",
      "luJ\n",
      "51\n",
      "]GL.sc[\n",
      "5v81070.8081:viXra\n",
      "Hypernetwork Knowledge Graph Embeddings\n",
      "Ivana Balaˇzevi´c1, Carl Allen1, and Timothy M. Hospedales1,2\n",
      "1 School of Informatics, Universityof Edinburgh,UK\n",
      "2 Samsung AI Centre, Cambridge, UK\n",
      "{ivana.balazevic, carl.allen, t.hospedales}@ed.ac.uk\n",
      "Abstract. Knowledgegraphsaregraphicalrepresentationsoflargedata-\n",
      "basesoffacts,whichtypicallysufferfromincompleteness.Inferringmiss-\n",
      "ingrelations(links)betweenentities(nodes)isthetaskoflinkprediction.\n",
      "Arecentstate-of-the-artapproachtolinkprediction,ConvE,implements\n",
      "a convolutional neural network to extract features from concatenated\n",
      "subject and relation vectors. Whilst results are impressive, the method\n",
      "is unintuitive and poorly understood. We propose a hypernetwork ar-\n",
      "chitecturethatgenerates simplified relation-specific convolutionalfilters\n",
      "that(i)outperformsConvEandallpreviousapproachesacrossstandard\n",
      "datasets; and (ii) can be framed as tensor factorization and thus set\n",
      "within a well established family of factorization models for link predic-\n",
      "tion. We thus demonstrate that convolution simply offers a convenient\n",
      "computationalmeansofintroducingsparsityandparametertyingtofind\n",
      "an effective trade-off between non-linear expressiveness and the number\n",
      "of parameters to learn.\n",
      "1 Introduction\n",
      "Knowledge graphs, such as WordNet, Freebase, and Google Knowledge Graph,\n",
      "arelargegraph-structureddatabasesoffacts,containinginformationintheform\n",
      "oftriples(e,r,e ),withe ande representingsubjectandobjectentitiesandr\n",
      "1 2 1 2\n",
      "a relation between them. They are considered important information resources,\n",
      "used for a wide variety of tasks ranging from question answering to informa-\n",
      "tion retrieval and text summarization. One of the main challenges with existing\n",
      "knowledge graphs is their incompleteness: many of the links between entities in\n",
      "the graph are missing. This has inspired substantial work in the field of link\n",
      "prediction, i.e. the task of inferring missing links in knowledge graphs.\n",
      "Until recently, many approaches to link prediction have been based on dif-\n",
      "ferent factorizations of a 3-moded binary tensor representation of the training\n",
      "triples [12,17,23,22]. Such approaches are shallow and linear, with limited ex-\n",
      "pressiveness.However,attempts to increase expressiveness with additional fully\n",
      "connectedlayersandnon-linearitiesoftenleadtooverfitting[12,17].Forthisrea-\n",
      "son, Dettmers et al. introduce ConvE, a model that uses 2D convolutions over\n",
      "reshaped and concatenated entity and relation embeddings [3]. They motivate\n",
      "the use of convolutions by being parameter efficient and fast to compute on a\n",
      "GPU,aswellashavingvariousrobustmethodsfromcomputervisiontoprevent\n",
      "2 Balaˇzevi´c et al.\n",
      "overfitting. Even though results achieved by ConvE are impressive, it is highly\n",
      "unintuitive that convolution – particularly 2D convolution – should be effective\n",
      "for extracting information from 1D entity and relation embeddings.\n",
      "In this paper, we introduce HypER, a model that uses a hypernetwork [5]\n",
      "to generate convolutional filter weights for each relation. A hypernetwork is an\n",
      "approach by which one network generates weights for another network, that\n",
      "can be used to enable weight-sharing across layers and to dynamically synthe-\n",
      "size weights given an input. In our context, we generate relation-specific filter\n",
      "weights to process input entities, and also achieve multi-task knowledge shar-\n",
      "ing across relations in the knowledge graph. Our proposed HypER model uses\n",
      "a hypernetwork to generate a set of 1D relation-specific filters to process the\n",
      "subject entity embeddings. This simplifies the interaction between subject en-\n",
      "tity and relation embeddings compared to ConvE, in which a global set of 2D\n",
      "filters are convolved over reshaped and concatenated subject entity and relation\n",
      "embeddings, which is unintuitive as it suggests the presence of 2D structure in\n",
      "wordembeddings.Moreover,interactionbetweensubjectandrelationinConvE\n",
      "depends on an arbitrarychoice about how they are reshaped and concatenated.\n",
      "In contrast, HypER’s hypernetwork generates relation-specific filters, and thus\n",
      "extracts relation-specific features from the subject entity embedding. This ne-\n",
      "cessitatesno 2Dreshaping,andallowsentity andrelationtointeractmorecom-\n",
      "pletely,ratherthanonlyaroundthe concatenationboundary.We showthatthis\n",
      "simplified approach, in addition to improving link prediction performance, can\n",
      "be understood in terms of tensor factorization, thus placing HypER within a\n",
      "well established family of factorizationmodels. The apparent obscurity of using\n",
      "convolutionwithinwordembeddingsistherebyexplainedassimplyaconvenient\n",
      "computational means of introducing sparsity and parameter tying.\n",
      "WeevaluateHypERagainstseveralpreviouslyproposedlinkpredictionmod-\n",
      "elsusingstandarddatasets(FB15k-237,WN18RR,FB15k,WN18,YAGO3-10),\n",
      "across which it consistently achieves state-of-the-art performance. In summary,\n",
      "our key contributions are:\n",
      "– proposinganew modelfor link prediction(HypER)whichachievesstate-of-\n",
      "the-art performance across all standard datasets;\n",
      "– showing that the benefit of using convolutional instead of fully connected\n",
      "layers is due to restricting the number of dimensions that interact (i.e. ex-\n",
      "plicitregularization),ratherthanfindinghigherdimensionalstructureinthe\n",
      "embeddings (as implied by ConvE); and\n",
      "– showingthatHypERinfactfallswithinabroadclassoftensorfactorization\n",
      "models despite the use ofconvolution,whichservesto providea goodtrade-\n",
      "off between expressiveness and number of parameters to learn.\n",
      "2 Related Work\n",
      "Numerous matrix factorization approaches to link prediction have been pro-\n",
      "posed. An early model, RESCAL [12], tackles the link prediction task by opti-\n",
      "mizingascoringfunctioncontainingabilinearproductbetweenvectorsforeach\n",
      "HypernetworkKnowledge Graph Embeddings 3\n",
      "of the subject and object entities and a full rank matrix for each relation. Dist-\n",
      "Mult[23]canbeviewedasaspecialcaseofRESCALwithadiagonalmatrixper\n",
      "relationtype,whichlimitsthelineartransformationperformedonentityvectors\n",
      "toa stretch.ComplEx[22]extends DistMult tothe complexdomain.TransE[1]\n",
      "is an affine model that represents a relation as a translation operation between\n",
      "subject and object entity vectors.\n",
      "A somewhat separate line of link prediction research introduces Relational\n",
      "GraphConvolutionalNetworks(R-GCNs)[15].R-GCNsuseaconvolutionoper-\n",
      "atortocapturelocalityinformationingraphs.Themodelclosesttoourownand\n",
      "which we draw inspiration from, is ConvE [3], where a convolution operation is\n",
      "performed on the subject entity vector and the relation vector, after they are\n",
      "each reshaped to a matrix and lengthwise concatenated. The obtained feature\n",
      "maps are flattened, put through a fully connected layer, and the inner product\n",
      "is taken with all object entity vectors to generate a score for each triple. Ad-\n",
      "vantagesofConvEoverpreviousapproachesinclude itsexpressiveness,achieved\n",
      "by using multiple layersof non-linearfeatures, its scalability to largeknowledge\n",
      "graphs, and its robustness to overfitting. However, it is not intuitive why con-\n",
      "volving across concatenated and reshaped subject entity and relation vectors\n",
      "should be effective.\n",
      "The proposed HypER model does no such reshaping or concatenation and\n",
      "thus avoidsboth implying any inherent 2D structure in the embeddings and re-\n",
      "stricting interaction to the concatenation boundary. Instead, HypER convolves\n",
      "every dimension of the subject entity embedding with relation-specific convo-\n",
      "lutional filters generated by the hypernetwork. This way, entity and relation\n",
      "embeddings are combined in a non-linear (quadratic) manner, unlike the lin-\n",
      "ear combination (weighted sum) in ConvE. This gives HypER more expressive\n",
      "power, while also reducing parameters.\n",
      "Interestingly, we find that the differences in moving from ConvE to HypER\n",
      "in fact bring the factorization and convolutional approaches together, since the\n",
      "1D convolution process is equivalent to multiplication by a highly sparse tensor\n",
      "withtiedweights(seeFigure2).Themultiplicationofthis“convolutionaltensor”\n",
      "(defined by the relation embedding and hypernetwork) and other weights gives\n",
      "an implicit relation matrix, corresponding to those in e.g. RESCAL, DistMult\n",
      "and ComplEx. Other than the method of deriving these relation matrices, the\n",
      "key difference to existing factorization approaches is the ReLU non-linearity\n",
      "applied prior to interaction with the object embedding.\n",
      "3 Link Prediction\n",
      "In link prediction, the aim is to learn a scoring function φ that assigns a score\n",
      "s = φ(e,r,e ) ∈ R to each input triple (e,r,e ), where e,e ∈ E are sub-\n",
      "1 2 1 2 1 2\n",
      "ject and object entities and r ∈ R a relation. The score indicates the strength\n",
      "of prediction that the given triple corresponds to a true fact, with positive\n",
      "scores meaning true and negative scores,false. Link prediction models typically\n",
      "map entity pair e,e to their corresponding distributed embedding represen-\n",
      "1 2\n",
      "tations e 1,e\n",
      "2\n",
      "∈ Rde and a score is assigned using a relation-specific function,\n",
      "4 Balaˇzevi´c et al.\n",
      "Table 1. Scoring functions of state-of-the-art link prediction models, the dimension-\n",
      "ality of their relation parameters, and their space complexity. d e and d r are the di-\n",
      "mensionsofentityandrelationembeddingsrespectively,e ∈Cde denotesthecomplex\n",
      "2\n",
      "conjugateofe 2,ande 1,w r ∈Rdw×dh denotea2Dreshapingofe 1andw r respectively.\n",
      "∗ is theconvolution operator, F r =vec−1(w rH) thematrix of relation specificconvo-\n",
      "lutionalfilters,vecisavectorizationofamatrixandvec−1 itsinverse,f isanon-linear\n",
      "function, and n e and n r respectively denotethenumberof entities and relations.\n",
      "Model ScoringFunction RelationParameters SpaceComplexity\n",
      "RESCAL[12] e⊤ 1Wre2 Wr∈Rde2 O(nede+nrd2 e)\n",
      "TransE[1] ke1+wr−e2k wr∈Rde O(nede+nrde)\n",
      "NTN[17] u⊤ rf(e1Wr[1..k]e2+Vr(cid:20)e e1 2(cid:21)+br) Wr u∈ rR ∈de R2 kk,, bV rr ∈∈ RR k2dek, O(nede+nrde2k)\n",
      "DistMult[23] he1,wr,e2i wr∈Rde O(nede+nrde)\n",
      "ComplEx[22] Re(he1,wr,e2i) wr∈Cde O(nede+nrde)\n",
      "C Ho yn pv EE R[3 (o]\n",
      "urs)\n",
      "ff (v(v ece (c e(f 1( ∗[e v1 e; cw −1r (] w∗ rw H)) )W )W)e )e2\n",
      "2\n",
      "ww rr ∈∈ RR dd rr OO (( nn ee dd ee ++ nn rr dd rr ))\n",
      "s=φ (e,e ).Themajorityoflinkpredictionmodelsapplythelogisticsigmoid\n",
      "r 1 2\n",
      "function σ(·) to the score to give a probabilistically interpretable prediction\n",
      "p = σ(s) ∈ [0,1] as to whether the queried fact is true. The scoring functions\n",
      "for models from across the literature and HypER are summarized in Table 1,\n",
      "togetherwiththe dimensionalityoftheirrelationparametersandthe significant\n",
      "terms of their space complexity.\n",
      "4 Hypernetwork Knowledge Graph Embeddings\n",
      "In this work, we propose a novel hypernetwork model for link prediction in\n",
      "knowledge graphs. In summary, the hypernetwork projects a vector embedding\n",
      "of each relation via a fully connected layer, the result of which is reshaped to\n",
      "give a set of convolutional filter weight vectors for each relation. We explain\n",
      "this process in more detail below. The idea of using convolutions on entity and\n",
      "relation embeddings stems from computer vision, where feature maps reflect\n",
      "patterns in the image such as lines or edges. Their role in the text domain is\n",
      "hardertointerpret,sincelittleisknownofthemeaningofasingledimensionina\n",
      "wordembedding.Webelieveconvolutionalfiltershavearegularizingeffectwhen\n",
      "appliedto wordembeddings (comparedto the correspondingfull tensor),as the\n",
      "filter size restricts which dimensions of embeddings can interact. This allows\n",
      "nonlinear expressiveness while limiting overfitting by using few parameters. A\n",
      "visualization of HypER is given in Figure 1.\n",
      "4.1 Scoring Function and Model Architecture\n",
      "The relation-specific scoring function for the HypER model is:\n",
      "φ (e,e )=f(vec(e ∗F )W)e\n",
      "r 1 2 1 r 2\n",
      "(1)\n",
      "=f(vec(e ∗vec−1(w H))W)e,\n",
      "1 r 2\n",
      "where the vec−1 operator reshapes a vector to a matrix, and non-linearity f is\n",
      "chosen to be a rectified linear unit (ReLU).\n",
      "HypernetworkKnowledge Graph Embeddings 5\n",
      "e1 Mr\n",
      "Convolve W f 0 0..3 1\n",
      "0.3\n",
      "0.2\n",
      "wr H × σ 0 0 0...5 9\n",
      "1\n",
      "0.2\n",
      "0.8\n",
      "Fr... 0.4\n",
      "E\n",
      "Fig.1.VisualizationoftheHypERmodelarchitecture.Subjectentityembeddinge is\n",
      "1\n",
      "convolvedwithfiltersF r,createdbythehypernetworkHfromrelationembeddingw r.\n",
      "The obtained feature maps M r are mapped to d e-dimensional space via W and the\n",
      "non-linearity f applied before being combined with all object vectors e ∈E through\n",
      "2\n",
      "an inner product to give a score for each triple. Predictions are obtained by applying\n",
      "thelogistic sigmoid function to each score.\n",
      "H Fr W\n",
      "dr\n",
      "Fr\n",
      "lm lm\n",
      "wr ⊗z nf ⊗yz\n",
      "nf nf Fr nf\n",
      "lf\n",
      "lf de de\n",
      "f\n",
      "y explicitlylearnedparameters\n",
      "⊗x ⊗x\n",
      "x inducedfilterweights e1 e2\n",
      "z zeroweights\n",
      "Fig.2. Interpretation of the HypER model in terms of tensor operations. Each rela-\n",
      "tion embedding w r generates a set of filters F r via the hypernetwork H. The act of\n",
      "convolving F r over e 1 is equivalent to multiplication of e 1 by a tensor Fr (in which\n",
      "F r is diagonally duplicated and zero elsewhere). The tensor product Fr ⊗yzW gives\n",
      "a d e ×d e matrix specific to each relation. Axes labels indicate the modes of tensor\n",
      "interaction (via inner product).\n",
      "In the feed-forward pass, the model obtains embeddings for the input triple\n",
      "from the entity and relation embedding matrices E ∈Rne×de and R ∈Rnr×dr.\n",
      "The hypernetwork is a fully connected layer H ∈ Rdr×lfnf (l\n",
      "f\n",
      "denotes filter\n",
      "length and n the number of filters per relation, i.e. output channels of the\n",
      "f\n",
      "convolution) that is applied to the relation embedding w\n",
      "r\n",
      "∈ Rdr. The result\n",
      "is reshaped to generate a matrix of convolutional filters F = vec−1(w H) ∈\n",
      "r r\n",
      "Rlf×nf. Whilst the overall dimensionality of the filter set is l fn f, the rank is\n",
      "restricted to d to encourage parameter sharing between relations.\n",
      "r\n",
      "Thesubjectentityembeddinge isconvolvedwiththesetofrelation-specific\n",
      "1\n",
      "filters F\n",
      "r\n",
      "to give a 2D feature map M\n",
      "r\n",
      "∈ Rlm×nf, where l\n",
      "m\n",
      "= d\n",
      "e\n",
      "−l\n",
      "f\n",
      "+1 is\n",
      "the feature maplength.The feature mapis vectorizedto vec(M r)∈Rlmnf,and\n",
      "projected to d e-dimensional space by the weight matrix W ∈ Rlmnf×de. After\n",
      "applying a ReLU activation function, the result is combined by way of inner\n",
      "productwitheachandeveryobjectentityembeddinge (i),whereivariesoverall\n",
      "2\n",
      "entitiesinthedataset(ofsizen ),togiveavectorofscores.Thelogisticsigmoid\n",
      "e\n",
      "isappliedelement-wisetothe scorevectortoobtainthe predictedprobabilityof\n",
      "each prospective triple being true p =σ(φ (e,e (i))).\n",
      "i r 1 2\n",
      "6 Balaˇzevi´c et al.\n",
      "4.2 Understanding HypER as Tensor Factorization\n",
      "Having described the HypER architecture, we can view it as a series of tensor\n",
      "operations by considering the hypernetworkH and weightmatrix W as tensors\n",
      "H ∈ Rdr×lf×nf and W ∈ Rlm×nf×de respectively. The act of convolving F\n",
      "r\n",
      "=\n",
      "w ⊗H overthe subject entity embedding e is equivalent to the multiplication\n",
      "r 1\n",
      "of e by a sparse tensor F within which F is diagonally duplicated with zeros\n",
      "1 r r\n",
      "elsewhere (see Figure 2). The result is multiplied by W to give a vector, which\n",
      "is subject to ReLU before the final dot product with e. Linearity allows the\n",
      "2\n",
      "product F ⊗W to be considered separately as generating a d ×d matrix for\n",
      "r e e\n",
      "each relation. Further, rather than duplicating entries of F within F, we can\n",
      "r r\n",
      "generalize F\n",
      "r\n",
      "to a relation-agnostic sparse 4 moded tensor F ∈ Rdr×de×nf×lm\n",
      "by replacing entries with d -dimensional strands of H. Thus, the HypER model\n",
      "r\n",
      "can be described explicitly as tensor multiplication of e,e and w with a core\n",
      "1 2 r\n",
      "tensorF⊗W ∈Rde×de×dr,whereF isheavilyconstrainedintermsofitsnumber\n",
      "of free variables.This insight allows HypER to be viewed in a very similar light\n",
      "to the family of factorization approaches to link prediction, such as RESCAL,\n",
      "DistMult and ComplEx.\n",
      "4.3 Training Procedure\n",
      "Followingthe training procedureintroducedby [3],we use 1-N scoring with the\n",
      "Adam optimizer [8] to minimize the binary cross-entropyloss:\n",
      "1\n",
      "L(p,y)=− X(y ilog(p i)+(1−y i)log(1−p i)), (2)\n",
      "n\n",
      "e\n",
      "i\n",
      "where y∈Rne is the label vector containing ones for true triples and zeros oth-\n",
      "erwise,subjecttolabel smoothing.Label smoothingisawidelyusedtechnique\n",
      "shown to improve generalization [20,14]. Label smoothing changes the ground-\n",
      "truth label distributionby adding a uniform priorto encouragethe model to be\n",
      "less confident, achieving a regularizing effect. 1-N scoring refers to simultane-\n",
      "ously scoring (e,r,E), i.e. for all entities e ∈E, in contrast to 1-1 scoring, the\n",
      "1 2\n",
      "practice of training individual triples (e,r,e ) one at a time. As shown by [3],\n",
      "1 2\n",
      "1-N scoring offers a significant speedup (3x on trainand 300xon test time) and\n",
      "improvedaccuracycomparedto1-1scoring.ApotentialextensionoftheHypER\n",
      "model described above would be to apply convolutional filters to both subject\n",
      "andobjectentityembeddings.However,sincethisisnottriviallyimplementable\n",
      "with 1-N scoring and wanting to keep its benefits, we leave this to future work.\n",
      "4.4 Number of Parameters\n",
      "Table 2 compares the number of parameters of ConvE and HypER (for the\n",
      "FB15k-237 dataset, which determines n and n ). It can be seen that, overall,\n",
      "e r\n",
      "HypERhasfewerparameters(4.3M)thanConvE(5.1M)duetothewayHypER\n",
      "directly transforms relations to convolutional filters.\n",
      "HypernetworkKnowledge Graph Embeddings 7\n",
      "Table 2.Comparison ofnumberofparametersforConvEandHypERonFB15k-237.\n",
      "h m and w m are height and width of the ConvEfeature maps respectively.\n",
      "Model E R Filters W\n",
      "ConvE\n",
      "ne×de nr×dr lfnf hmwmnf×de\n",
      "2.9M 0.1M 0.0M 2.1M\n",
      "HypER\n",
      "ne×de nr×dr dr×lfnf lmnf×de\n",
      "2.9M 0.1M 0.1M 1.2M\n",
      "5 Experiments\n",
      "5.1 Datasets\n",
      "We evaluate our HypER model on the standard link prediction task using the\n",
      "following datasets (see Table 3):\n",
      "FB15k [1]asubsetofFreebase,alargedatabaseoffactsabouttherealworld.\n",
      "WN18 [1] a subset of WordNet, containing lexical relations between words.\n",
      "FB15k-237 createdby[21],notingthatthevalidationandtestsetsofFB15k\n",
      "and WN18 contain the inverse of many relations present in the training set,\n",
      "making it easy for simple models to do well. FB15k-237 is a subset of FB15k\n",
      "with the inverse relations removed.\n",
      "WN18RR [3] a subset of WN18, created by removing the inverse relations.\n",
      "YAGO3-10 [3] a subset of YAGO3 [10], containing entities which have a\n",
      "minimum of 10 relations each.\n",
      "Table 3. Summary of dataset statistics.\n",
      "Dataset Entities(ne) Relations(nr)\n",
      "FB15k 14,951 1,345\n",
      "WN18 40,943 18\n",
      "FB15k-237 14,541 237\n",
      "WN18RR 40,943 11\n",
      "YAGO3-10 123,182 37\n",
      "5.2 Experimental Setup\n",
      "We implement HypER in PyTorch[13] and make our code publicly available.1\n",
      "Implementation Details We train our model with 200 dimension entity\n",
      "and relation embeddings (d = d = 200) and 1-N scoring. Whilst the relation\n",
      "e r\n",
      "embedding dimension does not have to equal the entity embedding dimension,\n",
      "we set d =200 to match ConvE for fairness of comparison.\n",
      "r\n",
      "Toacceleratetrainingandpreventoverfitting,weusebatchnormalization[6]\n",
      "and dropout [18] on the input embeddings, feature maps and the hidden layer.\n",
      "We perform a hyperparameter search and select the best performing model by\n",
      "mean reciprocal rank (MRR) on the validation set. Having tested the values\n",
      "{0.,0.1,0.2,0.3}, we find that the following combination of parameters works\n",
      "1 https://github.com/ibalazevic/HypER\n",
      "8 Balaˇzevi´c et al.\n",
      "wellacrossalldatasets:input dropout0.2,feature mapdropout0.2,andhidden\n",
      "dropout0.3,apartfromFB15k-237,wherewesetinputdropoutto0.3.Weselect\n",
      "the learning rate from {0.01,0.005,0.003,0.001,0.0005,0.0001}and exponential\n",
      "learning rate decay from {1.,0.99,0.995}for eachdatasetand find the best per-\n",
      "forming learning rate and learning rate decay to be dataset-specific. We set the\n",
      "convolution stride to 1, number of feature maps to 32 with the filter size 3×3\n",
      "for ConvEand1×9 for HypER,after testing different numbers offeature maps\n",
      "n ∈ {16,32,64} and filter sizes l ∈ {1×1,1×2,1×3,1×6,1×9,1×12}\n",
      "f f\n",
      "(see Table 9). We train all models using the Adam optimizer with batch size\n",
      "128.One epoch on FB15k-237takes approximately 12 seconds on a single GPU\n",
      "compared to 1 minute for e.g. RESCAL, largely due to 1-N scoring.\n",
      "Evaluation Results are obtained by iterating over all triples in the test set.\n",
      "Aparticulartriple isevaluatedbyreplacingthe objectentity e withallentities\n",
      "2\n",
      "E while keeping the subject entity e fixed and vice versa, obtaining scores for\n",
      "1\n",
      "eachcombination.Thesescoresarethenrankedusingthe“filtered”settingonly,\n",
      "i.e. we remove all true cases other than the current test triple [1].\n",
      "We evaluate HypER on five different metrics found throughout the link pre-\n",
      "diction literature: mean rank (MR), mean reciprocal rank (MRR), hits@10,\n",
      "hits@3, and hits@1. Mean rank is the average rank assigned to the true triple,\n",
      "overalltesttriples.Meanreciprocalranktakestheaverageofthereciprocalrank\n",
      "assigned to the true triple. Hits@k measures the percentage of cases in which\n",
      "the true triple appearsin the topk rankedtriples.Overall,the aimis to achieve\n",
      "highmeanreciprocalrankandhits@k andlow meanrank.For amore extensive\n",
      "description of how each of these metrics is calculated, we refer to [3].\n",
      "5.3 Results\n",
      "LinkpredictionresultsforallmodelsacrossthefivedatasetsareshowninTables\n",
      "4, 5 and 6. Our key findings are:\n",
      "– whilst having fewer parameters than the closest comparator ConvE, Hy-\n",
      "pER consistently outperforms all other models across all datasets, thereby\n",
      "achieving state-of-the-artresults on the link prediction task; and\n",
      "– our filter dimension study suggests that no benefit is gained by convolving\n",
      "over reshaped 2D entity embeddings in comparison with 1D entity embed-\n",
      "ding vectors and that most information can be extracted with very small\n",
      "convolutional filters (Table 9).\n",
      "Overall,HypERoutperformsallothermodelsonallmetricsapartfrommean\n",
      "reciprocalrank on WN18 and mean rank on WN18RR, FB15k-237,WN18, and\n",
      "YAGO3-10. Given that mean rank is known to be highly sensitive to outliers\n",
      "[11], this suggests that HypER correctly ranks many true triples in the top 10,\n",
      "but makes larger ranking errors elsewhere.\n",
      "Giventhat mostmodels inthe literature,withthe exceptionofConvE,were\n",
      "trained with 100 dimension embeddings and 1-1 scoring,we reimplement previ-\n",
      "ous models (DistMult, ComplEx and ConvE) with 200 dimension embeddings\n",
      "HypernetworkKnowledge Graph Embeddings 9\n",
      "and1-NscoringforfaircomparisonandreporttheobtainedresultsonWN18RR\n",
      "in Table 7. We perform the same hyperparameter search for every model and\n",
      "present the mean and standard deviation of each result across five runs (differ-\n",
      "ent random seeds). This improves most previously published results, except for\n",
      "ConvEwherewefailtoreplicatesomevalues.Notwithstanding,HypERremains\n",
      "the best performing model overalldespite better tuning of the competitors.\n",
      "Table4.LinkpredictionresultsonWN18RRandFB15k-237.TheRotatE[19]results\n",
      "are reported without their self-adversarial negative sampling (see Appendix H in the\n",
      "original paper) for fair comparison, given that it is not specific to that model only.\n",
      "WN18RR FB15k-237\n",
      "MR MRRH@10H@3H@1 MRMRRH@10H@3H@1\n",
      "DistMult[23] 5110.430.490.440.390 254.241.419.263.155\n",
      "ComplEx[22] 5261.440.510.460.410 339.247.428.275.158\n",
      "NeuralLP[24] − − − − − −.250.408 − −\n",
      "R-GCN[15] − − − − − −.248.417.264.151\n",
      "MINERVA[2] − − − − − − −.456 − −\n",
      "ConvE[3] 4187.430.520.440.400 244.325.501.356.237\n",
      "M-Walk[16] −.437 −.445.414 − − − − −\n",
      "RotatE[19] − − − − − 185.297.480.328.205\n",
      "HypER(ours) 5798.465.522.477.436 250.341.520.376.252\n",
      "Table 5. Link prediction results on WN18 and FB15k.\n",
      "WN18 FB15k\n",
      "MRMRRH@10H@3H@1 MRMRRH@10H@3H@1\n",
      "TransE[1] 251 −.892 − − 125 −.471 − −\n",
      "DistMult[23] 902.822.936.914.728 97.654.824.733.546\n",
      "ComplEx[22] −.941.947.936.936 −.692.840.759.599\n",
      "ANALOGY[9] −.942.947.944.939 −.725.854.785.646\n",
      "NeuralLP[24] −.940.945 − − −.760.837 − −\n",
      "R-GCN[15] −.819.964.929.697 −.696.842.760.601\n",
      "TorusE[4] −.947.954.950.943 −.733.832.771.674\n",
      "ConvE[3] 374.943.956.946.935 51.657.831.723.558\n",
      "SimplE[7] −.942.947.944.939 −.727.838.773.660\n",
      "HypER(ours) 431.951.958.955.947 44.790.885.829.734\n",
      "Table 6. Link prediction results on YAGO3-10.\n",
      "YAGO3-10\n",
      "MR MRRH@10H@3H@1\n",
      "DistMult[23] 5926.340.540.380.240\n",
      "ComplEx[22] 6351.360.550.400.260\n",
      "ConvE[3] 1676.440.620.490.350\n",
      "HypER(ours) 2529.533.678.580.455\n",
      "ToensurethatthedifferencebetweenreportedresultsforHypERandConvE\n",
      "is not simply due to HypER having a reduced number of parameters (implicit\n",
      "regularization), we trained ConvE reducing the number of feature maps to 16\n",
      "instead of 32 to have a comparable number of parameters to HypER (explicit\n",
      "10 Balaˇzevi´c et al.\n",
      "Table 7. Link prediction results on WN18RR;all models trained with 200 dimension\n",
      "embeddings and 1-N scoring.\n",
      "WN18RR\n",
      "MR MRR H@10 H@3 H@1\n",
      "DistMult[23] 4911±109.434±.002.508±.002.447±.001.399±.002\n",
      "ComplEx[22] 5930±125.446±.001.523±.002.462±.001.409±.001\n",
      "ConvE[3] 4997± 99.431±.001.504±.002.443±.002.396±.001\n",
      "HypER(ours) 5798±124.465±.002.522±.003.477±.002.436±.003\n",
      "regularization). This showed no improvement in ConvE results, indicating Hy-\n",
      "pER’s architecture does more than merely reducing the number of parameters.\n",
      "Table 8. Results with and without hypernetworkon WN18RR and FB15k-237.\n",
      "WN18RR FB15k-237\n",
      "MRR H@10 MRR H@10\n",
      "HypER.465±.002.522±.003.341±.001.520±.002\n",
      "HypER(noH).459±.002.511±.002.338±.001.515±.001\n",
      "Hypernetwork Influence To test the influence of the hypernetwork and,\n",
      "thereby, knowledge sharing between relations, we compare HypER results on\n",
      "WN18RRandFB15k-237withthehypernetworkcomponentremoved,i.e.with-\n",
      "outthefirstfullyconnectedlayerandwiththerelationembeddingsdirectlycor-\n",
      "respondingtoasetofconvolutionalfilters.ResultspresentedinTable8showthat\n",
      "thehypernetworkcomponentimprovesperformance,demonstratingthevalueof\n",
      "multi-task learning across different relations.\n",
      "FilterDimensionStudy Table9showsresultsofourstudyinvestigatingthe\n",
      "influenceofdifferentconvolutionalfiltersizesontheperformanceofHypER.The\n",
      "lowerpartofthetableshowsresultsfor2Dfiltersconvolvedoverreshaped(10×\n",
      "20)2Dsubjectentityembeddings.Itcanbeseenthatreshapingtheembeddings\n",
      "is of no benefit, especially onWN18RR.These results indicate that the purpose\n",
      "ofconvolutiononwordembeddingsisnottofindpatternsina2Dembedding(as\n",
      "with images), but perhaps to limit the number of dimensions that can interact\n",
      "with each other, thereby avoiding overfitting.In the upper part of the table, we\n",
      "vary the length of 1D filters, showing that comparable results can be achieved\n",
      "with filter<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,    605,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['FB15k', 'WN18', 'FB15k-237', 'WN18RR', 'YAGO3-10']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: asetofconvolutionalfilters.ResultspresentedinTable8showthat\n",
      "thehypernetworkcomponentimprovesperformance,demonstratingthevalueof\n",
      "multi-task learning across different relations.\n",
      "FilterDimensionStudy Table9showsresultsofourstudyinvestigatingthe\n",
      "influenceofdifferentconvolutionalfiltersizesontheperformanceofHypER.The\n",
      "lowerpartofthetableshowsresultsfor2Dfiltersconvolvedoverreshaped(10×\n",
      "20)2Dsubjectentityembeddings.Itcanbeseenthatreshapingtheembeddings\n",
      "is of no benefit, especially onWN18RR.These results indicate that the purpose\n",
      "ofconvolutiononwordembeddingsisnottofindpatternsina2Dembedding(as\n",
      "with images), but perhaps to limit the number of dimensions that can interact\n",
      "with each other, thereby avoiding overfitting.In the upper part of the table, we\n",
      "vary the length of 1D filters, showing that comparable results can be achieved\n",
      "with filter sizes 1×6 and 1×9, with diminishing results for smaller (e.g. 1×1)\n",
      "and larger (e.g. 1×12) filters.\n",
      "LabelSmoothing Contrarytotheablationstudyof[3],showingtheinfluence\n",
      "of hyperparameters on mean reciprocal rank for FB15k-237, from which they\n",
      "deemlabelsmoothingunimportant,wefindlabelsmoothingtogiveasignificant\n",
      "improvement in prediction scores for WN18RR. However, we find it does have\n",
      "a negative influence on the FB15k scores and as such, exclude label smoothing\n",
      "from our experiments on that dataset. We therefore recommend evaluating the\n",
      "influence of label smoothing on a per dataset basis and leave to future work\n",
      "analysis of the utility of label smoothing in the general case.\n",
      "HypernetworkKnowledge Graph Embeddings 11\n",
      "Table 9. Influenceof different filterdimension choices on prediction results.\n",
      "WN18RR FB15k-237\n",
      "FilterSizeMRRH@1 MRRH@1\n",
      "1×1.455.422.337.248\n",
      "1×2.458.428.337.248\n",
      "1×3.457.427.339.250\n",
      "1×6.459.429.340.251\n",
      "1×9.465.436.341.252\n",
      "1×12.457.428.341.252\n",
      "2×2.456.429.340.250\n",
      "3×3.458.430.339.250\n",
      "5×5.452.423.340.252\n",
      "6 Conclusion\n",
      "Inthis work,we introduce HypER,ahypernetworkmodelforlink predictionon\n",
      "knowledge graphs. HypER generates relation-specific convolutional filters and\n",
      "appliesthemtosubjectentityembeddings.Thehypernetworkcomponentallows\n",
      "information to be shared between relation vectors, enabling multi-task learning\n",
      "across relations. To our knowledge, HypER is the first link prediction model\n",
      "that creates non-linear interaction between entity and relation embeddings by\n",
      "convolving relation-specific filters over the entity embeddings.\n",
      "We show that no benefit is gained from 2D convolutional filters over 1D,\n",
      "dispelling the suggestion that 2D structure exists in entity embeddings implied\n",
      "by ConvE. We also recast HypER in terms of tensor operations showing that,\n",
      "despitetheconvolutionoperation,itiscloselyrelatedtotheestablishedfamilyof\n",
      "tensorfactorizationmodels.Ourresultssuggestthatconvolutionprovidesagood\n",
      "trade-off between expressiveness and parameter number compared to a dense\n",
      "network. HypER is fast, robust to overfitting, has relatively few parameters,\n",
      "and achieves state-of-the-art results across almost all metrics on multiple link\n",
      "prediction datasets.\n",
      "Future work might include expanding the current architecture by applying\n",
      "convolutional filters to both subject and object entity embeddings. We may\n",
      "also analyze the influence of label smoothing and explore the interpretability of\n",
      "convolutional feature maps to gain insight and potentially improve the model.\n",
      "Acknowledgements\n",
      "We thank Ivan Titov for helpful discussions on this work. Ivana Balaˇzevi´c and\n",
      "CarlAllen weresupportedby the Centre for DoctoralTrainingin Data Science,\n",
      "funded by EPSRC (grant EP/L016427/1)and the University of Edinburgh.\n",
      "References\n",
      "1. Bordes,A.,Usunier,N.,Garcia-Duran,A.,Weston,J.,Yakhnenko,O.:Translating\n",
      "Embeddingsfor Modeling Multi-relational Data. In:Advancesin Neural Informa-\n",
      "tion Processing Systems (2013)\n",
      "12 Balaˇzevi´c et al.\n",
      "2. Das, R., Dhuliawala, S., Zaheer, M., Vilnis, L., Durugkar, I., Krishnamurthy, A.,\n",
      "Smola, A., McCallum, A.: Go for a Walk and Arrive at the Answer: Reasoning\n",
      "over Paths in Knowledge Bases Using Reinforcement Learning. In: International\n",
      "Conference on Learning Representations (2018)\n",
      "3. Dettmers,T.,Minervini,P.,Stenetorp,P.,Riedel,S.:Convolutional2DKnowledge\n",
      "Graph Embeddings. In:Association for theAdvancementof Artificial Intelligence\n",
      "(2018)\n",
      "4. Ebisu, T., Ichise, R.: TorusE: Knowledge Graph Embedding on a Lie Group. In:\n",
      "Association for theAdvancementof Artificial Intelligence (2018)\n",
      "5. Ha,D.,Dai,A.,Le,Q.V.:Hypernetworks.In:InternationalConferenceonLearning\n",
      "Representations (2017)\n",
      "6. Ioffe, S., Szegedy, C.: Batch Normalization: Accelerating Deep Network Training\n",
      "by Reducing Internal Covariate Shift. In: International Conference on Machine\n",
      "Learning (2015)\n",
      "7. Kazemi, S.M., Poole, D.: SimplE Embedding for Link Prediction in Knowledge\n",
      "Graphs. In:Advancesin NeuralInformation Processing Systems(2018)\n",
      "8. Kingma, D.P., Ba, J.: Adam: A Method for Stochastic Optimization. In: Interna-\n",
      "tional Conference on Learning Representations (2015)\n",
      "9. Liu, H., Wu, Y., Yang, Y.: Analogical Inference for Multi-relational Embeddings.\n",
      "In:International Conference on Machine Learning (2017)\n",
      "10. Mahdisoltani,F.,Biega, J.,Suchanek,F.M.:Yago3:AKnowledgeBasefromMul-\n",
      "tilingual Wikipedias. In: Conference on InnovativeData Systems Research (2013)\n",
      "11. Nickel, M., Rosasco, L., Poggio, T.A.: Holographic Embeddings of Knowledge\n",
      "Graphs. In:Association for theAdvancementof Artificial Intelligence (2016)\n",
      "12. Nickel,M.,Tresp,V.,Kriegel,H.P.:AThree-WayModelforCollectiveLearningon\n",
      "Multi-Relational Data. In:International Conference on Machine Learning (2011)\n",
      "13. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,\n",
      "Desmaison, A., Antiga, L., Lerer, A.: Automatic Differentiation in PyTorch. In:\n",
      "NIPS-W(2017)\n",
      "14. Pereyra, G., Tucker, G., Chorowski, J., Kaiser, L., Hinton, G.: Regularizing\n",
      "neural networks by penalizing confident output distributions. arXiv preprint\n",
      "arXiv:1701.06548 (2017)\n",
      "15. Schlichtkrull, M., Kipf, T.N., Bloem, P., van den Berg, R., Titov, I., Welling,\n",
      "M.: Modeling Relational Data with Graph Convolutional Networks. In: European\n",
      "SemanticWeb Conference (2018)\n",
      "16. Shen, Y., Chen, J., Huang, P.S., Guo, Y., Gao, J.: M-Walk: Learning to Walk\n",
      "over Graphs using Monte Carlo Tree Search. In: Advances in Neural Information\n",
      "Processing Systems (2018)\n",
      "17. Socher,R.,Chen, D.,Manning, C.D., Ng,A.: Reasoning with NeuralTensor Net-\n",
      "works for Knowledge Base Completion. In: Advances in Neural Information Pro-\n",
      "cessing Systems(2013)\n",
      "18. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.:\n",
      "Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal\n",
      "of Machine Learning Research 15(1), 1929–1958 (2014)\n",
      "19. Sun,Z.,Deng,Z.H.,Nie,J.Y.,Tang,J.:RotatE:KnowledgeGraphEmbeddingby\n",
      "Relational Rotation in Complex Space. In: International Conference on Learning\n",
      "Representations (2019)\n",
      "20. Szegedy,C., Vanhoucke,V.,Ioffe,S.,Shlens,J., Wojna,Z.: RethinkingtheIncep-\n",
      "tion Architecture for Computer Vision. In: Computer Vision and Pattern Recog-\n",
      "nition (2016)\n",
      "HypernetworkKnowledge Graph Embeddings 13\n",
      "21. Toutanova, K., Chen, D., Pantel, P., Poon, H., Choudhury, P., Gamon, M.: Rep-\n",
      "resenting Text for Joint Embedding of Text and Knowledge Bases. In: Empirical\n",
      "Methods in Natural Language Processing (2015)\n",
      "22. Trouillon, T., Welbl, J., Riedel, S., Gaussier, E´., Bouchard, G.: Complex Embed-\n",
      "dingsforSimpleLinkPrediction.In:InternationalConferenceonMachineLearning\n",
      "(2016)\n",
      "23. Yang,B., Yih,W.t.,He,X.,Gao, J., Deng,L.: EmbeddingEntities andRelations\n",
      "for Learning and Inference in Knowledge Bases. In: International Conference on\n",
      "Learning Representations (2015)\n",
      "24. Yang, F., Yang, Z., Cohen, W.W.: Differentiable Learning of Logical Rules for\n",
      "Knowledge Base Reasoning. In: Advances in Neural Information Processing Sys-\n",
      "tems (2017)<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   8268,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['FB15k-237', 'WN18RR']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: 9102\n",
      "luJ\n",
      "51\n",
      "]GL.sc[\n",
      "5v81070.8081:viXra\n",
      "Hypernetwork Knowledge Graph Embeddings\n",
      "Ivana Balaˇzevi´c1, Carl Allen1, and Timothy M. Hospedales1,2\n",
      "1 School of Informatics, Universityof Edinburgh,UK\n",
      "2 Samsung AI Centre, Cambridge, UK\n",
      "{ivana.balazevic, carl.allen, t.hospedales}@ed.ac.uk\n",
      "Abstract. Knowledgegraphsaregraphicalrepresentationsoflargedata-\n",
      "basesoffacts,whichtypicallysufferfromincompleteness.Inferringmiss-\n",
      "ingrelations(links)betweenentities(nodes)isthetaskoflinkprediction.\n",
      "Arecentstate-of-the-artapproachtolinkprediction,ConvE,implements\n",
      "a convolutional neural network to extract features from concatenated\n",
      "subject and relation vectors. Whilst results are impressive, the method\n",
      "is unintuitive and poorly understood. We propose a hypernetwork ar-\n",
      "chitecturethatgenerates simplified relation-specific convolutionalfilters\n",
      "that(i)outperformsConvEandallpreviousapproachesacrossstandard\n",
      "datasets; and (ii) can be framed as tensor factorization and thus set\n",
      "within a well established family of factorization models for link predic-\n",
      "tion. We thus demonstrate that convolution simply offers a convenient\n",
      "computationalmeansofintroducingsparsityandparametertyingtofind\n",
      "an effective trade-off between non-linear expressiveness and the number\n",
      "of parameters to learn.\n",
      "1 Introduction\n",
      "Knowledge graphs, such as WordNet, Freebase, and Google Knowledge Graph,\n",
      "arelargegraph-structureddatabasesoffacts,containinginformationintheform\n",
      "oftriples(e,r,e ),withe ande representingsubjectandobjectentitiesandr\n",
      "1 2 1 2\n",
      "a relation between them. They are considered important information resources,\n",
      "used for a wide variety of tasks ranging from question answering to informa-\n",
      "tion retrieval and text summarization. One of the main challenges with existing\n",
      "knowledge graphs is their incompleteness: many of the links between entities in\n",
      "the graph are missing. This has inspired substantial work in the field of link\n",
      "prediction, i.e. the task of inferring missing links in knowledge graphs.\n",
      "Until recently, many approaches to link prediction have been based on dif-\n",
      "ferent factorizations of a 3-moded binary tensor representation of the training\n",
      "triples [12,17,23,22]. Such approaches are shallow and linear, with limited ex-\n",
      "pressiveness.However,attempts to increase expressiveness with additional fully\n",
      "connectedlayersandnon-linearitiesoftenleadtooverfitting[12,17].Forthisrea-\n",
      "son, Dettmers et al. introduce ConvE, a model that uses 2D convolutions over\n",
      "reshaped and concatenated entity and relation embeddings [3]. They motivate\n",
      "the use of convolutions by being parameter efficient and fast to compute on a\n",
      "GPU,aswellashavingvariousrobustmethodsfromcomputervisiontoprevent\n",
      "2 Balaˇzevi´c et al.\n",
      "overfitting. Even though results achieved by ConvE are impressive, it is highly\n",
      "unintuitive that convolution – particularly 2D convolution – should be effective\n",
      "for extracting information from 1D entity and relation embeddings.\n",
      "In this paper, we introduce HypER, a model that uses a hypernetwork [5]\n",
      "to generate convolutional filter weights for each relation. A hypernetwork is an\n",
      "approach by which one network generates weights for another network, that\n",
      "can be used to enable weight-sharing across layers and to dynamically synthe-\n",
      "size weights given an input. In our context, we generate relation-specific filter\n",
      "weights to process input entities, and also achieve multi-task knowledge shar-\n",
      "ing across relations in the knowledge graph. Our proposed HypER model uses\n",
      "a hypernetwork to generate a set of 1D relation-specific filters to process the\n",
      "subject entity embeddings. This simplifies the interaction between subject en-\n",
      "tity and relation embeddings compared to ConvE, in which a global set of 2D\n",
      "filters are convolved over reshaped and concatenated subject entity and relation\n",
      "embeddings, which is unintuitive as it suggests the presence of 2D structure in\n",
      "wordembeddings.Moreover,interactionbetweensubjectandrelationinConvE\n",
      "depends on an arbitrarychoice about how they are reshaped and concatenated.\n",
      "In contrast, HypER’s hypernetwork generates relation-specific filters, and thus\n",
      "extracts relation-specific features from the subject entity embedding. This ne-\n",
      "cessitatesno 2Dreshaping,andallowsentity andrelationtointeractmorecom-\n",
      "pletely,ratherthanonlyaroundthe concatenationboundary.We showthatthis\n",
      "simplified approach, in addition to improving link prediction performance, can\n",
      "be understood in terms of tensor factorization, thus placing HypER within a\n",
      "well established family of factorizationmodels. The apparent obscurity of using\n",
      "convolutionwithinwordembeddingsistherebyexplainedassimplyaconvenient\n",
      "computational means of introducing sparsity and parameter tying.\n",
      "WeevaluateHypERagainstseveralpreviouslyproposedlinkpredictionmod-\n",
      "elsusingstandarddatasets(FB15k-237,WN18RR,FB15k,WN18,YAGO3-10),\n",
      "across which it consistently achieves state-of-the-art performance. In summary,\n",
      "our key contributions are:\n",
      "– proposinganew modelfor link prediction(HypER)whichachievesstate-of-\n",
      "the-art performance across all standard datasets;\n",
      "– showing that the benefit of using convolutional instead of fully connected\n",
      "layers is due to restricting the number of dimensions that interact (i.e. ex-\n",
      "plicitregularization),ratherthanfindinghigherdimensionalstructureinthe\n",
      "embeddings (as implied by ConvE); and\n",
      "– showingthatHypERinfactfallswithinabroadclassoftensorfactorization\n",
      "models despite the use ofconvolution,whichservesto providea goodtrade-\n",
      "off between expressiveness and number of parameters to learn.\n",
      "2 Related Work\n",
      "Numerous matrix factorization approaches to link prediction have been pro-\n",
      "posed. An early model, RESCAL [12], tackles the link prediction task by opti-\n",
      "mizingascoringfunctioncontainingabilinearproductbetweenvectorsforeach\n",
      "HypernetworkKnowledge Graph Embeddings 3\n",
      "of the subject and object entities and a full rank matrix for each relation. Dist-\n",
      "Mult[23]canbeviewedasaspecialcaseofRESCALwithadiagonalmatrixper\n",
      "relationtype,whichlimitsthelineartransformationperformedonentityvectors\n",
      "toa stretch.ComplEx[22]extends DistMult tothe complexdomain.TransE[1]\n",
      "is an affine model that represents a relation as a translation operation between\n",
      "subject and object entity vectors.\n",
      "A somewhat separate line of link prediction research introduces Relational\n",
      "GraphConvolutionalNetworks(R-GCNs)[15].R-GCNsuseaconvolutionoper-\n",
      "atortocapturelocalityinformationingraphs.Themodelclosesttoourownand\n",
      "which we draw inspiration from, is ConvE [3], where a convolution operation is\n",
      "performed on the subject entity vector and the relation vector, after they are\n",
      "each reshaped to a matrix and lengthwise concatenated. The obtained feature\n",
      "maps are flattened, put through a fully connected layer, and the inner product\n",
      "is taken with all object entity vectors to generate a score for each triple. Ad-\n",
      "vantagesofConvEoverpreviousapproachesinclude itsexpressiveness,achieved\n",
      "by using multiple layersof non-linearfeatures, its scalability to largeknowledge\n",
      "graphs, and its robustness to overfitting. However, it is not intuitive why con-\n",
      "volving across concatenated and reshaped subject entity and relation vectors\n",
      "should be effective.\n",
      "The proposed HypER model does no such reshaping or concatenation and\n",
      "thus avoidsboth implying any inherent 2D structure in the embeddings and re-\n",
      "stricting interaction to the concatenation boundary. Instead, HypER convolves\n",
      "every dimension of the subject entity embedding with relation-specific convo-\n",
      "lutional filters generated by the hypernetwork. This way, entity and relation\n",
      "embeddings are combined in a non-linear (quadratic) manner, unlike the lin-\n",
      "ear combination (weighted sum) in ConvE. This gives HypER more expressive\n",
      "power, while also reducing parameters.\n",
      "Interestingly, we find that the differences in moving from ConvE to HypER\n",
      "in fact bring the factorization and convolutional approaches together, since the\n",
      "1D convolution process is equivalent to multiplication by a highly sparse tensor\n",
      "withtiedweights(seeFigure2).Themultiplicationofthis“convolutionaltensor”\n",
      "(defined by the relation embedding and hypernetwork) and other weights gives\n",
      "an implicit relation matrix, corresponding to those in e.g. RESCAL, DistMult\n",
      "and ComplEx. Other than the method of deriving these relation matrices, the\n",
      "key difference to existing factorization approaches is the ReLU non-linearity\n",
      "applied prior to interaction with the object embedding.\n",
      "3 Link Prediction\n",
      "In link prediction, the aim is to learn a scoring function φ that assigns a score\n",
      "s = φ(e,r,e ) ∈ R to each input triple (e,r,e ), where e,e ∈ E are sub-\n",
      "1 2 1 2 1 2\n",
      "ject and object entities and r ∈ R a relation. The score indicates the strength\n",
      "of prediction that the given triple corresponds to a true fact, with positive\n",
      "scores meaning true and negative scores,false. Link prediction models typically\n",
      "map entity pair e,e to their corresponding distributed embedding represen-\n",
      "1 2\n",
      "tations e 1,e\n",
      "2\n",
      "∈ Rde and a score is assigned using a relation-specific function,\n",
      "4 Balaˇzevi´c et al.\n",
      "Table 1. Scoring functions of state-of-the-art link prediction models, the dimension-\n",
      "ality of their relation parameters, and their space complexity. d e and d r are the di-\n",
      "mensionsofentityandrelationembeddingsrespectively,e ∈Cde denotesthecomplex\n",
      "2\n",
      "conjugateofe 2,ande 1,w r ∈Rdw×dh denotea2Dreshapingofe 1andw r respectively.\n",
      "∗ is theconvolution operator, F r =vec−1(w rH) thematrix of relation specificconvo-\n",
      "lutionalfilters,vecisavectorizationofamatrixandvec−1 itsinverse,f isanon-linear\n",
      "function, and n e and n r respectively denotethenumberof entities and relations.\n",
      "Model ScoringFunction RelationParameters SpaceComplexity\n",
      "RESCAL[12] e⊤ 1Wre2 Wr∈Rde2 O(nede+nrd2 e)\n",
      "TransE[1] ke1+wr−e2k wr∈Rde O(nede+nrde)\n",
      "NTN[17] u⊤ rf(e1Wr[1..k]e2+Vr(cid:20)e e1 2(cid:21)+br) Wr u∈ rR ∈de R2 kk,, bV rr ∈∈ RR k2dek, O(nede+nrde2k)\n",
      "DistMult[23] he1,wr,e2i wr∈Rde O(nede+nrde)\n",
      "ComplEx[22] Re(he1,wr,e2i) wr∈Cde O(nede+nrde)\n",
      "C Ho yn pv EE R[3 (o]\n",
      "urs)\n",
      "ff (v(v ece (c e(f 1( ∗[e v1 e; cw −1r (] w∗ rw H)) )W )W)e )e2\n",
      "2\n",
      "ww rr ∈∈ RR dd rr OO (( nn ee dd ee ++ nn rr dd rr ))\n",
      "s=φ (e,e ).Themajorityoflinkpredictionmodelsapplythelogisticsigmoid\n",
      "r 1 2\n",
      "function σ(·) to the score to give a probabilistically interpretable prediction\n",
      "p = σ(s) ∈ [0,1] as to whether the queried fact is true. The scoring functions\n",
      "for models from across the literature and HypER are summarized in Table 1,\n",
      "togetherwiththe dimensionalityoftheirrelationparametersandthe significant\n",
      "terms of their space complexity.\n",
      "4 Hypernetwork Knowledge Graph Embeddings\n",
      "In this work, we propose a novel hypernetwork model for link prediction in\n",
      "knowledge graphs. In summary, the hypernetwork projects a vector embedding\n",
      "of each relation via a fully connected layer, the result of which is reshaped to\n",
      "give a set of convolutional filter weight vectors for each relation. We explain\n",
      "this process in more detail below. The idea of using convolutions on entity and\n",
      "relation embeddings stems from computer vision, where feature maps reflect\n",
      "patterns in the image such as lines or edges. Their role in the text domain is\n",
      "hardertointerpret,sincelittleisknownofthemeaningofasingledimensionina\n",
      "wordembedding.Webelieveconvolutionalfiltershavearegularizingeffectwhen\n",
      "appliedto wordembeddings (comparedto the correspondingfull tensor),as the\n",
      "filter size restricts which dimensions of embeddings can interact. This allows\n",
      "nonlinear expressiveness while limiting overfitting by using few parameters. A\n",
      "visualization of HypER is given in Figure 1.\n",
      "4.1 Scoring Function and Model Architecture\n",
      "The relation-specific scoring function for the HypER model is:\n",
      "φ (e,e )=f(vec(e ∗F )W)e\n",
      "r 1 2 1 r 2\n",
      "(1)\n",
      "=f(vec(e ∗vec−1(w H))W)e,\n",
      "1 r 2\n",
      "where the vec−1 operator reshapes a vector to a matrix, and non-linearity f is\n",
      "chosen to be a rectified linear unit (ReLU).\n",
      "HypernetworkKnowledge Graph Embeddings 5\n",
      "e1 Mr\n",
      "Convolve W f 0 0..3 1\n",
      "0.3\n",
      "0.2\n",
      "wr H × σ 0 0 0...5 9\n",
      "1\n",
      "0.2\n",
      "0.8\n",
      "Fr... 0.4\n",
      "E\n",
      "Fig.1.VisualizationoftheHypERmodelarchitecture.Subjectentityembeddinge is\n",
      "1\n",
      "convolvedwithfiltersF r,createdbythehypernetworkHfromrelationembeddingw r.\n",
      "The obtained feature maps M r are mapped to d e-dimensional space via W and the\n",
      "non-linearity f applied before being combined with all object vectors e ∈E through\n",
      "2\n",
      "an inner product to give a score for each triple. Predictions are obtained by applying\n",
      "thelogistic sigmoid function to each score.\n",
      "H Fr W\n",
      "dr\n",
      "Fr\n",
      "lm lm\n",
      "wr ⊗z nf ⊗yz\n",
      "nf nf Fr nf\n",
      "lf\n",
      "lf de de\n",
      "f\n",
      "y explicitlylearnedparameters\n",
      "⊗x ⊗x\n",
      "x inducedfilterweights e1 e2\n",
      "z zeroweights\n",
      "Fig.2. Interpretation of the HypER model in terms of tensor operations. Each rela-\n",
      "tion embedding w r generates a set of filters F r via the hypernetwork H. The act of\n",
      "convolving F r over e 1 is equivalent to multiplication of e 1 by a tensor Fr (in which\n",
      "F r is diagonally duplicated and zero elsewhere). The tensor product Fr ⊗yzW gives\n",
      "a d e ×d e matrix specific to each relation. Axes labels indicate the modes of tensor\n",
      "interaction (via inner product).\n",
      "In the feed-forward pass, the model obtains embeddings for the input triple\n",
      "from the entity and relation embedding matrices E ∈Rne×de and R ∈Rnr×dr.\n",
      "The hypernetwork is a fully connected layer H ∈ Rdr×lfnf (l\n",
      "f\n",
      "denotes filter\n",
      "length and n the number of filters per relation, i.e. output channels of the\n",
      "f\n",
      "convolution) that is applied to the relation embedding w\n",
      "r\n",
      "∈ Rdr. The result\n",
      "is reshaped to generate a matrix of convolutional filters F = vec−1(w H) ∈\n",
      "r r\n",
      "Rlf×nf. Whilst the overall dimensionality of the filter set is l fn f, the rank is\n",
      "restricted to d to encourage parameter sharing between relations.\n",
      "r\n",
      "Thesubjectentityembeddinge isconvolvedwiththesetofrelation-specific\n",
      "1\n",
      "filters F\n",
      "r\n",
      "to give a 2D feature map M\n",
      "r\n",
      "∈ Rlm×nf, where l\n",
      "m\n",
      "= d\n",
      "e\n",
      "−l\n",
      "f\n",
      "+1 is\n",
      "the feature maplength.The feature mapis vectorizedto vec(M r)∈Rlmnf,and\n",
      "projected to d e-dimensional space by the weight matrix W ∈ Rlmnf×de. After\n",
      "applying a ReLU activation function, the result is combined by way of inner\n",
      "productwitheachandeveryobjectentityembeddinge (i),whereivariesoverall\n",
      "2\n",
      "entitiesinthedataset(ofsizen ),togiveavectorofscores.Thelogisticsigmoid\n",
      "e\n",
      "isappliedelement-wisetothe scorevectortoobtainthe predictedprobabilityof\n",
      "each prospective triple being true p =σ(φ (e,e (i))).\n",
      "i r 1 2\n",
      "6 Balaˇzevi´c et al.\n",
      "4.2 Understanding HypER as Tensor Factorization\n",
      "Having described the HypER architecture, we can view it as a series of tensor\n",
      "operations by considering the hypernetworkH and weightmatrix W as tensors\n",
      "H ∈ Rdr×lf×nf and W ∈ Rlm×nf×de respectively. The act of convolving F\n",
      "r\n",
      "=\n",
      "w ⊗H overthe subject entity embedding e is equivalent to the multiplication\n",
      "r 1\n",
      "of e by a sparse tensor F within which F is diagonally duplicated with zeros\n",
      "1 r r\n",
      "elsewhere (see Figure 2). The result is multiplied by W to give a vector, which\n",
      "is subject to ReLU before the final dot product with e. Linearity allows the\n",
      "2\n",
      "product F ⊗W to be considered separately as generating a d ×d matrix for\n",
      "r e e\n",
      "each relation. Further, rather than duplicating entries of F within F, we can\n",
      "r r\n",
      "generalize F\n",
      "r\n",
      "to a relation-agnostic sparse 4 moded tensor F ∈ Rdr×de×nf×lm\n",
      "by replacing entries with d -dimensional strands of H. Thus, the HypER model\n",
      "r\n",
      "can be described explicitly as tensor multiplication of e,e and w with a core\n",
      "1 2 r\n",
      "tensorF⊗W ∈Rde×de×dr,whereF isheavilyconstrainedintermsofitsnumber\n",
      "of free variables.This insight allows HypER to be viewed in a very similar light\n",
      "to the family of factorization approaches to link prediction, such as RESCAL,\n",
      "DistMult and ComplEx.\n",
      "4.3 Training Procedure\n",
      "Followingthe training procedureintroducedby [3],we use 1-N scoring with the\n",
      "Adam optimizer [8] to minimize the binary cross-entropyloss:\n",
      "1\n",
      "L(p,y)=− X(y ilog(p i)+(1−y i)log(1−p i)), (2)\n",
      "n\n",
      "e\n",
      "i\n",
      "where y∈Rne is the label vector containing ones for true triples and zeros oth-\n",
      "erwise,subjecttolabel smoothing.Label smoothingisawidelyusedtechnique\n",
      "shown to improve generalization [20,14]. Label smoothing changes the ground-\n",
      "truth label distributionby adding a uniform priorto encouragethe model to be\n",
      "less confident, achieving a regularizing effect. 1-N scoring refers to simultane-\n",
      "ously scoring (e,r,E), i.e. for all entities e ∈E, in contrast to 1-1 scoring, the\n",
      "1 2\n",
      "practice of training individual triples (e,r,e ) one at a time. As shown by [3],\n",
      "1 2\n",
      "1-N scoring offers a significant speedup (3x on trainand 300xon test time) and\n",
      "improvedaccuracycomparedto1-1scoring.ApotentialextensionoftheHypER\n",
      "model described above would be to apply convolutional filters to both subject\n",
      "andobjectentityembeddings.However,sincethisisnottriviallyimplementable\n",
      "with 1-N scoring and wanting to keep its benefits, we leave this to future work.\n",
      "4.4 Number of Parameters\n",
      "Table 2 compares the number of parameters of ConvE and HypER (for the\n",
      "FB15k-237 dataset, which determines n and n ). It can be seen that, overall,\n",
      "e r\n",
      "HypERhasfewerparameters(4.3M)thanConvE(5.1M)duetothewayHypER\n",
      "directly transforms relations to convolutional filters.\n",
      "HypernetworkKnowledge Graph Embeddings 7\n",
      "Table 2.Comparison ofnumberofparametersforConvEandHypERonFB15k-237.\n",
      "h m and w m are height and width of the ConvEfeature maps respectively.\n",
      "Model E R Filters W\n",
      "ConvE\n",
      "ne×de nr×dr lfnf hmwmnf×de\n",
      "2.9M 0.1M 0.0M 2.1M\n",
      "HypER\n",
      "ne×de nr×dr dr×lfnf lmnf×de\n",
      "2.9M 0.1M 0.1M 1.2M\n",
      "5 Experiments\n",
      "5.1 Datasets\n",
      "We evaluate our HypER model on the standard link prediction task using the\n",
      "following datasets (see Table 3):\n",
      "FB15k [1]asubsetofFreebase,alargedatabaseoffactsabouttherealworld.\n",
      "WN18 [1] a subset of WordNet, containing lexical relations between words.\n",
      "FB15k-237 createdby[21],notingthatthevalidationandtestsetsofFB15k\n",
      "and WN18 contain the inverse of many relations present in the training set,\n",
      "making it easy for simple models to do well. FB15k-237 is a subset of FB15k\n",
      "with the inverse relations removed.\n",
      "WN18RR [3] a subset of WN18, created by removing the inverse relations.\n",
      "YAGO3-10 [3] a subset of YAGO3 [10], containing entities which have a\n",
      "minimum of 10 relations each.\n",
      "Table 3. Summary of dataset statistics.\n",
      "Dataset Entities(ne) Relations(nr)\n",
      "FB15k 14,951 1,345\n",
      "WN18 40,943 18\n",
      "FB15k-237 14,541 237\n",
      "WN18RR 40,943 11\n",
      "YAGO3-10 123,182 37\n",
      "5.2 Experimental Setup\n",
      "We implement HypER in PyTorch[13] and make our code publicly available.1\n",
      "Implementation Details We train our model with 200 dimension entity\n",
      "and relation embeddings (d = d = 200) and 1-N scoring. Whilst the relation\n",
      "e r\n",
      "embedding dimension does not have to equal the entity embedding dimension,\n",
      "we set d =200 to match ConvE for fairness of comparison.\n",
      "r\n",
      "Toacceleratetrainingandpreventoverfitting,weusebatchnormalization[6]\n",
      "and dropout [18] on the input embeddings, feature maps and the hidden layer.\n",
      "We perform a hyperparameter search and select the best performing model by\n",
      "mean reciprocal rank (MRR) on the validation set. Having tested the values\n",
      "{0.,0.1,0.2,0.3}, we find that the following combination of parameters works\n",
      "1 https://github.com/ibalazevic/HypER\n",
      "8 Balaˇzevi´c et al.\n",
      "wellacrossalldatasets:input dropout0.2,feature mapdropout0.2,andhidden\n",
      "dropout0.3,apartfromFB15k-237,wherewesetinputdropoutto0.3.Weselect\n",
      "the learning rate from {0.01,0.005,0.003,0.001,0.0005,0.0001}and exponential\n",
      "learning rate decay from {1.,0.99,0.995}for eachdatasetand find the best per-\n",
      "forming learning rate and learning rate decay to be dataset-specific. We set the\n",
      "convolution stride to 1, number of feature maps to 32 with the filter size 3×3\n",
      "for ConvEand1×9 for HypER,after testing different numbers offeature maps\n",
      "n ∈ {16,32,64} and filter sizes l ∈ {1×1,1×2,1×3,1×6,1×9,1×12}\n",
      "f f\n",
      "(see Table 9). We train all models using the Adam optimizer with batch size\n",
      "128.One epoch on FB15k-237takes approximately 12 seconds on a single GPU\n",
      "compared to 1 minute for e.g. RESCAL, largely due to 1-N scoring.\n",
      "Evaluation Results are obtained by iterating over all triples in the test set.\n",
      "Aparticulartriple isevaluatedbyreplacingthe objectentity e withallentities\n",
      "2\n",
      "E while keeping the subject entity e fixed and vice versa, obtaining scores for\n",
      "1\n",
      "eachcombination.Thesescoresarethenrankedusingthe“filtered”settingonly,\n",
      "i.e. we remove all true cases other than the current test triple [1].\n",
      "We evaluate HypER on five different metrics found throughout the link pre-\n",
      "diction literature: mean rank (MR), mean reciprocal rank (MRR), hits@10,\n",
      "hits@3, and hits@1. Mean rank is the average rank assigned to the true triple,\n",
      "overalltesttriples.Meanreciprocalranktakestheaverageofthereciprocalrank\n",
      "assigned to the true triple. Hits@k measures the percentage of cases in which\n",
      "the true triple appearsin the topk rankedtriples.Overall,the aimis to achieve\n",
      "highmeanreciprocalrankandhits@k andlow meanrank.For amore extensive\n",
      "description of how each of these metrics is calculated, we refer to [3].\n",
      "5.3 Results\n",
      "LinkpredictionresultsforallmodelsacrossthefivedatasetsareshowninTables\n",
      "4, 5 and 6. Our key findings are:\n",
      "– whilst having fewer parameters than the closest comparator ConvE, Hy-\n",
      "pER consistently outperforms all other models across all datasets, thereby\n",
      "achieving state-of-the-artresults on the link prediction task; and\n",
      "– our filter dimension study suggests that no benefit is gained by convolving\n",
      "over reshaped 2D entity embeddings in comparison with 1D entity embed-\n",
      "ding vectors and that most information can be extracted with very small\n",
      "convolutional filters (Table 9).\n",
      "Overall,HypERoutperformsallothermodelsonallmetricsapartfrommean\n",
      "reciprocalrank on WN18 and mean rank on WN18RR, FB15k-237,WN18, and\n",
      "YAGO3-10. Given that mean rank is known to be highly sensitive to outliers\n",
      "[11], this suggests that HypER correctly ranks many true triples in the top 10,\n",
      "but makes larger ranking errors elsewhere.\n",
      "Giventhat mostmodels inthe literature,withthe exceptionofConvE,were\n",
      "trained with 100 dimension embeddings and 1-1 scoring,we reimplement previ-\n",
      "ous models (DistMult, ComplEx and ConvE) with 200 dimension embeddings\n",
      "HypernetworkKnowledge Graph Embeddings 9\n",
      "and1-NscoringforfaircomparisonandreporttheobtainedresultsonWN18RR\n",
      "in Table 7. We perform the same hyperparameter search for every model and\n",
      "present the mean and standard deviation of each result across five runs (differ-\n",
      "ent random seeds). This improves most previously published results, except for\n",
      "ConvEwherewefailtoreplicatesomevalues.Notwithstanding,HypERremains\n",
      "the best performing model overalldespite better tuning of the competitors.\n",
      "Table4.LinkpredictionresultsonWN18RRandFB15k-237.TheRotatE[19]results\n",
      "are reported without their self-adversarial negative sampling (see Appendix H in the\n",
      "original paper) for fair comparison, given that it is not specific to that model only.\n",
      "WN18RR FB15k-237\n",
      "MR MRRH@10H@3H@1 MRMRRH@10H@3H@1\n",
      "DistMult[23] 5110.430.490.440.390 254.241.419.263.155\n",
      "ComplEx[22] 5261.440.510.460.410 339.247.428.275.158\n",
      "NeuralLP[24] − − − − − −.250.408 − −\n",
      "R-GCN[15] − − − − − −.248.417.264.151\n",
      "MINERVA[2] − − − − − − −.456 − −\n",
      "ConvE[3] 4187.430.520.440.400 244.325.501.356.237\n",
      "M-Walk[16] −.437 −.445.414 − − − − −\n",
      "RotatE[19] − − − − − 185.297.480.328.205\n",
      "HypER(ours) 5798.465.522.477.436 250.341.520.376.252\n",
      "Table 5. Link prediction results on WN18 and FB15k.\n",
      "WN18 FB15k\n",
      "MRMRRH@10H@3H@1 MRMRRH@10H@3H@1\n",
      "TransE[1] 251 −.892 − − 125 −.471 − −\n",
      "DistMult[23] 902.822.936.914.728 97.654.824.733.546\n",
      "ComplEx[22] −.941.947.936.936 −.692.840.759.599\n",
      "ANALOGY[9] −.942.947.944.939 −.725.854.785.646\n",
      "NeuralLP[24] −.940.945 − − −.760.837 − −\n",
      "R-GCN[15] −.819.964.929.697 −.696.842.760.601\n",
      "TorusE[4] −.947.954.950.943 −.733.832.771.674\n",
      "ConvE[3] 374.943.956.946.935 51.657.831.723.558\n",
      "SimplE[7] −.942.947.944.939 −.727.838.773.660\n",
      "HypER(ours) 431.951.958.955.947 44.790.885.829.734\n",
      "Table 6. Link prediction results on YAGO3-10.\n",
      "YAGO3-10\n",
      "MR MRRH@10H@3H@1\n",
      "DistMult[23] 5926.340.540.380.240\n",
      "ComplEx[22] 6351.360.550.400.260\n",
      "ConvE[3] 1676.440.620.490.350\n",
      "HypER(ours) 2529.533.678.580.455\n",
      "ToensurethatthedifferencebetweenreportedresultsforHypERandConvE\n",
      "is not simply due to HypER having a reduced number of parameters (implicit\n",
      "regularization), we trained ConvE reducing the number of feature maps to 16\n",
      "instead of 32 to have a comparable number of parameters to HypER (explicit\n",
      "10 Balaˇzevi´c et al.\n",
      "Table 7. Link prediction results on WN18RR;all models trained with 200 dimension\n",
      "embeddings and 1-N scoring.\n",
      "WN18RR\n",
      "MR MRR H@10 H@3 H@1\n",
      "DistMult[23] 4911±109.434±.002.508±.002.447±.001.399±.002\n",
      "ComplEx[22] 5930±125.446±.001.523±.002.462±.001.409±.001\n",
      "ConvE[3] 4997± 99.431±.001.504±.002.443±.002.396±.001\n",
      "HypER(ours) 5798±124.465±.002.522±.003.477±.002.436±.003\n",
      "regularization). This showed no improvement in ConvE results, indicating Hy-\n",
      "pER’s architecture does more than merely reducing the number of parameters.\n",
      "Table 8. Results with and without hypernetworkon WN18RR and FB15k-237.\n",
      "WN18RR FB15k-237\n",
      "MRR H@10 MRR H@10\n",
      "HypER.465±.002.522±.003.341±.001.520±.002\n",
      "HypER(noH).459±.002.511±.002.338±.001.515±.001\n",
      "Hypernetwork Influence To test the influence of the hypernetwork and,\n",
      "thereby, knowledge sharing between relations, we compare HypER results on\n",
      "WN18RRandFB15k-237withthehypernetworkcomponentremoved,i.e.with-\n",
      "outthefirstfullyconnectedlayerandwiththerelationembeddingsdirectlycor-\n",
      "respondingtoasetofconvolutionalfilters.ResultspresentedinTable8showthat\n",
      "thehypernetworkcomponentimprovesperformance,demonstratingthevalueof\n",
      "multi-task learning across different relations.\n",
      "FilterDimensionStudy Table9showsresultsofourstudyinvestigatingthe\n",
      "influenceofdifferentconvolutionalfiltersizesontheperformanceofHypER.The\n",
      "lowerpartofthetableshowsresultsfor2Dfiltersconvolvedoverreshaped(10×\n",
      "20)2Dsubjectentityembeddings.Itcanbeseenthatreshapingtheembeddings\n",
      "is of no benefit, especially onWN18RR.These results indicate that the purpose\n",
      "ofconvolutiononwordembeddingsisnottofindpatternsina2Dembedding(as\n",
      "with images), but perhaps to limit the number of dimensions that can interact\n",
      "with each other, thereby avoiding overfitting.In the upper part of the table, we\n",
      "vary the length of 1D filters, showing that comparable results can be achieved\n",
      "with filter<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  62965,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Link Prediction']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: asetofconvolutionalfilters.ResultspresentedinTable8showthat\n",
      "thehypernetworkcomponentimprovesperformance,demonstratingthevalueof\n",
      "multi-task learning across different relations.\n",
      "FilterDimensionStudy Table9showsresultsofourstudyinvestigatingthe\n",
      "influenceofdifferentconvolutionalfiltersizesontheperformanceofHypER.The\n",
      "lowerpartofthetableshowsresultsfor2Dfiltersconvolvedoverreshaped(10×\n",
      "20)2Dsubjectentityembeddings.Itcanbeseenthatreshapingtheembeddings\n",
      "is of no benefit, especially onWN18RR.These results indicate that the purpose\n",
      "ofconvolutiononwordembeddingsisnottofindpatternsina2Dembedding(as\n",
      "with images), but perhaps to limit the number of dimensions that can interact\n",
      "with each other, thereby avoiding overfitting.In the upper part of the table, we\n",
      "vary the length of 1D filters, showing that comparable results can be achieved\n",
      "with filter sizes 1×6 and 1×9, with diminishing results for smaller (e.g. 1×1)\n",
      "and larger (e.g. 1×12) filters.\n",
      "LabelSmoothing Contrarytotheablationstudyof[3],showingtheinfluence\n",
      "of hyperparameters on mean reciprocal rank for FB15k-237, from which they\n",
      "deemlabelsmoothingunimportant,wefindlabelsmoothingtogiveasignificant\n",
      "improvement in prediction scores for WN18RR. However, we find it does have\n",
      "a negative influence on the FB15k scores and as such, exclude label smoothing\n",
      "from our experiments on that dataset. We therefore recommend evaluating the\n",
      "influence of label smoothing on a per dataset basis and leave to future work\n",
      "analysis of the utility of label smoothing in the general case.\n",
      "HypernetworkKnowledge Graph Embeddings 11\n",
      "Table 9. Influenceof different filterdimension choices on prediction results.\n",
      "WN18RR FB15k-237\n",
      "FilterSizeMRRH@1 MRRH@1\n",
      "1×1.455.422.337.248\n",
      "1×2.458.428.337.248\n",
      "1×3.457.427.339.250\n",
      "1×6.459.429.340.251\n",
      "1×9.465.436.341.252\n",
      "1×12.457.428.341.252\n",
      "2×2.456.429.340.250\n",
      "3×3.458.430.339.250\n",
      "5×5.452.423.340.252\n",
      "6 Conclusion\n",
      "Inthis work,we introduce HypER,ahypernetworkmodelforlink predictionon\n",
      "knowledge graphs. HypER generates relation-specific convolutional filters and\n",
      "appliesthemtosubjectentityembeddings.Thehypernetworkcomponentallows\n",
      "information to be shared between relation vectors, enabling multi-task learning\n",
      "across relations. To our knowledge, HypER is the first link prediction model\n",
      "that creates non-linear interaction between entity and relation embeddings by\n",
      "convolving relation-specific filters over the entity embeddings.\n",
      "We show that no benefit is gained from 2D convolutional filters over 1D,\n",
      "dispelling the suggestion that 2D structure exists in entity embeddings implied\n",
      "by ConvE. We also recast HypER in terms of tensor operations showing that,\n",
      "despitetheconvolutionoperation,itiscloselyrelatedtotheestablishedfamilyof\n",
      "tensorfactorizationmodels.Ourresultssuggestthatconvolutionprovidesagood\n",
      "trade-off between expressiveness and parameter number compared to a dense\n",
      "network. HypER is fast, robust to overfitting, has relatively few parameters,\n",
      "and achieves state-of-the-art results across almost all metrics on multiple link\n",
      "prediction datasets.\n",
      "Future work might include expanding the current architecture by applying\n",
      "convolutional filters to both subject and object entity embeddings. We may\n",
      "also analyze the influence of label smoothing and explore the interpretability of\n",
      "convolutional feature maps to gain insight and potentially improve the model.\n",
      "Acknowledgements\n",
      "We thank Ivan Titov for helpful discussions on this work. Ivana Balaˇzevi´c and\n",
      "CarlAllen weresupportedby the Centre for DoctoralTrainingin Data Science,\n",
      "funded by EPSRC (grant EP/L016427/1)and the University of Edinburgh.\n",
      "References\n",
      "1. Bordes,A.,Usunier,N.,Garcia-Duran,A.,Weston,J.,Yakhnenko,O.:Translating\n",
      "Embeddingsfor Modeling Multi-relational Data. In:Advancesin Neural Informa-\n",
      "tion Processing Systems (2013)\n",
      "12 Balaˇzevi´c et al.\n",
      "2. Das, R., Dhuliawala, S., Zaheer, M., Vilnis, L., Durugkar, I., Krishnamurthy, A.,\n",
      "Smola, A., McCallum, A.: Go for a Walk and Arrive at the Answer: Reasoning\n",
      "over Paths in Knowledge Bases Using Reinforcement Learning. In: International\n",
      "Conference on Learning Representations (2018)\n",
      "3. Dettmers,T.,Minervini,P.,Stenetorp,P.,Riedel,S.:Convolutional2DKnowledge\n",
      "Graph Embeddings. In:Association for theAdvancementof Artificial Intelligence\n",
      "(2018)\n",
      "4. Ebisu, T., Ichise, R.: TorusE: Knowledge Graph Embedding on a Lie Group. In:\n",
      "Association for theAdvancementof Artificial Intelligence (2018)\n",
      "5. Ha,D.,Dai,A.,Le,Q.V.:Hypernetworks.In:InternationalConferenceonLearning\n",
      "Representations (2017)\n",
      "6. Ioffe, S., Szegedy, C.: Batch Normalization: Accelerating Deep Network Training\n",
      "by Reducing Internal Covariate Shift. In: International Conference on Machine\n",
      "Learning (2015)\n",
      "7. Kazemi, S.M., Poole, D.: SimplE Embedding for Link Prediction in Knowledge\n",
      "Graphs. In:Advancesin NeuralInformation Processing Systems(2018)\n",
      "8. Kingma, D.P., Ba, J.: Adam: A Method for Stochastic Optimization. In: Interna-\n",
      "tional Conference on Learning Representations (2015)\n",
      "9. Liu, H., Wu, Y., Yang, Y.: Analogical Inference for Multi-relational Embeddings.\n",
      "In:International Conference on Machine Learning (2017)\n",
      "10. Mahdisoltani,F.,Biega, J.,Suchanek,F.M.:Yago3:AKnowledgeBasefromMul-\n",
      "tilingual Wikipedias. In: Conference on InnovativeData Systems Research (2013)\n",
      "11. Nickel, M., Rosasco, L., Poggio, T.A.: Holographic Embeddings of Knowledge\n",
      "Graphs. In:Association for theAdvancementof Artificial Intelligence (2016)\n",
      "12. Nickel,M.,Tresp,V.,Kriegel,H.P.:AThree-WayModelforCollectiveLearningon\n",
      "Multi-Relational Data. In:International Conference on Machine Learning (2011)\n",
      "13. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,\n",
      "Desmaison, A., Antiga, L., Lerer, A.: Automatic Differentiation in PyTorch. In:\n",
      "NIPS-W(2017)\n",
      "14. Pereyra, G., Tucker, G., Chorowski, J., Kaiser, L., Hinton, G.: Regularizing\n",
      "neural networks by penalizing confident output distributions. arXiv preprint\n",
      "arXiv:1701.06548 (2017)\n",
      "15. Schlichtkrull, M., Kipf, T.N., Bloem, P., van den Berg, R., Titov, I., Welling,\n",
      "M.: Modeling Relational Data with Graph Convolutional Networks. In: European\n",
      "SemanticWeb Conference (2018)\n",
      "16. Shen, Y., Chen, J., Huang, P.S., Guo, Y., Gao, J.: M-Walk: Learning to Walk\n",
      "over Graphs using Monte Carlo Tree Search. In: Advances in Neural Information\n",
      "Processing Systems (2018)\n",
      "17. Socher,R.,Chen, D.,Manning, C.D., Ng,A.: Reasoning with NeuralTensor Net-\n",
      "works for Knowledge Base Completion. In: Advances in Neural Information Pro-\n",
      "cessing Systems(2013)\n",
      "18. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.:\n",
      "Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal\n",
      "of Machine Learning Research 15(1), 1929–1958 (2014)\n",
      "19. Sun,Z.,Deng,Z.H.,Nie,J.Y.,Tang,J.:RotatE:KnowledgeGraphEmbeddingby\n",
      "Relational Rotation in Complex Space. In: International Conference on Learning\n",
      "Representations (2019)\n",
      "20. Szegedy,C., Vanhoucke,V.,Ioffe,S.,Shlens,J., Wojna,Z.: RethinkingtheIncep-\n",
      "tion Architecture for Computer Vision. In: Computer Vision and Pattern Recog-\n",
      "nition (2016)\n",
      "HypernetworkKnowledge Graph Embeddings 13\n",
      "21. Toutanova, K., Chen, D., Pantel, P., Poon, H., Choudhury, P., Gamon, M.: Rep-\n",
      "resenting Text for Joint Embedding of Text and Knowledge Bases. In: Empirical\n",
      "Methods in Natural Language Processing (2015)\n",
      "22. Trouillon, T., Welbl, J., Riedel, S., Gaussier, E´., Bouchard, G.: Complex Embed-\n",
      "dingsforSimpleLinkPrediction.In:InternationalConferenceonMachineLearning\n",
      "(2016)\n",
      "23. Yang,B., Yih,W.t.,He,X.,Gao, J., Deng,L.: EmbeddingEntities andRelations\n",
      "for Learning and Inference in Knowledge Bases. In: International Conference on\n",
      "Learning Representations (2015)\n",
      "24. Yang, F., Yang, Z., Cohen, W.W.: Differentiable Learning of Logical Rules for\n",
      "Knowledge Base Reasoning. In: Advances in Neural Information Processing Sys-\n",
      "tems (2017)<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  40099,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['link prediction on knowledge graphs']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: 9102\n",
      "luJ\n",
      "51\n",
      "]GL.sc[\n",
      "5v81070.8081:viXra\n",
      "Hypernetwork Knowledge Graph Embeddings\n",
      "Ivana Balaˇzevi´c1, Carl Allen1, and Timothy M. Hospedales1,2\n",
      "1 School of Informatics, Universityof Edinburgh,UK\n",
      "2 Samsung AI Centre, Cambridge, UK\n",
      "{ivana.balazevic, carl.allen, t.hospedales}@ed.ac.uk\n",
      "Abstract. Knowledgegraphsaregraphicalrepresentationsoflargedata-\n",
      "basesoffacts,whichtypicallysufferfromincompleteness.Inferringmiss-\n",
      "ingrelations(links)betweenentities(nodes)isthetaskoflinkprediction.\n",
      "Arecentstate-of-the-artapproachtolinkprediction,ConvE,implements\n",
      "a convolutional neural network to extract features from concatenated\n",
      "subject and relation vectors. Whilst results are impressive, the method\n",
      "is unintuitive and poorly understood. We propose a hypernetwork ar-\n",
      "chitecturethatgenerates simplified relation-specific convolutionalfilters\n",
      "that(i)outperformsConvEandallpreviousapproachesacrossstandard\n",
      "datasets; and (ii) can be framed as tensor factorization and thus set\n",
      "within a well established family of factorization models for link predic-\n",
      "tion. We thus demonstrate that convolution simply offers a convenient\n",
      "computationalmeansofintroducingsparsityandparametertyingtofind\n",
      "an effective trade-off between non-linear expressiveness and the number\n",
      "of parameters to learn.\n",
      "1 Introduction\n",
      "Knowledge graphs, such as WordNet, Freebase, and Google Knowledge Graph,\n",
      "arelargegraph-structureddatabasesoffacts,containinginformationintheform\n",
      "oftriples(e,r,e ),withe ande representingsubjectandobjectentitiesandr\n",
      "1 2 1 2\n",
      "a relation between them. They are considered important information resources,\n",
      "used for a wide variety of tasks ranging from question answering to informa-\n",
      "tion retrieval and text summarization. One of the main challenges with existing\n",
      "knowledge graphs is their incompleteness: many of the links between entities in\n",
      "the graph are missing. This has inspired substantial work in the field of link\n",
      "prediction, i.e. the task of inferring missing links in knowledge graphs.\n",
      "Until recently, many approaches to link prediction have been based on dif-\n",
      "ferent factorizations of a 3-moded binary tensor representation of the training\n",
      "triples [12,17,23,22]. Such approaches are shallow and linear, with limited ex-\n",
      "pressiveness.However,attempts to increase expressiveness with additional fully\n",
      "connectedlayersandnon-linearitiesoftenleadtooverfitting[12,17].Forthisrea-\n",
      "son, Dettmers et al. introduce ConvE, a model that uses 2D convolutions over\n",
      "reshaped and concatenated entity and relation embeddings [3]. They motivate\n",
      "the use of convolutions by being parameter efficient and fast to compute on a\n",
      "GPU,aswellashavingvariousrobustmethodsfromcomputervisiontoprevent\n",
      "2 Balaˇzevi´c et al.\n",
      "overfitting. Even though results achieved by ConvE are impressive, it is highly\n",
      "unintuitive that convolution – particularly 2D convolution – should be effective\n",
      "for extracting information from 1D entity and relation embeddings.\n",
      "In this paper, we introduce HypER, a model that uses a hypernetwork [5]\n",
      "to generate convolutional filter weights for each relation. A hypernetwork is an\n",
      "approach by which one network generates weights for another network, that\n",
      "can be used to enable weight-sharing across layers and to dynamically synthe-\n",
      "size weights given an input. In our context, we generate relation-specific filter\n",
      "weights to process input entities, and also achieve multi-task knowledge shar-\n",
      "ing across relations in the knowledge graph. Our proposed HypER model uses\n",
      "a hypernetwork to generate a set of 1D relation-specific filters to process the\n",
      "subject entity embeddings. This simplifies the interaction between subject en-\n",
      "tity and relation embeddings compared to ConvE, in which a global set of 2D\n",
      "filters are convolved over reshaped and concatenated subject entity and relation\n",
      "embeddings, which is unintuitive as it suggests the presence of 2D structure in\n",
      "wordembeddings.Moreover,interactionbetweensubjectandrelationinConvE\n",
      "depends on an arbitrarychoice about how they are reshaped and concatenated.\n",
      "In contrast, HypER’s hypernetwork generates relation-specific filters, and thus\n",
      "extracts relation-specific features from the subject entity embedding. This ne-\n",
      "cessitatesno 2Dreshaping,andallowsentity andrelationtointeractmorecom-\n",
      "pletely,ratherthanonlyaroundthe concatenationboundary.We showthatthis\n",
      "simplified approach, in addition to improving link prediction performance, can\n",
      "be understood in terms of tensor factorization, thus placing HypER within a\n",
      "well established family of factorizationmodels. The apparent obscurity of using\n",
      "convolutionwithinwordembeddingsistherebyexplainedassimplyaconvenient\n",
      "computational means of introducing sparsity and parameter tying.\n",
      "WeevaluateHypERagainstseveralpreviouslyproposedlinkpredictionmod-\n",
      "elsusingstandarddatasets(FB15k-237,WN18RR,FB15k,WN18,YAGO3-10),\n",
      "across which it consistently achieves state-of-the-art performance. In summary,\n",
      "our key contributions are:\n",
      "– proposinganew modelfor link prediction(HypER)whichachievesstate-of-\n",
      "the-art performance across all standard datasets;\n",
      "– showing that the benefit of using convolutional instead of fully connected\n",
      "layers is due to restricting the number of dimensions that interact (i.e. ex-\n",
      "plicitregularization),ratherthanfindinghigherdimensionalstructureinthe\n",
      "embeddings (as implied by ConvE); and\n",
      "– showingthatHypERinfactfallswithinabroadclassoftensorfactorization\n",
      "models despite the use ofconvolution,whichservesto providea goodtrade-\n",
      "off between expressiveness and number of parameters to learn.\n",
      "2 Related Work\n",
      "Numerous matrix factorization approaches to link prediction have been pro-\n",
      "posed. An early model, RESCAL [12], tackles the link prediction task by opti-\n",
      "mizingascoringfunctioncontainingabilinearproductbetweenvectorsforeach\n",
      "HypernetworkKnowledge Graph Embeddings 3\n",
      "of the subject and object entities and a full rank matrix for each relation. Dist-\n",
      "Mult[23]canbeviewedasaspecialcaseofRESCALwithadiagonalmatrixper\n",
      "relationtype,whichlimitsthelineartransformationperformedonentityvectors\n",
      "toa stretch.ComplEx[22]extends DistMult tothe complexdomain.TransE[1]\n",
      "is an affine model that represents a relation as a translation operation between\n",
      "subject and object entity vectors.\n",
      "A somewhat separate line of link prediction research introduces Relational\n",
      "GraphConvolutionalNetworks(R-GCNs)[15].R-GCNsuseaconvolutionoper-\n",
      "atortocapturelocalityinformationingraphs.Themodelclosesttoourownand\n",
      "which we draw inspiration from, is ConvE [3], where a convolution operation is\n",
      "performed on the subject entity vector and the relation vector, after they are\n",
      "each reshaped to a matrix and lengthwise concatenated. The obtained feature\n",
      "maps are flattened, put through a fully connected layer, and the inner product\n",
      "is taken with all object entity vectors to generate a score for each triple. Ad-\n",
      "vantagesofConvEoverpreviousapproachesinclude itsexpressiveness,achieved\n",
      "by using multiple layersof non-linearfeatures, its scalability to largeknowledge\n",
      "graphs, and its robustness to overfitting. However, it is not intuitive why con-\n",
      "volving across concatenated and reshaped subject entity and relation vectors\n",
      "should be effective.\n",
      "The proposed HypER model does no such reshaping or concatenation and\n",
      "thus avoidsboth implying any inherent 2D structure in the embeddings and re-\n",
      "stricting interaction to the concatenation boundary. Instead, HypER convolves\n",
      "every dimension of the subject entity embedding with relation-specific convo-\n",
      "lutional filters generated by the hypernetwork. This way, entity and relation\n",
      "embeddings are combined in a non-linear (quadratic) manner, unlike the lin-\n",
      "ear combination (weighted sum) in ConvE. This gives HypER more expressive\n",
      "power, while also reducing parameters.\n",
      "Interestingly, we find that the differences in moving from ConvE to HypER\n",
      "in fact bring the factorization and convolutional approaches together, since the\n",
      "1D convolution process is equivalent to multiplication by a highly sparse tensor\n",
      "withtiedweights(seeFigure2).Themultiplicationofthis“convolutionaltensor”\n",
      "(defined by the relation embedding and hypernetwork) and other weights gives\n",
      "an implicit relation matrix, corresponding to those in e.g. RESCAL, DistMult\n",
      "and ComplEx. Other than the method of deriving these relation matrices, the\n",
      "key difference to existing factorization approaches is the ReLU non-linearity\n",
      "applied prior to interaction with the object embedding.\n",
      "3 Link Prediction\n",
      "In link prediction, the aim is to learn a scoring function φ that assigns a score\n",
      "s = φ(e,r,e ) ∈ R to each input triple (e,r,e ), where e,e ∈ E are sub-\n",
      "1 2 1 2 1 2\n",
      "ject and object entities and r ∈ R a relation. The score indicates the strength\n",
      "of prediction that the given triple corresponds to a true fact, with positive\n",
      "scores meaning true and negative scores,false. Link prediction models typically\n",
      "map entity pair e,e to their corresponding distributed embedding represen-\n",
      "1 2\n",
      "tations e 1,e\n",
      "2\n",
      "∈ Rde and a score is assigned using a relation-specific function,\n",
      "4 Balaˇzevi´c et al.\n",
      "Table 1. Scoring functions of state-of-the-art link prediction models, the dimension-\n",
      "ality of their relation parameters, and their space complexity. d e and d r are the di-\n",
      "mensionsofentityandrelationembeddingsrespectively,e ∈Cde denotesthecomplex\n",
      "2\n",
      "conjugateofe 2,ande 1,w r ∈Rdw×dh denotea2Dreshapingofe 1andw r respectively.\n",
      "∗ is theconvolution operator, F r =vec−1(w rH) thematrix of relation specificconvo-\n",
      "lutionalfilters,vecisavectorizationofamatrixandvec−1 itsinverse,f isanon-linear\n",
      "function, and n e and n r respectively denotethenumberof entities and relations.\n",
      "Model ScoringFunction RelationParameters SpaceComplexity\n",
      "RESCAL[12] e⊤ 1Wre2 Wr∈Rde2 O(nede+nrd2 e)\n",
      "TransE[1] ke1+wr−e2k wr∈Rde O(nede+nrde)\n",
      "NTN[17] u⊤ rf(e1Wr[1..k]e2+Vr(cid:20)e e1 2(cid:21)+br) Wr u∈ rR ∈de R2 kk,, bV rr ∈∈ RR k2dek, O(nede+nrde2k)\n",
      "DistMult[23] he1,wr,e2i wr∈Rde O(nede+nrde)\n",
      "ComplEx[22] Re(he1,wr,e2i) wr∈Cde O(nede+nrde)\n",
      "C Ho yn pv EE R[3 (o]\n",
      "urs)\n",
      "ff (v(v ece (c e(f 1( ∗[e v1 e; cw −1r (] w∗ rw H)) )W )W)e )e2\n",
      "2\n",
      "ww rr ∈∈ RR dd rr OO (( nn ee dd ee ++ nn rr dd rr ))\n",
      "s=φ (e,e ).Themajorityoflinkpredictionmodelsapplythelogisticsigmoid\n",
      "r 1 2\n",
      "function σ(·) to the score to give a probabilistically interpretable prediction\n",
      "p = σ(s) ∈ [0,1] as to whether the queried fact is true. The scoring functions\n",
      "for models from across the literature and HypER are summarized in Table 1,\n",
      "togetherwiththe dimensionalityoftheirrelationparametersandthe significant\n",
      "terms of their space complexity.\n",
      "4 Hypernetwork Knowledge Graph Embeddings\n",
      "In this work, we propose a novel hypernetwork model for link prediction in\n",
      "knowledge graphs. In summary, the hypernetwork projects a vector embedding\n",
      "of each relation via a fully connected layer, the result of which is reshaped to\n",
      "give a set of convolutional filter weight vectors for each relation. We explain\n",
      "this process in more detail below. The idea of using convolutions on entity and\n",
      "relation embeddings stems from computer vision, where feature maps reflect\n",
      "patterns in the image such as lines or edges. Their role in the text domain is\n",
      "hardertointerpret,sincelittleisknownofthemeaningofasingledimensionina\n",
      "wordembedding.Webelieveconvolutionalfiltershavearegularizingeffectwhen\n",
      "appliedto wordembeddings (comparedto the correspondingfull tensor),as the\n",
      "filter size restricts which dimensions of embeddings can interact. This allows\n",
      "nonlinear expressiveness while limiting overfitting by using few parameters. A\n",
      "visualization of HypER is given in Figure 1.\n",
      "4.1 Scoring Function and Model Architecture\n",
      "The relation-specific scoring function for the HypER model is:\n",
      "φ (e,e )=f(vec(e ∗F )W)e\n",
      "r 1 2 1 r 2\n",
      "(1)\n",
      "=f(vec(e ∗vec−1(w H))W)e,\n",
      "1 r 2\n",
      "where the vec−1 operator reshapes a vector to a matrix, and non-linearity f is\n",
      "chosen to be a rectified linear unit (ReLU).\n",
      "HypernetworkKnowledge Graph Embeddings 5\n",
      "e1 Mr\n",
      "Convolve W f 0 0..3 1\n",
      "0.3\n",
      "0.2\n",
      "wr H × σ 0 0 0...5 9\n",
      "1\n",
      "0.2\n",
      "0.8\n",
      "Fr... 0.4\n",
      "E\n",
      "Fig.1.VisualizationoftheHypERmodelarchitecture.Subjectentityembeddinge is\n",
      "1\n",
      "convolvedwithfiltersF r,createdbythehypernetworkHfromrelationembeddingw r.\n",
      "The obtained feature maps M r are mapped to d e-dimensional space via W and the\n",
      "non-linearity f applied before being combined with all object vectors e ∈E through\n",
      "2\n",
      "an inner product to give a score for each triple. Predictions are obtained by applying\n",
      "thelogistic sigmoid function to each score.\n",
      "H Fr W\n",
      "dr\n",
      "Fr\n",
      "lm lm\n",
      "wr ⊗z nf ⊗yz\n",
      "nf nf Fr nf\n",
      "lf\n",
      "lf de de\n",
      "f\n",
      "y explicitlylearnedparameters\n",
      "⊗x ⊗x\n",
      "x inducedfilterweights e1 e2\n",
      "z zeroweights\n",
      "Fig.2. Interpretation of the HypER model in terms of tensor operations. Each rela-\n",
      "tion embedding w r generates a set of filters F r via the hypernetwork H. The act of\n",
      "convolving F r over e 1 is equivalent to multiplication of e 1 by a tensor Fr (in which\n",
      "F r is diagonally duplicated and zero elsewhere). The tensor product Fr ⊗yzW gives\n",
      "a d e ×d e matrix specific to each relation. Axes labels indicate the modes of tensor\n",
      "interaction (via inner product).\n",
      "In the feed-forward pass, the model obtains embeddings for the input triple\n",
      "from the entity and relation embedding matrices E ∈Rne×de and R ∈Rnr×dr.\n",
      "The hypernetwork is a fully connected layer H ∈ Rdr×lfnf (l\n",
      "f\n",
      "denotes filter\n",
      "length and n the number of filters per relation, i.e. output channels of the\n",
      "f\n",
      "convolution) that is applied to the relation embedding w\n",
      "r\n",
      "∈ Rdr. The result\n",
      "is reshaped to generate a matrix of convolutional filters F = vec−1(w H) ∈\n",
      "r r\n",
      "Rlf×nf. Whilst the overall dimensionality of the filter set is l fn f, the rank is\n",
      "restricted to d to encourage parameter sharing between relations.\n",
      "r\n",
      "Thesubjectentityembeddinge isconvolvedwiththesetofrelation-specific\n",
      "1\n",
      "filters F\n",
      "r\n",
      "to give a 2D feature map M\n",
      "r\n",
      "∈ Rlm×nf, where l\n",
      "m\n",
      "= d\n",
      "e\n",
      "−l\n",
      "f\n",
      "+1 is\n",
      "the feature maplength.The feature mapis vectorizedto vec(M r)∈Rlmnf,and\n",
      "projected to d e-dimensional space by the weight matrix W ∈ Rlmnf×de. After\n",
      "applying a ReLU activation function, the result is combined by way of inner\n",
      "productwitheachandeveryobjectentityembeddinge (i),whereivariesoverall\n",
      "2\n",
      "entitiesinthedataset(ofsizen ),togiveavectorofscores.Thelogisticsigmoid\n",
      "e\n",
      "isappliedelement-wisetothe scorevectortoobtainthe predictedprobabilityof\n",
      "each prospective triple being true p =σ(φ (e,e (i))).\n",
      "i r 1 2\n",
      "6 Balaˇzevi´c et al.\n",
      "4.2 Understanding HypER as Tensor Factorization\n",
      "Having described the HypER architecture, we can view it as a series of tensor\n",
      "operations by considering the hypernetworkH and weightmatrix W as tensors\n",
      "H ∈ Rdr×lf×nf and W ∈ Rlm×nf×de respectively. The act of convolving F\n",
      "r\n",
      "=\n",
      "w ⊗H overthe subject entity embedding e is equivalent to the multiplication\n",
      "r 1\n",
      "of e by a sparse tensor F within which F is diagonally duplicated with zeros\n",
      "1 r r\n",
      "elsewhere (see Figure 2). The result is multiplied by W to give a vector, which\n",
      "is subject to ReLU before the final dot product with e. Linearity allows the\n",
      "2\n",
      "product F ⊗W to be considered separately as generating a d ×d matrix for\n",
      "r e e\n",
      "each relation. Further, rather than duplicating entries of F within F, we can\n",
      "r r\n",
      "generalize F\n",
      "r\n",
      "to a relation-agnostic sparse 4 moded tensor F ∈ Rdr×de×nf×lm\n",
      "by replacing entries with d -dimensional strands of H. Thus, the HypER model\n",
      "r\n",
      "can be described explicitly as tensor multiplication of e,e and w with a core\n",
      "1 2 r\n",
      "tensorF⊗W ∈Rde×de×dr,whereF isheavilyconstrainedintermsofitsnumber\n",
      "of free variables.This insight allows HypER to be viewed in a very similar light\n",
      "to the family of factorization approaches to link prediction, such as RESCAL,\n",
      "DistMult and ComplEx.\n",
      "4.3 Training Procedure\n",
      "Followingthe training procedureintroducedby [3],we use 1-N scoring with the\n",
      "Adam optimizer [8] to minimize the binary cross-entropyloss:\n",
      "1\n",
      "L(p,y)=− X(y ilog(p i)+(1−y i)log(1−p i)), (2)\n",
      "n\n",
      "e\n",
      "i\n",
      "where y∈Rne is the label vector containing ones for true triples and zeros oth-\n",
      "erwise,subjecttolabel smoothing.Label smoothingisawidelyusedtechnique\n",
      "shown to improve generalization [20,14]. Label smoothing changes the ground-\n",
      "truth label distributionby adding a uniform priorto encouragethe model to be\n",
      "less confident, achieving a regularizing effect. 1-N scoring refers to simultane-\n",
      "ously scoring (e,r,E), i.e. for all entities e ∈E, in contrast to 1-1 scoring, the\n",
      "1 2\n",
      "practice of training individual triples (e,r,e ) one at a time. As shown by [3],\n",
      "1 2\n",
      "1-N scoring offers a significant speedup (3x on trainand 300xon test time) and\n",
      "improvedaccuracycomparedto1-1scoring.ApotentialextensionoftheHypER\n",
      "model described above would be to apply convolutional filters to both subject\n",
      "andobjectentityembeddings.However,sincethisisnottriviallyimplementable\n",
      "with 1-N scoring and wanting to keep its benefits, we leave this to future work.\n",
      "4.4 Number of Parameters\n",
      "Table 2 compares the number of parameters of ConvE and HypER (for the\n",
      "FB15k-237 dataset, which determines n and n ). It can be seen that, overall,\n",
      "e r\n",
      "HypERhasfewerparameters(4.3M)thanConvE(5.1M)duetothewayHypER\n",
      "directly transforms relations to convolutional filters.\n",
      "HypernetworkKnowledge Graph Embeddings 7\n",
      "Table 2.Comparison ofnumberofparametersforConvEandHypERonFB15k-237.\n",
      "h m and w m are height and width of the ConvEfeature maps respectively.\n",
      "Model E R Filters W\n",
      "ConvE\n",
      "ne×de nr×dr lfnf hmwmnf×de\n",
      "2.9M 0.1M 0.0M 2.1M\n",
      "HypER\n",
      "ne×de nr×dr dr×lfnf lmnf×de\n",
      "2.9M 0.1M 0.1M 1.2M\n",
      "5 Experiments\n",
      "5.1 Datasets\n",
      "We evaluate our HypER model on the standard link prediction task using the\n",
      "following datasets (see Table 3):\n",
      "FB15k [1]asubsetofFreebase,alargedatabaseoffactsabouttherealworld.\n",
      "WN18 [1] a subset of WordNet, containing lexical relations between words.\n",
      "FB15k-237 createdby[21],notingthatthevalidationandtestsetsofFB15k\n",
      "and WN18 contain the inverse of many relations present in the training set,\n",
      "making it easy for simple models to do well. FB15k-237 is a subset of FB15k\n",
      "with the inverse relations removed.\n",
      "WN18RR [3] a subset of WN18, created by removing the inverse relations.\n",
      "YAGO3-10 [3] a subset of YAGO3 [10], containing entities which have a\n",
      "minimum of 10 relations each.\n",
      "Table 3. Summary of dataset statistics.\n",
      "Dataset Entities(ne) Relations(nr)\n",
      "FB15k 14,951 1,345\n",
      "WN18 40,943 18\n",
      "FB15k-237 14,541 237\n",
      "WN18RR 40,943 11\n",
      "YAGO3-10 123,182 37\n",
      "5.2 Experimental Setup\n",
      "We implement HypER in PyTorch[13] and make our code publicly available.1\n",
      "Implementation Details We train our model with 200 dimension entity\n",
      "and relation embeddings (d = d = 200) and 1-N scoring. Whilst the relation\n",
      "e r\n",
      "embedding dimension does not have to equal the entity embedding dimension,\n",
      "we set d =200 to match ConvE for fairness of comparison.\n",
      "r\n",
      "Toacceleratetrainingandpreventoverfitting,weusebatchnormalization[6]\n",
      "and dropout [18] on the input embeddings, feature maps and the hidden layer.\n",
      "We perform a hyperparameter search and select the best performing model by\n",
      "mean reciprocal rank (MRR) on the validation set. Having tested the values\n",
      "{0.,0.1,0.2,0.3}, we find that the following combination of parameters works\n",
      "1 https://github.com/ibalazevic/HypER\n",
      "8 Balaˇzevi´c et al.\n",
      "wellacrossalldatasets:input dropout0.2,feature mapdropout0.2,andhidden\n",
      "dropout0.3,apartfromFB15k-237,wherewesetinputdropoutto0.3.Weselect\n",
      "the learning rate from {0.01,0.005,0.003,0.001,0.0005,0.0001}and exponential\n",
      "learning rate decay from {1.,0.99,0.995}for eachdatasetand find the best per-\n",
      "forming learning rate and learning rate decay to be dataset-specific. We set the\n",
      "convolution stride to 1, number of feature maps to 32 with the filter size 3×3\n",
      "for ConvEand1×9 for HypER,after testing different numbers offeature maps\n",
      "n ∈ {16,32,64} and filter sizes l ∈ {1×1,1×2,1×3,1×6,1×9,1×12}\n",
      "f f\n",
      "(see Table 9). We train all models using the Adam optimizer with batch size\n",
      "128.One epoch on FB15k-237takes approximately 12 seconds on a single GPU\n",
      "compared to 1 minute for e.g. RESCAL, largely due to 1-N scoring.\n",
      "Evaluation Results are obtained by iterating over all triples in the test set.\n",
      "Aparticulartriple isevaluatedbyreplacingthe objectentity e withallentities\n",
      "2\n",
      "E while keeping the subject entity e fixed and vice versa, obtaining scores for\n",
      "1\n",
      "eachcombination.Thesescoresarethenrankedusingthe“filtered”settingonly,\n",
      "i.e. we remove all true cases other than the current test triple [1].\n",
      "We evaluate HypER on five different metrics found throughout the link pre-\n",
      "diction literature: mean rank (MR), mean reciprocal rank (MRR), hits@10,\n",
      "hits@3, and hits@1. Mean rank is the average rank assigned to the true triple,\n",
      "overalltesttriples.Meanreciprocalranktakestheaverageofthereciprocalrank\n",
      "assigned to the true triple. Hits@k measures the percentage of cases in which\n",
      "the true triple appearsin the topk rankedtriples.Overall,the aimis to achieve\n",
      "highmeanreciprocalrankandhits@k andlow meanrank.For amore extensive\n",
      "description of how each of these metrics is calculated, we refer to [3].\n",
      "5.3 Results\n",
      "LinkpredictionresultsforallmodelsacrossthefivedatasetsareshowninTables\n",
      "4, 5 and 6. Our key findings are:\n",
      "– whilst having fewer parameters than the closest comparator ConvE, Hy-\n",
      "pER consistently outperforms all other models across all datasets, thereby\n",
      "achieving state-of-the-artresults on the link prediction task; and\n",
      "– our filter dimension study suggests that no benefit is gained by convolving\n",
      "over reshaped 2D entity embeddings in comparison with 1D entity embed-\n",
      "ding vectors and that most information can be extracted with very small\n",
      "convolutional filters (Table 9).\n",
      "Overall,HypERoutperformsallothermodelsonallmetricsapartfrommean\n",
      "reciprocalrank on WN18 and mean rank on WN18RR, FB15k-237,WN18, and\n",
      "YAGO3-10. Given that mean rank is known to be highly sensitive to outliers\n",
      "[11], this suggests that HypER correctly ranks many true triples in the top 10,\n",
      "but makes larger ranking errors elsewhere.\n",
      "Giventhat mostmodels inthe literature,withthe exceptionofConvE,were\n",
      "trained with 100 dimension embeddings and 1-1 scoring,we reimplement previ-\n",
      "ous models (DistMult, ComplEx and ConvE) with 200 dimension embeddings\n",
      "HypernetworkKnowledge Graph Embeddings 9\n",
      "and1-NscoringforfaircomparisonandreporttheobtainedresultsonWN18RR\n",
      "in Table 7. We perform the same hyperparameter search for every model and\n",
      "present the mean and standard deviation of each result across five runs (differ-\n",
      "ent random seeds). This improves most previously published results, except for\n",
      "ConvEwherewefailtoreplicatesomevalues.Notwithstanding,HypERremains\n",
      "the best performing model overalldespite better tuning of the competitors.\n",
      "Table4.LinkpredictionresultsonWN18RRandFB15k-237.TheRotatE[19]results\n",
      "are reported without their self-adversarial negative sampling (see Appendix H in the\n",
      "original paper) for fair comparison, given that it is not specific to that model only.\n",
      "WN18RR FB15k-237\n",
      "MR MRRH@10H@3H@1 MRMRRH@10H@3H@1\n",
      "DistMult[23] 5110.430.490.440.390 254.241.419.263.155\n",
      "ComplEx[22] 5261.440.510.460.410 339.247.428.275.158\n",
      "NeuralLP[24] − − − − − −.250.408 − −\n",
      "R-GCN[15] − − − − − −.248.417.264.151\n",
      "MINERVA[2] − − − − − − −.456 − −\n",
      "ConvE[3] 4187.430.520.440.400 244.325.501.356.237\n",
      "M-Walk[16] −.437 −.445.414 − − − − −\n",
      "RotatE[19] − − − − − 185.297.480.328.205\n",
      "HypER(ours) 5798.465.522.477.436 250.341.520.376.252\n",
      "Table 5. Link prediction results on WN18 and FB15k.\n",
      "WN18 FB15k\n",
      "MRMRRH@10H@3H@1 MRMRRH@10H@3H@1\n",
      "TransE[1] 251 −.892 − − 125 −.471 − −\n",
      "DistMult[23] 902.822.936.914.728 97.654.824.733.546\n",
      "ComplEx[22] −.941.947.936.936 −.692.840.759.599\n",
      "ANALOGY[9] −.942.947.944.939 −.725.854.785.646\n",
      "NeuralLP[24] −.940.945 − − −.760.837 − −\n",
      "R-GCN[15] −.819.964.929.697 −.696.842.760.601\n",
      "TorusE[4] −.947.954.950.943 −.733.832.771.674\n",
      "ConvE[3] 374.943.956.946.935 51.657.831.723.558\n",
      "SimplE[7] −.942.947.944.939 −.727.838.773.660\n",
      "HypER(ours) 431.951.958.955.947 44.790.885.829.734\n",
      "Table 6. Link prediction results on YAGO3-10.\n",
      "YAGO3-10\n",
      "MR MRRH@10H@3H@1\n",
      "DistMult[23] 5926.340.540.380.240\n",
      "ComplEx[22] 6351.360.550.400.260\n",
      "ConvE[3] 1676.440.620.490.350\n",
      "HypER(ours) 2529.533.678.580.455\n",
      "ToensurethatthedifferencebetweenreportedresultsforHypERandConvE\n",
      "is not simply due to HypER having a reduced number of parameters (implicit\n",
      "regularization), we trained ConvE reducing the number of feature maps to 16\n",
      "instead of 32 to have a comparable number of parameters to HypER (explicit\n",
      "10 Balaˇzevi´c et al.\n",
      "Table 7. Link prediction results on WN18RR;all models trained with 200 dimension\n",
      "embeddings and 1-N scoring.\n",
      "WN18RR\n",
      "MR MRR H@10 H@3 H@1\n",
      "DistMult[23] 4911±109.434±.002.508±.002.447±.001.399±.002\n",
      "ComplEx[22] 5930±125.446±.001.523±.002.462±.001.409±.001\n",
      "ConvE[3] 4997± 99.431±.001.504±.002.443±.002.396±.001\n",
      "HypER(ours) 5798±124.465±.002.522±.003.477±.002.436±.003\n",
      "regularization). This showed no improvement in ConvE results, indicating Hy-\n",
      "pER’s architecture does more than merely reducing the number of parameters.\n",
      "Table 8. Results with and without hypernetworkon WN18RR and FB15k-237.\n",
      "WN18RR FB15k-237\n",
      "MRR H@10 MRR H@10\n",
      "HypER.465±.002.522±.003.341±.001.520±.002\n",
      "HypER(noH).459±.002.511±.002.338±.001.515±.001\n",
      "Hypernetwork Influence To test the influence of the hypernetwork and,\n",
      "thereby, knowledge sharing between relations, we compare HypER results on\n",
      "WN18RRandFB15k-237withthehypernetworkcomponentremoved,i.e.with-\n",
      "outthefirstfullyconnectedlayerandwiththerelationembeddingsdirectlycor-\n",
      "respondingtoasetofconvolutionalfilters.ResultspresentedinTable8showthat\n",
      "thehypernetworkcomponentimprovesperformance,demonstratingthevalueof\n",
      "multi-task learning across different relations.\n",
      "FilterDimensionStudy Table9showsresultsofourstudyinvestigatingthe\n",
      "influenceofdifferentconvolutionalfiltersizesontheperformanceofHypER.The\n",
      "lowerpartofthetableshowsresultsfor2Dfiltersconvolvedoverreshaped(10×\n",
      "20)2Dsubjectentityembeddings.Itcanbeseenthatreshapingtheembeddings\n",
      "is of no benefit, especially onWN18RR.These results indicate that the purpose\n",
      "ofconvolutiononwordembeddingsisnottofindpatternsina2Dembedding(as\n",
      "with images), but perhaps to limit the number of dimensions that can interact\n",
      "with each other, thereby avoiding overfitting.In the upper part of the table, we\n",
      "vary the length of 1D filters, showing that comparable results can be achieved\n",
      "with filter<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,   3916,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Ivana Balažević', 'Carl Allen', 'Timothy M. Hospedales']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: asetofconvolutionalfilters.ResultspresentedinTable8showthat\n",
      "thehypernetworkcomponentimprovesperformance,demonstratingthevalueof\n",
      "multi-task learning across different relations.\n",
      "FilterDimensionStudy Table9showsresultsofourstudyinvestigatingthe\n",
      "influenceofdifferentconvolutionalfiltersizesontheperformanceofHypER.The\n",
      "lowerpartofthetableshowsresultsfor2Dfiltersconvolvedoverreshaped(10×\n",
      "20)2Dsubjectentityembeddings.Itcanbeseenthatreshapingtheembeddings\n",
      "is of no benefit, especially onWN18RR.These results indicate that the purpose\n",
      "ofconvolutiononwordembeddingsisnottofindpatternsina2Dembedding(as\n",
      "with images), but perhaps to limit the number of dimensions that can interact\n",
      "with each other, thereby avoiding overfitting.In the upper part of the table, we\n",
      "vary the length of 1D filters, showing that comparable results can be achieved\n",
      "with filter sizes 1×6 and 1×9, with diminishing results for smaller (e.g. 1×1)\n",
      "and larger (e.g. 1×12) filters.\n",
      "LabelSmoothing Contrarytotheablationstudyof[3],showingtheinfluence\n",
      "of hyperparameters on mean reciprocal rank for FB15k-237, from which they\n",
      "deemlabelsmoothingunimportant,wefindlabelsmoothingtogiveasignificant\n",
      "improvement in prediction scores for WN18RR. However, we find it does have\n",
      "a negative influence on the FB15k scores and as such, exclude label smoothing\n",
      "from our experiments on that dataset. We therefore recommend evaluating the\n",
      "influence of label smoothing on a per dataset basis and leave to future work\n",
      "analysis of the utility of label smoothing in the general case.\n",
      "HypernetworkKnowledge Graph Embeddings 11\n",
      "Table 9. Influenceof different filterdimension choices on prediction results.\n",
      "WN18RR FB15k-237\n",
      "FilterSizeMRRH@1 MRRH@1\n",
      "1×1.455.422.337.248\n",
      "1×2.458.428.337.248\n",
      "1×3.457.427.339.250\n",
      "1×6.459.429.340.251\n",
      "1×9.465.436.341.252\n",
      "1×12.457.428.341.252\n",
      "2×2.456.429.340.250\n",
      "3×3.458.430.339.250\n",
      "5×5.452.423.340.252\n",
      "6 Conclusion\n",
      "Inthis work,we introduce HypER,ahypernetworkmodelforlink predictionon\n",
      "knowledge graphs. HypER generates relation-specific convolutional filters and\n",
      "appliesthemtosubjectentityembeddings.Thehypernetworkcomponentallows\n",
      "information to be shared between relation vectors, enabling multi-task learning\n",
      "across relations. To our knowledge, HypER is the first link prediction model\n",
      "that creates non-linear interaction between entity and relation embeddings by\n",
      "convolving relation-specific filters over the entity embeddings.\n",
      "We show that no benefit is gained from 2D convolutional filters over 1D,\n",
      "dispelling the suggestion that 2D structure exists in entity embeddings implied\n",
      "by ConvE. We also recast HypER in terms of tensor operations showing that,\n",
      "despitetheconvolutionoperation,itiscloselyrelatedtotheestablishedfamilyof\n",
      "tensorfactorizationmodels.Ourresultssuggestthatconvolutionprovidesagood\n",
      "trade-off between expressiveness and parameter number compared to a dense\n",
      "network. HypER is fast, robust to overfitting, has relatively few parameters,\n",
      "and achieves state-of-the-art results across almost all metrics on multiple link\n",
      "prediction datasets.\n",
      "Future work might include expanding the current architecture by applying\n",
      "convolutional filters to both subject and object entity embeddings. We may\n",
      "also analyze the influence of label smoothing and explore the interpretability of\n",
      "convolutional feature maps to gain insight and potentially improve the model.\n",
      "Acknowledgements\n",
      "We thank Ivan Titov for helpful discussions on this work. Ivana Balaˇzevi´c and\n",
      "CarlAllen weresupportedby the Centre for DoctoralTrainingin Data Science,\n",
      "funded by EPSRC (grant EP/L016427/1)and the University of Edinburgh.\n",
      "References\n",
      "1. Bordes,A.,Usunier,N.,Garcia-Duran,A.,Weston,J.,Yakhnenko,O.:Translating\n",
      "Embeddingsfor Modeling Multi-relational Data. In:Advancesin Neural Informa-\n",
      "tion Processing Systems (2013)\n",
      "12 Balaˇzevi´c et al.\n",
      "2. Das, R., Dhuliawala, S., Zaheer, M., Vilnis, L., Durugkar, I., Krishnamurthy, A.,\n",
      "Smola, A., McCallum, A.: Go for a Walk and Arrive at the Answer: Reasoning\n",
      "over Paths in Knowledge Bases Using Reinforcement Learning. In: International\n",
      "Conference on Learning Representations (2018)\n",
      "3. Dettmers,T.,Minervini,P.,Stenetorp,P.,Riedel,S.:Convolutional2DKnowledge\n",
      "Graph Embeddings. In:Association for theAdvancementof Artificial Intelligence\n",
      "(2018)\n",
      "4. Ebisu, T., Ichise, R.: TorusE: Knowledge Graph Embedding on a Lie Group. In:\n",
      "Association for theAdvancementof Artificial Intelligence (2018)\n",
      "5. Ha,D.,Dai,A.,Le,Q.V.:Hypernetworks.In:InternationalConferenceonLearning\n",
      "Representations (2017)\n",
      "6. Ioffe, S., Szegedy, C.: Batch Normalization: Accelerating Deep Network Training\n",
      "by Reducing Internal Covariate Shift. In: International Conference on Machine\n",
      "Learning (2015)\n",
      "7. Kazemi, S.M., Poole, D.: SimplE Embedding for Link Prediction in Knowledge\n",
      "Graphs. In:Advancesin NeuralInformation Processing Systems(2018)\n",
      "8. Kingma, D.P., Ba, J.: Adam: A Method for Stochastic Optimization. In: Interna-\n",
      "tional Conference on Learning Representations (2015)\n",
      "9. Liu, H., Wu, Y., Yang, Y.: Analogical Inference for Multi-relational Embeddings.\n",
      "In:International Conference on Machine Learning (2017)\n",
      "10. Mahdisoltani,F.,Biega, J.,Suchanek,F.M.:Yago3:AKnowledgeBasefromMul-\n",
      "tilingual Wikipedias. In: Conference on InnovativeData Systems Research (2013)\n",
      "11. Nickel, M., Rosasco, L., Poggio, T.A.: Holographic Embeddings of Knowledge\n",
      "Graphs. In:Association for theAdvancementof Artificial Intelligence (2016)\n",
      "12. Nickel,M.,Tresp,V.,Kriegel,H.P.:AThree-WayModelforCollectiveLearningon\n",
      "Multi-Relational Data. In:International Conference on Machine Learning (2011)\n",
      "13. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,\n",
      "Desmaison, A., Antiga, L., Lerer, A.: Automatic Differentiation in PyTorch. In:\n",
      "NIPS-W(2017)\n",
      "14. Pereyra, G., Tucker, G., Chorowski, J., Kaiser, L., Hinton, G.: Regularizing\n",
      "neural networks by penalizing confident output distributions. arXiv preprint\n",
      "arXiv:1701.06548 (2017)\n",
      "15. Schlichtkrull, M., Kipf, T.N., Bloem, P., van den Berg, R., Titov, I., Welling,\n",
      "M.: Modeling Relational Data with Graph Convolutional Networks. In: European\n",
      "SemanticWeb Conference (2018)\n",
      "16. Shen, Y., Chen, J., Huang, P.S., Guo, Y., Gao, J.: M-Walk: Learning to Walk\n",
      "over Graphs using Monte Carlo Tree Search. In: Advances in Neural Information\n",
      "Processing Systems (2018)\n",
      "17. Socher,R.,Chen, D.,Manning, C.D., Ng,A.: Reasoning with NeuralTensor Net-\n",
      "works for Knowledge Base Completion. In: Advances in Neural Information Pro-\n",
      "cessing Systems(2013)\n",
      "18. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.:\n",
      "Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal\n",
      "of Machine Learning Research 15(1), 1929–1958 (2014)\n",
      "19. Sun,Z.,Deng,Z.H.,Nie,J.Y.,Tang,J.:RotatE:KnowledgeGraphEmbeddingby\n",
      "Relational Rotation in Complex Space. In: International Conference on Learning\n",
      "Representations (2019)\n",
      "20. Szegedy,C., Vanhoucke,V.,Ioffe,S.,Shlens,J., Wojna,Z.: RethinkingtheIncep-\n",
      "tion Architecture for Computer Vision. In: Computer Vision and Pattern Recog-\n",
      "nition (2016)\n",
      "HypernetworkKnowledge Graph Embeddings 13\n",
      "21. Toutanova, K., Chen, D., Pantel, P., Poon, H., Choudhury, P., Gamon, M.: Rep-\n",
      "resenting Text for Joint Embedding of Text and Knowledge Bases. In: Empirical\n",
      "Methods in Natural Language Processing (2015)\n",
      "22. Trouillon, T., Welbl, J., Riedel, S., Gaussier, E´., Bouchard, G.: Complex Embed-\n",
      "dingsforSimpleLinkPrediction.In:InternationalConferenceonMachineLearning\n",
      "(2016)\n",
      "23. Yang,B., Yih,W.t.,He,X.,Gao, J., Deng,L.: EmbeddingEntities andRelations\n",
      "for Learning and Inference in Knowledge Bases. In: International Conference on\n",
      "Learning Representations (2015)\n",
      "24. Yang, F., Yang, Z., Cohen, W.W.: Differentiable Learning of Logical Rules for\n",
      "Knowledge Base Reasoning. In: Advances in Neural Information Processing Sys-\n",
      "tems (2017)<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  20661,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Ivana Balažević', 'Carl Allen']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Multi-Hop Knowledge Graph Reasoning with Reward Shaping\n",
      "XiVictoriaLin RichardSocher CaimingXiong\n",
      "SalesforceResearch\n",
      "{xilin,rsocher,cxiong}@salesforce.com\n",
      "Abstract belong_to?\n",
      "U.S. Government Rudy_Giulian\n",
      "belong_to belong_to collaborate\n",
      "Multi-hop reasoning is an effective approach _with\n",
      "collaborate_with\n",
      "for query answering (QA) over incomplete Barack_Obama John_McCain\n",
      "knowledgegraphs(KGs). Theproblemcanbe\n",
      "endorsed_by\n",
      "formulated in a reinforcement learning (RL) born_in collaborate_with? live_in col _la wb io thrate\n",
      "setup,whereapolicy-basedagentsequentially\n",
      "locate_in live_in\n",
      "extends its inference path until it reaches a Hawaii U.S. Hillary_Clinton\n",
      "target. However, in an incomplete KG en-\n",
      "Figure1: Exampleofanincompleteknowledgegraph\n",
      "vironment, the agent receives low-quality re-\n",
      "which contains missing links (dashed lines) that can\n",
      "wardscorruptedbyfalsenegativesinthetrain-\n",
      "possiblybeinferredfromexistingfacts(solidlines).\n",
      "ing data, which harms generalization at test\n",
      "time. Furthermore,sincenogoldenactionse-\n",
      "quence is used for training, the agent can be\n",
      "Embedding based approaches ignore the sym-\n",
      "misled by spurious search trajectories that in-\n",
      "bolic compositionality of KG relations, which\n",
      "cidentallyleadtothecorrectanswer. Wepro-\n",
      "limit their application in more complex rea-\n",
      "pose two modeling advances to address both\n",
      "soning tasks. An alternative solution for KG\n",
      "issues: (1)wereducetheimpactoffalsenega-\n",
      "tivesupervisionbyadoptingapretrainedone- reasoning is to infer missing facts by synthe-\n",
      "hop embedding model to estimate the reward sizing information from multi-hop paths, e.g.\n",
      "of unobserved facts; (2) we counter the sen- bornIn(Obama,Hawaii)∧locatedIn(Hawaii,US)\n",
      "sitivity to spurious paths of on-policy RL by ⇒ bornIn(Obama, US), as shown in Figure 1.\n",
      "forcing the agent to explore a diverse set of\n",
      "Path-basedreasoningofferslogicalinsightsofthe\n",
      "paths using randomly generated edge masks.\n",
      "underlyingKGandaremoredirectlyinterpretable.\n",
      "Our approach significantly improves over ex-\n",
      "Early work treats it as a link prediction prob-\n",
      "isting path-based KGQA models on several\n",
      "lem and perform maximum-likelihood classifica-\n",
      "benchmarkdatasetsandiscomparableorbet-\n",
      "terthanembedding-basedmodels. tion over either discrete path features (Lao et al.,\n",
      "2011, 2012; Gardner et al., 2013) or their hidden\n",
      "1 Introduction representationsinavectorspace(Guuetal.,2015;\n",
      "Toutanovaetal.,2016;McCallumetal.,2017).\n",
      "Large-scale knowledge graphs (KGs) support a Morerecentworkformulatesmulti-hopreason-\n",
      "variety of downstream NLP applications such as ing as a sequential decision problem, and lever-\n",
      "semanticsearch(Berantetal.,2013)anddialogue ages reinforcement learning (RL) to perform ef-\n",
      "generation (He et al., 2017). Whether curated au- fective path search (Xiong et al., 2017; Das et al.,\n",
      "tomatically or manually, practical KGs often fail 2018;Shenetal.,2018;Chenetal.,2018). Inpar-\n",
      "to include many relevant facts. A popular ap- ticular,MINERVA(Dasetal.,2018)usestheRE-\n",
      "proach for modeling incomplete KGs is knowl- INFORCE algorithm (Williams, 1992) to train an\n",
      "edge graph embeddings, which map both entities end-to-endmodelformulti-hopKGqueryanswer-\n",
      "and relations in the KG to a vector space and ing: givenaqueryrelationandasourceentity,the\n",
      "learn a truth value function for any potential KG trained agent searches over the KG starting from\n",
      "tripleparameterizedbytheentityandrelationvec- the source and arrives at the candidate answers\n",
      "tors(Yangetal.,2014;Dettmersetal.,2018). withoutaccesstoanypre-computedpaths.\n",
      "8102\n",
      "peS\n",
      "11\n",
      "]IA.sc[\n",
      "2v86501.8081:viXra\n",
      "art multi-hop reasoning approaches on four out\n",
      "false negative of five benchmark KG datasets (UMLS, Kinship,\n",
      "0.25\n",
      "FB15k-237, WN18RR). It is also the first path-\n",
      "0.20 based model that achieves consistently compara-\n",
      "ble or better performance than embedding-based\n",
      "0.15\n",
      "models. In addition, we perform a thorough ab-\n",
      "0 5 10 15\n",
      "# epochs lationstudyandresultanalysis,demonstratingthe\n",
      "effectofeachmodelinginnovation.\n",
      "Figure2: Percentageoffalsenegativeshit(wherethe\n",
      "modelpredictedananswerthatexistsinthefullKGbut\n",
      "2 Approach\n",
      "cannot be identified by the training subset) in the first\n",
      "20 epochs of walk-based QA training on the UMLS\n",
      "Inthissection,wefirstreviewthewalk-basedQA\n",
      "knowledgegraph(KokandDomingos,2007).\n",
      "framework(§2.2)andtheon-policyreinforcement\n",
      "learning approach proposed by Das et al. (2018)\n",
      "(§2.3,§2.4). Then we describe our proposed so-\n",
      "We refer to the RL formulation adopted by\n",
      "lutions to the false negative reward and spurious\n",
      "MINERVA as “learning to walk towards the an-\n",
      "path problems: knowledge-based reward shaping\n",
      "swer” or “walk-based query-answering (QA)”.\n",
      "(§2.5)andactiondropout(§2.6).\n",
      "Walk-based QA eliminates the need to pre-\n",
      "compute path features, yet this setup poses sev-\n",
      "2.1 FormalProblemDefinition\n",
      "eral challenges for training. First, because prac-\n",
      "tical KGs are intrinsically incomplete, the agent Weformallyrepresent aknowledgegraphasG =\n",
      "may arrive at a correct answer whose link to the (E,R), where E is the set of entities and R is the\n",
      "source entity is missing from the training graph set of relations. Each directed link in the knowl-\n",
      "without receiving any reward (false negative tar- edge graph l = (e s,r,e o) ∈ G represents a fact\n",
      "gets,Figure2). Second,sincenogroundtruthpath (alsocalledatriple).\n",
      "is available for training, the agent may traverse Givenaquery(e s,r q,?),wheree s isthesource\n",
      "spurious paths that lead to a correct answer only entity and r q is the relation of interest, the goal\n",
      "incidentally(falsepositivepaths). BecauseREIN- is to perform an efficient search over G and col-\n",
      "FORCE (Williams, 1992) is an on-policy (Sutton lect the set of possible answers E o = {e o} s.t.\n",
      "and Barto, 1998) RL algorithm which encourages (e s,r q,e o) ∈/ G duetoincompleteness.\n",
      "past actions with high reward, it can bias the pol-\n",
      "2.2 ReinforcementLearningFormulation\n",
      "icy toward spurious paths found early in training\n",
      "(Guuetal.,2017). The search can be viewed as a Markov Decision\n",
      "Process(MDP)(SuttonandBarto,1998): starting\n",
      "WeproposetwomodelingadvancesforRLap-\n",
      "frome,theagentsequentiallyselectsanoutgoing\n",
      "proaches in the walk-based QA framework to ad- s\n",
      "edgel andtraversestoanewentityuntilitarrives\n",
      "dress the previously mentioned problems. First,\n",
      "at a target. Specifically, the MDP consists of the\n",
      "insteadofusingabinaryrewardbasedonwhether\n",
      "followingcomponents(Dasetal.,2018).\n",
      "the agent has reached a correct answer or not,\n",
      "we adopt pre-trained state-of-the-art embedding-\n",
      "States Each state s = (e,(e,r )) ∈ S is a\n",
      "t t s q\n",
      "based models (Dettmers et al., 2018; Trouillon\n",
      "tuple where e is the entity visited at step t and\n",
      "t\n",
      "et al., 2016) to estimate a soft reward for target\n",
      "(e,r ) are the source entity and query relation.\n",
      "s q\n",
      "entities whose correctness cannot be determined.\n",
      "e can be viewed as state-dependent information\n",
      "t\n",
      "As embedding-based models capture link seman-\n",
      "while (e,r ) are the global context shared by all\n",
      "s q\n",
      "tics well, unobserved but correct answers would\n",
      "states.\n",
      "receive a higher reward score compared to a true\n",
      "negative entity using a well-trained model. Sec- Actions The set of possible actions A ∈ A of\n",
      "t\n",
      "ond, we perform action dropout which randomly atsteptconsistsoftheoutgoingedgesofe inG.\n",
      "t\n",
      "blocks some outgoing edges of the agent at each Concretely, A = {(r(cid:48),e(cid:48))|(e,r(cid:48),e(cid:48)) ∈ G}. To\n",
      "t t\n",
      "trainingstepsoastoenforceeffectiveexploration give the agent the option of terminating a search,\n",
      "ofadiversesetofpathsanddilutethenegativeim- a self-loop edge is added to every A. Because\n",
      "t\n",
      "pact of the spurious ones. Empirically, our over- search is unrolled for a fixed number of steps T,\n",
      "allmodelsignificantlyimprovesoverstate-of-the- theself-loopactssimilarlytoa“stop”action.\n",
      "Transition Atransitionfunctionδ : S×A → S θ withthefollowingstochasticgradient:\n",
      "is defined by δ(s,A ) = δ(e,(e,r ),A ). For\n",
      "t t t s q t (cid:88)\n",
      "∇ J(θ) ≈ ∇ R(s |e,r)logπ (a |s ).\n",
      "walk-based QA, the transition is entirely deter- θ θ T s θ t t\n",
      "minedbyG. t\n",
      "(6)\n",
      "Rewards In the default formulation, the agent\n",
      "2.5 Knowledge-BasedRewardShaping\n",
      "receives a terminal reward of 1 if it arrives at a\n",
      "correcttargetentityattheendofsearchand0oth- According to equation 1, the agent receives a bi-\n",
      "erwise. naryrewardbasedonsolelytheobservedanswers\n",
      "in G. However, G is intrinsically incomplete and\n",
      "R (s ) = 1{(e,r,e ) ∈ G}. (1)\n",
      "b T s q T this approach rewards the false negative search\n",
      "2.3 PolicyNetwork results identically to true negatives. To allevi-\n",
      "ate this problem, we use existing KG embedding\n",
      "The search policy is parameterized using state in-\n",
      "models designed for the purpose of KG comple-\n",
      "formation and global context, plus the search his-\n",
      "tion (Trouillon et al., 2016; Dettmers et al., 2018)\n",
      "tory(Dasetal.,2018).\n",
      "to estimate a soft reward for target entities whose\n",
      "Specifically, every entity and relation in G is\n",
      "correctnessisunknown.\n",
      "assigned a dense vector embedding e ∈ Rd and\n",
      "Formally, the embedding models map E and R\n",
      "r ∈ Rd. Theactiona = (r,e ) ∈ A isrep-\n",
      "t t+1 t+1 t\n",
      "to a vector space, and estimate the likelihood of\n",
      "resented as the concatenation of the relation em-\n",
      "each fact l = (e,r,e ) ∈ G using a composi-\n",
      "beddingandtheendnodeembeddinga = [r;e(cid:48)]. s t\n",
      "t t tionfunctionoftheentityandrelationembeddings\n",
      "The search history h =\n",
      "t f(e,r,e ). f istrainedbymaximizingthelikeli-\n",
      "(e,r,e,...,r,e ) ∈ H consists of the s t\n",
      "s 1 1 t t hood of all facts in G. We propose the following\n",
      "sequence of observations and actions taken up to\n",
      "rewardshapingstrategy(Ngetal.,1999):\n",
      "stept,andcanbeencodedusinganLSTM:\n",
      "R(s ) = R (s )+(1−R (s ))f(e,r,e ).\n",
      "h = LSTM(0,[r ;e ]) (2) T b T b T s q T\n",
      "0 0 s\n",
      "(7)\n",
      "h = LSTM(h,a ), t > 0, (3)\n",
      "t t−1 t−1 Namely, if the destination e is a correct answer\n",
      "T\n",
      "where r is a special start relation introduced to according to G, the agent receives reward 1. Oth-\n",
      "0\n",
      "formastartactionwithe. erwisetheagentreceivesafactscoreestimatedby\n",
      "s\n",
      "Theactionspaceisencodedbystackingtheem- f(e s,r q,e T), which is pre-trained. Here we keep\n",
      "beddingsofallactionsinA t: A\n",
      "t\n",
      "∈ R|At|×2d. And f in its general form and it can be replaced by\n",
      "thepolicynetworkπ isdefinedas: any state-of-the-art model (Trouillon et al., 2016;\n",
      "Dettmersetal.,2018)orensemblethereof.\n",
      "π (a |s ) = σ(A ×W ReLU(W [e ;h ;r ])),\n",
      "θ t t t 2 1 t t q\n",
      "(4) 2.6 ActionDropout\n",
      "whereσ isthesoftmaxoperator. The REINFORCE training algorithm performs\n",
      "on-policy sampling according to π (a |s ), and\n",
      "θ t t\n",
      "2.4 Optimization\n",
      "updates θ stochastically using equation 6. Be-\n",
      "The policy network is trained by maximizing the cause the agent does not have access to any or-\n",
      "expectedrewardoverallqueriesinG: acle path, it is possible for it to arrive at a cor-\n",
      "rect answer e via a path which is irrelevant to\n",
      "J(θ) = E [E [R(s |e,r)]]. o\n",
      "(es,r,eo)∈G a1,...,aT∼π θ T s the query relation. As shown in Figure 1, the\n",
      "(5)\n",
      "path Obama −endorsedBy→ McCain −liveIn→\n",
      "The optimization is done using the REIN-\n",
      "U.S. ←locatedIn− Hawaii does not infer the fact\n",
      "FORCE (Williams, 1992) algorithm, which iter-\n",
      "bornIn(Obama,Hawaii).\n",
      "atesthroughall(e,r,e )triplesinG1andupdates\n",
      "s o Discriminating paths of different qualities is\n",
      "1This training strategy treats a query with n > 1 an- non-trivial, and existing RL approaches for walk-\n",
      "swersasnsingle-answerqueries.Inparticular,givenaquery\n",
      "based KGQA largely rely on the terminal reward\n",
      "(e,r,?)withmultipleanswers{e,...e },whentrain-\n",
      "ings wq.r.t. the example (e,r,e ),t M1 INERt Vn A removes all to bias the search. Since there are usually more\n",
      "s q ti\n",
      "{e tj|j (cid:54)= i}observedinthetrainingdatafromthepossible spurious paths than correct ones, spurious paths\n",
      "setoftargetentitiesinthelastsearchstepsoastoforcethe\n",
      "areoftenfoundfirst,andfollowingexplorationcan\n",
      "agenttowalktowardse. Weadoptthesametechniquein\n",
      "ti\n",
      "ourtraining. beincreasinglybiasedtowardsthem(Equation6).\n",
      "r?\n",
      "q\n",
      "Path sam ~pled … …\n",
      "w/ π θ if (es, rq, eT)\n",
      "r 0 e s r t e t r T e T observed\n",
      "LSTM path Reward +1\n",
      "… …\n",
      "encoder\n",
      "h 0 h t h T\n",
      "otherwise\n",
      "action selection\n",
      "Policy Network with Action Dropout Reward Shaping\n",
      "X f(e, r, e )\n",
      "s q T\n",
      "X ~\n",
      "~\n",
      "r q h t e t A t π θ(a t|s t) m π θ(a t|s t) e s r q e T\n",
      "Figure3:Overalltrainingapproach.Ateachtimestept,theagentsamplesanoutgoinglinkaccordingtoπ˜ (a |s ),\n",
      "θ t t\n",
      "whichisthestochasticREINFORCEpolicyπ (a |s )perturbedbyarandombinarymaskm. Theagentreceives\n",
      "θ t t\n",
      "reward 1 if stopped at an observed answer of the query (e,r,?); otherwise, it receives reward f(e,r,e )\n",
      "s q s q T\n",
      "estimatedbytherewardshaping(RS)network.TheRSnetworkispre-trainedanddoesn’treceivegradientupdates.\n",
      "Entities with larger fan-in (in-degree) and fan-out 3.1 KnowledgeGraphEmbeddings\n",
      "(out-degree)oftenexacerbatethisproblem.\n",
      "KG embeddings (Bordes et al., 2013; Socher\n",
      "Guu et al. (2017) identified a similar issue in\n",
      "et al., 2013; Yang et al., 2014; Trouillon et al.,\n",
      "RL-based semantic parsing with weak supervi-\n",
      "2016; Dettmers et al., 2018) are one-hop KG\n",
      "sion, where programs that do not semantically\n",
      "modeling approaches which learn a scoring func-\n",
      "match the user utterance frequently pass the tests.\n",
      "tion f(e,r,e ) to define a fuzzy truth value of\n",
      "s o\n",
      "Tosolvethisproblem,Guuetal.(2017)proposed\n",
      "a triple in the embedding space. These mod-\n",
      "randomized beam search combined with a meri-\n",
      "els can be adapted for query answering by sim-\n",
      "tocratic update rule to ensure all trajectories that\n",
      "ply return the e ’s with the highest f(e,r,e )\n",
      "o s o\n",
      "obtainrewardsareup-weightedroughlyequally.\n",
      "scores. Despitetheirsimplicity,embedding-based\n",
      "Here we propose the action dropout tech-\n",
      "models achieved state-of-the-art performance on\n",
      "niquewhichachievessimilareffectasrandomized\n",
      "KGQA (Das et al., 2018). However, such models\n",
      "search and is simpler to implement over graphs.\n",
      "ignore the symbolic compositionality of KG rela-\n",
      "Action dropout randomly masks some outgoing\n",
      "tions, which limits their usage in more complex\n",
      "edgesfortheagentinthesamplingstepofREIN-\n",
      "reasoningtasks. Therewardshaping(RS)strategy\n",
      "FORCE. The agent then performs sampling2 ac-\n",
      "we proposed is a step to combine their capabil-\n",
      "cordingtotheadjustedactiondistribution\n",
      "ityinmodelingtriplesemanticswiththesymbolic\n",
      "π˜ (a |s ) ∝ (π (a |s )·m+(cid:15)) (8) reasoningcapabilityofthepath-basedapproach.\n",
      "θ t t θ t t\n",
      "m ∼ Bernoulli(1−α),i = 1,...|A |, (9)\n",
      "i t 3.2 Multi-HopReasoning\n",
      "where each entry of m ∈ {0,1}|At| is a binary Multi-hop reasoning focus on learning symbolic\n",
      "variable sampled from the Bernoulli distribution inferencerulesfromrelationalpathsintheKGand\n",
      "with parameter 1 − α. A small value (cid:15) is used has been formulated as sequential decision prob-\n",
      "to smooth the distribution in case m = 0, where lemsinrecentworks(Xiongetal.,2017;Dasetal.,\n",
      "π˜ θ(a t|s t)becomesuniform. 2018;Shenetal.,2018;Chenetal.,2018). Inpar-\n",
      "Ouroverallapproachisillustratedinfigure3. ticular,DeepPath(Xiongetal.,2017)firstadopted\n",
      "REINFORCE to search for generic representative\n",
      "3 RelatedWork\n",
      "pathsbetweenpairsofentities. DIVA(Chenetal.,\n",
      "Inthissection,wesummarizetherelatedworkand 2018) also performs generic path search between\n",
      "discusstheirconnectionstoourapproach. entities using RL and its variational objective can\n",
      "beinterpretedasmodel-basedrewardassignment.\n",
      "2We only modify the sampling distribution and still use\n",
      "π (a |s )tocomputethegradientupdateinequation6. MINERVA (Das et al., 2018) first introduced RL\n",
      "θ t t\n",
      "to search for answer entities of a particular KG Dataset #Ent #Rel #Fact #degree\n",
      "mean median\n",
      "query end-to-end. MINERVA uses entropy reg-\n",
      "Kinship 104 25 8,544 85.15 82\n",
      "ularization to softly encourage the policy to sam- UMLS 135 46 5,216 38.63 28\n",
      "ple diverse paths, and we show that hard action FB15k-237 14,505 237 272,115 19.74 14\n",
      "WN18RR 40,945 11 86,835 2.19 2\n",
      "dropoutismoreeffectiveinthissetup. Reinforce-\n",
      "NELL-995 75,492 200 154,213 4.07 1\n",
      "Walk(Shenetal.,2018)furtherproposedtosolve\n",
      "Table 1: KGs used in the experiments sorted by in-\n",
      "the reward sparsity problem in walk-based QA\n",
      "creasingsparsitylevel.\n",
      "using off-policy learning. ReinforceWalk scores\n",
      "the search targets with a value function which is\n",
      "995 (Xiong et al., 2017). The statistics of the\n",
      "updated based on the search history accumulated\n",
      "datasetsareshowninTable1.\n",
      "through epochs. In comparison, we leveraged ex-\n",
      "isting embedding-based models for reward shap-\n",
      "4.2 BaselinesandModelVariations\n",
      "ing,whichmakesthetrainingmoreefficient.\n",
      "Wecomparewiththreeembeddingbasedmodels:\n",
      "3.3 ReinforcementLearning DistMult(Yangetal.,2014),ComplEx(Trouillon\n",
      "et al., 2016) and ConvE (Dettmers et al., 2018).\n",
      "Recently, RL has seen a variety of applications in\n",
      "Wealsocomparewiththreemulti-hopneuralsym-\n",
      "NLPincludingmachinetranslation(Ranzatoetal.,\n",
      "bolic models: (a) NTP-λ, an improved version of\n",
      "2015), summarization (Paulus et al., 2017), and\n",
      "Neural Theorem Prover (Rockta¨schel and Riedel,\n",
      "semanticparsing(Guuetal.,2017). Comparedto\n",
      "2017), (b) Neural Logical Programming (Neu-\n",
      "thedomainofgames(Mnihetal.,2013)andmany\n",
      "ralLP)(Yangetal.,2017)and(c)MINERVA.For\n",
      "other applications, RL formulations in NLP often\n",
      "our own approach, we include two model vari-\n",
      "havealargeactionspace(e.g.,inmachinetransla-\n",
      "ations that use ComplEx and ConvE as the re-\n",
      "tion,thespaceofpossibleactionsistheentirevo-\n",
      "ward shaping modules respectively, denoted as\n",
      "cabulary of a language). This also holds for KGs,\n",
      "Ours(ComplEx) and Ours(ConvE). We quote the\n",
      "assomeentitiesmayhavethousandsofneighbors\n",
      "results of NeuralLP, NTP-λ and MINERVA re-\n",
      "(e.g. U.S.). Since often there is no golden path\n",
      "portedinDasetal.(2018),andreplicatedtheem-\n",
      "available for a KG reasoning problem, we cannot\n",
      "beddingbasedsystems.3\n",
      "usesupervisedpre-trainingtogivethepathsearch\n",
      "abetterstartpositionfollowingthecommonprac-\n",
      "4.3 ImplementationDetails\n",
      "ticeadoptedinRL-basednaturallanguagegenera-\n",
      "Beam Search Decoding We perform beam\n",
      "tion(Ranzatoetal.,2015). Ontheotherhand,the\n",
      "search decoding to obtain a list of unique en-\n",
      "inference paths being studied in a KG are often\n",
      "tity predictions. Because multiple paths may lead\n",
      "much shorter (usually containing 2-5 steps) com-\n",
      "to the same target entity, we compute the list of\n",
      "paredtotheNLsentencesinthesequencegenera-\n",
      "uniqueentitiesreachedinthefinalsearchstepand\n",
      "tionproblems(oftencontaining20-30words).\n",
      "assigneachofthemthemaximumscoreamongall\n",
      "4 ExperimentSetup pathsthatledtoit. Wethenoutputthetop-ranked\n",
      "unique entities. We find this approach to improve\n",
      "We evaluate our modeling contributions on five overoutputtingtheentitiesrankedatthebeamtop\n",
      "KGsfromdifferentdomainsandexhibitingdiffer- directly,asmanyofthemarerepetitions.\n",
      "entgraphproperties(§4.1). Wecomparewithtwo\n",
      "KG Setup Following previous work, we treat\n",
      "classes of state-of-the-art KG models: multi-hop\n",
      "every KG link as bidirectional and augment the\n",
      "neural symbolic approaches and KG embeddings\n",
      "graphwiththereversed(e,r−1,e )links. Weuse\n",
      "(§4.2). Inthissection,wedescribethedatasetsand o s\n",
      "thesametrain,dev,andtestsetsplitsasDasetal.\n",
      "ourexperimentsetupindetail.\n",
      "(2018). Weexcludeanylinkfromthedevandtest\n",
      "4.1 Dataset set(anditsreversedlink)fromthetrainsetincase\n",
      "there is an overlap. Following Das et al. (2018),\n",
      "We adopt five benchmark KG datasets for query\n",
      "answering: (1) Alyawarra Kinship, (2) Unified 3Dasetal.(2018)reportedMINERVAresultswiththeen-\n",
      "Medical Language Systems (Kok and Domingos, tityembeddingusageasanextrahyperparameter–thequoted\n",
      "performanceofMINERVAinTable2onUMLSandKinship\n",
      "2007),(3)FB15k-237(Toutanovaetal.,2015),(4)\n",
      "wereobtainedwithentityembeddingssettingtozero.Incon-\n",
      "WN18RR (Dettmers et al., 2018), and (5) NELL- trast,oursystemalwaysusestrainedentityembeddings.\n",
      "UMLS Kinship FB15k-237 WN18RR NELL-995\n",
      "Model\n",
      "@1 @10 MRR @1 @10 MRR @1 @10 MRR @1 @10 MRR @1 @10 MRR\n",
      "DistMult(Yangetal.,2014) 82.1 96.7 86.8 48.7 90.4 61.4 32.4 60.0 41.7 43.1 52.4 46.2 55.2 78.3 64.1\n",
      "ComplEx(Trouillonetal.,2016) 89.0 99.2 93.4 81.8 98.1 88.4 32.8 61.6 42.5 41.8 48.0 43.7 64.3 86.0 72.6\n",
      "ConvE(Dettmersetal.,2018) 93.2 99.4 95.7 79.7 98.1 87.1 34.1 62.2 43.5 40.3 54.0 44.9 67.8 88.6 76.1\n",
      "NeuralLP(Yangetal.,2017) 64.3 96.2 77.8 47.5 91.2 61.9 16.6 34.8 22.7 37.6 65.7 46.3 – – –\n",
      "NTP-λ(Rockta¨schelet.al.2017) 84.3 100 91.2 75.9 87.8 79.3 – – – – – – – – –\n",
      "MINERVA(Dasetal.,2018) 72.8 96.8 82.5 60.5 92.4 72.0 21.7 45.6 29.3 41.3 51.3 44.8 66.3 83.1 72.5\n",
      "Ours(ComplEx) 88.7 98.5 92.9 81.1 98.2 87.8 32.9 54.4 39.3 43.7 54.2 47.2 65.5 83.6 72.2\n",
      "Ours(ConvE) 90.2 99.2 94.0 78.9 98.2 86.5 32.7 56.4 40.7 41.8 51.7 45.0 65.6 84.4 72.7\n",
      "Table 2: Query answering performance compared to state-of-the-art embedding based approaches (top part) and\n",
      "multi-hop reasoning approaches (bottom part). The @1, @10 and MRR metrics were multiplied by 100. We\n",
      "highlightthebestapproachineachcategory.\n",
      "wecutthemaximumnumberofoutgoingedgesof other correct answers from E and use it to com-\n",
      "o\n",
      "anentitybyathresholdηtopreventGPUmemory pute two types of metrics: (1) Hits@k which is\n",
      "overflow: foreachentityweretainitstop-ηneigh- thepercentageofexampleswherer ≤ kand(2)\n",
      "eo\n",
      "borswiththehighestPageRankscores(Pageetal., meanreciprocalrank(MRR)whichisthemeanof\n",
      "1999). 1/r for all examples in the test set. We use the\n",
      "eo\n",
      "entiretestsetforevaluation,withtheexceptionof\n",
      "Hyperparameters Wesettheentityandrelation\n",
      "NELL-995,wheretesttripleswithunseenentities\n",
      "embedding size to 200 for all models. We use\n",
      "areremovedfollowingDasetal.(2018).\n",
      "Xavierinitialization(GlorotandBengio,2010)for\n",
      "We will release the Pytorch implementation of\n",
      "theembeddingsandtheNNlayers. ForConvE,we\n",
      "all experiments. Please check the authors’ web\n",
      "use the same convolution layer and label smooth-\n",
      "pageforupdates.\n",
      "inghyperparametersasDettmersetal.(2018). For\n",
      "path-basedmodels,weuseathree-layerLSTMas 5 Results\n",
      "the path encoder and set its hidden dimension to\n",
      "5.1 ModelComparison\n",
      "200. Weperformgridsearchonthereasoningpath\n",
      "length (2,3), the node fan-out threshold η (256- Table 2 shows the evaluation results of our pro-\n",
      "512) and the action dropout rate α (0.1-0.9). Fol- posed approach and the baselines. The top\n",
      "lowingDasetal.(2018),weaddanentropyregu- part presents embedding based approaches and\n",
      "larizationtermintheobjectiveandtunetheweight the bottom part presents multi-hop reasoning ap-\n",
      "parameterβwithin0-0.1. WeuseAdamoptimiza- proaches.5\n",
      "tion (Kingma and Ba, 2014) and search the learn- We find embedding based models perform\n",
      "ing rate (0.001-0.003) and mini-batch size (128- stronglyonseveraldatasets,achievingoverallbest\n",
      "512).4 Forallmodelsweapplydropouttotheen- evaluation metrics on UMLS, Kinship, FB15K-\n",
      "tity and relation embeddings and all feed-forward 237andNELL-995despitetheirsimplicity. While\n",
      "layers, and search the dropout rates within 0-0.5. previous path based approaches achieve com-\n",
      "We use a decoding beam size of 512 for NELL- parable performance on some of the datasets\n",
      "995and128fortheotherdatasets. (WN18RR, NELL-995, and UMLS), the perfor-\n",
      "mance gaps to the embedding based models on\n",
      "Evaluation Protocol We convert each triple\n",
      "the other datasets (Kinship and FB15k-237) are\n",
      "(e,r,e ) in the test set into a query and com-\n",
      "s o considerable(9.1and14.2absolutepointsrespec-\n",
      "pute ranking-based evaluation metrics. The mod-\n",
      "tively). A possible reason for this is that embed-\n",
      "els take e,r as the input and output a list of can-\n",
      "s dingbasedmethodsmapeverylinkintheKGinto\n",
      "didate answers E = [e1,...,eL] ranked in de-\n",
      "o the same embedding space, which implicitly en-\n",
      "creasing order of confidence score. We compute\n",
      "codestheconnectivityofthewholegraph. Incon-\n",
      "r, the rank of e among E, after removing the\n",
      "eo o o trast,pathbasedmodelsusethediscreterepresen-\n",
      "4On some datasets, we found larger batch size to con- tationofaKGasinput,andthereforehavetoleave\n",
      "tinueimprovingtheperformancebuthadtostopat512due\n",
      "tomemoryconstraints. 5Wereportthemodelrobustnessmeasurementsin§A.1.\n",
      "Model UMLSKinshipFB15k237WN18RRNELL995 995 does not change the results significantly. In\n",
      "general,removingactiondropouthasagreaterim-\n",
      "Ours(ConvE) 73.0 75.0 38.2 43.8 78.8\n",
      "−RS 67.7 66.5 35.1 45.7 78.4 pact, suggesting that thorough exploration of the\n",
      "−AD 61.3 65.4 31.0 39.1 76.1 pathspaceisimportantacrossdatasets.\n",
      "Table3: ComparisonofdevsetMRRofOurs(ConvE)\n",
      "5.3 Analysis\n",
      "andmodelswithoutrewardshapingandactiondropout.\n",
      "5.3.1 Con<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  22101,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Alyawarra Kinship', 'Unified Medical Language Systems (UMLS)', 'FB15k-237', 'WN18RR', 'NELL-995']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: tinueimprovingtheperformancebuthadtostopat512due\n",
      "tomemoryconstraints. 5Wereportthemodelrobustnessmeasurementsin§A.1.\n",
      "Model UMLSKinshipFB15k237WN18RRNELL995 995 does not change the results significantly. In\n",
      "general,removingactiondropouthasagreaterim-\n",
      "Ours(ConvE) 73.0 75.0 38.2 43.8 78.8\n",
      "−RS 67.7 66.5 35.1 45.7 78.4 pact, suggesting that thorough exploration of the\n",
      "−AD 61.3 65.4 31.0 39.1 76.1 pathspaceisimportantacrossdatasets.\n",
      "Table3: ComparisonofdevsetMRRofOurs(ConvE)\n",
      "5.3 Analysis\n",
      "andmodelswithoutrewardshapingandactiondropout.\n",
      "5.3.1 ConvergenceRate\n",
      "out a significant proportion of the combinatorial We are interested in studying the impact of each\n",
      "path space by selection. For some path based ap- proposed enhancement on the training conver-\n",
      "proaches,computationcostisabottleneck. Inpar- gence rate. In particular, we expect reward shap-\n",
      "ticular,NeuralLPandNTP-λfailedtoscaletothe ingtoacceleratetheconvergenceofRL(toabetter\n",
      "larger datasets and their results are omitted from performance level) as it propagates prior knowl-\n",
      "thetable,asDasetal.(2018)reported. edge about the underlying KG to the agent. On\n",
      "Ours is the first multi-hop reasoning approach theotherhand,afairconcernforactiondropoutis\n",
      "which is consistently comparable or better than thatitcanbeslowertotrain,astheagentisforced\n",
      "embedding based approaches on all five datasets. to explore a more diverse set of paths. Figure 4\n",
      "Thebestsinglemodel,Ours(ConvE),improvesthe eliminatesthisconcern.\n",
      "SOTAperformanceofpath-basedmodelsonthree The first row of Figure 4 shows the changes in\n",
      "datasets (UMLS, Kinship, and FB15k-237) by devsetMRRofOurs(ConvE)(green)andthetwo\n",
      "4%,9%,and39%respectively. OnWN18RRand ablated models w.r.t. # epochs. In general, the\n",
      "NELL-995,ourapproachdidnotsignificantlyim- proposed approachis ableto converge toa higher\n",
      "proveoverexistingSOTA.TheNELL-995dataset accuracy level much faster than either of the ab-\n",
      "consists of only 12 relations in the test set and, as lated models and the performance gap often per-\n",
      "we further detail in the analysis (§ 5.3.3), our ap- sistsuntiltheendoftraining(onUMLS,Kinship,\n",
      "proachislesseffectiveforthoserelationtypes. andFB15k-237). Particularly,onFB15k-237,our\n",
      "The model variations using different reward approach still shows improvement even after the\n",
      "shapingmodulesperformsimilarly. Whileabetter twoablatedmodelsstarttooverfit,with−ADbe-\n",
      "rewardshapingmoduletypicallyresultsinabetter ginningtooverfitsooner. OnWN18RR,introduc-\n",
      "overall model, an exception is WN18RR, where ingrewardshapinghurtdevsetperformancefrom\n",
      "ComplEx performs slightly worse on its own but the beginning, as discussed in § 5.2. On NELL-\n",
      "is more helpful for reward shaping. We left the 995,Ours(ConvE)performssignificantlybetterin\n",
      "study of the relationship between the accuracy of the beginning, but −RS gradually reaches a com-\n",
      "the reward shaping module and the overall model parableperformancelevel.\n",
      "performanceasfuturework. It is especially interesting that introducing ac-\n",
      "tiondropoutimmediatelyimprovesthemodelper-\n",
      "5.2 AblationStudy formance on all datasets. A possible explanation\n",
      "for this is that by exploring a more diverse set of\n",
      "We perform an ablation study where we remove\n",
      "pathstheagentlearnssearchpoliciesthatgeneral-\n",
      "rewardshaping(−RS)andactiondropout(−AD)\n",
      "izebetter.\n",
      "from Ours(ConvE) and compare their MRRs to\n",
      "the whole model on the dev sets.6 As shown in\n",
      "5.3.2 PathDiversity\n",
      "Table 3, on most datasets, removing each com-\n",
      "Wealsocomputethetotalnumberofuniquepaths\n",
      "ponent results in a significant performance drop.\n",
      "theagentexploresduringtrainingandvisualizeits\n",
      "The exception is WN18RR, where removing the\n",
      "change w.r.t. # training epochs in the second row\n",
      "ConvE reward shaping module improves the per-\n",
      "formance.7 Removing reward shaping on NELL- ofFigure4. Whencountingauniquepath,wein-\n",
      "clude both the edge label and intermediate entity.\n",
      "6According to Table 3 and Table 2, the dev and test set Firstweobservethat,onalldatasets,theagentex-\n",
      "evaluationmetricsdiffersignificantlyonseveraldatasets.We\n",
      "discussthecauseofthisin§A.2. thataddingtheComplExrewardshapingmodulehelps, de-\n",
      "7Apossibleexplanationforthisisthataspath-basedmod- spite the fact that ComplEx performs slightly worse than\n",
      "els tend to outperform the embedding based approaches on ConvEonthisdataset. Thisindicatesthatdevsetaccuracy\n",
      "WN18RR,ConvEmaybesupplyingmorenoisethanuseful is not the only factor which determines the effectiveness of\n",
      "informationabouttheKG.Yetcounter-intuitively,wefound rewardshaping.\n",
      "0.7\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0 50 100 150 200\n",
      "RRM\n",
      "tes ved\n",
      "UMLS Kinship FB15k-237 WN18RR NELL-995\n",
      "0.46\n",
      "0.38 0.7 0.78\n",
      "0.6 0.36 0.44\n",
      "0.76\n",
      "0.5 0.34 0.42 0.4 0.74 0.3 0.32 0.40 0.72\n",
      "0.2 0.30 0.38\n",
      "0.1 0.28 0.70\n",
      "0 50 100 150 200 0 5 10 15 0 5 10 15 20 0 10 20 30\n",
      "1.2\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "0 50 100 150 200\n",
      "detisiv\n",
      "shtap\n",
      "euqinu\n",
      "#\n",
      "Ours(ConvE) -AD -RS\n",
      "1e6 1e6 1e7 1e7 1e6\n",
      "1.50 1.75\n",
      "1.0 1.50\n",
      "6 1.25\n",
      "0.8 1.00 1.25\n",
      "0.6 4 0.75 1.00\n",
      "0.75\n",
      "0.4 2 0.50 0.50\n",
      "0.25 0.25\n",
      "0.2\n",
      "0 0.00 0.00\n",
      "0 50 100 150 200 0 5 10 15 0 5 10 15 20 0 10 20 30\n",
      "# epochs\n",
      "Figure 4: Illustration of convergence rate and path exploration efficiency. The three curves in each subplot\n",
      "representsOurs(ConvE)(green)andthetwoablatedmodels: −RS(blue)and−AD(orange). Thetoprowshows\n",
      "thechangeofdevsetMRRandthebottomrowshowsthegrowthof#uniquepathsexploredw.r.t. #epochs.\n",
      "To-many To-one\n",
      "Dataset\n",
      "% Ours(ConvE) −RS −AD % Ours(ConvE) −RS −AD\n",
      "UMLS 99.1 73.1 67.9(-7%) 61.3(-16%) 0.9 62.5 55.5(-11%) 54.4(-13%)\n",
      "Kinship 100 75 66.5(-11%) 65.4(-13%) 0 – – –\n",
      "FB15k-237 76.6 28.3 24.5(-13%) 20.9(-26%) 23.4 72 69.8(-3%) 63.9(-11%)\n",
      "WN18RR 52.8 65 65.7(+1%) 57.9(-11%) 47.2 20.1 23.2(+16%) 18.1(-10%)\n",
      "NELL-995 12.9 55.7 62.1(+12%) 56.9(+2%) 87.1 81.4 80.7(-1%) 80.5(-1%)\n",
      "Table4: MRRevaluationofdifferentrelationtypes(to-manyvs. to-one)onfivedatasets. The%columnsshow\n",
      "thepercentageofexamplesofeachrelationtypefoundinthedevelopmentsplitofthecorrespondingdataset. In\n",
      "general,ourproposedtechniquesimprovethepredictionresultsforto-manyrelationsmoresignificantly.\n",
      "plores a large number of paths before reaching a ploredanddevsetperformanceisnotstrictlypos-\n",
      "goodperformancelevel. Thespeedofpathdiscov- itive. Thebestperformingmodelingeneralisnot\n",
      "ery slowly decreases as training progresses. On themodelthatexploredthelargest#paths. Italso\n",
      "smallerKGs(UMLSandKinship),therateofen- demonstratestheroleofrewardshapingasaregu-\n",
      "counteringnewpathsissignificantlyloweraftera larizerwhichguidestheagenttoavoidnoisypaths\n",
      "certainnumberofepochs,andthedevsetaccuracy withitspriorknowledge.\n",
      "plateaus correspondingly. On much larger KGs\n",
      "5.3.3 Performancew.r.t. RelationTypes\n",
      "(FB15k-237, WN18RR, and NELL-995), we did\n",
      "not observe a significant slowdown before severe We investigate the behaviors of our proposed ap-\n",
      "overfitting occurs and the dev set performance proachw.r.tdifferentrelationtypes. ForeachKG,\n",
      "startstodrop. Apossiblereasonforthisisthatthe we classify its set of relations into two categories\n",
      "largerKGsaremoresparselyconnectedcompared based on the answer set cardinality. Specifically,\n",
      "to the smaller KGs (Table 1), therefore it is less we define the metric ξ r as the average answer set\n",
      "efficienttogaingeneralizableknowledgefromthe cardinalityofallquerieswithtopicrelationr. We\n",
      "KG by exploring a limited proportion of the path countr asa“to-many”relationifξ r > 1.5,which\n",
      "spacethroughsampling. indicates that most queries in relation r has more\n",
      "than 1 correct answer; we count r as a “to-one”\n",
      "Second,itisinterestingtoseethatwhileremov- relation otherwise, meaning most queries of this\n",
      "ing action dropout significantly lowers the effec- relationhaveonly1correctanswer.\n",
      "tiveness of path exploration (orange vs. green), Table4showsthepercentageofexamplesofto-\n",
      "removing reward shaping slightly improves the # manyandto-onerelationsoneachdevdatasetand\n",
      "paths visited during training for all datasets. This theMRRevaluationmetricsofpreviouslystudied\n",
      "indicates that the correlation between # paths ex- modelscomputedontheexamplesofeachrelation\n",
      "SeenQueries UnseenQueries\n",
      "Dataset\n",
      "% Ours(ConvE) −RS −AD % Ours(ConvE) −RS −AD\n",
      "UMLS 97.2 73.1 67.9(-7%) 61.4(-16%) 2.8 68.5 61.5(-10%) 58.7(-14%)\n",
      "Kinship 96.8 75.1 66.5(-11%) 65.8(-12%) 3.2 73.6 64.3(-13%) 53.3(-27%)\n",
      "FB15k-237 76.1 28.3 24.3(-14%) 20.6(-27%) 23.9 70.9 69.1(-2%) 63.9(-10%)\n",
      "WN18RR 41.8 60.8 62.0(+2%) 53.4(-12%) 58.2 31.5 33.9(+7%) 28.8(-9%)\n",
      "NELL-995 15.3 40.4 45.9(+14%) 42.5(+5%) 84.7 85.5 84.7(-1%) 84.3(-1%)\n",
      "Table5: MRRevaluationofseenqueriesvs. unseenqueriesonfivedatasets. The%columnsshowthepercentage\n",
      "ofexamplesofseen/unseenqueriesfoundinthedevelopmentsplitofthecorrespondingdataset.\n",
      "type. Since UMLS and Kinship are densely con- mostdatasets,theratioofseenvs. unseenqueries\n",
      "nected, they almost exclusively contain to-many is similar to that of to-many vs. to-one relations\n",
      "relations. FB15k-237mostlycontainsto-manyre- (Table 4) as a result of random data split, with\n",
      "lations. In Figure 4, we observe the biggest rela- the exception of WN18RR. On some datasets, all\n",
      "tive gains from the ablated models on these three models perform better on seen queries (UMLS,\n",
      "datasets. WN18RR is more balanced and con- Kinship, WN18RR)whileothersrevealtheoppo-\n",
      "sists of slightly more to-many relations than to- sitetrend. OnNELL-995bothofourproposeden-\n",
      "one relations. The NELL-995 dev set is a unique hancementsarenoteffectiveovertheseenqueries.\n",
      "one which almost exclusively consists of to-one We leave the study of these model behaviors to\n",
      "relations. There is no common performance pat- future work. In most cases, our proposed en-\n",
      "tern over the two relation types across datasets: hancementsimprovetheperformanceoverunseen\n",
      "on some datasets all models perform better on queries,withADbeingmoreeffective.\n",
      "to-many relations (UMLS, WN18RR) while oth-\n",
      "6 Conclusions\n",
      "ers reveal the opposite trend (FB15k-237, NELL-\n",
      "995). We leave the study of these differences to\n",
      "We propose two modeling advances for end-to-\n",
      "futurework.\n",
      "endRL-basedknowledge graphqueryanswering:\n",
      "Weshowtherelativeperformancechangeofthe rewardshapingandactiondropout. Ourapproach\n",
      "ablatedmodels−RSand−ADw.r.t. Ours(ConvE) improvesoverstate-of-the-artmulti-hopreasoning\n",
      "in parentheses. We observe that in general our models consistently on several benchmark KGs.\n",
      "proposedenhancementsareeffectiveinimproving A detailed analysis indicates that the access to a\n",
      "query-answering over both relation types (more moreaccurateenvironmentrepresentation(reward\n",
      "effective for to-many relations). However, adding shaping) and a more thorough exploration of the\n",
      "the ConvE reward shaping module on WN18RR searchspace(actiondropout)areimportanttothe\n",
      "hurts the performance over both to-many and to- performanceboost.\n",
      "one relations (more for to-one relations). On On the other hand, the performance gap be-\n",
      "NELL-995, both techniques hurt the performance tween RL-based approaches and the embedding-\n",
      "overto-manyrelations. based approaches for KGQA remains. In future\n",
      "work, we would like to investigate learnable re-\n",
      "5.3.4 Performancew.r.t. SeenQueriesvs.\n",
      "wardshapingandactiondropoutschemesandap-\n",
      "UnseenQueries\n",
      "plymodel-basedRLtothisdomain.\n",
      "Sincemostbenchmarkdatasetsrandomlysplitthe\n",
      "Acknowledgements\n",
      "KGtriplesintotrain,devandtestsets,thequeries\n",
      "that have multiple answers may fall into multi- We thank Mark O. Riedl, Yingbo Zhou, James\n",
      "ple splits. As a result, some of the test queries Bradbury and Vena Jia Li for their feedback on\n",
      "(e s,r q,?) are seen in the training set (with a dif- early draft of the paper, and Mark O. Riedl for\n",
      "ferentsetofanswers)whiletheothersarenot. We helpful conversations on reward shaping. We\n",
      "investigatethebehaviorsofourproposedapproach thanktheanonymousreviewersandtheSalesforce\n",
      "w.r.t. seenandunseenqueries. research team members for their thoughtful com-\n",
      "Table 5 shows the percentage of examples as- ments and discussions. We thank Fre´deric Godin\n",
      "sociated with seen and unseen queries on each forpointingoutanerrorinEquation8inanearly\n",
      "dev dataset and the corresponding MRR evalua- versionofthepaper.\n",
      "tion metrics of previously studied models. On\n",
      "References Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,\n",
      "Hanna M. Wallach, Rob Fergus, S. V. N. Vish-\n",
      "ReginaBarzilayandMin-YenKan,editors.2017. Pro-\n",
      "wanathan, and Roman Garnett, editors. 2017. Ad-\n",
      "ceedingsofthe55thAnnualMeetingoftheAssocia-\n",
      "vances in Neural Information Processing Systems\n",
      "tionforComputationalLinguistics,ACL2017,Van-\n",
      "30: AnnualConferenceonNeuralInformationPro-\n",
      "couver,Canada,July30-August4,Volume1: Long\n",
      "cessing Systems 2017, 4-9 December 2017, Long\n",
      "Papers.AssociationforComputationalLinguistics.\n",
      "Beach,CA,USA.\n",
      "JonathanBerant,AndrewChou,RoyFrostig,andPercy\n",
      "He He, Anusha Balakrishnan, Mihail Eric, and Percy\n",
      "Liang. 2013. Semantic parsing on freebase from\n",
      "Liang. 2017. Learning symmetric collaborative di-\n",
      "question-answer pairs. In (Yarowsky et al., 2013),\n",
      "alogue agents with dynamic knowledge graph em-\n",
      "pages1533–1544.\n",
      "beddings. In(BarzilayandKan,2017),pages1766–\n",
      "Antoine Bordes, Nicolas Usunier, Alberto Garcia- 1776.\n",
      "Dura´n, Jason Weston, and Oksana Yakhnenko.\n",
      "2013. Translating embeddings for modeling multi- Diederik P. Kingma and Jimmy Ba. 2014. Adam:\n",
      "relationaldata. InProceedingsofthe26thInterna- A method for stochastic optimization. CoRR,\n",
      "tional Conference on Neural Information Process- abs/1412.6980.\n",
      "ingSystems-Volume2,NIPS’13,pages2787–2795,\n",
      "StanleyKokandPedroM.Domingos.2007. Statistical\n",
      "USA.CurranAssociatesInc.\n",
      "predicateinvention. InICML,volume227ofACM\n",
      "Wenhu Chen, Wenhan Xiong, Xifeng Yan, and International Conference Proceeding Series, pages\n",
      "William Yang Wang. 2018. Variational knowledge 433–440.ACM.\n",
      "graphreasoning. CoRR,abs/1803.06581.\n",
      "Ni Lao, Tom Mitchell, and William W. Cohen. 2011.\n",
      "Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Randomwalkinferenceandlearninginalargescale\n",
      "Luke Vilnis, Ishan Durugkar, Akshay Krishna- knowledge base. In Proceedings of the Conference\n",
      "murthy,AlexSmola,andAndrewMcCallum.2018. on Empirical Methods in Natural Language Pro-\n",
      "Go for a walk and arrive at the answer: Reasoning cessing, EMNLP’11, pages529–539, Stroudsburg,\n",
      "over paths in knowledge bases using reinforcement PA, USA. Association for Computational Linguis-\n",
      "learning. In International Conference on Learning tics.\n",
      "Representations.\n",
      "NiLao,AmarnagSubramanya,FernandoC.N.Pereira,\n",
      "Tim Dettmers, Pasquale Minervini, Pontus Stenetorp,\n",
      "andWilliamW.Cohen.2012. Readingthewebwith\n",
      "and Sebastian Riedel. 2018. Convolutional 2d\n",
      "learned syntactic-semantic inference rules. In Pro-\n",
      "knowledge graph embeddings. In Proceedings of\n",
      "ceedingsofthe2012JointConferenceonEmpirical\n",
      "theThirty-SecondAAAIConferenceonArtificialIn-\n",
      "MethodsinNaturalLanguageProcessingandCom-\n",
      "telligence,NewOrleans,Louisiana,USA,February\n",
      "putational Natural Language Learning, EMNLP-\n",
      "2-7,2018.AAAIPress.\n",
      "CoNLL2012, July12-14, 2012, JejuIsland, Korea,\n",
      "pages1017–1026.ACL.\n",
      "Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,\n",
      "and Tom M. Mitchell. 2013. Improving learning\n",
      "Andrew McCallum, Arvind Neelakantan, Rajarshi\n",
      "and inference in a large knowledge-base using la-\n",
      "Das, and David Belanger. 2017. Chains of reason-\n",
      "tentsyntacticcues. In(Yarowskyetal.,2013),pages\n",
      "ing over entities, relations, and text using recurrent\n",
      "833–838.\n",
      "neural networks. In Proceedings of the 15th Con-\n",
      "XavierGlorotandYoshuaBengio.2010. Understand- ferenceoftheEuropeanChapteroftheAssociation\n",
      "ing the difficulty of training deep feedforward neu- for Computational Linguistics, EACL 2017, Valen-\n",
      "ral networks. In Proceedings of the Thirteenth In- cia,Spain,April3-7,2017,Volume1: LongPapers,\n",
      "ternationalConferenceonArtificialIntelligenceand pages132–141.AssociationforComputationalLin-\n",
      "Statistics,AISTATS2010,ChiaLagunaResort,Sar- guistics.\n",
      "dinia, Italy, May 13-15, 2010, volume 9 of JMLR\n",
      "Volodymyr Mnih, Koray Kavukcuoglu, David Silver,\n",
      "Proceedings,pages249–256.JMLR.org.\n",
      "Alex Graves, Ioannis Antonoglou, Daan Wierstra,\n",
      "Kelvin Guu, John Miller, and Percy Liang. 2015. and Martin A. Riedmiller. 2013. Playing atari with\n",
      "Traversing knowledge graphs in vector space. In deepreinforcementlearning. CoRR,abs/1312.5602.\n",
      "Proceedings of the 2015 Conference on Empirical\n",
      "Methods in Natural Language Processing, EMNLP Andrew Y. Ng, Daishi Harada, and Stuart J. Russell.\n",
      "2015, Lisbon, Portugal, September 17-21, 2015, 1999. Policy invariance under reward transforma-\n",
      "pages318–327.TheAssociationforComputational tions: Theory and application to reward shaping.\n",
      "Linguistics. In Proceedings of the Sixteenth International Con-\n",
      "ference on Machine Learning (ICML 1999), Bled,\n",
      "Kelvin Guu, Panupong Pasupat, Evan Zheran Liu, Slovenia, June27-30, 1999, pages278–287.Mor-\n",
      "and Percy Liang. 2017. From language to pro- ganKaufmann.\n",
      "grams: Bridging reinforcement learning and max-\n",
      "imum marginal likelihood. In (Barzilay and Kan, Lawrence Page, Sergey Brin, Rajeev Motwani, and\n",
      "2017),pages1051–1062. TerryWinograd.1999. Thepagerankcitationrank-\n",
      "ing: Bringing order to the web. Technical Re- method for knowledge graph reasoning. In Pro-\n",
      "port 1999-66, Stanford InfoLab. Previous number ceedingsofthe2017ConferenceonEmpiricalMeth-\n",
      "=SIDL-WP-1999-0120. odsinNaturalLanguageProcessing,EMNLP2017,\n",
      "Copenhagen, Denmark, September 9-11, 2017,\n",
      "Romain Paulus, Caiming Xiong, and Richard Socher. pages564–573.AssociationforComputationalLin-\n",
      "2017. Adeepreinforcedmodelforabstractivesum- guistics.\n",
      "marization. CoRR,abs/1705.04304.\n",
      "Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\n",
      "Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, Gao, and Li Deng. 2014. Embedding entities and\n",
      "and Wojciech Zaremba. 2015. Sequence level relations for learning and inference in knowledge\n",
      "training with recurrent neural networks. CoRR, bases. CoRR,abs/1412.6575.\n",
      "abs/1511.06732.\n",
      "Fan Yang, Zhilin Yang, and William W. Cohen. 2017.\n",
      "TimRockta¨schelandSebastianRiedel.2017. End-to- Differentiable learning of logical rules for knowl-\n",
      "enddifferentiableproving. In(Guyonetal.,2017), edgebasereasoning. In(Guyonetal.,2017),pages\n",
      "pages3791–3803. 2316–2325.\n",
      "Yelong Shen, Jianshu Chen, Po-Sen Huang, Yuqing David Yarowsky, Timothy Baldwin, Anna Korhonen,\n",
      "Guo, and Jianfeng Gao. 2018. Reinforcewalk: Karen Livescu, and Steven Bethard, editors. 2013.\n",
      "Learning to walk in graph with monte carlo tree Proceedings of the 2013 Conference on Empirical\n",
      "search. CoRR,abs/1802.04394. Methods in Natural Language Processing, EMNLP\n",
      "2013, 18-21 October 2013, Grand Hyatt Seattle,\n",
      "RichardSocher,DanqiChen,ChristopherD.Manning, Seattle, Washington, USA, A meeting of SIGDAT, a\n",
      "and Andrew Y. Ng. 2013. Reasoning with neural SpecialInterestGroupoftheACL.ACL.\n",
      "tensornetworksforknowledgebasecompletion. In\n",
      "Proceedingsofthe26thInternationalConferenceon\n",
      "NeuralInformationProcessingSystems-Volume1,\n",
      "NIPS’13, pages 926–934, USA. Curran Associates\n",
      "Inc.\n",
      "Richard S. Sutton and Andrew G. Barto. 1998. Re-\n",
      "inforcement learning - an introduction. Adaptive\n",
      "computationandmachinelearning.MITPress.\n",
      "Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-\n",
      "fungPoon,PallaviChoudhury,andMichaelGamon.\n",
      "2015. Representingtextforjointembeddingoftext\n",
      "and knowledge bases. In EMNLP, pages 1499–\n",
      "1509. The Association for Computational Linguis-\n",
      "tics.\n",
      "KristinaToutanova,XiVictoriaLin,Wen-tauYih,Hoi-\n",
      "fung Poon, and Chris Quirk. 2016. Compositional\n",
      "learningofembeddingsforrelationpathsinknowl-\n",
      "edge base and text. In Proceedings of the 54th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics, ACL 2016, August 7-12, 2016, Berlin,\n",
      "Germany,Volume1: LongPapers.TheAssociation\n",
      "forComputerLinguistics.\n",
      "The´oTrouillon,JohannesWelbl,SebastianRiedel,E´ric\n",
      "Gaussier,andGuillaumeBouchard.2016. Complex\n",
      "embeddingsforsimplelinkprediction. InProceed-\n",
      "ings of the 33nd International Conference on Ma-\n",
      "chine Learning, ICML 2016, New York City, NY,\n",
      "USA,June19-24, 2016, volume48ofJMLRWork-\n",
      "shop and Conference Proceedings, pages 2071–\n",
      "2080.JMLR.org.\n",
      "RonaldJ. Williams.1992. Simplestatistical gradient-\n",
      "following algorithms for connectionist reinforce-\n",
      "mentlearning. MachineLearning,8:229–256.\n",
      "Wenhan Xiong, Thien Hoang, and William Yang\n",
      "Wang. 2017. Deeppath: A reinforcement learning\n",
      "A Appendix datasets, the evaluation metrics increases signifi-\n",
      "cantly to the level that is comparable to those on\n",
      "A.1 ModelRobustness\n",
      "the test set, with the relative improvement corre-\n",
      "We run the best embedding-based model ConvE latedwiththeaveragenodefan-outintheKG(Ta-\n",
      "and Ours(ConvE) on all datasets using 5 differ- ble1).\n",
      "ent random seeds with all other hyperparameters NoticethatTable7isgeneratedafterallhyper-\n",
      "fixed. Table6reportsthemeanandstandarddevi- parameters were fixed and the purpose is to show\n",
      "ationofeachmodel. Weobservethatbothmodels the effects of such dataset peculiarities. To avoid\n",
      "demonstrate a small standard deviation (< 0.01) potential test set leakage, hyperparameter search\n",
      "onalldatasets. shouldbedonewiththetestsettripleshidden(Ta-\n",
      "ble1)insteadofwiththefullKG.\n",
      "Dataset ConvE Ours(ConvE)\n",
      "A.3 ActionDropoutRatesUsedforDifferent\n",
      "UMLS 95.5±0.4 93.7±0.2 KGs\n",
      "Kinship 86.9±0.3 86.2±0.7\n",
      "Table 8 show the action dropout rates used for all\n",
      "FB15k-237 43.5±0.1 40.7±0.2\n",
      "KGdatasetsinourexperiments. Ingeneral,larger\n",
      "WN18RR 45.3±0.4 44.7±0.2\n",
      "actiondropoutratesarenecessaryforKGsthatare\n",
      "NELL-995 76.2±0.3 72.7±0.4\n",
      "densely connected. We find a positive correlation\n",
      "Table6: TestsetMRR×100meanandstandarddevia- between the optimal action dropout rate and the\n",
      "tionacrossfiverunsonalldatasets. averagenodefan-out(Table1).\n",
      "ForUMLSandKinship,wetriedsettingtheac-\n",
      "tion dropout rate to 1.0 (completely random sam-\n",
      "A.2 DevelopmentSetEvaluationUsing\n",
      "pling) and observed small but significant perfor-\n",
      "CompleteKGs\n",
      "mance drop. Random sampling performs reason-\n",
      "Comparing Table 2 and Table 3 reveals that the ablywellonthesetwodatasetspossiblyduetothe\n",
      "devsetMRRsaresignificantlylowerthanthetest fact that they are small. For larger KGs (FB15k-\n",
      "set MRRs on some datasets (UMLS, Kinship and 237, WN18RR, NELL-995), policy-guided sam-\n",
      "FB15k-237). Suchdiscrepanciesarecausedbythe plingisnecessary.\n",
      "multi-answer queries in these datasets. As most\n",
      "benchmark datasets randomly split the KG triples Dataset α\n",
      "intotrain/dev/testsets,thequeriesthathavemulti-\n",
      "UMLS 0.95\n",
      "pleanswersmayfallintomultiplesplits. Because\n",
      "Kinship 0.9\n",
      "wehidealltriplesinthetestsetduringthedevset\n",
      "FB15k-237 0.5\n",
      "evaluation,somepredictionsgeneratedduringdev\n",
      "WN18RR 0.1\n",
      "setevaluationwerewronglypunishedasfalseneg-\n",
      "NELL-995 0.1\n",
      "atives. In contrast, the test set evaluation metrics\n",
      "are computed using the complete KGs. Access to Table8: Actiondropoutratesusedinourexperiments.\n",
      "thecompleteKGeliminatesmostofthefalseneg-\n",
      "ativescasesandhenceincreasestheperformance.\n",
      "Model UMLSKinshipFB15k237WN18RRNELL995\n",
      "Ours(ConvE) 95.1 86.8 41.8 44.1 78.8\n",
      "−RS 85.6 75.7 37.1 46.1 78.4\n",
      "−AD 76.2 75.9 32.4 39.3 76.1\n",
      "Table7: ComparisonofdevsetMRRcomputedusing\n",
      "thecompleteKGsofOurs(ConvE)andmodelswithout\n",
      "rewardshapingandactiondropout.\n",
      "Table 7 shows the dev set MRR of the same\n",
      "systems shown in Table 3 with the MRRs com-\n",
      "puted using the complete KGs. On four of the<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the name of datasets used in the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  22101,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['UMLS', 'Kinship', 'FB15k-237', 'WN18RR', 'NELL-995']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Multi-Hop Knowledge Graph Reasoning with Reward Shaping\n",
      "XiVictoriaLin RichardSocher CaimingXiong\n",
      "SalesforceResearch\n",
      "{xilin,rsocher,cxiong}@salesforce.com\n",
      "Abstract belong_to?\n",
      "U.S. Government Rudy_Giulian\n",
      "belong_to belong_to collaborate\n",
      "Multi-hop reasoning is an effective approach _with\n",
      "collaborate_with\n",
      "for query answering (QA) over incomplete Barack_Obama John_McCain\n",
      "knowledgegraphs(KGs). Theproblemcanbe\n",
      "endorsed_by\n",
      "formulated in a reinforcement learning (RL) born_in collaborate_with? live_in col _la wb io thrate\n",
      "setup,whereapolicy-basedagentsequentially\n",
      "locate_in live_in\n",
      "extends its inference path until it reaches a Hawaii U.S. Hillary_Clinton\n",
      "target. However, in an incomplete KG en-\n",
      "Figure1: Exampleofanincompleteknowledgegraph\n",
      "vironment, the agent receives low-quality re-\n",
      "which contains missing links (dashed lines) that can\n",
      "wardscorruptedbyfalsenegativesinthetrain-\n",
      "possiblybeinferredfromexistingfacts(solidlines).\n",
      "ing data, which harms generalization at test\n",
      "time. Furthermore,sincenogoldenactionse-\n",
      "quence is used for training, the agent can be\n",
      "Embedding based approaches ignore the sym-\n",
      "misled by spurious search trajectories that in-\n",
      "bolic compositionality of KG relations, which\n",
      "cidentallyleadtothecorrectanswer. Wepro-\n",
      "limit their application in more complex rea-\n",
      "pose two modeling advances to address both\n",
      "soning tasks. An alternative solution for KG\n",
      "issues: (1)wereducetheimpactoffalsenega-\n",
      "tivesupervisionbyadoptingapretrainedone- reasoning is to infer missing facts by synthe-\n",
      "hop embedding model to estimate the reward sizing information from multi-hop paths, e.g.\n",
      "of unobserved facts; (2) we counter the sen- bornIn(Obama,Hawaii)∧locatedIn(Hawaii,US)\n",
      "sitivity to spurious paths of on-policy RL by ⇒ bornIn(Obama, US), as shown in Figure 1.\n",
      "forcing the agent to explore a diverse set of\n",
      "Path-basedreasoningofferslogicalinsightsofthe\n",
      "paths using randomly generated edge masks.\n",
      "underlyingKGandaremoredirectlyinterpretable.\n",
      "Our approach significantly improves over ex-\n",
      "Early work treats it as a link prediction prob-\n",
      "isting path-based KGQA models on several\n",
      "lem and perform maximum-likelihood classifica-\n",
      "benchmarkdatasetsandiscomparableorbet-\n",
      "terthanembedding-basedmodels. tion over either discrete path features (Lao et al.,\n",
      "2011, 2012; Gardner et al., 2013) or their hidden\n",
      "1 Introduction representationsinavectorspace(Guuetal.,2015;\n",
      "Toutanovaetal.,2016;McCallumetal.,2017).\n",
      "Large-scale knowledge graphs (KGs) support a Morerecentworkformulatesmulti-hopreason-\n",
      "variety of downstream NLP applications such as ing as a sequential decision problem, and lever-\n",
      "semanticsearch(Berantetal.,2013)anddialogue ages reinforcement learning (RL) to perform ef-\n",
      "generation (He et al., 2017). Whether curated au- fective path search (Xiong et al., 2017; Das et al.,\n",
      "tomatically or manually, practical KGs often fail 2018;Shenetal.,2018;Chenetal.,2018). Inpar-\n",
      "to include many relevant facts. A popular ap- ticular,MINERVA(Dasetal.,2018)usestheRE-\n",
      "proach for modeling incomplete KGs is knowl- INFORCE algorithm (Williams, 1992) to train an\n",
      "edge graph embeddings, which map both entities end-to-endmodelformulti-hopKGqueryanswer-\n",
      "and relations in the KG to a vector space and ing: givenaqueryrelationandasourceentity,the\n",
      "learn a truth value function for any potential KG trained agent searches over the KG starting from\n",
      "tripleparameterizedbytheentityandrelationvec- the source and arrives at the candidate answers\n",
      "tors(Yangetal.,2014;Dettmersetal.,2018). withoutaccesstoanypre-computedpaths.\n",
      "8102\n",
      "peS\n",
      "11\n",
      "]IA.sc[\n",
      "2v86501.8081:viXra\n",
      "art multi-hop reasoning approaches on four out\n",
      "false negative of five benchmark KG datasets (UMLS, Kinship,\n",
      "0.25\n",
      "FB15k-237, WN18RR). It is also the first path-\n",
      "0.20 based model that achieves consistently compara-\n",
      "ble or better performance than embedding-based\n",
      "0.15\n",
      "models. In addition, we perform a thorough ab-\n",
      "0 5 10 15\n",
      "# epochs lationstudyandresultanalysis,demonstratingthe\n",
      "effectofeachmodelinginnovation.\n",
      "Figure2: Percentageoffalsenegativeshit(wherethe\n",
      "modelpredictedananswerthatexistsinthefullKGbut\n",
      "2 Approach\n",
      "cannot be identified by the training subset) in the first\n",
      "20 epochs of walk-based QA training on the UMLS\n",
      "Inthissection,wefirstreviewthewalk-basedQA\n",
      "knowledgegraph(KokandDomingos,2007).\n",
      "framework(§2.2)andtheon-policyreinforcement\n",
      "learning approach proposed by Das et al. (2018)\n",
      "(§2.3,§2.4). Then we describe our proposed so-\n",
      "We refer to the RL formulation adopted by\n",
      "lutions to the false negative reward and spurious\n",
      "MINERVA as “learning to walk towards the an-\n",
      "path problems: knowledge-based reward shaping\n",
      "swer” or “walk-based query-answering (QA)”.\n",
      "(§2.5)andactiondropout(§2.6).\n",
      "Walk-based QA eliminates the need to pre-\n",
      "compute path features, yet this setup poses sev-\n",
      "2.1 FormalProblemDefinition\n",
      "eral challenges for training. First, because prac-\n",
      "tical KGs are intrinsically incomplete, the agent Weformallyrepresent aknowledgegraphasG =\n",
      "may arrive at a correct answer whose link to the (E,R), where E is the set of entities and R is the\n",
      "source entity is missing from the training graph set of relations. Each directed link in the knowl-\n",
      "without receiving any reward (false negative tar- edge graph l = (e s,r,e o) ∈ G represents a fact\n",
      "gets,Figure2). Second,sincenogroundtruthpath (alsocalledatriple).\n",
      "is available for training, the agent may traverse Givenaquery(e s,r q,?),wheree s isthesource\n",
      "spurious paths that lead to a correct answer only entity and r q is the relation of interest, the goal\n",
      "incidentally(falsepositivepaths). BecauseREIN- is to perform an efficient search over G and col-\n",
      "FORCE (Williams, 1992) is an on-policy (Sutton lect the set of possible answers E o = {e o} s.t.\n",
      "and Barto, 1998) RL algorithm which encourages (e s,r q,e o) ∈/ G duetoincompleteness.\n",
      "past actions with high reward, it can bias the pol-\n",
      "2.2 ReinforcementLearningFormulation\n",
      "icy toward spurious paths found early in training\n",
      "(Guuetal.,2017). The search can be viewed as a Markov Decision\n",
      "Process(MDP)(SuttonandBarto,1998): starting\n",
      "WeproposetwomodelingadvancesforRLap-\n",
      "frome,theagentsequentiallyselectsanoutgoing\n",
      "proaches in the walk-based QA framework to ad- s\n",
      "edgel andtraversestoanewentityuntilitarrives\n",
      "dress the previously mentioned problems. First,\n",
      "at a target. Specifically, the MDP consists of the\n",
      "insteadofusingabinaryrewardbasedonwhether\n",
      "followingcomponents(Dasetal.,2018).\n",
      "the agent has reached a correct answer or not,\n",
      "we adopt pre-trained state-of-the-art embedding-\n",
      "States Each state s = (e,(e,r )) ∈ S is a\n",
      "t t s q\n",
      "based models (Dettmers et al., 2018; Trouillon\n",
      "tuple where e is the entity visited at step t and\n",
      "t\n",
      "et al., 2016) to estimate a soft reward for target\n",
      "(e,r ) are the source entity and query relation.\n",
      "s q\n",
      "entities whose correctness cannot be determined.\n",
      "e can be viewed as state-dependent information\n",
      "t\n",
      "As embedding-based models capture link seman-\n",
      "while (e,r ) are the global context shared by all\n",
      "s q\n",
      "tics well, unobserved but correct answers would\n",
      "states.\n",
      "receive a higher reward score compared to a true\n",
      "negative entity using a well-trained model. Sec- Actions The set of possible actions A ∈ A of\n",
      "t\n",
      "ond, we perform action dropout which randomly atsteptconsistsoftheoutgoingedgesofe inG.\n",
      "t\n",
      "blocks some outgoing edges of the agent at each Concretely, A = {(r(cid:48),e(cid:48))|(e,r(cid:48),e(cid:48)) ∈ G}. To\n",
      "t t\n",
      "trainingstepsoastoenforceeffectiveexploration give the agent the option of terminating a search,\n",
      "ofadiversesetofpathsanddilutethenegativeim- a self-loop edge is added to every A. Because\n",
      "t\n",
      "pact of the spurious ones. Empirically, our over- search is unrolled for a fixed number of steps T,\n",
      "allmodelsignificantlyimprovesoverstate-of-the- theself-loopactssimilarlytoa“stop”action.\n",
      "Transition Atransitionfunctionδ : S×A → S θ withthefollowingstochasticgradient:\n",
      "is defined by δ(s,A ) = δ(e,(e,r ),A ). For\n",
      "t t t s q t (cid:88)\n",
      "∇ J(θ) ≈ ∇ R(s |e,r)logπ (a |s ).\n",
      "walk-based QA, the transition is entirely deter- θ θ T s θ t t\n",
      "minedbyG. t\n",
      "(6)\n",
      "Rewards In the default formulation, the agent\n",
      "2.5 Knowledge-BasedRewardShaping\n",
      "receives a terminal reward of 1 if it arrives at a\n",
      "correcttargetentityattheendofsearchand0oth- According to equation 1, the agent receives a bi-\n",
      "erwise. naryrewardbasedonsolelytheobservedanswers\n",
      "in G. However, G is intrinsically incomplete and\n",
      "R (s ) = 1{(e,r,e ) ∈ G}. (1)\n",
      "b T s q T this approach rewards the false negative search\n",
      "2.3 PolicyNetwork results identically to true negatives. To allevi-\n",
      "ate this problem, we use existing KG embedding\n",
      "The search policy is parameterized using state in-\n",
      "models designed for the purpose of KG comple-\n",
      "formation and global context, plus the search his-\n",
      "tion (Trouillon et al., 2016; Dettmers et al., 2018)\n",
      "tory(Dasetal.,2018).\n",
      "to estimate a soft reward for target entities whose\n",
      "Specifically, every entity and relation in G is\n",
      "correctnessisunknown.\n",
      "assigned a dense vector embedding e ∈ Rd and\n",
      "Formally, the embedding models map E and R\n",
      "r ∈ Rd. Theactiona = (r,e ) ∈ A isrep-\n",
      "t t+1 t+1 t\n",
      "to a vector space, and estimate the likelihood of\n",
      "resented as the concatenation of the relation em-\n",
      "each fact l = (e,r,e ) ∈ G using a composi-\n",
      "beddingandtheendnodeembeddinga = [r;e(cid:48)]. s t\n",
      "t t tionfunctionoftheentityandrelationembeddings\n",
      "The search history h =\n",
      "t f(e,r,e ). f istrainedbymaximizingthelikeli-\n",
      "(e,r,e,...,r,e ) ∈ H consists of the s t\n",
      "s 1 1 t t hood of all facts in G. We propose the following\n",
      "sequence of observations and actions taken up to\n",
      "rewardshapingstrategy(Ngetal.,1999):\n",
      "stept,andcanbeencodedusinganLSTM:\n",
      "R(s ) = R (s )+(1−R (s ))f(e,r,e ).\n",
      "h = LSTM(0,[r ;e ]) (2) T b T b T s q T\n",
      "0 0 s\n",
      "(7)\n",
      "h = LSTM(h,a ), t > 0, (3)\n",
      "t t−1 t−1 Namely, if the destination e is a correct answer\n",
      "T\n",
      "where r is a special start relation introduced to according to G, the agent receives reward 1. Oth-\n",
      "0\n",
      "formastartactionwithe. erwisetheagentreceivesafactscoreestimatedby\n",
      "s\n",
      "Theactionspaceisencodedbystackingtheem- f(e s,r q,e T), which is pre-trained. Here we keep\n",
      "beddingsofallactionsinA t: A\n",
      "t\n",
      "∈ R|At|×2d. And f in its general form and it can be replaced by\n",
      "thepolicynetworkπ isdefinedas: any state-of-the-art model (Trouillon et al., 2016;\n",
      "Dettmersetal.,2018)orensemblethereof.\n",
      "π (a |s ) = σ(A ×W ReLU(W [e ;h ;r ])),\n",
      "θ t t t 2 1 t t q\n",
      "(4) 2.6 ActionDropout\n",
      "whereσ isthesoftmaxoperator. The REINFORCE training algorithm performs\n",
      "on-policy sampling according to π (a |s ), and\n",
      "θ t t\n",
      "2.4 Optimization\n",
      "updates θ stochastically using equation 6. Be-\n",
      "The policy network is trained by maximizing the cause the agent does not have access to any or-\n",
      "expectedrewardoverallqueriesinG: acle path, it is possible for it to arrive at a cor-\n",
      "rect answer e via a path which is irrelevant to\n",
      "J(θ) = E [E [R(s |e,r)]]. o\n",
      "(es,r,eo)∈G a1,...,aT∼π θ T s the query relation. As shown in Figure 1, the\n",
      "(5)\n",
      "path Obama −endorsedBy→ McCain −liveIn→\n",
      "The optimization is done using the REIN-\n",
      "U.S. ←locatedIn− Hawaii does not infer the fact\n",
      "FORCE (Williams, 1992) algorithm, which iter-\n",
      "bornIn(Obama,Hawaii).\n",
      "atesthroughall(e,r,e )triplesinG1andupdates\n",
      "s o Discriminating paths of different qualities is\n",
      "1This training strategy treats a query with n > 1 an- non-trivial, and existing RL approaches for walk-\n",
      "swersasnsingle-answerqueries.Inparticular,givenaquery\n",
      "based KGQA largely rely on the terminal reward\n",
      "(e,r,?)withmultipleanswers{e,...e },whentrain-\n",
      "ings wq.r.t. the example (e,r,e ),t M1 INERt Vn A removes all to bias the search. Since there are usually more\n",
      "s q ti\n",
      "{e tj|j (cid:54)= i}observedinthetrainingdatafromthepossible spurious paths than correct ones, spurious paths\n",
      "setoftargetentitiesinthelastsearchstepsoastoforcethe\n",
      "areoftenfoundfirst,andfollowingexplorationcan\n",
      "agenttowalktowardse. Weadoptthesametechniquein\n",
      "ti\n",
      "ourtraining. beincreasinglybiasedtowardsthem(Equation6).\n",
      "r?\n",
      "q\n",
      "Path sam ~pled … …\n",
      "w/ π θ if (es, rq, eT)\n",
      "r 0 e s r t e t r T e T observed\n",
      "LSTM path Reward +1\n",
      "… …\n",
      "encoder\n",
      "h 0 h t h T\n",
      "otherwise\n",
      "action selection\n",
      "Policy Network with Action Dropout Reward Shaping\n",
      "X f(e, r, e )\n",
      "s q T\n",
      "X ~\n",
      "~\n",
      "r q h t e t A t π θ(a t|s t) m π θ(a t|s t) e s r q e T\n",
      "Figure3:Overalltrainingapproach.Ateachtimestept,theagentsamplesanoutgoinglinkaccordingtoπ˜ (a |s ),\n",
      "θ t t\n",
      "whichisthestochasticREINFORCEpolicyπ (a |s )perturbedbyarandombinarymaskm. Theagentreceives\n",
      "θ t t\n",
      "reward 1 if stopped at an observed answer of the query (e,r,?); otherwise, it receives reward f(e,r,e )\n",
      "s q s q T\n",
      "estimatedbytherewardshaping(RS)network.TheRSnetworkispre-trainedanddoesn’treceivegradientupdates.\n",
      "Entities with larger fan-in (in-degree) and fan-out 3.1 KnowledgeGraphEmbeddings\n",
      "(out-degree)oftenexacerbatethisproblem.\n",
      "KG embeddings (Bordes et al., 2013; Socher\n",
      "Guu et al. (2017) identified a similar issue in\n",
      "et al., 2013; Yang et al., 2014; Trouillon et al.,\n",
      "RL-based semantic parsing with weak supervi-\n",
      "2016; Dettmers et al., 2018) are one-hop KG\n",
      "sion, where programs that do not semantically\n",
      "modeling approaches which learn a scoring func-\n",
      "match the user utterance frequently pass the tests.\n",
      "tion f(e,r,e ) to define a fuzzy truth value of\n",
      "s o\n",
      "Tosolvethisproblem,Guuetal.(2017)proposed\n",
      "a triple in the embedding space. These mod-\n",
      "randomized beam search combined with a meri-\n",
      "els can be adapted for query answering by sim-\n",
      "tocratic update rule to ensure all trajectories that\n",
      "ply return the e ’s with the highest f(e,r,e )\n",
      "o s o\n",
      "obtainrewardsareup-weightedroughlyequally.\n",
      "scores. Despitetheirsimplicity,embedding-based\n",
      "Here we propose the action dropout tech-\n",
      "models achieved state-of-the-art performance on\n",
      "niquewhichachievessimilareffectasrandomized\n",
      "KGQA (Das et al., 2018). However, such models\n",
      "search and is simpler to implement over graphs.\n",
      "ignore the symbolic compositionality of KG rela-\n",
      "Action dropout randomly masks some outgoing\n",
      "tions, which limits their usage in more complex\n",
      "edgesfortheagentinthesamplingstepofREIN-\n",
      "reasoningtasks. Therewardshaping(RS)strategy\n",
      "FORCE. The agent then performs sampling2 ac-\n",
      "we proposed is a step to combine their capabil-\n",
      "cordingtotheadjustedactiondistribution\n",
      "ityinmodelingtriplesemanticswiththesymbolic\n",
      "π˜ (a |s ) ∝ (π (a |s )·m+(cid:15)) (8) reasoningcapabilityofthepath-basedapproach.\n",
      "θ t t θ t t\n",
      "m ∼ Bernoulli(1−α),i = 1,...|A |, (9)\n",
      "i t 3.2 Multi-HopReasoning\n",
      "where each entry of m ∈ {0,1}|At| is a binary Multi-hop reasoning focus on learning symbolic\n",
      "variable sampled from the Bernoulli distribution inferencerulesfromrelationalpathsintheKGand\n",
      "with parameter 1 − α. A small value (cid:15) is used has been formulated as sequential decision prob-\n",
      "to smooth the distribution in case m = 0, where lemsinrecentworks(Xiongetal.,2017;Dasetal.,\n",
      "π˜ θ(a t|s t)becomesuniform. 2018;Shenetal.,2018;Chenetal.,2018). Inpar-\n",
      "Ouroverallapproachisillustratedinfigure3. ticular,DeepPath(Xiongetal.,2017)firstadopted\n",
      "REINFORCE to search for generic representative\n",
      "3 RelatedWork\n",
      "pathsbetweenpairsofentities. DIVA(Chenetal.,\n",
      "Inthissection,wesummarizetherelatedworkand 2018) also performs generic path search between\n",
      "discusstheirconnectionstoourapproach. entities using RL and its variational objective can\n",
      "beinterpretedasmodel-basedrewardassignment.\n",
      "2We only modify the sampling distribution and still use\n",
      "π (a |s )tocomputethegradientupdateinequation6. MINERVA (Das et al., 2018) first introduced RL\n",
      "θ t t\n",
      "to search for answer entities of a particular KG Dataset #Ent #Rel #Fact #degree\n",
      "mean median\n",
      "query end-to-end. MINERVA uses entropy reg-\n",
      "Kinship 104 25 8,544 85.15 82\n",
      "ularization to softly encourage the policy to sam- UMLS 135 46 5,216 38.63 28\n",
      "ple diverse paths, and we show that hard action FB15k-237 14,505 237 272,115 19.74 14\n",
      "WN18RR 40,945 11 86,835 2.19 2\n",
      "dropoutismoreeffectiveinthissetup. Reinforce-\n",
      "NELL-995 75,492 200 154,213 4.07 1\n",
      "Walk(Shenetal.,2018)furtherproposedtosolve\n",
      "Table 1: KGs used in the experiments sorted by in-\n",
      "the reward sparsity problem in walk-based QA\n",
      "creasingsparsitylevel.\n",
      "using off-policy learning. ReinforceWalk scores\n",
      "the search targets with a value function which is\n",
      "995 (Xiong et al., 2017). The statistics of the\n",
      "updated based on the search history accumulated\n",
      "datasetsareshowninTable1.\n",
      "through epochs. In comparison, we leveraged ex-\n",
      "isting embedding-based models for reward shap-\n",
      "4.2 BaselinesandModelVariations\n",
      "ing,whichmakesthetrainingmoreefficient.\n",
      "Wecomparewiththreeembeddingbasedmodels:\n",
      "3.3 ReinforcementLearning DistMult(Yangetal.,2014),ComplEx(Trouillon\n",
      "et al., 2016) and ConvE (Dettmers et al., 2018).\n",
      "Recently, RL has seen a variety of applications in\n",
      "Wealsocomparewiththreemulti-hopneuralsym-\n",
      "NLPincludingmachinetranslation(Ranzatoetal.,\n",
      "bolic models: (a) NTP-λ, an improved version of\n",
      "2015), summarization (Paulus et al., 2017), and\n",
      "Neural Theorem Prover (Rockta¨schel and Riedel,\n",
      "semanticparsing(Guuetal.,2017). Comparedto\n",
      "2017), (b) Neural Logical Programming (Neu-\n",
      "thedomainofgames(Mnihetal.,2013)andmany\n",
      "ralLP)(Yangetal.,2017)and(c)MINERVA.For\n",
      "other applications, RL formulations in NLP often\n",
      "our own approach, we include two model vari-\n",
      "havealargeactionspace(e.g.,inmachinetransla-\n",
      "ations that use ComplEx and ConvE as the re-\n",
      "tion,thespaceofpossibleactionsistheentirevo-\n",
      "ward shaping modules respectively, denoted as\n",
      "cabulary of a language). This also holds for KGs,\n",
      "Ours(ComplEx) and Ours(ConvE). We quote the\n",
      "assomeentitiesmayhavethousandsofneighbors\n",
      "results of NeuralLP, NTP-λ and MINERVA re-\n",
      "(e.g. U.S.). Since often there is no golden path\n",
      "portedinDasetal.(2018),andreplicatedtheem-\n",
      "available for a KG reasoning problem, we cannot\n",
      "beddingbasedsystems.3\n",
      "usesupervisedpre-trainingtogivethepathsearch\n",
      "abetterstartpositionfollowingthecommonprac-\n",
      "4.3 ImplementationDetails\n",
      "ticeadoptedinRL-basednaturallanguagegenera-\n",
      "Beam Search Decoding We perform beam\n",
      "tion(Ranzatoetal.,2015). Ontheotherhand,the\n",
      "search decoding to obtain a list of unique en-\n",
      "inference paths being studied in a KG are often\n",
      "tity predictions. Because multiple paths may lead\n",
      "much shorter (usually containing 2-5 steps) com-\n",
      "to the same target entity, we compute the list of\n",
      "paredtotheNLsentencesinthesequencegenera-\n",
      "uniqueentitiesreachedinthefinalsearchstepand\n",
      "tionproblems(oftencontaining20-30words).\n",
      "assigneachofthemthemaximumscoreamongall\n",
      "4 ExperimentSetup pathsthatledtoit. Wethenoutputthetop-ranked\n",
      "unique entities. We find this approach to improve\n",
      "We evaluate our modeling contributions on five overoutputtingtheentitiesrankedatthebeamtop\n",
      "KGsfromdifferentdomainsandexhibitingdiffer- directly,asmanyofthemarerepetitions.\n",
      "entgraphproperties(§4.1). Wecomparewithtwo\n",
      "KG Setup Following previous work, we treat\n",
      "classes of state-of-the-art KG models: multi-hop\n",
      "every KG link as bidirectional and augment the\n",
      "neural symbolic approaches and KG embeddings\n",
      "graphwiththereversed(e,r−1,e )links. Weuse\n",
      "(§4.2). Inthissection,wedescribethedatasetsand o s\n",
      "thesametrain,dev,andtestsetsplitsasDasetal.\n",
      "ourexperimentsetupindetail.\n",
      "(2018). Weexcludeanylinkfromthedevandtest\n",
      "4.1 Dataset set(anditsreversedlink)fromthetrainsetincase\n",
      "there is an overlap. Following Das et al. (2018),\n",
      "We adopt five benchmark KG datasets for query\n",
      "answering: (1) Alyawarra Kinship, (2) Unified 3Dasetal.(2018)reportedMINERVAresultswiththeen-\n",
      "Medical Language Systems (Kok and Domingos, tityembeddingusageasanextrahyperparameter–thequoted\n",
      "performanceofMINERVAinTable2onUMLSandKinship\n",
      "2007),(3)FB15k-237(Toutanovaetal.,2015),(4)\n",
      "wereobtainedwithentityembeddingssettingtozero.Incon-\n",
      "WN18RR (Dettmers et al., 2018), and (5) NELL- trast,oursystemalwaysusestrainedentityembeddings.\n",
      "UMLS Kinship FB15k-237 WN18RR NELL-995\n",
      "Model\n",
      "@1 @10 MRR @1 @10 MRR @1 @10 MRR @1 @10 MRR @1 @10 MRR\n",
      "DistMult(Yangetal.,2014) 82.1 96.7 86.8 48.7 90.4 61.4 32.4 60.0 41.7 43.1 52.4 46.2 55.2 78.3 64.1\n",
      "ComplEx(Trouillonetal.,2016) 89.0 99.2 93.4 81.8 98.1 88.4 32.8 61.6 42.5 41.8 48.0 43.7 64.3 86.0 72.6\n",
      "ConvE(Dettmersetal.,2018) 93.2 99.4 95.7 79.7 98.1 87.1 34.1 62.2 43.5 40.3 54.0 44.9 67.8 88.6 76.1\n",
      "NeuralLP(Yangetal.,2017) 64.3 96.2 77.8 47.5 91.2 61.9 16.6 34.8 22.7 37.6 65.7 46.3 – – –\n",
      "NTP-λ(Rockta¨schelet.al.2017) 84.3 100 91.2 75.9 87.8 79.3 – – – – – – – – –\n",
      "MINERVA(Dasetal.,2018) 72.8 96.8 82.5 60.5 92.4 72.0 21.7 45.6 29.3 41.3 51.3 44.8 66.3 83.1 72.5\n",
      "Ours(ComplEx) 88.7 98.5 92.9 81.1 98.2 87.8 32.9 54.4 39.3 43.7 54.2 47.2 65.5 83.6 72.2\n",
      "Ours(ConvE) 90.2 99.2 94.0 78.9 98.2 86.5 32.7 56.4 40.7 41.8 51.7 45.0 65.6 84.4 72.7\n",
      "Table 2: Query answering performance compared to state-of-the-art embedding based approaches (top part) and\n",
      "multi-hop reasoning approaches (bottom part). The @1, @10 and MRR metrics were multiplied by 100. We\n",
      "highlightthebestapproachineachcategory.\n",
      "wecutthemaximumnumberofoutgoingedgesof other correct answers from E and use it to com-\n",
      "o\n",
      "anentitybyathresholdηtopreventGPUmemory pute two types of metrics: (1) Hits@k which is\n",
      "overflow: foreachentityweretainitstop-ηneigh- thepercentageofexampleswherer ≤ kand(2)\n",
      "eo\n",
      "borswiththehighestPageRankscores(Pageetal., meanreciprocalrank(MRR)whichisthemeanof\n",
      "1999). 1/r for all examples in the test set. We use the\n",
      "eo\n",
      "entiretestsetforevaluation,withtheexceptionof\n",
      "Hyperparameters Wesettheentityandrelation\n",
      "NELL-995,wheretesttripleswithunseenentities\n",
      "embedding size to 200 for all models. We use\n",
      "areremovedfollowingDasetal.(2018).\n",
      "Xavierinitialization(GlorotandBengio,2010)for\n",
      "We will release the Pytorch implementation of\n",
      "theembeddingsandtheNNlayers. ForConvE,we\n",
      "all experiments. Please check the authors’ web\n",
      "use the same convolution layer and label smooth-\n",
      "pageforupdates.\n",
      "inghyperparametersasDettmersetal.(2018). For\n",
      "path-basedmodels,weuseathree-layerLSTMas 5 Results\n",
      "the path encoder and set its hidden dimension to\n",
      "5.1 ModelComparison\n",
      "200. Weperformgridsearchonthereasoningpath\n",
      "length (2,3), the node fan-out threshold η (256- Table 2 shows the evaluation results of our pro-\n",
      "512) and the action dropout rate α (0.1-0.9). Fol- posed approach and the baselines. The top\n",
      "lowingDasetal.(2018),weaddanentropyregu- part presents embedding based approaches and\n",
      "larizationtermintheobjectiveandtunetheweight the bottom part presents multi-hop reasoning ap-\n",
      "parameterβwithin0-0.1. WeuseAdamoptimiza- proaches.5\n",
      "tion (Kingma and Ba, 2014) and search the learn- We find embedding based models perform\n",
      "ing rate (0.001-0.003) and mini-batch size (128- stronglyonseveraldatasets,achievingoverallbest\n",
      "512).4 Forallmodelsweapplydropouttotheen- evaluation metrics on UMLS, Kinship, FB15K-\n",
      "tity and relation embeddings and all feed-forward 237andNELL-995despitetheirsimplicity. While\n",
      "layers, and search the dropout rates within 0-0.5. previous path based approaches achieve com-\n",
      "We use a decoding beam size of 512 for NELL- parable performance on some of the datasets\n",
      "995and128fortheotherdatasets. (WN18RR, NELL-995, and UMLS), the perfor-\n",
      "mance gaps to the embedding based models on\n",
      "Evaluation Protocol We convert each triple\n",
      "the other datasets (Kinship and FB15k-237) are\n",
      "(e,r,e ) in the test set into a query and com-\n",
      "s o considerable(9.1and14.2absolutepointsrespec-\n",
      "pute ranking-based evaluation metrics. The mod-\n",
      "tively). A possible reason for this is that embed-\n",
      "els take e,r as the input and output a list of can-\n",
      "s dingbasedmethodsmapeverylinkintheKGinto\n",
      "didate answers E = [e1,...,eL] ranked in de-\n",
      "o the same embedding space, which implicitly en-\n",
      "creasing order of confidence score. We compute\n",
      "codestheconnectivityofthewholegraph. Incon-\n",
      "r, the rank of e among E, after removing the\n",
      "eo o o trast,pathbasedmodelsusethediscreterepresen-\n",
      "4On some datasets, we found larger batch size to con- tationofaKGasinput,andthereforehavetoleave\n",
      "tinueimprovingtheperformancebuthadtostopat512due\n",
      "tomemoryconstraints. 5Wereportthemodelrobustnessmeasurementsin§A.1.\n",
      "Model UMLSKinshipFB15k237WN18RRNELL995 995 does not change the results significantly. In\n",
      "general,removingactiondropouthasagreaterim-\n",
      "Ours(ConvE) 73.0 75.0 38.2 43.8 78.8\n",
      "−RS 67.7 66.5 35.1 45.7 78.4 pact, suggesting that thorough exploration of the\n",
      "−AD 61.3 65.4 31.0 39.1 76.1 pathspaceisimportantacrossdatasets.\n",
      "Table3: ComparisonofdevsetMRRofOurs(ConvE)\n",
      "5.3 Analysis\n",
      "andmodelswithoutrewardshapingandactiondropout.\n",
      "5.3.1 Con<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,    287,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Multi-Hop Knowledge Graph Reasoning', 'Query Answering']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: tinueimprovingtheperformancebuthadtostopat512due\n",
      "tomemoryconstraints. 5Wereportthemodelrobustnessmeasurementsin§A.1.\n",
      "Model UMLSKinshipFB15k237WN18RRNELL995 995 does not change the results significantly. In\n",
      "general,removingactiondropouthasagreaterim-\n",
      "Ours(ConvE) 73.0 75.0 38.2 43.8 78.8\n",
      "−RS 67.7 66.5 35.1 45.7 78.4 pact, suggesting that thorough exploration of the\n",
      "−AD 61.3 65.4 31.0 39.1 76.1 pathspaceisimportantacrossdatasets.\n",
      "Table3: ComparisonofdevsetMRRofOurs(ConvE)\n",
      "5.3 Analysis\n",
      "andmodelswithoutrewardshapingandactiondropout.\n",
      "5.3.1 ConvergenceRate\n",
      "out a significant proportion of the combinatorial We are interested in studying the impact of each\n",
      "path space by selection. For some path based ap- proposed enhancement on the training conver-\n",
      "proaches,computationcostisabottleneck. Inpar- gence rate. In particular, we expect reward shap-\n",
      "ticular,NeuralLPandNTP-λfailedtoscaletothe ingtoacceleratetheconvergenceofRL(toabetter\n",
      "larger datasets and their results are omitted from performance level) as it propagates prior knowl-\n",
      "thetable,asDasetal.(2018)reported. edge about the underlying KG to the agent. On\n",
      "Ours is the first multi-hop reasoning approach theotherhand,afairconcernforactiondropoutis\n",
      "which is consistently comparable or better than thatitcanbeslowertotrain,astheagentisforced\n",
      "embedding based approaches on all five datasets. to explore a more diverse set of paths. Figure 4\n",
      "Thebestsinglemodel,Ours(ConvE),improvesthe eliminatesthisconcern.\n",
      "SOTAperformanceofpath-basedmodelsonthree The first row of Figure 4 shows the changes in\n",
      "datasets (UMLS, Kinship, and FB15k-237) by devsetMRRofOurs(ConvE)(green)andthetwo\n",
      "4%,9%,and39%respectively. OnWN18RRand ablated models w.r.t. # epochs. In general, the\n",
      "NELL-995,ourapproachdidnotsignificantlyim- proposed approachis ableto converge toa higher\n",
      "proveoverexistingSOTA.TheNELL-995dataset accuracy level much faster than either of the ab-\n",
      "consists of only 12 relations in the test set and, as lated models and the performance gap often per-\n",
      "we further detail in the analysis (§ 5.3.3), our ap- sistsuntiltheendoftraining(onUMLS,Kinship,\n",
      "proachislesseffectiveforthoserelationtypes. andFB15k-237). Particularly,onFB15k-237,our\n",
      "The model variations using different reward approach still shows improvement even after the\n",
      "shapingmodulesperformsimilarly. Whileabetter twoablatedmodelsstarttooverfit,with−ADbe-\n",
      "rewardshapingmoduletypicallyresultsinabetter ginningtooverfitsooner. OnWN18RR,introduc-\n",
      "overall model, an exception is WN18RR, where ingrewardshapinghurtdevsetperformancefrom\n",
      "ComplEx performs slightly worse on its own but the beginning, as discussed in § 5.2. On NELL-\n",
      "is more helpful for reward shaping. We left the 995,Ours(ConvE)performssignificantlybetterin\n",
      "study of the relationship between the accuracy of the beginning, but −RS gradually reaches a com-\n",
      "the reward shaping module and the overall model parableperformancelevel.\n",
      "performanceasfuturework. It is especially interesting that introducing ac-\n",
      "tiondropoutimmediatelyimprovesthemodelper-\n",
      "5.2 AblationStudy formance on all datasets. A possible explanation\n",
      "for this is that by exploring a more diverse set of\n",
      "We perform an ablation study where we remove\n",
      "pathstheagentlearnssearchpoliciesthatgeneral-\n",
      "rewardshaping(−RS)andactiondropout(−AD)\n",
      "izebetter.\n",
      "from Ours(ConvE) and compare their MRRs to\n",
      "the whole model on the dev sets.6 As shown in\n",
      "5.3.2 PathDiversity\n",
      "Table 3, on most datasets, removing each com-\n",
      "Wealsocomputethetotalnumberofuniquepaths\n",
      "ponent results in a significant performance drop.\n",
      "theagentexploresduringtrainingandvisualizeits\n",
      "The exception is WN18RR, where removing the\n",
      "change w.r.t. # training epochs in the second row\n",
      "ConvE reward shaping module improves the per-\n",
      "formance.7 Removing reward shaping on NELL- ofFigure4. Whencountingauniquepath,wein-\n",
      "clude both the edge label and intermediate entity.\n",
      "6According to Table 3 and Table 2, the dev and test set Firstweobservethat,onalldatasets,theagentex-\n",
      "evaluationmetricsdiffersignificantlyonseveraldatasets.We\n",
      "discussthecauseofthisin§A.2. thataddingtheComplExrewardshapingmodulehelps, de-\n",
      "7Apossibleexplanationforthisisthataspath-basedmod- spite the fact that ComplEx performs slightly worse than\n",
      "els tend to outperform the embedding based approaches on ConvEonthisdataset. Thisindicatesthatdevsetaccuracy\n",
      "WN18RR,ConvEmaybesupplyingmorenoisethanuseful is not the only factor which determines the effectiveness of\n",
      "informationabouttheKG.Yetcounter-intuitively,wefound rewardshaping.\n",
      "0.7\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0 50 100 150 200\n",
      "RRM\n",
      "tes ved\n",
      "UMLS Kinship FB15k-237 WN18RR NELL-995\n",
      "0.46\n",
      "0.38 0.7 0.78\n",
      "0.6 0.36 0.44\n",
      "0.76\n",
      "0.5 0.34 0.42 0.4 0.74 0.3 0.32 0.40 0.72\n",
      "0.2 0.30 0.38\n",
      "0.1 0.28 0.70\n",
      "0 50 100 150 200 0 5 10 15 0 5 10 15 20 0 10 20 30\n",
      "1.2\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "0 50 100 150 200\n",
      "detisiv\n",
      "shtap\n",
      "euqinu\n",
      "#\n",
      "Ours(ConvE) -AD -RS\n",
      "1e6 1e6 1e7 1e7 1e6\n",
      "1.50 1.75\n",
      "1.0 1.50\n",
      "6 1.25\n",
      "0.8 1.00 1.25\n",
      "0.6 4 0.75 1.00\n",
      "0.75\n",
      "0.4 2 0.50 0.50\n",
      "0.25 0.25\n",
      "0.2\n",
      "0 0.00 0.00\n",
      "0 50 100 150 200 0 5 10 15 0 5 10 15 20 0 10 20 30\n",
      "# epochs\n",
      "Figure 4: Illustration of convergence rate and path exploration efficiency. The three curves in each subplot\n",
      "representsOurs(ConvE)(green)andthetwoablatedmodels: −RS(blue)and−AD(orange). Thetoprowshows\n",
      "thechangeofdevsetMRRandthebottomrowshowsthegrowthof#uniquepathsexploredw.r.t. #epochs.\n",
      "To-many To-one\n",
      "Dataset\n",
      "% Ours(ConvE) −RS −AD % Ours(ConvE) −RS −AD\n",
      "UMLS 99.1 73.1 67.9(-7%) 61.3(-16%) 0.9 62.5 55.5(-11%) 54.4(-13%)\n",
      "Kinship 100 75 66.5(-11%) 65.4(-13%) 0 – – –\n",
      "FB15k-237 76.6 28.3 24.5(-13%) 20.9(-26%) 23.4 72 69.8(-3%) 63.9(-11%)\n",
      "WN18RR 52.8 65 65.7(+1%) 57.9(-11%) 47.2 20.1 23.2(+16%) 18.1(-10%)\n",
      "NELL-995 12.9 55.7 62.1(+12%) 56.9(+2%) 87.1 81.4 80.7(-1%) 80.5(-1%)\n",
      "Table4: MRRevaluationofdifferentrelationtypes(to-manyvs. to-one)onfivedatasets. The%columnsshow\n",
      "thepercentageofexamplesofeachrelationtypefoundinthedevelopmentsplitofthecorrespondingdataset. In\n",
      "general,ourproposedtechniquesimprovethepredictionresultsforto-manyrelationsmoresignificantly.\n",
      "plores a large number of paths before reaching a ploredanddevsetperformanceisnotstrictlypos-\n",
      "goodperformancelevel. Thespeedofpathdiscov- itive. Thebestperformingmodelingeneralisnot\n",
      "ery slowly decreases as training progresses. On themodelthatexploredthelargest#paths. Italso\n",
      "smallerKGs(UMLSandKinship),therateofen- demonstratestheroleofrewardshapingasaregu-\n",
      "counteringnewpathsissignificantlyloweraftera larizerwhichguidestheagenttoavoidnoisypaths\n",
      "certainnumberofepochs,andthedevsetaccuracy withitspriorknowledge.\n",
      "plateaus correspondingly. On much larger KGs\n",
      "5.3.3 Performancew.r.t. RelationTypes\n",
      "(FB15k-237, WN18RR, and NELL-995), we did\n",
      "not observe a significant slowdown before severe We investigate the behaviors of our proposed ap-\n",
      "overfitting occurs and the dev set performance proachw.r.tdifferentrelationtypes. ForeachKG,\n",
      "startstodrop. Apossiblereasonforthisisthatthe we classify its set of relations into two categories\n",
      "largerKGsaremoresparselyconnectedcompared based on the answer set cardinality. Specifically,\n",
      "to the smaller KGs (Table 1), therefore it is less we define the metric ξ r as the average answer set\n",
      "efficienttogaingeneralizableknowledgefromthe cardinalityofallquerieswithtopicrelationr. We\n",
      "KG by exploring a limited proportion of the path countr asa“to-many”relationifξ r > 1.5,which\n",
      "spacethroughsampling. indicates that most queries in relation r has more\n",
      "than 1 correct answer; we count r as a “to-one”\n",
      "Second,itisinterestingtoseethatwhileremov- relation otherwise, meaning most queries of this\n",
      "ing action dropout significantly lowers the effec- relationhaveonly1correctanswer.\n",
      "tiveness of path exploration (orange vs. green), Table4showsthepercentageofexamplesofto-\n",
      "removing reward shaping slightly improves the # manyandto-onerelationsoneachdevdatasetand\n",
      "paths visited during training for all datasets. This theMRRevaluationmetricsofpreviouslystudied\n",
      "indicates that the correlation between # paths ex- modelscomputedontheexamplesofeachrelation\n",
      "SeenQueries UnseenQueries\n",
      "Dataset\n",
      "% Ours(ConvE) −RS −AD % Ours(ConvE) −RS −AD\n",
      "UMLS 97.2 73.1 67.9(-7%) 61.4(-16%) 2.8 68.5 61.5(-10%) 58.7(-14%)\n",
      "Kinship 96.8 75.1 66.5(-11%) 65.8(-12%) 3.2 73.6 64.3(-13%) 53.3(-27%)\n",
      "FB15k-237 76.1 28.3 24.3(-14%) 20.6(-27%) 23.9 70.9 69.1(-2%) 63.9(-10%)\n",
      "WN18RR 41.8 60.8 62.0(+2%) 53.4(-12%) 58.2 31.5 33.9(+7%) 28.8(-9%)\n",
      "NELL-995 15.3 40.4 45.9(+14%) 42.5(+5%) 84.7 85.5 84.7(-1%) 84.3(-1%)\n",
      "Table5: MRRevaluationofseenqueriesvs. unseenqueriesonfivedatasets. The%columnsshowthepercentage\n",
      "ofexamplesofseen/unseenqueriesfoundinthedevelopmentsplitofthecorrespondingdataset.\n",
      "type. Since UMLS and Kinship are densely con- mostdatasets,theratioofseenvs. unseenqueries\n",
      "nected, they almost exclusively contain to-many is similar to that of to-many vs. to-one relations\n",
      "relations. FB15k-237mostlycontainsto-manyre- (Table 4) as a result of random data split, with\n",
      "lations. In Figure 4, we observe the biggest rela- the exception of WN18RR. On some datasets, all\n",
      "tive gains from the ablated models on these three models perform better on seen queries (UMLS,\n",
      "datasets. WN18RR is more balanced and con- Kinship, WN18RR)whileothersrevealtheoppo-\n",
      "sists of slightly more to-many relations than to- sitetrend. OnNELL-995bothofourproposeden-\n",
      "one relations. The NELL-995 dev set is a unique hancementsarenoteffectiveovertheseenqueries.\n",
      "one which almost exclusively consists of to-one We leave the study of these model behaviors to\n",
      "relations. There is no common performance pat- future work. In most cases, our proposed en-\n",
      "tern over the two relation types across datasets: hancementsimprovetheperformanceoverunseen\n",
      "on some datasets all models perform better on queries,withADbeingmoreeffective.\n",
      "to-many relations (UMLS, WN18RR) while oth-\n",
      "6 Conclusions\n",
      "ers reveal the opposite trend (FB15k-237, NELL-\n",
      "995). We leave the study of these differences to\n",
      "We propose two modeling advances for end-to-\n",
      "futurework.\n",
      "endRL-basedknowledge graphqueryanswering:\n",
      "Weshowtherelativeperformancechangeofthe rewardshapingandactiondropout. Ourapproach\n",
      "ablatedmodels−RSand−ADw.r.t. Ours(ConvE) improvesoverstate-of-the-artmulti-hopreasoning\n",
      "in parentheses. We observe that in general our models consistently on several benchmark KGs.\n",
      "proposedenhancementsareeffectiveinimproving A detailed analysis indicates that the access to a\n",
      "query-answering over both relation types (more moreaccurateenvironmentrepresentation(reward\n",
      "effective for to-many relations). However, adding shaping) and a more thorough exploration of the\n",
      "the ConvE reward shaping module on WN18RR searchspace(actiondropout)areimportanttothe\n",
      "hurts the performance over both to-many and to- performanceboost.\n",
      "one relations (more for to-one relations). On On the other hand, the performance gap be-\n",
      "NELL-995, both techniques hurt the performance tween RL-based approaches and the embedding-\n",
      "overto-manyrelations. based approaches for KGQA remains. In future\n",
      "work, we would like to investigate learnable re-\n",
      "5.3.4 Performancew.r.t. SeenQueriesvs.\n",
      "wardshapingandactiondropoutschemesandap-\n",
      "UnseenQueries\n",
      "plymodel-basedRLtothisdomain.\n",
      "Sincemostbenchmarkdatasetsrandomlysplitthe\n",
      "Acknowledgements\n",
      "KGtriplesintotrain,devandtestsets,thequeries\n",
      "that have multiple answers may fall into multi- We thank Mark O. Riedl, Yingbo Zhou, James\n",
      "ple splits. As a result, some of the test queries Bradbury and Vena Jia Li for their feedback on\n",
      "(e s,r q,?) are seen in the training set (with a dif- early draft of the paper, and Mark O. Riedl for\n",
      "ferentsetofanswers)whiletheothersarenot. We helpful conversations on reward shaping. We\n",
      "investigatethebehaviorsofourproposedapproach thanktheanonymousreviewersandtheSalesforce\n",
      "w.r.t. seenandunseenqueries. research team members for their thoughtful com-\n",
      "Table 5 shows the percentage of examples as- ments and discussions. We thank Fre´deric Godin\n",
      "sociated with seen and unseen queries on each forpointingoutanerrorinEquation8inanearly\n",
      "dev dataset and the corresponding MRR evalua- versionofthepaper.\n",
      "tion metrics of previously studied models. On\n",
      "References Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,\n",
      "Hanna M. Wallach, Rob Fergus, S. V. N. Vish-\n",
      "ReginaBarzilayandMin-YenKan,editors.2017. Pro-\n",
      "wanathan, and Roman Garnett, editors. 2017. Ad-\n",
      "ceedingsofthe55thAnnualMeetingoftheAssocia-\n",
      "vances in Neural Information Processing Systems\n",
      "tionforComputationalLinguistics,ACL2017,Van-\n",
      "30: AnnualConferenceonNeuralInformationPro-\n",
      "couver,Canada,July30-August4,Volume1: Long\n",
      "cessing Systems 2017, 4-9 December 2017, Long\n",
      "Papers.AssociationforComputationalLinguistics.\n",
      "Beach,CA,USA.\n",
      "JonathanBerant,AndrewChou,RoyFrostig,andPercy\n",
      "He He, Anusha Balakrishnan, Mihail Eric, and Percy\n",
      "Liang. 2013. Semantic parsing on freebase from\n",
      "Liang. 2017. Learning symmetric collaborative di-\n",
      "question-answer pairs. In (Yarowsky et al., 2013),\n",
      "alogue agents with dynamic knowledge graph em-\n",
      "pages1533–1544.\n",
      "beddings. In(BarzilayandKan,2017),pages1766–\n",
      "Antoine Bordes, Nicolas Usunier, Alberto Garcia- 1776.\n",
      "Dura´n, Jason Weston, and Oksana Yakhnenko.\n",
      "2013. Translating embeddings for modeling multi- Diederik P. Kingma and Jimmy Ba. 2014. Adam:\n",
      "relationaldata. InProceedingsofthe26thInterna- A method for stochastic optimization. CoRR,\n",
      "tional Conference on Neural Information Process- abs/1412.6980.\n",
      "ingSystems-Volume2,NIPS’13,pages2787–2795,\n",
      "StanleyKokandPedroM.Domingos.2007. Statistical\n",
      "USA.CurranAssociatesInc.\n",
      "predicateinvention. InICML,volume227ofACM\n",
      "Wenhu Chen, Wenhan Xiong, Xifeng Yan, and International Conference Proceeding Series, pages\n",
      "William Yang Wang. 2018. Variational knowledge 433–440.ACM.\n",
      "graphreasoning. CoRR,abs/1803.06581.\n",
      "Ni Lao, Tom Mitchell, and William W. Cohen. 2011.\n",
      "Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Randomwalkinferenceandlearninginalargescale\n",
      "Luke Vilnis, Ishan Durugkar, Akshay Krishna- knowledge base. In Proceedings of the Conference\n",
      "murthy,AlexSmola,andAndrewMcCallum.2018. on Empirical Methods in Natural Language Pro-\n",
      "Go for a walk and arrive at the answer: Reasoning cessing, EMNLP’11, pages529–539, Stroudsburg,\n",
      "over paths in knowledge bases using reinforcement PA, USA. Association for Computational Linguis-\n",
      "learning. In International Conference on Learning tics.\n",
      "Representations.\n",
      "NiLao,AmarnagSubramanya,FernandoC.N.Pereira,\n",
      "Tim Dettmers, Pasquale Minervini, Pontus Stenetorp,\n",
      "andWilliamW.Cohen.2012. Readingthewebwith\n",
      "and Sebastian Riedel. 2018. Convolutional 2d\n",
      "learned syntactic-semantic inference rules. In Pro-\n",
      "knowledge graph embeddings. In Proceedings of\n",
      "ceedingsofthe2012JointConferenceonEmpirical\n",
      "theThirty-SecondAAAIConferenceonArtificialIn-\n",
      "MethodsinNaturalLanguageProcessingandCom-\n",
      "telligence,NewOrleans,Louisiana,USA,February\n",
      "putational Natural Language Learning, EMNLP-\n",
      "2-7,2018.AAAIPress.\n",
      "CoNLL2012, July12-14, 2012, JejuIsland, Korea,\n",
      "pages1017–1026.ACL.\n",
      "Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,\n",
      "and Tom M. Mitchell. 2013. Improving learning\n",
      "Andrew McCallum, Arvind Neelakantan, Rajarshi\n",
      "and inference in a large knowledge-base using la-\n",
      "Das, and David Belanger. 2017. Chains of reason-\n",
      "tentsyntacticcues. In(Yarowskyetal.,2013),pages\n",
      "ing over entities, relations, and text using recurrent\n",
      "833–838.\n",
      "neural networks. In Proceedings of the 15th Con-\n",
      "XavierGlorotandYoshuaBengio.2010. Understand- ferenceoftheEuropeanChapteroftheAssociation\n",
      "ing the difficulty of training deep feedforward neu- for Computational Linguistics, EACL 2017, Valen-\n",
      "ral networks. In Proceedings of the Thirteenth In- cia,Spain,April3-7,2017,Volume1: LongPapers,\n",
      "ternationalConferenceonArtificialIntelligenceand pages132–141.AssociationforComputationalLin-\n",
      "Statistics,AISTATS2010,ChiaLagunaResort,Sar- guistics.\n",
      "dinia, Italy, May 13-15, 2010, volume 9 of JMLR\n",
      "Volodymyr Mnih, Koray Kavukcuoglu, David Silver,\n",
      "Proceedings,pages249–256.JMLR.org.\n",
      "Alex Graves, Ioannis Antonoglou, Daan Wierstra,\n",
      "Kelvin Guu, John Miller, and Percy Liang. 2015. and Martin A. Riedmiller. 2013. Playing atari with\n",
      "Traversing knowledge graphs in vector space. In deepreinforcementlearning. CoRR,abs/1312.5602.\n",
      "Proceedings of the 2015 Conference on Empirical\n",
      "Methods in Natural Language Processing, EMNLP Andrew Y. Ng, Daishi Harada, and Stuart J. Russell.\n",
      "2015, Lisbon, Portugal, September 17-21, 2015, 1999. Policy invariance under reward transforma-\n",
      "pages318–327.TheAssociationforComputational tions: Theory and application to reward shaping.\n",
      "Linguistics. In Proceedings of the Sixteenth International Con-\n",
      "ference on Machine Learning (ICML 1999), Bled,\n",
      "Kelvin Guu, Panupong Pasupat, Evan Zheran Liu, Slovenia, June27-30, 1999, pages278–287.Mor-\n",
      "and Percy Liang. 2017. From language to pro- ganKaufmann.\n",
      "grams: Bridging reinforcement learning and max-\n",
      "imum marginal likelihood. In (Barzilay and Kan, Lawrence Page, Sergey Brin, Rajeev Motwani, and\n",
      "2017),pages1051–1062. TerryWinograd.1999. Thepagerankcitationrank-\n",
      "ing: Bringing order to the web. Technical Re- method for knowledge graph reasoning. In Pro-\n",
      "port 1999-66, Stanford InfoLab. Previous number ceedingsofthe2017ConferenceonEmpiricalMeth-\n",
      "=SIDL-WP-1999-0120. odsinNaturalLanguageProcessing,EMNLP2017,\n",
      "Copenhagen, Denmark, September 9-11, 2017,\n",
      "Romain Paulus, Caiming Xiong, and Richard Socher. pages564–573.AssociationforComputationalLin-\n",
      "2017. Adeepreinforcedmodelforabstractivesum- guistics.\n",
      "marization. CoRR,abs/1705.04304.\n",
      "Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\n",
      "Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, Gao, and Li Deng. 2014. Embedding entities and\n",
      "and Wojciech Zaremba. 2015. Sequence level relations for learning and inference in knowledge\n",
      "training with recurrent neural networks. CoRR, bases. CoRR,abs/1412.6575.\n",
      "abs/1511.06732.\n",
      "Fan Yang, Zhilin Yang, and William W. Cohen. 2017.\n",
      "TimRockta¨schelandSebastianRiedel.2017. End-to- Differentiable learning of logical rules for knowl-\n",
      "enddifferentiableproving. In(Guyonetal.,2017), edgebasereasoning. In(Guyonetal.,2017),pages\n",
      "pages3791–3803. 2316–2325.\n",
      "Yelong Shen, Jianshu Chen, Po-Sen Huang, Yuqing David Yarowsky, Timothy Baldwin, Anna Korhonen,\n",
      "Guo, and Jianfeng Gao. 2018. Reinforcewalk: Karen Livescu, and Steven Bethard, editors. 2013.\n",
      "Learning to walk in graph with monte carlo tree Proceedings of the 2013 Conference on Empirical\n",
      "search. CoRR,abs/1802.04394. Methods in Natural Language Processing, EMNLP\n",
      "2013, 18-21 October 2013, Grand Hyatt Seattle,\n",
      "RichardSocher,DanqiChen,ChristopherD.Manning, Seattle, Washington, USA, A meeting of SIGDAT, a\n",
      "and Andrew Y. Ng. 2013. Reasoning with neural SpecialInterestGroupoftheACL.ACL.\n",
      "tensornetworksforknowledgebasecompletion. In\n",
      "Proceedingsofthe26thInternationalConferenceon\n",
      "NeuralInformationProcessingSystems-Volume1,\n",
      "NIPS’13, pages 926–934, USA. Curran Associates\n",
      "Inc.\n",
      "Richard S. Sutton and Andrew G. Barto. 1998. Re-\n",
      "inforcement learning - an introduction. Adaptive\n",
      "computationandmachinelearning.MITPress.\n",
      "Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-\n",
      "fungPoon,PallaviChoudhury,andMichaelGamon.\n",
      "2015. Representingtextforjointembeddingoftext\n",
      "and knowledge bases. In EMNLP, pages 1499–\n",
      "1509. The Association for Computational Linguis-\n",
      "tics.\n",
      "KristinaToutanova,XiVictoriaLin,Wen-tauYih,Hoi-\n",
      "fung Poon, and Chris Quirk. 2016. Compositional\n",
      "learningofembeddingsforrelationpathsinknowl-\n",
      "edge base and text. In Proceedings of the 54th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics, ACL 2016, August 7-12, 2016, Berlin,\n",
      "Germany,Volume1: LongPapers.TheAssociation\n",
      "forComputerLinguistics.\n",
      "The´oTrouillon,JohannesWelbl,SebastianRiedel,E´ric\n",
      "Gaussier,andGuillaumeBouchard.2016. Complex\n",
      "embeddingsforsimplelinkprediction. InProceed-\n",
      "ings of the 33nd International Conference on Ma-\n",
      "chine Learning, ICML 2016, New York City, NY,\n",
      "USA,June19-24, 2016, volume48ofJMLRWork-\n",
      "shop and Conference Proceedings, pages 2071–\n",
      "2080.JMLR.org.\n",
      "RonaldJ. Williams.1992. Simplestatistical gradient-\n",
      "following algorithms for connectionist reinforce-\n",
      "mentlearning. MachineLearning,8:229–256.\n",
      "Wenhan Xiong, Thien Hoang, and William Yang\n",
      "Wang. 2017. Deeppath: A reinforcement learning\n",
      "A Appendix datasets, the evaluation metrics increases signifi-\n",
      "cantly to the level that is comparable to those on\n",
      "A.1 ModelRobustness\n",
      "the test set, with the relative improvement corre-\n",
      "We run the best embedding-based model ConvE latedwiththeaveragenodefan-outintheKG(Ta-\n",
      "and Ours(ConvE) on all datasets using 5 differ- ble1).\n",
      "ent random seeds with all other hyperparameters NoticethatTable7isgeneratedafterallhyper-\n",
      "fixed. Table6reportsthemeanandstandarddevi- parameters were fixed and the purpose is to show\n",
      "ationofeachmodel. Weobservethatbothmodels the effects of such dataset peculiarities. To avoid\n",
      "demonstrate a small standard deviation (< 0.01) potential test set leakage, hyperparameter search\n",
      "onalldatasets. shouldbedonewiththetestsettripleshidden(Ta-\n",
      "ble1)insteadofwiththefullKG.\n",
      "Dataset ConvE Ours(ConvE)\n",
      "A.3 ActionDropoutRatesUsedforDifferent\n",
      "UMLS 95.5±0.4 93.7±0.2 KGs\n",
      "Kinship 86.9±0.3 86.2±0.7\n",
      "Table 8 show the action dropout rates used for all\n",
      "FB15k-237 43.5±0.1 40.7±0.2\n",
      "KGdatasetsinourexperiments. Ingeneral,larger\n",
      "WN18RR 45.3±0.4 44.7±0.2\n",
      "actiondropoutratesarenecessaryforKGsthatare\n",
      "NELL-995 76.2±0.3 72.7±0.4\n",
      "densely connected. We find a positive correlation\n",
      "Table6: TestsetMRR×100meanandstandarddevia- between the optimal action dropout rate and the\n",
      "tionacrossfiverunsonalldatasets. averagenodefan-out(Table1).\n",
      "ForUMLSandKinship,wetriedsettingtheac-\n",
      "tion dropout rate to 1.0 (completely random sam-\n",
      "A.2 DevelopmentSetEvaluationUsing\n",
      "pling) and observed small but significant perfor-\n",
      "CompleteKGs\n",
      "mance drop. Random sampling performs reason-\n",
      "Comparing Table 2 and Table 3 reveals that the ablywellonthesetwodatasetspossiblyduetothe\n",
      "devsetMRRsaresignificantlylowerthanthetest fact that they are small. For larger KGs (FB15k-\n",
      "set MRRs on some datasets (UMLS, Kinship and 237, WN18RR, NELL-995), policy-guided sam-\n",
      "FB15k-237). Suchdiscrepanciesarecausedbythe plingisnecessary.\n",
      "multi-answer queries in these datasets. As most\n",
      "benchmark datasets randomly split the KG triples Dataset α\n",
      "intotrain/dev/testsets,thequeriesthathavemulti-\n",
      "UMLS 0.95\n",
      "pleanswersmayfallintomultiplesplits. Because\n",
      "Kinship 0.9\n",
      "wehidealltriplesinthetestsetduringthedevset\n",
      "FB15k-237 0.5\n",
      "evaluation,somepredictionsgeneratedduringdev\n",
      "WN18RR 0.1\n",
      "setevaluationwerewronglypunishedasfalseneg-\n",
      "NELL-995 0.1\n",
      "atives. In contrast, the test set evaluation metrics\n",
      "are computed using the complete KGs. Access to Table8: Actiondropoutratesusedinourexperiments.\n",
      "thecompleteKGeliminatesmostofthefalseneg-\n",
      "ativescasesandhenceincreasestheperformance.\n",
      "Model UMLSKinshipFB15k237WN18RRNELL995\n",
      "Ours(ConvE) 95.1 86.8 41.8 44.1 78.8\n",
      "−RS 85.6 75.7 37.1 46.1 78.4\n",
      "−AD 76.2 75.9 32.4 39.3 76.1\n",
      "Table7: ComparisonofdevsetMRRcomputedusing\n",
      "thecompleteKGsofOurs(ConvE)andmodelswithout\n",
      "rewardshapingandactiondropout.\n",
      "Table 7 shows the dev set MRR of the same\n",
      "systems shown in Table 3 with the MRRs com-\n",
      "puted using the complete KGs. On four of the<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: What are the tasks that the model is trained for?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  21579,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Knowledge Graph Query Answering', 'Multi-Hop Reasoning', 'End-to-End Reinforcement Learning']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: Multi-Hop Knowledge Graph Reasoning with Reward Shaping\n",
      "XiVictoriaLin RichardSocher CaimingXiong\n",
      "SalesforceResearch\n",
      "{xilin,rsocher,cxiong}@salesforce.com\n",
      "Abstract belong_to?\n",
      "U.S. Government Rudy_Giulian\n",
      "belong_to belong_to collaborate\n",
      "Multi-hop reasoning is an effective approach _with\n",
      "collaborate_with\n",
      "for query answering (QA) over incomplete Barack_Obama John_McCain\n",
      "knowledgegraphs(KGs). Theproblemcanbe\n",
      "endorsed_by\n",
      "formulated in a reinforcement learning (RL) born_in collaborate_with? live_in col _la wb io thrate\n",
      "setup,whereapolicy-basedagentsequentially\n",
      "locate_in live_in\n",
      "extends its inference path until it reaches a Hawaii U.S. Hillary_Clinton\n",
      "target. However, in an incomplete KG en-\n",
      "Figure1: Exampleofanincompleteknowledgegraph\n",
      "vironment, the agent receives low-quality re-\n",
      "which contains missing links (dashed lines) that can\n",
      "wardscorruptedbyfalsenegativesinthetrain-\n",
      "possiblybeinferredfromexistingfacts(solidlines).\n",
      "ing data, which harms generalization at test\n",
      "time. Furthermore,sincenogoldenactionse-\n",
      "quence is used for training, the agent can be\n",
      "Embedding based approaches ignore the sym-\n",
      "misled by spurious search trajectories that in-\n",
      "bolic compositionality of KG relations, which\n",
      "cidentallyleadtothecorrectanswer. Wepro-\n",
      "limit their application in more complex rea-\n",
      "pose two modeling advances to address both\n",
      "soning tasks. An alternative solution for KG\n",
      "issues: (1)wereducetheimpactoffalsenega-\n",
      "tivesupervisionbyadoptingapretrainedone- reasoning is to infer missing facts by synthe-\n",
      "hop embedding model to estimate the reward sizing information from multi-hop paths, e.g.\n",
      "of unobserved facts; (2) we counter the sen- bornIn(Obama,Hawaii)∧locatedIn(Hawaii,US)\n",
      "sitivity to spurious paths of on-policy RL by ⇒ bornIn(Obama, US), as shown in Figure 1.\n",
      "forcing the agent to explore a diverse set of\n",
      "Path-basedreasoningofferslogicalinsightsofthe\n",
      "paths using randomly generated edge masks.\n",
      "underlyingKGandaremoredirectlyinterpretable.\n",
      "Our approach significantly improves over ex-\n",
      "Early work treats it as a link prediction prob-\n",
      "isting path-based KGQA models on several\n",
      "lem and perform maximum-likelihood classifica-\n",
      "benchmarkdatasetsandiscomparableorbet-\n",
      "terthanembedding-basedmodels. tion over either discrete path features (Lao et al.,\n",
      "2011, 2012; Gardner et al., 2013) or their hidden\n",
      "1 Introduction representationsinavectorspace(Guuetal.,2015;\n",
      "Toutanovaetal.,2016;McCallumetal.,2017).\n",
      "Large-scale knowledge graphs (KGs) support a Morerecentworkformulatesmulti-hopreason-\n",
      "variety of downstream NLP applications such as ing as a sequential decision problem, and lever-\n",
      "semanticsearch(Berantetal.,2013)anddialogue ages reinforcement learning (RL) to perform ef-\n",
      "generation (He et al., 2017). Whether curated au- fective path search (Xiong et al., 2017; Das et al.,\n",
      "tomatically or manually, practical KGs often fail 2018;Shenetal.,2018;Chenetal.,2018). Inpar-\n",
      "to include many relevant facts. A popular ap- ticular,MINERVA(Dasetal.,2018)usestheRE-\n",
      "proach for modeling incomplete KGs is knowl- INFORCE algorithm (Williams, 1992) to train an\n",
      "edge graph embeddings, which map both entities end-to-endmodelformulti-hopKGqueryanswer-\n",
      "and relations in the KG to a vector space and ing: givenaqueryrelationandasourceentity,the\n",
      "learn a truth value function for any potential KG trained agent searches over the KG starting from\n",
      "tripleparameterizedbytheentityandrelationvec- the source and arrives at the candidate answers\n",
      "tors(Yangetal.,2014;Dettmersetal.,2018). withoutaccesstoanypre-computedpaths.\n",
      "8102\n",
      "peS\n",
      "11\n",
      "]IA.sc[\n",
      "2v86501.8081:viXra\n",
      "art multi-hop reasoning approaches on four out\n",
      "false negative of five benchmark KG datasets (UMLS, Kinship,\n",
      "0.25\n",
      "FB15k-237, WN18RR). It is also the first path-\n",
      "0.20 based model that achieves consistently compara-\n",
      "ble or better performance than embedding-based\n",
      "0.15\n",
      "models. In addition, we perform a thorough ab-\n",
      "0 5 10 15\n",
      "# epochs lationstudyandresultanalysis,demonstratingthe\n",
      "effectofeachmodelinginnovation.\n",
      "Figure2: Percentageoffalsenegativeshit(wherethe\n",
      "modelpredictedananswerthatexistsinthefullKGbut\n",
      "2 Approach\n",
      "cannot be identified by the training subset) in the first\n",
      "20 epochs of walk-based QA training on the UMLS\n",
      "Inthissection,wefirstreviewthewalk-basedQA\n",
      "knowledgegraph(KokandDomingos,2007).\n",
      "framework(§2.2)andtheon-policyreinforcement\n",
      "learning approach proposed by Das et al. (2018)\n",
      "(§2.3,§2.4). Then we describe our proposed so-\n",
      "We refer to the RL formulation adopted by\n",
      "lutions to the false negative reward and spurious\n",
      "MINERVA as “learning to walk towards the an-\n",
      "path problems: knowledge-based reward shaping\n",
      "swer” or “walk-based query-answering (QA)”.\n",
      "(§2.5)andactiondropout(§2.6).\n",
      "Walk-based QA eliminates the need to pre-\n",
      "compute path features, yet this setup poses sev-\n",
      "2.1 FormalProblemDefinition\n",
      "eral challenges for training. First, because prac-\n",
      "tical KGs are intrinsically incomplete, the agent Weformallyrepresent aknowledgegraphasG =\n",
      "may arrive at a correct answer whose link to the (E,R), where E is the set of entities and R is the\n",
      "source entity is missing from the training graph set of relations. Each directed link in the knowl-\n",
      "without receiving any reward (false negative tar- edge graph l = (e s,r,e o) ∈ G represents a fact\n",
      "gets,Figure2). Second,sincenogroundtruthpath (alsocalledatriple).\n",
      "is available for training, the agent may traverse Givenaquery(e s,r q,?),wheree s isthesource\n",
      "spurious paths that lead to a correct answer only entity and r q is the relation of interest, the goal\n",
      "incidentally(falsepositivepaths). BecauseREIN- is to perform an efficient search over G and col-\n",
      "FORCE (Williams, 1992) is an on-policy (Sutton lect the set of possible answers E o = {e o} s.t.\n",
      "and Barto, 1998) RL algorithm which encourages (e s,r q,e o) ∈/ G duetoincompleteness.\n",
      "past actions with high reward, it can bias the pol-\n",
      "2.2 ReinforcementLearningFormulation\n",
      "icy toward spurious paths found early in training\n",
      "(Guuetal.,2017). The search can be viewed as a Markov Decision\n",
      "Process(MDP)(SuttonandBarto,1998): starting\n",
      "WeproposetwomodelingadvancesforRLap-\n",
      "frome,theagentsequentiallyselectsanoutgoing\n",
      "proaches in the walk-based QA framework to ad- s\n",
      "edgel andtraversestoanewentityuntilitarrives\n",
      "dress the previously mentioned problems. First,\n",
      "at a target. Specifically, the MDP consists of the\n",
      "insteadofusingabinaryrewardbasedonwhether\n",
      "followingcomponents(Dasetal.,2018).\n",
      "the agent has reached a correct answer or not,\n",
      "we adopt pre-trained state-of-the-art embedding-\n",
      "States Each state s = (e,(e,r )) ∈ S is a\n",
      "t t s q\n",
      "based models (Dettmers et al., 2018; Trouillon\n",
      "tuple where e is the entity visited at step t and\n",
      "t\n",
      "et al., 2016) to estimate a soft reward for target\n",
      "(e,r ) are the source entity and query relation.\n",
      "s q\n",
      "entities whose correctness cannot be determined.\n",
      "e can be viewed as state-dependent information\n",
      "t\n",
      "As embedding-based models capture link seman-\n",
      "while (e,r ) are the global context shared by all\n",
      "s q\n",
      "tics well, unobserved but correct answers would\n",
      "states.\n",
      "receive a higher reward score compared to a true\n",
      "negative entity using a well-trained model. Sec- Actions The set of possible actions A ∈ A of\n",
      "t\n",
      "ond, we perform action dropout which randomly atsteptconsistsoftheoutgoingedgesofe inG.\n",
      "t\n",
      "blocks some outgoing edges of the agent at each Concretely, A = {(r(cid:48),e(cid:48))|(e,r(cid:48),e(cid:48)) ∈ G}. To\n",
      "t t\n",
      "trainingstepsoastoenforceeffectiveexploration give the agent the option of terminating a search,\n",
      "ofadiversesetofpathsanddilutethenegativeim- a self-loop edge is added to every A. Because\n",
      "t\n",
      "pact of the spurious ones. Empirically, our over- search is unrolled for a fixed number of steps T,\n",
      "allmodelsignificantlyimprovesoverstate-of-the- theself-loopactssimilarlytoa“stop”action.\n",
      "Transition Atransitionfunctionδ : S×A → S θ withthefollowingstochasticgradient:\n",
      "is defined by δ(s,A ) = δ(e,(e,r ),A ). For\n",
      "t t t s q t (cid:88)\n",
      "∇ J(θ) ≈ ∇ R(s |e,r)logπ (a |s ).\n",
      "walk-based QA, the transition is entirely deter- θ θ T s θ t t\n",
      "minedbyG. t\n",
      "(6)\n",
      "Rewards In the default formulation, the agent\n",
      "2.5 Knowledge-BasedRewardShaping\n",
      "receives a terminal reward of 1 if it arrives at a\n",
      "correcttargetentityattheendofsearchand0oth- According to equation 1, the agent receives a bi-\n",
      "erwise. naryrewardbasedonsolelytheobservedanswers\n",
      "in G. However, G is intrinsically incomplete and\n",
      "R (s ) = 1{(e,r,e ) ∈ G}. (1)\n",
      "b T s q T this approach rewards the false negative search\n",
      "2.3 PolicyNetwork results identically to true negatives. To allevi-\n",
      "ate this problem, we use existing KG embedding\n",
      "The search policy is parameterized using state in-\n",
      "models designed for the purpose of KG comple-\n",
      "formation and global context, plus the search his-\n",
      "tion (Trouillon et al., 2016; Dettmers et al., 2018)\n",
      "tory(Dasetal.,2018).\n",
      "to estimate a soft reward for target entities whose\n",
      "Specifically, every entity and relation in G is\n",
      "correctnessisunknown.\n",
      "assigned a dense vector embedding e ∈ Rd and\n",
      "Formally, the embedding models map E and R\n",
      "r ∈ Rd. Theactiona = (r,e ) ∈ A isrep-\n",
      "t t+1 t+1 t\n",
      "to a vector space, and estimate the likelihood of\n",
      "resented as the concatenation of the relation em-\n",
      "each fact l = (e,r,e ) ∈ G using a composi-\n",
      "beddingandtheendnodeembeddinga = [r;e(cid:48)]. s t\n",
      "t t tionfunctionoftheentityandrelationembeddings\n",
      "The search history h =\n",
      "t f(e,r,e ). f istrainedbymaximizingthelikeli-\n",
      "(e,r,e,...,r,e ) ∈ H consists of the s t\n",
      "s 1 1 t t hood of all facts in G. We propose the following\n",
      "sequence of observations and actions taken up to\n",
      "rewardshapingstrategy(Ngetal.,1999):\n",
      "stept,andcanbeencodedusinganLSTM:\n",
      "R(s ) = R (s )+(1−R (s ))f(e,r,e ).\n",
      "h = LSTM(0,[r ;e ]) (2) T b T b T s q T\n",
      "0 0 s\n",
      "(7)\n",
      "h = LSTM(h,a ), t > 0, (3)\n",
      "t t−1 t−1 Namely, if the destination e is a correct answer\n",
      "T\n",
      "where r is a special start relation introduced to according to G, the agent receives reward 1. Oth-\n",
      "0\n",
      "formastartactionwithe. erwisetheagentreceivesafactscoreestimatedby\n",
      "s\n",
      "Theactionspaceisencodedbystackingtheem- f(e s,r q,e T), which is pre-trained. Here we keep\n",
      "beddingsofallactionsinA t: A\n",
      "t\n",
      "∈ R|At|×2d. And f in its general form and it can be replaced by\n",
      "thepolicynetworkπ isdefinedas: any state-of-the-art model (Trouillon et al., 2016;\n",
      "Dettmersetal.,2018)orensemblethereof.\n",
      "π (a |s ) = σ(A ×W ReLU(W [e ;h ;r ])),\n",
      "θ t t t 2 1 t t q\n",
      "(4) 2.6 ActionDropout\n",
      "whereσ isthesoftmaxoperator. The REINFORCE training algorithm performs\n",
      "on-policy sampling according to π (a |s ), and\n",
      "θ t t\n",
      "2.4 Optimization\n",
      "updates θ stochastically using equation 6. Be-\n",
      "The policy network is trained by maximizing the cause the agent does not have access to any or-\n",
      "expectedrewardoverallqueriesinG: acle path, it is possible for it to arrive at a cor-\n",
      "rect answer e via a path which is irrelevant to\n",
      "J(θ) = E [E [R(s |e,r)]]. o\n",
      "(es,r,eo)∈G a1,...,aT∼π θ T s the query relation. As shown in Figure 1, the\n",
      "(5)\n",
      "path Obama −endorsedBy→ McCain −liveIn→\n",
      "The optimization is done using the REIN-\n",
      "U.S. ←locatedIn− Hawaii does not infer the fact\n",
      "FORCE (Williams, 1992) algorithm, which iter-\n",
      "bornIn(Obama,Hawaii).\n",
      "atesthroughall(e,r,e )triplesinG1andupdates\n",
      "s o Discriminating paths of different qualities is\n",
      "1This training strategy treats a query with n > 1 an- non-trivial, and existing RL approaches for walk-\n",
      "swersasnsingle-answerqueries.Inparticular,givenaquery\n",
      "based KGQA largely rely on the terminal reward\n",
      "(e,r,?)withmultipleanswers{e,...e },whentrain-\n",
      "ings wq.r.t. the example (e,r,e ),t M1 INERt Vn A removes all to bias the search. Since there are usually more\n",
      "s q ti\n",
      "{e tj|j (cid:54)= i}observedinthetrainingdatafromthepossible spurious paths than correct ones, spurious paths\n",
      "setoftargetentitiesinthelastsearchstepsoastoforcethe\n",
      "areoftenfoundfirst,andfollowingexplorationcan\n",
      "agenttowalktowardse. Weadoptthesametechniquein\n",
      "ti\n",
      "ourtraining. beincreasinglybiasedtowardsthem(Equation6).\n",
      "r?\n",
      "q\n",
      "Path sam ~pled … …\n",
      "w/ π θ if (es, rq, eT)\n",
      "r 0 e s r t e t r T e T observed\n",
      "LSTM path Reward +1\n",
      "… …\n",
      "encoder\n",
      "h 0 h t h T\n",
      "otherwise\n",
      "action selection\n",
      "Policy Network with Action Dropout Reward Shaping\n",
      "X f(e, r, e )\n",
      "s q T\n",
      "X ~\n",
      "~\n",
      "r q h t e t A t π θ(a t|s t) m π θ(a t|s t) e s r q e T\n",
      "Figure3:Overalltrainingapproach.Ateachtimestept,theagentsamplesanoutgoinglinkaccordingtoπ˜ (a |s ),\n",
      "θ t t\n",
      "whichisthestochasticREINFORCEpolicyπ (a |s )perturbedbyarandombinarymaskm. Theagentreceives\n",
      "θ t t\n",
      "reward 1 if stopped at an observed answer of the query (e,r,?); otherwise, it receives reward f(e,r,e )\n",
      "s q s q T\n",
      "estimatedbytherewardshaping(RS)network.TheRSnetworkispre-trainedanddoesn’treceivegradientupdates.\n",
      "Entities with larger fan-in (in-degree) and fan-out 3.1 KnowledgeGraphEmbeddings\n",
      "(out-degree)oftenexacerbatethisproblem.\n",
      "KG embeddings (Bordes et al., 2013; Socher\n",
      "Guu et al. (2017) identified a similar issue in\n",
      "et al., 2013; Yang et al., 2014; Trouillon et al.,\n",
      "RL-based semantic parsing with weak supervi-\n",
      "2016; Dettmers et al., 2018) are one-hop KG\n",
      "sion, where programs that do not semantically\n",
      "modeling approaches which learn a scoring func-\n",
      "match the user utterance frequently pass the tests.\n",
      "tion f(e,r,e ) to define a fuzzy truth value of\n",
      "s o\n",
      "Tosolvethisproblem,Guuetal.(2017)proposed\n",
      "a triple in the embedding space. These mod-\n",
      "randomized beam search combined with a meri-\n",
      "els can be adapted for query answering by sim-\n",
      "tocratic update rule to ensure all trajectories that\n",
      "ply return the e ’s with the highest f(e,r,e )\n",
      "o s o\n",
      "obtainrewardsareup-weightedroughlyequally.\n",
      "scores. Despitetheirsimplicity,embedding-based\n",
      "Here we propose the action dropout tech-\n",
      "models achieved state-of-the-art performance on\n",
      "niquewhichachievessimilareffectasrandomized\n",
      "KGQA (Das et al., 2018). However, such models\n",
      "search and is simpler to implement over graphs.\n",
      "ignore the symbolic compositionality of KG rela-\n",
      "Action dropout randomly masks some outgoing\n",
      "tions, which limits their usage in more complex\n",
      "edgesfortheagentinthesamplingstepofREIN-\n",
      "reasoningtasks. Therewardshaping(RS)strategy\n",
      "FORCE. The agent then performs sampling2 ac-\n",
      "we proposed is a step to combine their capabil-\n",
      "cordingtotheadjustedactiondistribution\n",
      "ityinmodelingtriplesemanticswiththesymbolic\n",
      "π˜ (a |s ) ∝ (π (a |s )·m+(cid:15)) (8) reasoningcapabilityofthepath-basedapproach.\n",
      "θ t t θ t t\n",
      "m ∼ Bernoulli(1−α),i = 1,...|A |, (9)\n",
      "i t 3.2 Multi-HopReasoning\n",
      "where each entry of m ∈ {0,1}|At| is a binary Multi-hop reasoning focus on learning symbolic\n",
      "variable sampled from the Bernoulli distribution inferencerulesfromrelationalpathsintheKGand\n",
      "with parameter 1 − α. A small value (cid:15) is used has been formulated as sequential decision prob-\n",
      "to smooth the distribution in case m = 0, where lemsinrecentworks(Xiongetal.,2017;Dasetal.,\n",
      "π˜ θ(a t|s t)becomesuniform. 2018;Shenetal.,2018;Chenetal.,2018). Inpar-\n",
      "Ouroverallapproachisillustratedinfigure3. ticular,DeepPath(Xiongetal.,2017)firstadopted\n",
      "REINFORCE to search for generic representative\n",
      "3 RelatedWork\n",
      "pathsbetweenpairsofentities. DIVA(Chenetal.,\n",
      "Inthissection,wesummarizetherelatedworkand 2018) also performs generic path search between\n",
      "discusstheirconnectionstoourapproach. entities using RL and its variational objective can\n",
      "beinterpretedasmodel-basedrewardassignment.\n",
      "2We only modify the sampling distribution and still use\n",
      "π (a |s )tocomputethegradientupdateinequation6. MINERVA (Das et al., 2018) first introduced RL\n",
      "θ t t\n",
      "to search for answer entities of a particular KG Dataset #Ent #Rel #Fact #degree\n",
      "mean median\n",
      "query end-to-end. MINERVA uses entropy reg-\n",
      "Kinship 104 25 8,544 85.15 82\n",
      "ularization to softly encourage the policy to sam- UMLS 135 46 5,216 38.63 28\n",
      "ple diverse paths, and we show that hard action FB15k-237 14,505 237 272,115 19.74 14\n",
      "WN18RR 40,945 11 86,835 2.19 2\n",
      "dropoutismoreeffectiveinthissetup. Reinforce-\n",
      "NELL-995 75,492 200 154,213 4.07 1\n",
      "Walk(Shenetal.,2018)furtherproposedtosolve\n",
      "Table 1: KGs used in the experiments sorted by in-\n",
      "the reward sparsity problem in walk-based QA\n",
      "creasingsparsitylevel.\n",
      "using off-policy learning. ReinforceWalk scores\n",
      "the search targets with a value function which is\n",
      "995 (Xiong et al., 2017). The statistics of the\n",
      "updated based on the search history accumulated\n",
      "datasetsareshowninTable1.\n",
      "through epochs. In comparison, we leveraged ex-\n",
      "isting embedding-based models for reward shap-\n",
      "4.2 BaselinesandModelVariations\n",
      "ing,whichmakesthetrainingmoreefficient.\n",
      "Wecomparewiththreeembeddingbasedmodels:\n",
      "3.3 ReinforcementLearning DistMult(Yangetal.,2014),ComplEx(Trouillon\n",
      "et al., 2016) and ConvE (Dettmers et al., 2018).\n",
      "Recently, RL has seen a variety of applications in\n",
      "Wealsocomparewiththreemulti-hopneuralsym-\n",
      "NLPincludingmachinetranslation(Ranzatoetal.,\n",
      "bolic models: (a) NTP-λ, an improved version of\n",
      "2015), summarization (Paulus et al., 2017), and\n",
      "Neural Theorem Prover (Rockta¨schel and Riedel,\n",
      "semanticparsing(Guuetal.,2017). Comparedto\n",
      "2017), (b) Neural Logical Programming (Neu-\n",
      "thedomainofgames(Mnihetal.,2013)andmany\n",
      "ralLP)(Yangetal.,2017)and(c)MINERVA.For\n",
      "other applications, RL formulations in NLP often\n",
      "our own approach, we include two model vari-\n",
      "havealargeactionspace(e.g.,inmachinetransla-\n",
      "ations that use ComplEx and ConvE as the re-\n",
      "tion,thespaceofpossibleactionsistheentirevo-\n",
      "ward shaping modules respectively, denoted as\n",
      "cabulary of a language). This also holds for KGs,\n",
      "Ours(ComplEx) and Ours(ConvE). We quote the\n",
      "assomeentitiesmayhavethousandsofneighbors\n",
      "results of NeuralLP, NTP-λ and MINERVA re-\n",
      "(e.g. U.S.). Since often there is no golden path\n",
      "portedinDasetal.(2018),andreplicatedtheem-\n",
      "available for a KG reasoning problem, we cannot\n",
      "beddingbasedsystems.3\n",
      "usesupervisedpre-trainingtogivethepathsearch\n",
      "abetterstartpositionfollowingthecommonprac-\n",
      "4.3 ImplementationDetails\n",
      "ticeadoptedinRL-basednaturallanguagegenera-\n",
      "Beam Search Decoding We perform beam\n",
      "tion(Ranzatoetal.,2015). Ontheotherhand,the\n",
      "search decoding to obtain a list of unique en-\n",
      "inference paths being studied in a KG are often\n",
      "tity predictions. Because multiple paths may lead\n",
      "much shorter (usually containing 2-5 steps) com-\n",
      "to the same target entity, we compute the list of\n",
      "paredtotheNLsentencesinthesequencegenera-\n",
      "uniqueentitiesreachedinthefinalsearchstepand\n",
      "tionproblems(oftencontaining20-30words).\n",
      "assigneachofthemthemaximumscoreamongall\n",
      "4 ExperimentSetup pathsthatledtoit. Wethenoutputthetop-ranked\n",
      "unique entities. We find this approach to improve\n",
      "We evaluate our modeling contributions on five overoutputtingtheentitiesrankedatthebeamtop\n",
      "KGsfromdifferentdomainsandexhibitingdiffer- directly,asmanyofthemarerepetitions.\n",
      "entgraphproperties(§4.1). Wecomparewithtwo\n",
      "KG Setup Following previous work, we treat\n",
      "classes of state-of-the-art KG models: multi-hop\n",
      "every KG link as bidirectional and augment the\n",
      "neural symbolic approaches and KG embeddings\n",
      "graphwiththereversed(e,r−1,e )links. Weuse\n",
      "(§4.2). Inthissection,wedescribethedatasetsand o s\n",
      "thesametrain,dev,andtestsetsplitsasDasetal.\n",
      "ourexperimentsetupindetail.\n",
      "(2018). Weexcludeanylinkfromthedevandtest\n",
      "4.1 Dataset set(anditsreversedlink)fromthetrainsetincase\n",
      "there is an overlap. Following Das et al. (2018),\n",
      "We adopt five benchmark KG datasets for query\n",
      "answering: (1) Alyawarra Kinship, (2) Unified 3Dasetal.(2018)reportedMINERVAresultswiththeen-\n",
      "Medical Language Systems (Kok and Domingos, tityembeddingusageasanextrahyperparameter–thequoted\n",
      "performanceofMINERVAinTable2onUMLSandKinship\n",
      "2007),(3)FB15k-237(Toutanovaetal.,2015),(4)\n",
      "wereobtainedwithentityembeddingssettingtozero.Incon-\n",
      "WN18RR (Dettmers et al., 2018), and (5) NELL- trast,oursystemalwaysusestrainedentityembeddings.\n",
      "UMLS Kinship FB15k-237 WN18RR NELL-995\n",
      "Model\n",
      "@1 @10 MRR @1 @10 MRR @1 @10 MRR @1 @10 MRR @1 @10 MRR\n",
      "DistMult(Yangetal.,2014) 82.1 96.7 86.8 48.7 90.4 61.4 32.4 60.0 41.7 43.1 52.4 46.2 55.2 78.3 64.1\n",
      "ComplEx(Trouillonetal.,2016) 89.0 99.2 93.4 81.8 98.1 88.4 32.8 61.6 42.5 41.8 48.0 43.7 64.3 86.0 72.6\n",
      "ConvE(Dettmersetal.,2018) 93.2 99.4 95.7 79.7 98.1 87.1 34.1 62.2 43.5 40.3 54.0 44.9 67.8 88.6 76.1\n",
      "NeuralLP(Yangetal.,2017) 64.3 96.2 77.8 47.5 91.2 61.9 16.6 34.8 22.7 37.6 65.7 46.3 – – –\n",
      "NTP-λ(Rockta¨schelet.al.2017) 84.3 100 91.2 75.9 87.8 79.3 – – – – – – – – –\n",
      "MINERVA(Dasetal.,2018) 72.8 96.8 82.5 60.5 92.4 72.0 21.7 45.6 29.3 41.3 51.3 44.8 66.3 83.1 72.5\n",
      "Ours(ComplEx) 88.7 98.5 92.9 81.1 98.2 87.8 32.9 54.4 39.3 43.7 54.2 47.2 65.5 83.6 72.2\n",
      "Ours(ConvE) 90.2 99.2 94.0 78.9 98.2 86.5 32.7 56.4 40.7 41.8 51.7 45.0 65.6 84.4 72.7\n",
      "Table 2: Query answering performance compared to state-of-the-art embedding based approaches (top part) and\n",
      "multi-hop reasoning approaches (bottom part). The @1, @10 and MRR metrics were multiplied by 100. We\n",
      "highlightthebestapproachineachcategory.\n",
      "wecutthemaximumnumberofoutgoingedgesof other correct answers from E and use it to com-\n",
      "o\n",
      "anentitybyathresholdηtopreventGPUmemory pute two types of metrics: (1) Hits@k which is\n",
      "overflow: foreachentityweretainitstop-ηneigh- thepercentageofexampleswherer ≤ kand(2)\n",
      "eo\n",
      "borswiththehighestPageRankscores(Pageetal., meanreciprocalrank(MRR)whichisthemeanof\n",
      "1999). 1/r for all examples in the test set. We use the\n",
      "eo\n",
      "entiretestsetforevaluation,withtheexceptionof\n",
      "Hyperparameters Wesettheentityandrelation\n",
      "NELL-995,wheretesttripleswithunseenentities\n",
      "embedding size to 200 for all models. We use\n",
      "areremovedfollowingDasetal.(2018).\n",
      "Xavierinitialization(GlorotandBengio,2010)for\n",
      "We will release the Pytorch implementation of\n",
      "theembeddingsandtheNNlayers. ForConvE,we\n",
      "all experiments. Please check the authors’ web\n",
      "use the same convolution layer and label smooth-\n",
      "pageforupdates.\n",
      "inghyperparametersasDettmersetal.(2018). For\n",
      "path-basedmodels,weuseathree-layerLSTMas 5 Results\n",
      "the path encoder and set its hidden dimension to\n",
      "5.1 ModelComparison\n",
      "200. Weperformgridsearchonthereasoningpath\n",
      "length (2,3), the node fan-out threshold η (256- Table 2 shows the evaluation results of our pro-\n",
      "512) and the action dropout rate α (0.1-0.9). Fol- posed approach and the baselines. The top\n",
      "lowingDasetal.(2018),weaddanentropyregu- part presents embedding based approaches and\n",
      "larizationtermintheobjectiveandtunetheweight the bottom part presents multi-hop reasoning ap-\n",
      "parameterβwithin0-0.1. WeuseAdamoptimiza- proaches.5\n",
      "tion (Kingma and Ba, 2014) and search the learn- We find embedding based models perform\n",
      "ing rate (0.001-0.003) and mini-batch size (128- stronglyonseveraldatasets,achievingoverallbest\n",
      "512).4 Forallmodelsweapplydropouttotheen- evaluation metrics on UMLS, Kinship, FB15K-\n",
      "tity and relation embeddings and all feed-forward 237andNELL-995despitetheirsimplicity. While\n",
      "layers, and search the dropout rates within 0-0.5. previous path based approaches achieve com-\n",
      "We use a decoding beam size of 512 for NELL- parable performance on some of the datasets\n",
      "995and128fortheotherdatasets. (WN18RR, NELL-995, and UMLS), the perfor-\n",
      "mance gaps to the embedding based models on\n",
      "Evaluation Protocol We convert each triple\n",
      "the other datasets (Kinship and FB15k-237) are\n",
      "(e,r,e ) in the test set into a query and com-\n",
      "s o considerable(9.1and14.2absolutepointsrespec-\n",
      "pute ranking-based evaluation metrics. The mod-\n",
      "tively). A possible reason for this is that embed-\n",
      "els take e,r as the input and output a list of can-\n",
      "s dingbasedmethodsmapeverylinkintheKGinto\n",
      "didate answers E = [e1,...,eL] ranked in de-\n",
      "o the same embedding space, which implicitly en-\n",
      "creasing order of confidence score. We compute\n",
      "codestheconnectivityofthewholegraph. Incon-\n",
      "r, the rank of e among E, after removing the\n",
      "eo o o trast,pathbasedmodelsusethediscreterepresen-\n",
      "4On some datasets, we found larger batch size to con- tationofaKGasinput,andthereforehavetoleave\n",
      "tinueimprovingtheperformancebuthadtostopat512due\n",
      "tomemoryconstraints. 5Wereportthemodelrobustnessmeasurementsin§A.1.\n",
      "Model UMLSKinshipFB15k237WN18RRNELL995 995 does not change the results significantly. In\n",
      "general,removingactiondropouthasagreaterim-\n",
      "Ours(ConvE) 73.0 75.0 38.2 43.8 78.8\n",
      "−RS 67.7 66.5 35.1 45.7 78.4 pact, suggesting that thorough exploration of the\n",
      "−AD 61.3 65.4 31.0 39.1 76.1 pathspaceisimportantacrossdatasets.\n",
      "Table3: ComparisonofdevsetMRRofOurs(ConvE)\n",
      "5.3 Analysis\n",
      "andmodelswithoutrewardshapingandactiondropout.\n",
      "5.3.1 Con<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,     70,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['Xi Victoria Lin', 'Richard Socher', 'Caiming Xiong']\n",
      "Formatted chat:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are an assistant for question-answering tasks. Use only the provided context information to form your response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Context chunk: tinueimprovingtheperformancebuthadtostopat512due\n",
      "tomemoryconstraints. 5Wereportthemodelrobustnessmeasurementsin§A.1.\n",
      "Model UMLSKinshipFB15k237WN18RRNELL995 995 does not change the results significantly. In\n",
      "general,removingactiondropouthasagreaterim-\n",
      "Ours(ConvE) 73.0 75.0 38.2 43.8 78.8\n",
      "−RS 67.7 66.5 35.1 45.7 78.4 pact, suggesting that thorough exploration of the\n",
      "−AD 61.3 65.4 31.0 39.1 76.1 pathspaceisimportantacrossdatasets.\n",
      "Table3: ComparisonofdevsetMRRofOurs(ConvE)\n",
      "5.3 Analysis\n",
      "andmodelswithoutrewardshapingandactiondropout.\n",
      "5.3.1 ConvergenceRate\n",
      "out a significant proportion of the combinatorial We are interested in studying the impact of each\n",
      "path space by selection. For some path based ap- proposed enhancement on the training conver-\n",
      "proaches,computationcostisabottleneck. Inpar- gence rate. In particular, we expect reward shap-\n",
      "ticular,NeuralLPandNTP-λfailedtoscaletothe ingtoacceleratetheconvergenceofRL(toabetter\n",
      "larger datasets and their results are omitted from performance level) as it propagates prior knowl-\n",
      "thetable,asDasetal.(2018)reported. edge about the underlying KG to the agent. On\n",
      "Ours is the first multi-hop reasoning approach theotherhand,afairconcernforactiondropoutis\n",
      "which is consistently comparable or better than thatitcanbeslowertotrain,astheagentisforced\n",
      "embedding based approaches on all five datasets. to explore a more diverse set of paths. Figure 4\n",
      "Thebestsinglemodel,Ours(ConvE),improvesthe eliminatesthisconcern.\n",
      "SOTAperformanceofpath-basedmodelsonthree The first row of Figure 4 shows the changes in\n",
      "datasets (UMLS, Kinship, and FB15k-237) by devsetMRRofOurs(ConvE)(green)andthetwo\n",
      "4%,9%,and39%respectively. OnWN18RRand ablated models w.r.t. # epochs. In general, the\n",
      "NELL-995,ourapproachdidnotsignificantlyim- proposed approachis ableto converge toa higher\n",
      "proveoverexistingSOTA.TheNELL-995dataset accuracy level much faster than either of the ab-\n",
      "consists of only 12 relations in the test set and, as lated models and the performance gap often per-\n",
      "we further detail in the analysis (§ 5.3.3), our ap- sistsuntiltheendoftraining(onUMLS,Kinship,\n",
      "proachislesseffectiveforthoserelationtypes. andFB15k-237). Particularly,onFB15k-237,our\n",
      "The model variations using different reward approach still shows improvement even after the\n",
      "shapingmodulesperformsimilarly. Whileabetter twoablatedmodelsstarttooverfit,with−ADbe-\n",
      "rewardshapingmoduletypicallyresultsinabetter ginningtooverfitsooner. OnWN18RR,introduc-\n",
      "overall model, an exception is WN18RR, where ingrewardshapinghurtdevsetperformancefrom\n",
      "ComplEx performs slightly worse on its own but the beginning, as discussed in § 5.2. On NELL-\n",
      "is more helpful for reward shaping. We left the 995,Ours(ConvE)performssignificantlybetterin\n",
      "study of the relationship between the accuracy of the beginning, but −RS gradually reaches a com-\n",
      "the reward shaping module and the overall model parableperformancelevel.\n",
      "performanceasfuturework. It is especially interesting that introducing ac-\n",
      "tiondropoutimmediatelyimprovesthemodelper-\n",
      "5.2 AblationStudy formance on all datasets. A possible explanation\n",
      "for this is that by exploring a more diverse set of\n",
      "We perform an ablation study where we remove\n",
      "pathstheagentlearnssearchpoliciesthatgeneral-\n",
      "rewardshaping(−RS)andactiondropout(−AD)\n",
      "izebetter.\n",
      "from Ours(ConvE) and compare their MRRs to\n",
      "the whole model on the dev sets.6 As shown in\n",
      "5.3.2 PathDiversity\n",
      "Table 3, on most datasets, removing each com-\n",
      "Wealsocomputethetotalnumberofuniquepaths\n",
      "ponent results in a significant performance drop.\n",
      "theagentexploresduringtrainingandvisualizeits\n",
      "The exception is WN18RR, where removing the\n",
      "change w.r.t. # training epochs in the second row\n",
      "ConvE reward shaping module improves the per-\n",
      "formance.7 Removing reward shaping on NELL- ofFigure4. Whencountingauniquepath,wein-\n",
      "clude both the edge label and intermediate entity.\n",
      "6According to Table 3 and Table 2, the dev and test set Firstweobservethat,onalldatasets,theagentex-\n",
      "evaluationmetricsdiffersignificantlyonseveraldatasets.We\n",
      "discussthecauseofthisin§A.2. thataddingtheComplExrewardshapingmodulehelps, de-\n",
      "7Apossibleexplanationforthisisthataspath-basedmod- spite the fact that ComplEx performs slightly worse than\n",
      "els tend to outperform the embedding based approaches on ConvEonthisdataset. Thisindicatesthatdevsetaccuracy\n",
      "WN18RR,ConvEmaybesupplyingmorenoisethanuseful is not the only factor which determines the effectiveness of\n",
      "informationabouttheKG.Yetcounter-intuitively,wefound rewardshaping.\n",
      "0.7\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0 50 100 150 200\n",
      "RRM\n",
      "tes ved\n",
      "UMLS Kinship FB15k-237 WN18RR NELL-995\n",
      "0.46\n",
      "0.38 0.7 0.78\n",
      "0.6 0.36 0.44\n",
      "0.76\n",
      "0.5 0.34 0.42 0.4 0.74 0.3 0.32 0.40 0.72\n",
      "0.2 0.30 0.38\n",
      "0.1 0.28 0.70\n",
      "0 50 100 150 200 0 5 10 15 0 5 10 15 20 0 10 20 30\n",
      "1.2\n",
      "1.0\n",
      "0.8\n",
      "0.6\n",
      "0.4\n",
      "0.2\n",
      "0.0\n",
      "0 50 100 150 200\n",
      "detisiv\n",
      "shtap\n",
      "euqinu\n",
      "#\n",
      "Ours(ConvE) -AD -RS\n",
      "1e6 1e6 1e7 1e7 1e6\n",
      "1.50 1.75\n",
      "1.0 1.50\n",
      "6 1.25\n",
      "0.8 1.00 1.25\n",
      "0.6 4 0.75 1.00\n",
      "0.75\n",
      "0.4 2 0.50 0.50\n",
      "0.25 0.25\n",
      "0.2\n",
      "0 0.00 0.00\n",
      "0 50 100 150 200 0 5 10 15 0 5 10 15 20 0 10 20 30\n",
      "# epochs\n",
      "Figure 4: Illustration of convergence rate and path exploration efficiency. The three curves in each subplot\n",
      "representsOurs(ConvE)(green)andthetwoablatedmodels: −RS(blue)and−AD(orange). Thetoprowshows\n",
      "thechangeofdevsetMRRandthebottomrowshowsthegrowthof#uniquepathsexploredw.r.t. #epochs.\n",
      "To-many To-one\n",
      "Dataset\n",
      "% Ours(ConvE) −RS −AD % Ours(ConvE) −RS −AD\n",
      "UMLS 99.1 73.1 67.9(-7%) 61.3(-16%) 0.9 62.5 55.5(-11%) 54.4(-13%)\n",
      "Kinship 100 75 66.5(-11%) 65.4(-13%) 0 – – –\n",
      "FB15k-237 76.6 28.3 24.5(-13%) 20.9(-26%) 23.4 72 69.8(-3%) 63.9(-11%)\n",
      "WN18RR 52.8 65 65.7(+1%) 57.9(-11%) 47.2 20.1 23.2(+16%) 18.1(-10%)\n",
      "NELL-995 12.9 55.7 62.1(+12%) 56.9(+2%) 87.1 81.4 80.7(-1%) 80.5(-1%)\n",
      "Table4: MRRevaluationofdifferentrelationtypes(to-manyvs. to-one)onfivedatasets. The%columnsshow\n",
      "thepercentageofexamplesofeachrelationtypefoundinthedevelopmentsplitofthecorrespondingdataset. In\n",
      "general,ourproposedtechniquesimprovethepredictionresultsforto-manyrelationsmoresignificantly.\n",
      "plores a large number of paths before reaching a ploredanddevsetperformanceisnotstrictlypos-\n",
      "goodperformancelevel. Thespeedofpathdiscov- itive. Thebestperformingmodelingeneralisnot\n",
      "ery slowly decreases as training progresses. On themodelthatexploredthelargest#paths. Italso\n",
      "smallerKGs(UMLSandKinship),therateofen- demonstratestheroleofrewardshapingasaregu-\n",
      "counteringnewpathsissignificantlyloweraftera larizerwhichguidestheagenttoavoidnoisypaths\n",
      "certainnumberofepochs,andthedevsetaccuracy withitspriorknowledge.\n",
      "plateaus correspondingly. On much larger KGs\n",
      "5.3.3 Performancew.r.t. RelationTypes\n",
      "(FB15k-237, WN18RR, and NELL-995), we did\n",
      "not observe a significant slowdown before severe We investigate the behaviors of our proposed ap-\n",
      "overfitting occurs and the dev set performance proachw.r.tdifferentrelationtypes. ForeachKG,\n",
      "startstodrop. Apossiblereasonforthisisthatthe we classify its set of relations into two categories\n",
      "largerKGsaremoresparselyconnectedcompared based on the answer set cardinality. Specifically,\n",
      "to the smaller KGs (Table 1), therefore it is less we define the metric ξ r as the average answer set\n",
      "efficienttogaingeneralizableknowledgefromthe cardinalityofallquerieswithtopicrelationr. We\n",
      "KG by exploring a limited proportion of the path countr asa“to-many”relationifξ r > 1.5,which\n",
      "spacethroughsampling. indicates that most queries in relation r has more\n",
      "than 1 correct answer; we count r as a “to-one”\n",
      "Second,itisinterestingtoseethatwhileremov- relation otherwise, meaning most queries of this\n",
      "ing action dropout significantly lowers the effec- relationhaveonly1correctanswer.\n",
      "tiveness of path exploration (orange vs. green), Table4showsthepercentageofexamplesofto-\n",
      "removing reward shaping slightly improves the # manyandto-onerelationsoneachdevdatasetand\n",
      "paths visited during training for all datasets. This theMRRevaluationmetricsofpreviouslystudied\n",
      "indicates that the correlation between # paths ex- modelscomputedontheexamplesofeachrelation\n",
      "SeenQueries UnseenQueries\n",
      "Dataset\n",
      "% Ours(ConvE) −RS −AD % Ours(ConvE) −RS −AD\n",
      "UMLS 97.2 73.1 67.9(-7%) 61.4(-16%) 2.8 68.5 61.5(-10%) 58.7(-14%)\n",
      "Kinship 96.8 75.1 66.5(-11%) 65.8(-12%) 3.2 73.6 64.3(-13%) 53.3(-27%)\n",
      "FB15k-237 76.1 28.3 24.3(-14%) 20.6(-27%) 23.9 70.9 69.1(-2%) 63.9(-10%)\n",
      "WN18RR 41.8 60.8 62.0(+2%) 53.4(-12%) 58.2 31.5 33.9(+7%) 28.8(-9%)\n",
      "NELL-995 15.3 40.4 45.9(+14%) 42.5(+5%) 84.7 85.5 84.7(-1%) 84.3(-1%)\n",
      "Table5: MRRevaluationofseenqueriesvs. unseenqueriesonfivedatasets. The%columnsshowthepercentage\n",
      "ofexamplesofseen/unseenqueriesfoundinthedevelopmentsplitofthecorrespondingdataset.\n",
      "type. Since UMLS and Kinship are densely con- mostdatasets,theratioofseenvs. unseenqueries\n",
      "nected, they almost exclusively contain to-many is similar to that of to-many vs. to-one relations\n",
      "relations. FB15k-237mostlycontainsto-manyre- (Table 4) as a result of random data split, with\n",
      "lations. In Figure 4, we observe the biggest rela- the exception of WN18RR. On some datasets, all\n",
      "tive gains from the ablated models on these three models perform better on seen queries (UMLS,\n",
      "datasets. WN18RR is more balanced and con- Kinship, WN18RR)whileothersrevealtheoppo-\n",
      "sists of slightly more to-many relations than to- sitetrend. OnNELL-995bothofourproposeden-\n",
      "one relations. The NELL-995 dev set is a unique hancementsarenoteffectiveovertheseenqueries.\n",
      "one which almost exclusively consists of to-one We leave the study of these model behaviors to\n",
      "relations. There is no common performance pat- future work. In most cases, our proposed en-\n",
      "tern over the two relation types across datasets: hancementsimprovetheperformanceoverunseen\n",
      "on some datasets all models perform better on queries,withADbeingmoreeffective.\n",
      "to-many relations (UMLS, WN18RR) while oth-\n",
      "6 Conclusions\n",
      "ers reveal the opposite trend (FB15k-237, NELL-\n",
      "995). We leave the study of these differences to\n",
      "We propose two modeling advances for end-to-\n",
      "futurework.\n",
      "endRL-basedknowledge graphqueryanswering:\n",
      "Weshowtherelativeperformancechangeofthe rewardshapingandactiondropout. Ourapproach\n",
      "ablatedmodels−RSand−ADw.r.t. Ours(ConvE) improvesoverstate-of-the-artmulti-hopreasoning\n",
      "in parentheses. We observe that in general our models consistently on several benchmark KGs.\n",
      "proposedenhancementsareeffectiveinimproving A detailed analysis indicates that the access to a\n",
      "query-answering over both relation types (more moreaccurateenvironmentrepresentation(reward\n",
      "effective for to-many relations). However, adding shaping) and a more thorough exploration of the\n",
      "the ConvE reward shaping module on WN18RR searchspace(actiondropout)areimportanttothe\n",
      "hurts the performance over both to-many and to- performanceboost.\n",
      "one relations (more for to-one relations). On On the other hand, the performance gap be-\n",
      "NELL-995, both techniques hurt the performance tween RL-based approaches and the embedding-\n",
      "overto-manyrelations. based approaches for KGQA remains. In future\n",
      "work, we would like to investigate learnable re-\n",
      "5.3.4 Performancew.r.t. SeenQueriesvs.\n",
      "wardshapingandactiondropoutschemesandap-\n",
      "UnseenQueries\n",
      "plymodel-basedRLtothisdomain.\n",
      "Sincemostbenchmarkdatasetsrandomlysplitthe\n",
      "Acknowledgements\n",
      "KGtriplesintotrain,devandtestsets,thequeries\n",
      "that have multiple answers may fall into multi- We thank Mark O. Riedl, Yingbo Zhou, James\n",
      "ple splits. As a result, some of the test queries Bradbury and Vena Jia Li for their feedback on\n",
      "(e s,r q,?) are seen in the training set (with a dif- early draft of the paper, and Mark O. Riedl for\n",
      "ferentsetofanswers)whiletheothersarenot. We helpful conversations on reward shaping. We\n",
      "investigatethebehaviorsofourproposedapproach thanktheanonymousreviewersandtheSalesforce\n",
      "w.r.t. seenandunseenqueries. research team members for their thoughtful com-\n",
      "Table 5 shows the percentage of examples as- ments and discussions. We thank Fre´deric Godin\n",
      "sociated with seen and unseen queries on each forpointingoutanerrorinEquation8inanearly\n",
      "dev dataset and the corresponding MRR evalua- versionofthepaper.\n",
      "tion metrics of previously studied models. On\n",
      "References Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,\n",
      "Hanna M. Wallach, Rob Fergus, S. V. N. Vish-\n",
      "ReginaBarzilayandMin-YenKan,editors.2017. Pro-\n",
      "wanathan, and Roman Garnett, editors. 2017. Ad-\n",
      "ceedingsofthe55thAnnualMeetingoftheAssocia-\n",
      "vances in Neural Information Processing Systems\n",
      "tionforComputationalLinguistics,ACL2017,Van-\n",
      "30: AnnualConferenceonNeuralInformationPro-\n",
      "couver,Canada,July30-August4,Volume1: Long\n",
      "cessing Systems 2017, 4-9 December 2017, Long\n",
      "Papers.AssociationforComputationalLinguistics.\n",
      "Beach,CA,USA.\n",
      "JonathanBerant,AndrewChou,RoyFrostig,andPercy\n",
      "He He, Anusha Balakrishnan, Mihail Eric, and Percy\n",
      "Liang. 2013. Semantic parsing on freebase from\n",
      "Liang. 2017. Learning symmetric collaborative di-\n",
      "question-answer pairs. In (Yarowsky et al., 2013),\n",
      "alogue agents with dynamic knowledge graph em-\n",
      "pages1533–1544.\n",
      "beddings. In(BarzilayandKan,2017),pages1766–\n",
      "Antoine Bordes, Nicolas Usunier, Alberto Garcia- 1776.\n",
      "Dura´n, Jason Weston, and Oksana Yakhnenko.\n",
      "2013. Translating embeddings for modeling multi- Diederik P. Kingma and Jimmy Ba. 2014. Adam:\n",
      "relationaldata. InProceedingsofthe26thInterna- A method for stochastic optimization. CoRR,\n",
      "tional Conference on Neural Information Process- abs/1412.6980.\n",
      "ingSystems-Volume2,NIPS’13,pages2787–2795,\n",
      "StanleyKokandPedroM.Domingos.2007. Statistical\n",
      "USA.CurranAssociatesInc.\n",
      "predicateinvention. InICML,volume227ofACM\n",
      "Wenhu Chen, Wenhan Xiong, Xifeng Yan, and International Conference Proceeding Series, pages\n",
      "William Yang Wang. 2018. Variational knowledge 433–440.ACM.\n",
      "graphreasoning. CoRR,abs/1803.06581.\n",
      "Ni Lao, Tom Mitchell, and William W. Cohen. 2011.\n",
      "Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Randomwalkinferenceandlearninginalargescale\n",
      "Luke Vilnis, Ishan Durugkar, Akshay Krishna- knowledge base. In Proceedings of the Conference\n",
      "murthy,AlexSmola,andAndrewMcCallum.2018. on Empirical Methods in Natural Language Pro-\n",
      "Go for a walk and arrive at the answer: Reasoning cessing, EMNLP’11, pages529–539, Stroudsburg,\n",
      "over paths in knowledge bases using reinforcement PA, USA. Association for Computational Linguis-\n",
      "learning. In International Conference on Learning tics.\n",
      "Representations.\n",
      "NiLao,AmarnagSubramanya,FernandoC.N.Pereira,\n",
      "Tim Dettmers, Pasquale Minervini, Pontus Stenetorp,\n",
      "andWilliamW.Cohen.2012. Readingthewebwith\n",
      "and Sebastian Riedel. 2018. Convolutional 2d\n",
      "learned syntactic-semantic inference rules. In Pro-\n",
      "knowledge graph embeddings. In Proceedings of\n",
      "ceedingsofthe2012JointConferenceonEmpirical\n",
      "theThirty-SecondAAAIConferenceonArtificialIn-\n",
      "MethodsinNaturalLanguageProcessingandCom-\n",
      "telligence,NewOrleans,Louisiana,USA,February\n",
      "putational Natural Language Learning, EMNLP-\n",
      "2-7,2018.AAAIPress.\n",
      "CoNLL2012, July12-14, 2012, JejuIsland, Korea,\n",
      "pages1017–1026.ACL.\n",
      "Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,\n",
      "and Tom M. Mitchell. 2013. Improving learning\n",
      "Andrew McCallum, Arvind Neelakantan, Rajarshi\n",
      "and inference in a large knowledge-base using la-\n",
      "Das, and David Belanger. 2017. Chains of reason-\n",
      "tentsyntacticcues. In(Yarowskyetal.,2013),pages\n",
      "ing over entities, relations, and text using recurrent\n",
      "833–838.\n",
      "neural networks. In Proceedings of the 15th Con-\n",
      "XavierGlorotandYoshuaBengio.2010. Understand- ferenceoftheEuropeanChapteroftheAssociation\n",
      "ing the difficulty of training deep feedforward neu- for Computational Linguistics, EACL 2017, Valen-\n",
      "ral networks. In Proceedings of the Thirteenth In- cia,Spain,April3-7,2017,Volume1: LongPapers,\n",
      "ternationalConferenceonArtificialIntelligenceand pages132–141.AssociationforComputationalLin-\n",
      "Statistics,AISTATS2010,ChiaLagunaResort,Sar- guistics.\n",
      "dinia, Italy, May 13-15, 2010, volume 9 of JMLR\n",
      "Volodymyr Mnih, Koray Kavukcuoglu, David Silver,\n",
      "Proceedings,pages249–256.JMLR.org.\n",
      "Alex Graves, Ioannis Antonoglou, Daan Wierstra,\n",
      "Kelvin Guu, John Miller, and Percy Liang. 2015. and Martin A. Riedmiller. 2013. Playing atari with\n",
      "Traversing knowledge graphs in vector space. In deepreinforcementlearning. CoRR,abs/1312.5602.\n",
      "Proceedings of the 2015 Conference on Empirical\n",
      "Methods in Natural Language Processing, EMNLP Andrew Y. Ng, Daishi Harada, and Stuart J. Russell.\n",
      "2015, Lisbon, Portugal, September 17-21, 2015, 1999. Policy invariance under reward transforma-\n",
      "pages318–327.TheAssociationforComputational tions: Theory and application to reward shaping.\n",
      "Linguistics. In Proceedings of the Sixteenth International Con-\n",
      "ference on Machine Learning (ICML 1999), Bled,\n",
      "Kelvin Guu, Panupong Pasupat, Evan Zheran Liu, Slovenia, June27-30, 1999, pages278–287.Mor-\n",
      "and Percy Liang. 2017. From language to pro- ganKaufmann.\n",
      "grams: Bridging reinforcement learning and max-\n",
      "imum marginal likelihood. In (Barzilay and Kan, Lawrence Page, Sergey Brin, Rajeev Motwani, and\n",
      "2017),pages1051–1062. TerryWinograd.1999. Thepagerankcitationrank-\n",
      "ing: Bringing order to the web. Technical Re- method for knowledge graph reasoning. In Pro-\n",
      "port 1999-66, Stanford InfoLab. Previous number ceedingsofthe2017ConferenceonEmpiricalMeth-\n",
      "=SIDL-WP-1999-0120. odsinNaturalLanguageProcessing,EMNLP2017,\n",
      "Copenhagen, Denmark, September 9-11, 2017,\n",
      "Romain Paulus, Caiming Xiong, and Richard Socher. pages564–573.AssociationforComputationalLin-\n",
      "2017. Adeepreinforcedmodelforabstractivesum- guistics.\n",
      "marization. CoRR,abs/1705.04304.\n",
      "Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng\n",
      "Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, Gao, and Li Deng. 2014. Embedding entities and\n",
      "and Wojciech Zaremba. 2015. Sequence level relations for learning and inference in knowledge\n",
      "training with recurrent neural networks. CoRR, bases. CoRR,abs/1412.6575.\n",
      "abs/1511.06732.\n",
      "Fan Yang, Zhilin Yang, and William W. Cohen. 2017.\n",
      "TimRockta¨schelandSebastianRiedel.2017. End-to- Differentiable learning of logical rules for knowl-\n",
      "enddifferentiableproving. In(Guyonetal.,2017), edgebasereasoning. In(Guyonetal.,2017),pages\n",
      "pages3791–3803. 2316–2325.\n",
      "Yelong Shen, Jianshu Chen, Po-Sen Huang, Yuqing David Yarowsky, Timothy Baldwin, Anna Korhonen,\n",
      "Guo, and Jianfeng Gao. 2018. Reinforcewalk: Karen Livescu, and Steven Bethard, editors. 2013.\n",
      "Learning to walk in graph with monte carlo tree Proceedings of the 2013 Conference on Empirical\n",
      "search. CoRR,abs/1802.04394. Methods in Natural Language Processing, EMNLP\n",
      "2013, 18-21 October 2013, Grand Hyatt Seattle,\n",
      "RichardSocher,DanqiChen,ChristopherD.Manning, Seattle, Washington, USA, A meeting of SIGDAT, a\n",
      "and Andrew Y. Ng. 2013. Reasoning with neural SpecialInterestGroupoftheACL.ACL.\n",
      "tensornetworksforknowledgebasecompletion. In\n",
      "Proceedingsofthe26thInternationalConferenceon\n",
      "NeuralInformationProcessingSystems-Volume1,\n",
      "NIPS’13, pages 926–934, USA. Curran Associates\n",
      "Inc.\n",
      "Richard S. Sutton and Andrew G. Barto. 1998. Re-\n",
      "inforcement learning - an introduction. Adaptive\n",
      "computationandmachinelearning.MITPress.\n",
      "Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-\n",
      "fungPoon,PallaviChoudhury,andMichaelGamon.\n",
      "2015. Representingtextforjointembeddingoftext\n",
      "and knowledge bases. In EMNLP, pages 1499–\n",
      "1509. The Association for Computational Linguis-\n",
      "tics.\n",
      "KristinaToutanova,XiVictoriaLin,Wen-tauYih,Hoi-\n",
      "fung Poon, and Chris Quirk. 2016. Compositional\n",
      "learningofembeddingsforrelationpathsinknowl-\n",
      "edge base and text. In Proceedings of the 54th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics, ACL 2016, August 7-12, 2016, Berlin,\n",
      "Germany,Volume1: LongPapers.TheAssociation\n",
      "forComputerLinguistics.\n",
      "The´oTrouillon,JohannesWelbl,SebastianRiedel,E´ric\n",
      "Gaussier,andGuillaumeBouchard.2016. Complex\n",
      "embeddingsforsimplelinkprediction. InProceed-\n",
      "ings of the 33nd International Conference on Ma-\n",
      "chine Learning, ICML 2016, New York City, NY,\n",
      "USA,June19-24, 2016, volume48ofJMLRWork-\n",
      "shop and Conference Proceedings, pages 2071–\n",
      "2080.JMLR.org.\n",
      "RonaldJ. Williams.1992. Simplestatistical gradient-\n",
      "following algorithms for connectionist reinforce-\n",
      "mentlearning. MachineLearning,8:229–256.\n",
      "Wenhan Xiong, Thien Hoang, and William Yang\n",
      "Wang. 2017. Deeppath: A reinforcement learning\n",
      "A Appendix datasets, the evaluation metrics increases signifi-\n",
      "cantly to the level that is comparable to those on\n",
      "A.1 ModelRobustness\n",
      "the test set, with the relative improvement corre-\n",
      "We run the best embedding-based model ConvE latedwiththeaveragenodefan-outintheKG(Ta-\n",
      "and Ours(ConvE) on all datasets using 5 differ- ble1).\n",
      "ent random seeds with all other hyperparameters NoticethatTable7isgeneratedafterallhyper-\n",
      "fixed. Table6reportsthemeanandstandarddevi- parameters were fixed and the purpose is to show\n",
      "ationofeachmodel. Weobservethatbothmodels the effects of such dataset peculiarities. To avoid\n",
      "demonstrate a small standard deviation (< 0.01) potential test set leakage, hyperparameter search\n",
      "onalldatasets. shouldbedonewiththetestsettripleshidden(Ta-\n",
      "ble1)insteadofwiththefullKG.\n",
      "Dataset ConvE Ours(ConvE)\n",
      "A.3 ActionDropoutRatesUsedforDifferent\n",
      "UMLS 95.5±0.4 93.7±0.2 KGs\n",
      "Kinship 86.9±0.3 86.2±0.7\n",
      "Table 8 show the action dropout rates used for all\n",
      "FB15k-237 43.5±0.1 40.7±0.2\n",
      "KGdatasetsinourexperiments. Ingeneral,larger\n",
      "WN18RR 45.3±0.4 44.7±0.2\n",
      "actiondropoutratesarenecessaryforKGsthatare\n",
      "NELL-995 76.2±0.3 72.7±0.4\n",
      "densely connected. We find a positive correlation\n",
      "Table6: TestsetMRR×100meanandstandarddevia- between the optimal action dropout rate and the\n",
      "tionacrossfiverunsonalldatasets. averagenodefan-out(Table1).\n",
      "ForUMLSandKinship,wetriedsettingtheac-\n",
      "tion dropout rate to 1.0 (completely random sam-\n",
      "A.2 DevelopmentSetEvaluationUsing\n",
      "pling) and observed small but significant perfor-\n",
      "CompleteKGs\n",
      "mance drop. Random sampling performs reason-\n",
      "Comparing Table 2 and Table 3 reveals that the ablywellonthesetwodatasetspossiblyduetothe\n",
      "devsetMRRsaresignificantlylowerthanthetest fact that they are small. For larger KGs (FB15k-\n",
      "set MRRs on some datasets (UMLS, Kinship and 237, WN18RR, NELL-995), policy-guided sam-\n",
      "FB15k-237). Suchdiscrepanciesarecausedbythe plingisnecessary.\n",
      "multi-answer queries in these datasets. As most\n",
      "benchmark datasets randomly split the KG triples Dataset α\n",
      "intotrain/dev/testsets,thequeriesthathavemulti-\n",
      "UMLS 0.95\n",
      "pleanswersmayfallintomultiplesplits. Because\n",
      "Kinship 0.9\n",
      "wehidealltriplesinthetestsetduringthedevset\n",
      "FB15k-237 0.5\n",
      "evaluation,somepredictionsgeneratedduringdev\n",
      "WN18RR 0.1\n",
      "setevaluationwerewronglypunishedasfalseneg-\n",
      "NELL-995 0.1\n",
      "atives. In contrast, the test set evaluation metrics\n",
      "are computed using the complete KGs. Access to Table8: Actiondropoutratesusedinourexperiments.\n",
      "thecompleteKGeliminatesmostofthefalseneg-\n",
      "ativescasesandhenceincreasestheperformance.\n",
      "Model UMLSKinshipFB15k237WN18RRNELL995\n",
      "Ours(ConvE) 95.1 86.8 41.8 44.1 78.8\n",
      "−RS 85.6 75.7 37.1 46.1 78.4\n",
      "−AD 76.2 75.9 32.4 39.3 76.1\n",
      "Table7: ComparisonofdevsetMRRcomputedusing\n",
      "thecompleteKGsofOurs(ConvE)andmodelswithout\n",
      "rewardshapingandactiondropout.\n",
      "Table 7 shows the dev set MRR of the same\n",
      "systems shown in Table 3 with the MRRs com-\n",
      "puted using the complete KGs. On four of the<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Now, given this question: Who are the authors of the paper?.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenized inputs:\n",
      " {'input_ids': tensor([[128000, 128006,   9125,  ...,  78191, 128007,    271]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n",
      "Generated tokens:\n",
      " tensor([[128000, 128006,   9125,  ...,  29346,    663, 128009]],\n",
      "       device='cuda:0')\n",
      "Decoded output:\n",
      " ['A. Bordes', 'N. Usunier', 'A. Garcia-Duran', 'J. Weston', 'O. Yakhnenko', 'D. P. Kingma', 'J. Ba', 'R. Das', 'S. Dhuliawala', 'M. Zaheer', 'L. Vilnis', 'A. Smola', 'A. McCallum', 'P. Liang', 'K. Guu', 'P. Pasupat', 'E. Z. Liu', 'M. Gardner', 'P. P. Talukdar', 'B. Kisiel', 'T. M. Mitchell', 'A. McCallum', 'A. Neelakantan', 'D. Belanger', 'R. Paulus', 'C. Xiong', 'R. Socher', 'F. Yang', 'Z. Yang', 'W. W. Cohen', 'T. Rocktaeschel', 'S. Riedel', 'Y. Shen', 'J. Chen', 'P. S. Huang', 'Y. Guo', 'J. F. Gao', 'R. S. Sutton', 'A. G. Barto', 'K. Toutanova', 'D. Chen', 'P. Pantel', 'H. Poon', 'C. Quirk', 'T. Trouillon', 'J. Welbl', 'E. Gaussier', 'G. Bouchard', 'R. J. Williams', 'W. Xiong', 'T. Hoang', 'W. Y. Wang']\n"
     ]
    }
   ],
   "source": [
    "for paper in papers_list[0:20]:\n",
    "    with pdfplumber.open(str(current_dir /paper['Local PDF Path'])) as pdf:\n",
    "        text = \"\\n\".join(page.extract_text() for page in pdf.pages)\n",
    "        \n",
    "    # Example Usage\n",
    "    chunks = chunk_text(text, tokenizer, max_tokens=max_context_tokens, overlap=200)\n",
    "    \n",
    "    # **Add context chunks iteratively**\n",
    "    for i, question in enumerate(questions):\n",
    "        response= []\n",
    "        for j, chunk in enumerate(chunks):\n",
    "\n",
    "            chat = [\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant for question-answering tasks. Use only the provided context information to form your response.\"}\n",
    "            ]\n",
    "\n",
    "\n",
    "            chat.append({\"role\": \"user\", \"content\": f\"Context chunk: {chunk}\"})\n",
    "\n",
    "            chat.append({\"role\": \"user\", \"content\": f\"Now, given this question: {question}.  Give back the answer only in a Python list format, for example: ['A','B']. If you don't know the answer, just return an empty list.\"})\n",
    "\n",
    "            # chat = [\n",
    "            #     {\"role\": \"system\", \"content\":\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question later on. If you don't know the answer, just say that you don't know.\" },\n",
    "            #     {\"role\": \"user\", \"content\": f\"Question: {question} Context: {context}\"}\n",
    "            # ]\n",
    "\n",
    "            # 2: Apply the chat template\n",
    "            formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "            print(\"Formatted chat:\\n\", formatted_chat)\n",
    "\n",
    "            # 3: Tokenize the chat (This can be combined with the previous step using tokenize=True)\n",
    "            inputs = tokenizer(formatted_chat, return_tensors=\"pt\", add_special_tokens=False)\n",
    "            # Move the tokenized inputs to the same device the model is on (GPU/CPU)\n",
    "            inputs = {key: tensor.to(model.device) for key, tensor in inputs.items()}\n",
    "            print(\"Tokenized inputs:\\n\", inputs)\n",
    "\n",
    "            # 4: Generate text from the model\n",
    "            outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.1)\n",
    "            print(\"Generated tokens:\\n\", outputs)\n",
    "\n",
    "            # 5: Decode the output back to a string\n",
    "            decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\n",
    "            print(\"Decoded output:\\n\", decoded_output)\n",
    "\n",
    "            response.append(decoded_output)\n",
    "        if i == 0:\n",
    "            responses_dataset.append(response)\n",
    "        elif i == 1:\n",
    "            responses_task.append(response)\n",
    "        else:\n",
    "            responses_authors.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [responses_authors, responses_dataset, responses_task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {\n",
    "    \"E_pred\": predictions\n",
    "}\n",
    "\n",
    "with open(\"qa_entities.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"qa_entities.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    predictions = data[\"E_pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from rapidfuzz import fuzz, process\n",
    "from itertools import chain\n",
    "def deduplicate_fuzzy(names, threshold=80):\n",
    "    unique = []\n",
    "    for name in names:\n",
    "        if all(fuzz.ratio(name, existing) < threshold for existing in unique):\n",
    "            unique.append(name)\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_dataset_copy = predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(responses_dataset_copy)):\n",
    "    for j in range(len(responses_dataset_copy[i])):\n",
    "        if isinstance(responses_dataset_copy[i][j], str):\n",
    "            responses_dataset_copy[i][j] = ast.literal_eval(responses_dataset_copy[i][j])\n",
    "    if len(responses_dataset_copy[i]) > 1:\n",
    "        merged= list(chain.from_iterable(responses_dataset_copy[i]))\n",
    "        responses_dataset_copy[i] = deduplicate_fuzzy(merged, threshold=80)\n",
    "    else:\n",
    "        responses_dataset_copy[i] = deduplicate_fuzzy(responses_dataset_copy[i][j], threshold=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_dataset_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_task_copy = predictions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(responses_task_copy)):\n",
    "    for j in range(len(responses_task_copy[i])):\n",
    "        if isinstance(responses_task_copy[i][j], str):\n",
    "\n",
    "            responses_task_copy[i][j] = ast.literal_eval(responses_task_copy[i][j])\n",
    "    if len(responses_task_copy[i]) > 1:\n",
    "        merged= list(chain.from_iterable(responses_task_copy[i]))\n",
    "        responses_task_copy[i] = deduplicate_fuzzy(merged, threshold=80)\n",
    "    else:\n",
    "        responses_task_copy[i] = deduplicate_fuzzy(responses_task_copy[i][j], threshold=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_task_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['Yuyu Zhang', 'Hanjun Dai', 'Kamil Toraman', 'Le Song']\",\n",
       " \"['Agustinus Kristiadi', 'Mohammad Asif Khan', 'Denis Lukovnikov', 'Jens Lehmann', 'Asja Fischer']\",\n",
       " \"['AvishekJoeyBose', 'HuanLing', 'YanshuaiCao']\",\n",
       " \"['Liwei Cai', 'William Yang Wang']\",\n",
       " \"['Tim Dettmers', 'Pasquale Minervini', 'Pontus Stenetorp', 'Sebastian Riedel']\",\n",
       " \"['Daniel Oñoro-Rubio', 'Mathias Niepert', 'Alberto García-Durán', 'Roberto González-Sánchez', 'Roberto J. López-Sastre']\",\n",
       " \"['Tommaso Soru', 'Stefano Ruberto', 'Diego Moussallem', 'André Valdestilhas', 'Alexander Bigerl', 'Edgard Marx', 'Diego Esteves']\",\n",
       " \"['Bhushan Kotnis', 'Vivi Nastase']\",\n",
       " \"['Bhushan Kotnis', 'Vivi Nastase']\",\n",
       " \"['Wenhan Xiong', 'Thien Hoang', 'William Yang Wang']\",\n",
       " \"['Chandrahas', 'Tathagata Sengupta', 'Cibi Pragadeesh', 'Partha Pratim Talukdar']\",\n",
       " \"['Armand Joulin', 'Edouard Grave', 'Piotr Bojanowski', 'Tomas Mikolov', 'Ajoulin@fb.com', 'Egrave@fb.com', 'Bojanowski@fb.com', 'Maxn@fb.com', 'Tmikolov@fb.com']\",\n",
       " \"['Théo Trouillon', 'Maximilian Nickel']\",\n",
       " \"['Muhao Chen', 'Yingtao Tian', 'Mohan Yang', 'Carlo Zaniolo']\",\n",
       " \"['HeHe', 'AnushaBalakrishnan', 'MihailEric', 'PercyLiang']\",\n",
       " \"['Naoya Takeishi', 'Kosuke Akimoto']\",\n",
       " \"['Yunpu Ma', 'Volker Trespa', 'Erik A. Daxberger']\",\n",
       " \"['Yue Liu', 'Tongtao Zhang', 'Zhicheng Liang', 'Heng Ji', 'Deborah L. McGuinness']\",\n",
       " \"['Ivana Balažević', 'Carl Allen', 'Timothy M. Hospedales']\",\n",
       " \"['Xi Victoria Lin', 'Richard Socher', 'Caiming Xiong']\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_authors_copy = predictions[0]\n",
    "# return the first response, because the authors are in the first chunk always\n",
    "for i, response in enumerate(responses_authors_copy):\n",
    "        responses_authors_copy[i] = response[0]\n",
    "\n",
    "responses_authors_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(responses_authors_copy)):\n",
    "\n",
    "#     responses_authors_copy[i] = ast.literal_eval(responses_authors_copy[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = papers_list[0:20]\n",
    "authors = []\n",
    "datasets = []\n",
    "tasks = []\n",
    "\n",
    "# get the authors from papers\n",
    "for i, paper in enumerate(papers):\n",
    "    authors.append(paper['Authors'])\n",
    "    datasets.append(paper['Datasets'])\n",
    "    tasks.append(paper['Tasks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [authors, datasets, tasks]\n",
    "predictions = [responses_authors_copy, responses_dataset_copy, responses_task_copy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "# Load sentence-level embedding model\n",
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: ['Hanjun Dai', 'Le Song', 'Kamil Toraman', 'Yuyu Zhang']\n",
      "Prediction: ['Yuyu Zhang', 'Hanjun Dai', 'Kamil Toraman', 'Le Song']\n",
      "Recall: 1.0000\n",
      "Reference: ['Jens Lehmann', 'Mohammad Asif Khan', 'Denis Lukovnikov', 'Asja Fischer', 'Agustinus Kristiadi']\n",
      "Prediction: ['Agustinus Kristiadi', 'Mohammad Asif Khan', 'Denis Lukovnikov', 'Jens Lehmann', 'Asja Fischer']\n",
      "Recall: 1.0000\n",
      "Reference: ['Yanshuai Cao', 'Avishek Joey Bose', 'Huan Ling']\n",
      "Prediction: ['AvishekJoeyBose', 'HuanLing', 'YanshuaiCao']\n",
      "Recall: 0.0000\n",
      "Reference: ['William Yang Wang', 'Liwei Cai']\n",
      "Prediction: ['Liwei Cai', 'William Yang Wang']\n",
      "Recall: 1.0000\n",
      "Reference: ['Pontus Stenetorp', 'Sebastian Riedel', 'Tim Dettmers', 'Pasquale Minervini']\n",
      "Prediction: ['Tim Dettmers', 'Pasquale Minervini', 'Pontus Stenetorp', 'Sebastian Riedel']\n",
      "Recall: 1.0000\n",
      "Reference: ['Roberto J. López-Sastre', 'Roberto González', 'Alberto García-Durán', 'Daniel Oñoro-Rubio', 'Mathias Niepert']\n",
      "Prediction: ['Daniel Oñoro-Rubio', 'Mathias Niepert', 'Alberto García-Durán', 'Roberto González-Sánchez', 'Roberto J. López-Sastre']\n",
      "Recall: 1.0000\n",
      "Reference: ['Edgard Marx', 'Alexander Bigerl', 'André Valdestilhas', 'Stefano Ruberto', 'Diego Esteves', 'Tommaso Soru', 'Diego Moussallem']\n",
      "Prediction: ['Tommaso Soru', 'Stefano Ruberto', 'Diego Moussallem', 'André Valdestilhas', 'Alexander Bigerl', 'Edgard Marx', 'Diego Esteves']\n",
      "Recall: 1.0000\n",
      "Reference: ['Vivi Nastase', 'Bhushan Kotnis']\n",
      "Prediction: ['Bhushan Kotnis', 'Vivi Nastase']\n",
      "Recall: 1.0000\n",
      "Reference: ['Vivi Nastase', 'Bhushan Kotnis']\n",
      "Prediction: ['Bhushan Kotnis', 'Vivi Nastase']\n",
      "Recall: 1.0000\n",
      "Reference: ['William Yang Wang', 'Wenhan Xiong', 'Thien Hoang']\n",
      "Prediction: ['Wenhan Xiong', 'Thien Hoang', 'William Yang Wang']\n",
      "Recall: 1.0000\n",
      "Reference: ['Partha Pratim Talukdar', 'Chandrahas', 'Cibi Pragadeesh', 'Tathagata Sengupta']\n",
      "Prediction: ['Chandrahas', 'Tathagata Sengupta', 'Cibi Pragadeesh', 'Partha Pratim Talukdar']\n",
      "Recall: 1.0000\n",
      "Reference: ['Maximilian Nickel', 'Edouard Grave', 'Armand Joulin', 'Piotr Bojanowski', 'Tomas Mikolov']\n",
      "Prediction: ['Armand Joulin', 'Edouard Grave', 'Piotr Bojanowski', 'Tomas Mikolov', 'Ajoulin@fb.com', 'Egrave@fb.com', 'Bojanowski@fb.com', 'Maxn@fb.com', 'Tmikolov@fb.com']\n",
      "Recall: 0.8000\n",
      "Reference: ['Maximilian Nickel', 'Théo Trouillon']\n",
      "Prediction: ['Théo Trouillon', 'Maximilian Nickel']\n",
      "Recall: 1.0000\n",
      "Reference: ['Carlo Zaniolo', 'Yingtao Tian', 'Mohan Yang', 'Muhao Chen']\n",
      "Prediction: ['Muhao Chen', 'Yingtao Tian', 'Mohan Yang', 'Carlo Zaniolo']\n",
      "Recall: 1.0000\n",
      "Reference: ['Anusha Balakrishnan', 'Percy Liang', 'Mihail Eric', 'He He']\n",
      "Prediction: ['HeHe', 'AnushaBalakrishnan', 'MihailEric', 'PercyLiang']\n",
      "Recall: 0.0000\n",
      "Reference: ['Kosuke Akimoto', 'Naoya Takeishi']\n",
      "Prediction: ['Naoya Takeishi', 'Kosuke Akimoto']\n",
      "Recall: 1.0000\n",
      "Reference: ['Erik Daxberger', 'Yunpu Ma', 'Volker Tresp']\n",
      "Prediction: ['Yunpu Ma', 'Volker Trespa', 'Erik A. Daxberger']\n",
      "Recall: 1.0000\n",
      "Reference: ['Heng Ji', 'Deborah L. McGuinness', 'Tongtao Zhang', 'Yue Liu', 'Zhicheng Liang']\n",
      "Prediction: ['Yue Liu', 'Tongtao Zhang', 'Zhicheng Liang', 'Heng Ji', 'Deborah L. McGuinness']\n",
      "Recall: 1.0000\n",
      "Reference: ['Timothy M. Hospedales', 'Ivana Balažević', 'Carl Allen']\n",
      "Prediction: ['Ivana Balažević', 'Carl Allen', 'Timothy M. Hospedales']\n",
      "Recall: 1.0000\n",
      "Reference: ['Xi Victoria Lin', 'Richard Socher', 'Caiming Xiong']\n",
      "Prediction: ['Xi Victoria Lin', 'Richard Socher', 'Caiming Xiong']\n",
      "Recall: 1.0000\n",
      "Reference: ['ARC (AI2 Reasoning Challenge)', 'SNLI', 'SQuAD']\n",
      "Prediction: ['ARC', 'SQuAD', 'SNLI', 'SciTail']\n",
      "Recall: 0.6667\n",
      "Reference: ['FB15k-237', 'FB15k']\n",
      "Prediction: ['FB15k', 'FB15k-237', 'YAGO3-10']\n",
      "Recall: 1.0000\n",
      "Reference: []\n",
      "Prediction: ['Rare Word', 'WS353', 'English Wikipedia', 'WN18', 'WordSim353']\n",
      "Recall: 1.0000\n",
      "Reference: ['WN18RR', 'FB15k', 'WN18', 'FB15k-237']\n",
      "Prediction: ['FB15k-237', 'WN18', 'WN18RR']\n",
      "Recall: 1.0000\n",
      "Reference: ['YAGO', 'YAGO3-10', 'UMLS', 'WN18RR', 'FB15k', 'FB15k-237', 'WN18']\n",
      "Prediction: ['WN18', 'FB15k', 'YAGO3-10', 'Countries', 'FB15k-237', 'WN18RR', 'UMLS', 'Kinship']\n",
      "Recall: 0.8571\n",
      "Reference: ['Visual Genome', 'FB15k', 'ImageNet']\n",
      "Prediction: ['FB15k', 'ImageGraph', 'VisualGenome', 'ImageNet', 'Freebase', 'DBpedia', 'SUN database', 'Drugbank']\n",
      "Recall: 1.0000\n",
      "Reference: []\n",
      "Prediction: ['AKSW-bib', 'DBpedia2016-04', 'DBpedia2015-10']\n",
      "Recall: 1.0000\n",
      "Reference: []\n",
      "Prediction: ['FB15K']\n",
      "Recall: 1.0000\n",
      "Reference: ['FB15k', 'NELL', 'WN18']\n",
      "Prediction: ['FB15k', 'WN18']\n",
      "Recall: 0.6667\n",
      "Reference: ['NELL', 'NELL-995']\n",
      "Prediction: ['FB15K-237', 'NELL-995']\n",
      "Recall: 0.5000\n",
      "Reference: []\n",
      "Prediction: ['FB15k-237']\n",
      "Recall: 1.0000\n",
      "Reference: ['WN18', 'FB15k', 'WikiMovies']\n",
      "Prediction: ['WN18', 'FB15k', 'FB15k-237', 'SVO', 'SimpleQuestion', 'WikiMovies', 'Freebase', 'WordNet']\n",
      "Recall: 1.0000\n",
      "Reference: ['WN18', 'FB15k']\n",
      "Prediction: ['WN18', 'FB15K']\n",
      "Recall: 1.0000\n",
      "Reference: ['DBP15K', 'MMKG']\n",
      "Prediction: ['WK3l-15k', 'WK3l-120k', 'CN3l', 'WK3l']\n",
      "Recall: 0.0000\n",
      "Reference: ['MutualFriends']\n",
      "Prediction: ['MutualFriends', 'CardsDialogueCorpus', 'UbuntuDialogueCorpus', 'WesternSugar', 'Mechanization', 'Engineering Cooperative', 'Mathematics', 'AVST Shopping Petroleum', 'Education', 'Parks Foreignlanguage', 'AdobeSystems American', 'Administration learning Petroleum', 'Broadcasting Origami', 'Agricultural BroncoWine Engineering', 'Shopping Company', 'Mechanization Company', 'TheWaltDisney', 'Metallurgical Foreignlanguage', 'ElectronicArts Company', 'Protestant', 'AcmeBrick Astronomy']\n",
      "Recall: 1.0000\n",
      "Reference: []\n",
      "Prediction: ['Climate Change Knowledge Portal', 'Countries', '1,380 objects']\n",
      "Recall: 1.0000\n",
      "Reference: []\n",
      "Prediction: ['ICEWS', 'GDELT']\n",
      "Recall: 1.0000\n",
      "Reference: []\n",
      "Prediction: ['NYT', 'ADE', 'Wiki-DBpedia']\n",
      "Recall: 1.0000\n",
      "Reference: ['WN18RR', 'FB15k', 'WN18', 'FB15k-237']\n",
      "Prediction: ['FB15k', 'WN18', 'FB15k-237', 'WN18RR', 'YAGO3-10']\n",
      "Recall: 1.0000\n",
      "Reference: ['NELL-995']\n",
      "Prediction: ['Alyawarra Kinship', 'Unified Medical Language Systems (UMLS)', 'FB15k-237', 'WN18RR', 'NELL-995', 'UMLS', 'Kinship']\n",
      "Recall: 1.0000\n",
      "Reference: ['ARC', 'Question Answering', 'Knowledge Graphs', 'AI2 Reasoning Challenge', 'Knowledge Graph Embeddings']\n",
      "Prediction: ['Generating Hypothesis', 'Searching Potential Supports', 'Constructing Knowledge Graphs', 'Learning with Graph Embeddings']\n",
      "Recall: 0.2000\n",
      "Reference: ['Knowledge Graphs', 'Knowledge Graph Embeddings', 'Link Prediction', 'Entity Embeddings']\n",
      "Prediction: ['Link prediction']\n",
      "Recall: 0.2500\n",
      "Reference: ['Contrastive Learning', 'Knowledge Graphs', 'Knowledge Graph Embeddings', 'Word Embeddings', 'Learning Word Embeddings']\n",
      "Prediction: ['Word Embeddings', 'Order Embeddings', 'Knowledge Graph Embeddings', 'Hypernym Prediction', 'Link Prediction']\n",
      "Recall: 0.6000\n",
      "Reference: ['Graph Embedding', 'Knowledge Base Completion', 'Knowledge Graphs', 'Knowledge Graph Embeddings', 'Link Prediction', 'Knowledge Graph Embedding']\n",
      "Prediction: ['Link prediction', 'Knowledge graph embedding']\n",
      "Recall: 0.5000\n",
      "Reference: ['Knowledge Graphs', 'Knowledge Graph Embeddings', 'Link Prediction']\n",
      "Prediction: ['Link prediction', '1-N scoring']\n",
      "Recall: 0.3333\n",
      "Reference: ['Knowledge Graphs', 'Retrieval', 'Graph Embedding', 'Image Retrieval', 'Knowledge Graph Embeddings', 'Knowledge Graph Embedding', 'Zero-Shot Learning', 'Representation Learning']\n",
      "Prediction: ['Query answering', 'KG completion', 'Zero-shot learning', 'Visual Relation Prediction', 'Multi-Relational Image Retrieval', 'Zero-Shot Visual Relation Prediction', 'Link prediction', 'Question answering', 'Image classification', 'Fine-grained image classification', 'Web image re-ranking', 'Scene recognition', 'Object affordances', 'Deep metric learning', 'Temporal reasoning', 'Extending knowledge bases using images', 'Learning visual clothing style', 'Relationship queries on extended knowledge graphs']\n",
      "Recall: 0.1250\n",
      "Reference: ['Graph Embedding', 'Question Answering', 'Knowledge Graph Completion', 'Knowledge Graph Embeddings', 'Link Prediction', 'Knowledge Graph Embedding', 'Triplet']\n",
      "Prediction: ['Knowledge Graph Embedding', 'Link Prediction', 'Entity Recommendation', 'Question Answering', 'Triples Classification', 'knowledge graph completion', 'knowledge-based question answering', 'link prediction', 'data mining']\n",
      "Recall: 0.7143\n",
      "Reference: ['Knowledge Graph Embeddings', 'Vocal Bursts Type Prediction']\n",
      "Prediction: ['Knowledge Base Completion (KBC)', 'Link Prediction', 'Knowledge Base Completion', 'Relation Extraction', 'Reasoning']\n",
      "Recall: 0.0000\n",
      "Reference: ['Knowledge Graphs', 'Knowledge Graph Embeddings', 'Link Prediction']\n",
      "Prediction: ['Link Prediction', 'Knowledge Graph Completion', 'Link Prediction in Knowledge Graphs']\n",
      "Recall: 0.6667\n",
      "Reference: ['Knowledge Graphs', 'Graph Embedding', 'reinforcement-learning', 'Knowledge Graph Embeddings', 'Diversity', 'Knowledge Graph Embedding', 'Reinforcement Learning', 'Reinforcement Learning (RL)']\n",
      "Prediction: ['link prediction', 'fact prediction', 'Relation Reasoning', 'Fact Prediction']\n",
      "Recall: 0.0000\n",
      "Reference: ['Knowledge Graph Embeddings']\n",
      "Prediction: ['KG inference', 'Link prediction', 'Triple classification', 'Inducing interpretability in KG embeddings']\n",
      "Recall: 0.0000\n",
      "Reference: ['Knowledge Graph Embeddings', 'model', 'Question Answering', 'Knowledge Base Completion', 'General Classification']\n",
      "Prediction: ['Knowledge base completion', 'Question answering']\n",
      "Recall: 0.4000\n",
      "Reference: ['Knowledge Graphs', 'Knowledge Graph Embeddings', 'Link Prediction', 'Entity Resolution']\n",
      "Prediction: ['Link prediction', 'Entity resolution']\n",
      "Recall: 0.5000\n",
      "Reference: ['Knowledge Graphs', 'Entity Alignment', 'Translation', 'Knowledge Graph Embeddings']\n",
      "Prediction: ['Cross-lingual entity matching', 'Triple-wise alignment verification', 'Cross-lingual triple-wise alignment verification', 'Cross-lingual relation matching', 'Cross-lingual triple completion', 'Cross-lingual transitions']\n",
      "Recall: 0.0000\n",
      "Reference: ['Knowledge Graph Embeddings']\n",
      "Prediction: ['MutualFriends', 'Task-Oriented Dialogue', 'Question-answering']\n",
      "Recall: 0.0000\n",
      "Reference: ['Knowledge Graph Embeddings', 'Inductive Bias']\n",
      "Prediction: ['Factor Analysis', 'Regularization', 'Regularized Factor Analysis', 'Knowledge Graph Embeddings', 'Latent Dirichlet Allocation', 'Generative Model Structure']\n",
      "Recall: 0.5000\n",
      "Reference: ['Knowledge Graphs', 'Knowledge Graph Embeddings', 'Inductive Learning']\n",
      "Prediction: ['Entity prediction', 'Predicate prediction', 'Episodic quadruple inference', 'Episodic tensor reconstruction', 'Semantic memory generation', 'Episodic-to-semantic projection']\n",
      "Recall: 0.0000\n",
      "Reference: ['Knowledge Graph Embeddings', 'Decoder', 'Translation']\n",
      "Prediction: ['Triple generation', 'Entity Linking', 'Relation Classification']\n",
      "Recall: 0.0000\n",
      "Reference: ['Knowledge Graphs', 'Prediction', 'Relation', 'Knowledge Graph Embeddings', 'Link Prediction']\n",
      "Prediction: ['Link Prediction', 'link prediction on knowledge graphs']\n",
      "Recall: 0.2000\n",
      "Reference: ['Knowledge Graphs', 'Knowledge Graph Embeddings', 'Reinforcement Learning', 'Reinforcement Learning (RL)']\n",
      "Prediction: ['Multi-Hop Knowledge Graph Reasoning', 'Query Answering', 'Knowledge Graph Query Answering', 'Multi-Hop Reasoning', 'End-to-End Reinforcement Learning']\n",
      "Recall: 0.0000\n"
     ]
    }
   ],
   "source": [
    "autor_recalls = []\n",
    "dataset_recalls = []\n",
    "task_recalls = []\n",
    "for i in range(len(predictions)):\n",
    "    for j in range(len(predictions[i])):\n",
    "        reference = references[i][j]\n",
    "        prediction = predictions[i][j]\n",
    "\n",
    "        if i != 0:\n",
    "            # Flatten reference list into a single string (first elements or all)\n",
    "            ref_text = [item[0] for item in reference]\n",
    "            pred_text = prediction\n",
    "        else:\n",
    "            ref_text = [name.strip() for name in reference.split(',')]\n",
    "            pred_text = ast.literal_eval(prediction)\n",
    "        if len(ref_text) == 0:\n",
    "            recall = 1\n",
    "        elif len(pred_text) == 0:\n",
    "            recall = 0\n",
    "        else:\n",
    "            # Compute embeddings\n",
    "            ref_emb = model.encode(ref_text, convert_to_tensor=True)\n",
    "            pred_emb = model.encode(pred_text, convert_to_tensor=True)\n",
    "\n",
    "            # Compute cosine similarities: shape (len(ref), len(pred))\n",
    "            cos_sim = util.pytorch_cos_sim(ref_emb, pred_emb)\n",
    "\n",
    "            # For each reference entity, find the max similarity to predicted entities\n",
    "            max_similarities = cos_sim.max(dim=1).values\n",
    "\n",
    "            # Apply threshold\n",
    "            threshold = 0.7\n",
    "            num_matched = (max_similarities >= threshold).sum().item()\n",
    "            recall = num_matched / len(ref_text) \n",
    "        if i == 0:\n",
    "            autor_recalls.append(recall)\n",
    "        elif i == 1:\n",
    "            dataset_recalls.append(recall)\n",
    "        elif i == 2:\n",
    "            task_recalls.append(recall)\n",
    "        print(f\"Reference: {ref_text}\")\n",
    "        print(f\"Prediction: {pred_text}\")\n",
    "        print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Author Recall: 0.8900\n",
      "Mean Dataset Recall: 0.8554\n",
      "Mean Task Recall: 0.2328\n"
     ]
    }
   ],
   "source": [
    "mean_autor_recall = sum(autor_recalls) / len(autor_recalls)\n",
    "mean_dataset_recall = sum(dataset_recalls) / len(dataset_recalls)\n",
    "mean_task_recall = sum(task_recalls) / len(task_recalls)\n",
    "print(f\"Mean Author Recall: {mean_autor_recall:.4f}\")\n",
    "print(f\"Mean Dataset Recall: {mean_dataset_recall:.4f}\")\n",
    "print(f\"Mean Task Recall: {mean_task_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
