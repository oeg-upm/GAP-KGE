{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc167309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.tei_extraction import extract_sections_fulltext, extract_abstract, tei_to_full_raw_text, extract_flat_sections_with_subtext, rank_sections_by_semantic_similarity\n",
    "from utils.grobid_service import GrobidService\n",
    "\n",
    "\n",
    "from rapidfuzz import fuzz, process\n",
    "import ast\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from grobid_client.grobid_client import GrobidClient\n",
    "from bs4 import BeautifulSoup\n",
    "import Levenshtein\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import util\n",
    "import torch\n",
    "\n",
    "\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e010e01",
   "metadata": {},
   "source": [
    "# Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d6a2093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_dir = Path(os.getcwd())\n",
    "parent_dir = current_dir.parent\n",
    "\n",
    "\n",
    "with open(\"../data/papers_data_copy.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    papers_list = json.load(f)\n",
    "\n",
    "# remove if Local PDF Path is None\n",
    "papers = [paper for paper in papers_list if paper.get(\"Local PDF Path\") is not None]\n",
    "authors = []\n",
    "datasets = []\n",
    "tasks = []\n",
    "\n",
    "# get the authors from papers\n",
    "for i, paper in enumerate(papers):\n",
    "    authors.append(paper['Authors'])\n",
    "    datasets.append(paper['Datasets'])\n",
    "    tasks.append(paper['Tasks'])\n",
    "references = [authors, datasets, tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b83f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Loading configuration file from ./Grobid/config.json\n",
      "2026-01-05 13:45:29,971 - INFO - Loading configuration file from ./Grobid/config.json\n",
      "INFO - Configuration file loaded successfully\n",
      "2026-01-05 13:45:29,973 - INFO - Configuration file loaded successfully\n",
      "2026-01-05 13:45:29,975 - INFO - Logging configured - Level: INFO, Console: True, File: disabled\n",
      "2026-01-05 13:45:30,000 - INFO - GROBID server http://localhost:8070 is up and running\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pdfs/KG^2- Learning to Reason Science Exam Questions with Contextual Knowledge Graph Embeddings.pdf\n",
      "['Yuyu Zhang', 'Hanjun Dai', 'Toraman Kamil', 'Le Song']\n",
      "data/pdfs/Incorporating Literals into Knowledge Graph Embeddings.pdf\n",
      "['Agustinus Kristiadi', 'Mohammad Asif Khan', 'Denis Lukovnikov', 'Jens Lehmann', 'Asja Fischer']\n",
      "data/pdfs/Adversarial Contrastive Estimation.pdf\n",
      "['Avishek Joey Bose', 'Huan Ling', 'Yanshuai Cao', 'Borealis Ai']\n",
      "data/pdfs/KBGAN- Adversarial Learning for Knowledge Graph Embeddings.pdf\n",
      "['Liwei Cai', 'William Yang Wang']\n",
      "data/pdfs/Convolutional 2D Knowledge Graph Embeddings.pdf\n",
      "['Tim Dettmers', 'Pasquale Minervini', 'Pontus Stenetorp', 'Sebastian Riedel']\n",
      "data/pdfs/Answering Visual-Relational Queries in Web-Extracted Knowledge Graphs.pdf\n",
      "['Daniel Oñoro-Rubio', 'Mathias Niepert', 'Alberto García-Durán', 'Roberto González-Sánchez', 'Roberto J López-Sastre']\n",
      "data/pdfs/Expeditious Generation of Knowledge Graph Embeddings.pdf\n",
      "['Tommaso Soru', 'Stefano Ruberto', 'Diego Moussallem', 'André Valdestilhas', 'Alexander Bigerl', 'Edgard Marx', 'Diego Esteves']\n",
      "data/pdfs/Learning Knowledge Graph Embeddings with Type Regularizer.pdf\n",
      "['Bhushan Kotnis', 'Vivi Nastase']\n",
      "data/pdfs/Analysis of the Impact of Negative Sampling on Link Prediction in Knowledge Graphs.pdf\n",
      "['Bhushan Kotnis', 'Vivi Nastase']\n",
      "data/pdfs/DeepPath- A Reinforcement Learning Method for Knowledge Graph Reasoning.pdf\n",
      "['Wenhan Xiong', 'Thien Hoang', 'William Yang Wang']\n",
      "data/pdfs/Inducing Interpretability in Knowledge Graph Embeddings.pdf\n",
      "['Tathagata Sengupta', 'Cibi Pragadeesh', 'Partha Pratim Talukdar']\n",
      "data/pdfs/Fast Linear Model for Knowledge Graph Embeddings.pdf\n",
      "['Armand Joulin', 'Piotr Bojanowski', 'Maximilian Nickel', 'Tomas Mikolov']\n",
      "data/pdfs/Complex and Holographic Embeddings of Knowledge Graphs- A Comparison.pdf\n",
      "['Théo Trouillon', 'Maximilian Nickel']\n",
      "data/pdfs/Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment.pdf\n",
      "['Muhao Chen', 'Yingtao Tian', 'Mohan Yang', 'Carlo Zaniolo']\n",
      "data/pdfs/Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings.pdf\n",
      "['He He', 'Anusha Balakrishnan', 'Mihail Eric', 'Percy Liang']\n",
      "data/pdfs/Knowledge-Based Distant Regularization in Learning Probabilistic Models.pdf\n",
      "['Naoya Takeishi', 'Kosuke Akimoto']\n",
      "data/pdfs/Embedding Models for Episodic Knowledge Graphs.pdf\n",
      "['Yunpu Ma', 'Volker Tresp', 'Erik A Daxberger']\n",
      "data/pdfs/Seq2RDF- An end-to-end application for deriving Triples from Natural Language Text.pdf\n",
      "['Yue Liu', 'Tongtao Zhang', 'Zhicheng Liang', 'Heng Ji', 'Deborah L Mcguinness']\n",
      "data/pdfs/Hypernetwork Knowledge Graph Embeddings.pdf\n",
      "['Ivana Balažević', 'Carl Allen', 'Timothy M Hospedales']\n",
      "data/pdfs/Multi-Hop Knowledge Graph Reasoning with Reward Shaping.pdf\n",
      "['Victoria Xi', 'Richard Lin', 'Caiming Socher', 'Xiong']\n",
      "data/pdfs/MOHONE- Modeling Higher Order Network Effects in KnowledgeGraphs via Network Infused Embeddings.pdf\n",
      "['Hao Yu', 'Vivek Kulkarni', 'William Yang Wang']\n",
      "data/pdfs/DOLORES- Deep Contextualized Knowledge Graph Embeddings.pdf\n",
      "['Haoyu Wang', 'Vivek Kulkarni', 'William Yang Wang']\n",
      "data/pdfs/Modelling Salient Features as Directions in Fine-Tuned Semantic Spaces.pdf\n",
      "['Thomas Ager', 'Ondřej Kuželka', 'Steven Schockaert']\n",
      "data/pdfs/Towards Understanding the Geometry of Knowledge Graph Embeddings.pdf\n",
      "['Aditya Sharma', 'Partha Talukdar']\n",
      "data/pdfs/Multimodal Named Entity Disambiguation for Noisy Social Media Posts.pdf\n",
      "['Seungwhan Moon', 'Leonardo Neves', 'Vitor Carvalho']\n",
      "data/pdfs/Recognizing Mentions of Adverse Drug Reaction in Social Media Using Knowledge-Infused Recurrent Models.pdf\n",
      "['Gabriel Stanovsky', 'Daniel Gruhl', 'Pablo N Mendes']\n",
      "data/pdfs/Sparsity and Noise- Where Knowledge Graph Embeddings Fall Short.pdf\n",
      "['Jay Pujara', 'Eriq Augustine', 'Lise Getoor']\n",
      "data/pdfs/Entity Hierarchy Embedding.pdf\n",
      "['Zhiting Hu', 'Poyao Huang', 'Yuntian Deng', 'Yingkai Gao', 'Eric P Xing']\n",
      "data/pdfs/Binarized Knowledge Graph Embeddings.pdf\n",
      "['Koki Kishimoto', 'Katsuhiko Hayashi', 'Genki Akai', 'Masashi Shimbo', 'Kazunori Komatani']\n",
      "data/pdfs/Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph Convolution Networks.pdf\n",
      "['Ningyu Zhang', 'Shumin Deng', 'Zhanlin Sun', 'Guanying Wang', 'Xi Chen', 'Wei Zhang', 'Huajun Chen']\n",
      "data/pdfs/Quaternion Knowledge Graph Embeddings.pdf\n",
      "['Shuai Zhang', 'Yi Tay', 'Lina Yao', 'Qi Liu']\n",
      "data/pdfs/Relation Embedding with Dihedral Group in Knowledge Graph.pdf\n",
      "['Canran Xu', 'Ruijiang Li']\n",
      "data/pdfs/Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs.pdf\n",
      "['Deepak Nathani', 'Jatin Chauhan', 'Charu Sharma', 'Manohar Kaul']\n",
      "data/pdfs/Neural Variational Inference For Estimating Uncertainty in Knowledge Graph Embeddings.pdf\n",
      "['Alexander I Cowen-Rivers', 'Pasquale Minervini', 'Tim Rocktäschel', 'Matko Bošnjak', 'Sebastian Riedel', 'Jun Wang']\n",
      "data/pdfs/Augmenting and Tuning Knowledge Graph Embeddings.pdf\n",
      "['Robert Bamler', 'Farnood Salehi', 'Stephan Mandt']\n",
      "data/pdfs/Adaptive Margin Ranking Loss for Knowledge Graph Embeddings via a Correntropy Objective Function.pdf\n",
      "['Mojtaba Nayyeri', 'Xiaotian Zhou', 'Sahar Vahdati', 'Shariat Hamed', 'Yazdi', 'Jens Lehmann']\n",
      "data/pdfs/Drug-Drug Interaction Prediction Based on Knowledge Graph Embeddings and Convolutional-LSTM Network.pdf\n",
      "['Md. Rezaul Karim', 'Michael Cochez', 'Joao Bosco Jares', 'Mamtaz Uddin', 'Oya Beyan', 'Stefan Decker']\n",
      "data/pdfs/Linking Physicians to Medical Research Results via Knowledge Graph Embeddings and Twitter.pdf\n",
      "['Afshin Sadeghi', 'Jens Lehmann']\n",
      "data/pdfs/RDF2Vec- RDF Graph Embeddings and Their Applications.pdf\n",
      "['Freddy Lecue', 'Achim Rettinger', 'Petar Ristoski', 'Jessica Rosati', 'Tommaso Di Noia', 'Renato De Leone', 'Heiko Paulheim']\n",
      "data/pdfs/HyperKG- Hyperbolic Knowledge Graph Embeddings for Knowledge Base Completion.pdf\n",
      "['Prodromos Kolyvakis', 'Alexandros Kalousis', 'Dimitris Kiritsis']\n"
     ]
    }
   ],
   "source": [
    "current_dir = Path(\".\")\n",
    "grobid = GrobidService(config_path=\"./Grobid/config.json\")\n",
    "authors_grobid = []\n",
    "\n",
    "for paper in papers:\n",
    "    pdf_path = str(paper[\"Local PDF Path\"])\n",
    "    authors = grobid.extract_authors_from_pdf(pdf_path)\n",
    "    authors_grobid.append(authors)\n",
    "    print(authors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c61e290",
   "metadata": {},
   "source": [
    "# Dataset and taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15acb41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, tokenizer, max_tokens=8000, overlap=200):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_tokens, len(tokens))\n",
    "        chunk = tokenizer.decode(tokens[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += max_tokens - overlap  # Overlapping context\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def deduplicate_fuzzy(list, threshold=80):\n",
    "    unique = []\n",
    "    for name in list:\n",
    "        if all(fuzz.ratio(name, existing) < threshold for existing in unique):\n",
    "            unique.append(name)\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78bf0504",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the name of datasets used in the paper?\"\n",
    "question2 = \"What are the tasks that the model is trained for?\"\n",
    "questions = [question, question2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9812b8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 41.34it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "max_context_tokens = 32768 - 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "066bcf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "def load_model_and_tokenizer(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device: str = \"cuda\"\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Load a model and tokenizer from pretrained checkpoints.\n",
    "\n",
    "    Args:\n",
    "        model_name:   HuggingFace model identifier.\n",
    "        tokenizer_name: If not provided, defaults to model_name.\n",
    "        device:       Device string, e.g. 'cuda' or 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        model, tokenizer\n",
    "    \"\"\"\n",
    "    # tokenizer_name = tokenizer_name or model_name\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    model = model\n",
    "    tokenizer = tokenizer\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def run_qa_on_chunk(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    chunk: str,\n",
    "    question: str,\n",
    "    unique_labels: list = None,\n",
    "    device: str = \"cuda\",\n",
    "    gen_kwargs: dict = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Run a single question–answer generation on one text chunk.\n",
    "    Returns the raw decoded content (Python list format expected).\n",
    "    \"\"\"\n",
    "    # Build chat history\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\":\n",
    "            \"You are an assistant for QA tasks. Use only provided context.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Context chunk: {chunk}\"}\n",
    "    ]\n",
    "\n",
    "    # Add question prompt\n",
    "    if unique_labels and question and isinstance(unique_labels, list):\n",
    "        prompt = (\n",
    "            f\"Now, given this question: {question}, and those possible tasks: {unique_labels}. \"\n",
    "            + \"Return answer only in a Python list format, e.g. ['A','B'].\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            f\"Now, given this question: {question}. \"\n",
    "            + \"Return answer only in a Python list format, e.g. ['A','B'].\"\n",
    "        )\n",
    "    chat.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # Format for model\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "    inputs = tokenizer([formatted], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generation kwargs\n",
    "    gen_kwargs = gen_kwargs or {\n",
    "        \"max_new_tokens\": 8192,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.8,\n",
    "        \"top_k\": 20\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "    # Strip input prefix and parse\n",
    "    output_ids = outputs[0][inputs.input_ids.shape[-1]:].tolist()\n",
    "    # detect </think> token id if present\n",
    "    try:\n",
    "        think_idx = len(output_ids) - output_ids[::-1].index(tokenizer.convert_tokens_to_ids('</think>'))\n",
    "    except ValueError:\n",
    "        think_idx = 0\n",
    "    raw = tokenizer.decode(output_ids[think_idx:], skip_special_tokens=True).strip()\n",
    "        # Parse JSON list\n",
    "    try:\n",
    "        answer_list = json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        # fallback to Python literal_eval\n",
    "        from ast import literal_eval\n",
    "        try:\n",
    "            answer_list = literal_eval(raw)\n",
    "        except Exception:\n",
    "            answer_list = []\n",
    "    return answer_list\n",
    "\n",
    "def process_paper(\n",
    "    pdf_path: str,\n",
    "    grobid: GrobidService,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    questions: list,\n",
    "    max_context_tokens: int,\n",
    "    unique_labels: list = None,\n",
    "    device: str = \"cuda\",\n",
    "    extraction_method: str = \"full_text\",\n",
    "    section_name: list = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Process one paper: extract text, chunk, run QA, and collect responses.\n",
    "    Returns a dict with keys 'dataset', 'task', 'authors'.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing paper: {pdf_path}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Extract full text via Grobid\n",
    "    tei = grobid.process_full_text(pdf_path)\n",
    "    print(f\"Grobid processing took: {time.time() - start_time:.2f}s\")\n",
    "    if extraction_method == \"full_text\":\n",
    "        # Extract full text from TEI XML\n",
    "        raw_text = extract_sections_fulltext(tei) # Quitado remove_ref=True como parámetro porque la función no lo acepta\n",
    "    elif extraction_method == \"abstract\":\n",
    "        # Extract abstract from TEI XML\n",
    "        raw_text = extract_abstract(tei) # Quitado remove_ref=True como parámetro porque la función no lo acepta\n",
    "    elif extraction_method == \"flat_sections\":\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        sim_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        sections = extract_flat_sections_with_subtext(tei) # extract sections with their text in a dictionaty\n",
    "        ranked_sections = rank_sections_by_semantic_similarity([sec['title'] for sec in sections], section_name,model = sim_model) # get the most similar sections to the queries\n",
    "        best_match_section, best_score = ranked_sections[0]\n",
    "        raw_text = sections[[sec['title'] for sec in sections].index(best_match_section)]['text']\n",
    "\n",
    "\n",
    "\n",
    "    chunks = chunk_text(raw_text, tokenizer, max_tokens=max_context_tokens, overlap=200)\n",
    "\n",
    "    responses = {\"dataset\": [], \"task\": [], \"authors\": []}\n",
    "    for i, question in enumerate(questions):\n",
    "        print(f\"  Question {i+1}/{len(questions)}: {question}\")\n",
    "        selected_chunks = chunks[:1] if i == 2 else chunks\n",
    "        answers = [\n",
    "            run_qa_on_chunk(\n",
    "                model, tokenizer, chunk, question,\n",
    "                unique_labels if i == 1 else None,\n",
    "                device=device\n",
    "            ) for chunk in selected_chunks\n",
    "        ]\n",
    "        key = [\"dataset\", \"task\", \"authors\"][i]\n",
    "        responses[key].append(answers)\n",
    "        print(f\"    Completed in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    return responses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43de67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the JSON file with paper metadata from paperswithcode\n",
    "with open(\"../data/papers_data_copy.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    papers_list = json.load(f)\n",
    "# remove if Local PDF Path is None\n",
    "papers_list = [paper for paper in papers_list if paper.get(\"Local PDF Path\") is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba944267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Loading configuration file from ./Grobid/config.json\n",
      "2026-01-05 13:52:19,847 - INFO - Loading configuration file from ./Grobid/config.json\n",
      "INFO - Configuration file loaded successfully\n",
      "2026-01-05 13:52:19,848 - INFO - Configuration file loaded successfully\n",
      "2026-01-05 13:52:19,851 - INFO - Logging configured - Level: INFO, Console: True, File: disabled\n",
      "2026-01-05 13:52:19,860 - INFO - GROBID server http://localhost:8070 is up and running\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing paper: data/pdfs/KG^2- Learning to Reason Science Exam Questions with Contextual Knowledge Graph Embeddings.pdf\n",
      "Grobid processing took: 1.62s\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m     14\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m paper \u001B[38;5;129;01min\u001B[39;00m papers_list[\u001B[32m0\u001B[39m:\u001B[32m2\u001B[39m]:\n\u001B[32m     15\u001B[39m     pdf_path = paper[\u001B[33m'\u001B[39m\u001B[33mLocal PDF Path\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m     res = \u001B[43mprocess_paper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpdf_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrobid\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[43m        \u001B[49m\u001B[43mquestions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m        \u001B[49m\u001B[32;43m2048\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     23\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcuda\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m     25\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     26\u001B[39m     all_results.append(res)\n\u001B[32m     28\u001B[39m \u001B[38;5;66;03m# Now you have a list of results per paper\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[23]\u001B[39m\u001B[32m, line 140\u001B[39m, in \u001B[36mprocess_paper\u001B[39m\u001B[34m(pdf_path, grobid, model, tokenizer, questions, max_context_tokens, unique_labels, device, extraction_method, section_name)\u001B[39m\n\u001B[32m    135\u001B[39m     best_match_section, best_score = ranked_sections[\u001B[32m0\u001B[39m]\n\u001B[32m    136\u001B[39m     raw_text = sections[[sec[\u001B[33m'\u001B[39m\u001B[33mtitle\u001B[39m\u001B[33m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m sec \u001B[38;5;129;01min\u001B[39;00m sections].index(best_match_section)][\u001B[33m'\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m--> \u001B[39m\u001B[32m140\u001B[39m chunks = \u001B[43mchunk_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_text\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_context_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moverlap\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m200\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    142\u001B[39m responses = {\u001B[33m\"\u001B[39m\u001B[33mdataset\u001B[39m\u001B[33m\"\u001B[39m: [], \u001B[33m\"\u001B[39m\u001B[33mtask\u001B[39m\u001B[33m\"\u001B[39m: [], \u001B[33m\"\u001B[39m\u001B[33mauthors\u001B[39m\u001B[33m\"\u001B[39m: []}\n\u001B[32m    143\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, question \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(questions):\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 2\u001B[39m, in \u001B[36mchunk_text\u001B[39m\u001B[34m(text, tokenizer, max_tokens, overlap)\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mchunk_text\u001B[39m(text, tokenizer, max_tokens=\u001B[32m8000\u001B[39m, overlap=\u001B[32m200\u001B[39m):\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m     tokens = \u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m     chunks = []\n\u001B[32m      5\u001B[39m     start = \u001B[32m0\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2732\u001B[39m, in \u001B[36mPreTrainedTokenizerBase.encode\u001B[39m\u001B[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001B[39m\n\u001B[32m   2694\u001B[39m \u001B[38;5;129m@add_end_docstrings\u001B[39m(\n\u001B[32m   2695\u001B[39m     ENCODE_KWARGS_DOCSTRING,\n\u001B[32m   2696\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   2715\u001B[39m     **kwargs,\n\u001B[32m   2716\u001B[39m ) -> \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mint\u001B[39m]:\n\u001B[32m   2717\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   2718\u001B[39m \u001B[33;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001B[39;00m\n\u001B[32m   2719\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   2730\u001B[39m \u001B[33;03m            method).\u001B[39;00m\n\u001B[32m   2731\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2732\u001B[39m     encoded_inputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2733\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2734\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2735\u001B[39m \u001B[43m        \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2736\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2737\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtruncation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2738\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2739\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2740\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2741\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2742\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2743\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2745\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m encoded_inputs[\u001B[33m\"\u001B[39m\u001B[33minput_ids\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:3123\u001B[39m, in \u001B[36mPreTrainedTokenizerBase.encode_plus\u001B[39m\u001B[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[39m\n\u001B[32m   3094\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   3095\u001B[39m \u001B[33;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001B[39;00m\n\u001B[32m   3096\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m   3111\u001B[39m \u001B[33;03m        method).\u001B[39;00m\n\u001B[32m   3112\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   3114\u001B[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001B[38;5;28mself\u001B[39m._get_padding_truncation_strategies(\n\u001B[32m   3115\u001B[39m     padding=padding,\n\u001B[32m   3116\u001B[39m     truncation=truncation,\n\u001B[32m   (...)\u001B[39m\u001B[32m   3120\u001B[39m     **kwargs,\n\u001B[32m   3121\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m3123\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3124\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3125\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3126\u001B[39m \u001B[43m    \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3127\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3128\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3129\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3130\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3131\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3132\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3133\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3134\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3135\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3136\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3137\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3138\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3139\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3140\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3141\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3142\u001B[39m \u001B[43m    \u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msplit_special_tokens\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3143\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3144\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_fast.py:627\u001B[39m, in \u001B[36mPreTrainedTokenizerFast._encode_plus\u001B[39m\u001B[34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[39m\n\u001B[32m    603\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_encode_plus\u001B[39m(\n\u001B[32m    604\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    605\u001B[39m     text: Union[TextInput, PreTokenizedInput],\n\u001B[32m   (...)\u001B[39m\u001B[32m    624\u001B[39m     **kwargs,\n\u001B[32m    625\u001B[39m ) -> BatchEncoding:\n\u001B[32m    626\u001B[39m     batched_input = [(text, text_pair)] \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;28;01melse\u001B[39;00m [text]\n\u001B[32m--> \u001B[39m\u001B[32m627\u001B[39m     batched_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_batch_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    628\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbatched_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    629\u001B[39m \u001B[43m        \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    630\u001B[39m \u001B[43m        \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    631\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    632\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    633\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    634\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    635\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    636\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    637\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    638\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    639\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    640\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    641\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    642\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    643\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    644\u001B[39m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    645\u001B[39m \u001B[43m        \u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    646\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    647\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    649\u001B[39m     \u001B[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001B[39;00m\n\u001B[32m    650\u001B[39m     \u001B[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001B[39;00m\n\u001B[32m    651\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m return_tensors \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_overflowing_tokens:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_fast.py:553\u001B[39m, in \u001B[36mPreTrainedTokenizerFast._batch_encode_plus\u001B[39m\u001B[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001B[39m\n\u001B[32m    550\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._tokenizer.encode_special_tokens != split_special_tokens:\n\u001B[32m    551\u001B[39m     \u001B[38;5;28mself\u001B[39m._tokenizer.encode_special_tokens = split_special_tokens\n\u001B[32m--> \u001B[39m\u001B[32m553\u001B[39m encodings = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_tokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode_batch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    554\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    555\u001B[39m \u001B[43m    \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    556\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_pretokenized\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    557\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    559\u001B[39m \u001B[38;5;66;03m# Convert encoding to dict\u001B[39;00m\n\u001B[32m    560\u001B[39m \u001B[38;5;66;03m# `Tokens` has type: tuple[\u001B[39;00m\n\u001B[32m    561\u001B[39m \u001B[38;5;66;03m#                       list[dict[str, list[list[int]]]] or list[dict[str, 2D-Tensor]],\u001B[39;00m\n\u001B[32m    562\u001B[39m \u001B[38;5;66;03m#                       list[EncodingFast]\u001B[39;00m\n\u001B[32m    563\u001B[39m \u001B[38;5;66;03m#                    ]\u001B[39;00m\n\u001B[32m    564\u001B[39m \u001B[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001B[39;00m\n\u001B[32m    565\u001B[39m tokens_and_encodings = [\n\u001B[32m    566\u001B[39m     \u001B[38;5;28mself\u001B[39m._convert_encoding(\n\u001B[32m    567\u001B[39m         encoding=encoding,\n\u001B[32m   (...)\u001B[39m\u001B[32m    576\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m encoding \u001B[38;5;129;01min\u001B[39;00m encodings\n\u001B[32m    577\u001B[39m ]\n",
      "\u001B[31mTypeError\u001B[39m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "    model, tokenizer, device=\"cuda\"\n",
    ")\n",
    "grobid = GrobidService(config_path=\"./Grobid/config.json\")\n",
    "\n",
    "# Define or load your papers_list, questions, unique_labels\n",
    "# papers_list = [...]  # List[dict]\n",
    "# questions = [...]    # List[str]\n",
    "# unique_labels = [...]\n",
    "\n",
    "all_results = []\n",
    "for paper in papers_list[0:2]:\n",
    "    pdf_path = paper['Local PDF Path']\n",
    "    res = process_paper(\n",
    "        pdf_path,\n",
    "        grobid,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        questions,\n",
    "        2048,\n",
    "        None,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "    all_results.append(res)\n",
    "\n",
    "# Now you have a list of results per paper\n",
    "for i in range(len(all_results)):\n",
    "    all_results[i]['dataset'] = deduplicate_fuzzy(\n",
    "        list(chain.from_iterable(all_results[i]['dataset'][0]))\n",
    "    )\n",
    "    all_results[i]['task'] = deduplicate_fuzzy(\n",
    "        list(chain.from_iterable(all_results[i]['task'][0]))\n",
    "    )\n",
    "    all_results[i]['authors'] = deduplicate_fuzzy(\n",
    "        list(chain.from_iterable(all_results[i]['authors'][0]))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3766c3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08075793",
   "metadata": {},
   "source": [
    "# Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fde182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the responses to a JSON file\n",
    "import json\n",
    "\n",
    "# open the CSV file\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"model_taxonomy_survey_2022\\index.csv\")\n",
    "df.head(5)\n",
    "labels = []\n",
    "# Create a mapping from category names to numerical labels\n",
    "category_mapping = {\n",
    "    \"Semantic matching models\": 0,\n",
    "    \"Translation models\": 1,\n",
    "    \"Internal side information inside KGs\": 2,\n",
    "    \"External extra information outside KGs\": 3,\n",
    "    \"Other models\": 4\n",
    "}\n",
    "# Initialize labels based on the category mapping\n",
    "labels = [category_mapping.get(category, 4) for category in df['category']]\n",
    "\n",
    "\n",
    "mask = ~df['url'].duplicated(keep=False)\n",
    "df_unique_only = df[mask]\n",
    "df_unique_only\n",
    "# Initialize labels based on the category mapping\n",
    "labels_unique = [category_mapping.get(category, 4) for category in df_unique_only['category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e66c34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Loading configuration file from ./Grobid/config.json\n",
      "2026-01-05 13:18:37,619 - INFO - Loading configuration file from ./Grobid/config.json\n",
      "INFO - Configuration file loaded successfully\n",
      "2026-01-05 13:18:37,620 - INFO - Configuration file loaded successfully\n",
      "2026-01-05 13:18:37,622 - INFO - Logging configured - Level: INFO, Console: True, File: disabled\n",
      "2026-01-05 13:18:37,633 - INFO - GROBID server http://localhost:8070 is up and running\n",
      "2026-01-05 13:18:37,635 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1901.09590.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1901.09590.pdf'\n",
      "2026-01-05 13:18:37,645 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\0a1bf96b7165e962e90cb14648c9462d-Paper.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\0a1bf96b7165e962e90cb14648c9462d-Paper.pdf'\n",
      "2026-01-05 13:18:37,646 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1506.00999.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1506.00999.pdf'\n",
      "2026-01-05 13:18:37,646 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\liu17d.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\liu17d.pdf'\n",
      "2026-01-05 13:18:37,647 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1412.6575.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1412.6575.pdf'\n",
      "2026-01-05 13:18:37,648 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\trouillon16.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\trouillon16.pdf'\n",
      "2026-01-05 13:18:37,650 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1802.04868.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1802.04868.pdf'\n",
      "2026-01-05 13:18:37,651 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\ds-paper-620.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\ds-paper-620.pdf'\n",
      "2026-01-05 13:18:37,652 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1705.10744.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1705.10744.pdf'\n",
      "2026-01-05 13:18:37,653 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1805.02408.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1805.02408.pdf'\n",
      "2026-01-05 13:18:37,654 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\lacroix18a.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\lacroix18a.pdf'\n",
      "2026-01-05 13:18:37,655 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1912.02686.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1912.02686.pdf'\n",
      "2026-01-05 13:18:37,655 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1904.10281.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1904.10281.pdf'\n",
      "2026-01-05 13:18:37,656 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1910.11583.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1910.11583.pdf'\n",
      "2026-01-05 13:18:37,657 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\b337e84de8752b27eda3a12363109e80-Paper.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\b337e84de8752b27eda3a12363109e80-Paper.pdf'\n",
      "2026-01-05 13:18:37,658 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\45634.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\45634.pdf'\n",
      "2026-01-05 13:18:37,659 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1808.04122.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1808.04122.pdf'\n",
      "2026-01-05 13:18:37,659 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1703.06103.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1703.06103.pdf'\n",
      "2026-01-05 13:18:37,660 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1911.03082.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1911.03082.pdf'\n",
      "2026-01-05 13:18:37,661 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1711.04071.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1711.04071.pdf'\n",
      "2026-01-05 13:18:37,662 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\85.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\85.pdf'\n",
      "2026-01-05 13:18:37,664 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf'\n",
      "2026-01-05 13:18:37,664 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\P15-1067.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\P15-1067.pdf'\n",
      "2026-01-05 13:18:37,665 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\N16-1105.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\N16-1105.pdf'\n",
      "2026-01-05 13:18:37,665 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1801.08641.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1801.08641.pdf'\n",
      "2026-01-05 13:18:37,666 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1606.08140.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1606.08140.pdf'\n",
      "2026-01-05 13:18:37,667 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\12887-57589-1-PB.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\12887-57589-1-PB.pdf'\n",
      "2026-01-05 13:18:37,667 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\PACLIC_28_Fan.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\PACLIC_28_Fan.pdf'\n",
      "2026-01-05 13:18:37,668 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1704.05908.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1704.05908.pdf'\n",
      "2026-01-05 13:18:37,669 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\0596.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\0596.pdf'\n",
      "2026-01-05 13:18:37,669 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1904.12211.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1904.12211.pdf'\n",
      "2026-01-05 13:18:37,670 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1509.05490.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1509.05490.pdf'\n",
      "2026-01-05 13:18:37,670 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1902.10197.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1902.10197.pdf'\n",
      "2026-01-05 13:18:37,672 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1709.04676.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1709.04676.pdf'\n",
      "2026-01-05 13:18:37,673 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1708.04828.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1708.04828.pdf'\n",
      "2026-01-05 13:18:37,674 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\W18-3017.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\W18-3017.pdf'\n",
      "2026-01-05 13:18:37,675 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1609.07028.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1609.07028.pdf'\n",
      "2026-01-05 13:18:37,675 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\S18-2027.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\S18-2027.pdf'\n",
      "2026-01-05 13:18:37,676 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1809.01341.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1809.01341.pdf'\n",
      "2026-01-05 13:18:37,677 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1903.05485.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1903.05485.pdf'\n",
      "2026-01-05 13:18:37,678 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1802.00934.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1802.00934.pdf'\n",
      "2026-01-05 13:18:37,679 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\D14-1165.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\D14-1165.pdf'\n",
      "2026-01-05 13:18:37,679 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1508.02593.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1508.02593.pdf'\n",
      "2026-01-05 13:18:37,681 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\paperID314.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\paperID314.pdf'\n",
      "2026-01-05 13:18:37,682 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\P17-2051.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\P17-2051.pdf'\n",
      "2026-01-05 13:18:37,682 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\download_file.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\download_file.pdf'\n",
      "2026-01-05 13:18:37,683 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\P15-1125.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\P15-1125.pdf'\n",
      "2026-01-05 13:18:37,684 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\P15-1009.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\P15-1009.pdf'\n",
      "2026-01-05 13:18:37,685 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\2018.KDD%202018%20Hierarchical%20Taxonomy%20Aware%20Network%20Embedding.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\2018.KDD%202018%20Hierarchical%20Taxonomy%20Aware%20Network%20Embedding.pdf'\n",
      "2026-01-05 13:18:37,686 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1805.09547.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1805.09547.pdf'\n",
      "2026-01-05 13:18:37,687 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\P19-1431.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\P19-1431.pdf'\n",
      "2026-01-05 13:18:37,687 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1906.01195.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1906.01195.pdf'\n",
      "2026-01-05 13:18:37,687 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1808.09040.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1808.09040.pdf'\n",
      "2026-01-05 13:18:37,688 - ERROR - Failed to open PDF file c:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\model_type_pdfs\\1911.04910.pdf: [Errno 2] No such file or directory: 'c:\\\\Users\\\\M\\\\Desktop\\\\Proyecto GAP-KGE\\\\GAP-KGE\\\\model_type_pdfs\\\\1911.04910.pdf'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing paper: model_type_pdfs/1901.09590.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/0a1bf96b7165e962e90cb14648c9462d-Paper.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1506.00999.pdf\n",
      "Grobid processing took: 0.0014858245849609375 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/liu17d.pdf\n",
      "Grobid processing took: 0.0012526512145996094 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1412.6575.pdf\n",
      "Grobid processing took: 0.001016855239868164 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/trouillon16.pdf\n",
      "Grobid processing took: 0.0015060901641845703 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1802.04868.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/ds-paper-620.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1705.10744.pdf\n",
      "Grobid processing took: 0.0010080337524414062 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1805.02408.pdf\n",
      "Grobid processing took: 0.001008749008178711 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/lacroix18a.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1912.02686.pdf\n",
      "Grobid processing took: 0.0010991096496582031 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1904.10281.pdf\n",
      "Grobid processing took: 0.0010116100311279297 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1910.11583.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/b337e84de8752b27eda3a12363109e80-Paper.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/45634.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1808.04122.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1703.06103.pdf\n",
      "Grobid processing took: 0.00099945068359375 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1911.03082.pdf\n",
      "Grobid processing took: 0.0010976791381835938 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1711.04071.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/85.pdf\n",
      "Grobid processing took: 0.001010894775390625 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf\n",
      "Grobid processing took: 0.0010790824890136719 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/P15-1067.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/N16-1105.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1801.08641.pdf\n",
      "Grobid processing took: 0.000997304916381836 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1606.08140.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/12887-57589-1-PB.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/PACLIC_28_Fan.pdf\n",
      "Grobid processing took: 0.0011429786682128906 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1704.05908.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/0596.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1904.12211.pdf\n",
      "Grobid processing took: 0.0015056133270263672 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1509.05490.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1902.10197.pdf\n",
      "Grobid processing took: 0.0015175342559814453 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1709.04676.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1708.04828.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/W18-3017.pdf\n",
      "Grobid processing took: 0.0010061264038085938 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1609.07028.pdf\n",
      "Grobid processing took: 0.0010063648223876953 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/S18-2027.pdf\n",
      "Grobid processing took: 0.0010077953338623047 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1809.01341.pdf\n",
      "Grobid processing took: 0.0009982585906982422 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1903.05485.pdf\n",
      "Grobid processing took: 0.0010018348693847656 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1802.00934.pdf\n",
      "Grobid processing took: 0.0010004043579101562 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/D14-1165.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1508.02593.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/paperID314.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/P17-2051.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/download_file.pdf\n",
      "Grobid processing took: 0.0010037422180175781 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/P15-1125.pdf\n",
      "Grobid processing took: 0.0009984970092773438 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/P15-1009.pdf\n",
      "Grobid processing took: 0.0009992122650146484 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/2018.KDD%202018%20Hierarchical%20Taxonomy%20Aware%20Network%20Embedding.pdf\n",
      "Grobid processing took: 0.0013921260833740234 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1805.09547.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/P19-1431.pdf\n",
      "Grobid processing took: 0.0010533332824707031 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1906.01195.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1808.09040.pdf\n",
      "Grobid processing took: 0.0014438629150390625 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n",
      "Processing paper: model_type_pdfs/1911.04910.pdf\n",
      "Grobid processing took: 0.0 seconds\n",
      "Error extracting abstract (XMLSyntaxError(\"Start tag expected, '<' not found, line 1, column 1\")), skipping.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "pop index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mXMLSyntaxError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 20\u001B[39m\n\u001B[32m     19\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m     raw_text = \u001B[43mextract_abstract\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtei\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m     \u001B[38;5;66;03m# if extract_abstract returns None or an empty string, treat as failure\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mc:\\Users\\M\\Desktop\\Proyecto GAP-KGE\\GAP-KGE\\utils\\tei_extraction.py:39\u001B[39m, in \u001B[36mextract_abstract\u001B[39m\u001B[34m(tei_xml_str)\u001B[39m\n\u001B[32m     38\u001B[39m ns = {\u001B[33m'\u001B[39m\u001B[33mtei\u001B[39m\u001B[33m'\u001B[39m: \u001B[33m'\u001B[39m\u001B[33mhttp://www.tei-c.org/ns/1.0\u001B[39m\u001B[33m'\u001B[39m}  \u001B[38;5;66;03m# TEI uses XML namespaces\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m root = \u001B[43metree\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfromstring\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtei_xml_str\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     41\u001B[39m \u001B[38;5;66;03m# Corrected XPath to include <div> between <abstract> and <p>\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32msrc\\lxml\\etree.pyx:3264\u001B[39m, in \u001B[36mlxml.etree.fromstring\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32msrc\\lxml\\parser.pxi:1916\u001B[39m, in \u001B[36mlxml.etree._parseMemoryDocument\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32msrc\\lxml\\parser.pxi:1803\u001B[39m, in \u001B[36mlxml.etree._parseDoc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32msrc\\lxml\\parser.pxi:1144\u001B[39m, in \u001B[36mlxml.etree._BaseParser._parseDoc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32msrc\\lxml\\parser.pxi:618\u001B[39m, in \u001B[36mlxml.etree._ParserContext._handleParseResultDoc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32msrc\\lxml\\parser.pxi:728\u001B[39m, in \u001B[36mlxml.etree._handleParseResult\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32msrc\\lxml\\parser.pxi:657\u001B[39m, in \u001B[36mlxml.etree._raiseParseError\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mXMLSyntaxError\u001B[39m: Start tag expected, '<' not found, line 1, column 1 (<string>, line 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mIndexError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 28\u001B[39m\n\u001B[32m     26\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     27\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mError extracting abstract (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[33m), skipping.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m     labels_unique.pop(i)  \u001B[38;5;66;03m# Remove the label for this paper\u001B[39;00m\n\u001B[32m     29\u001B[39m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m     31\u001B[39m \u001B[38;5;66;03m# sections\u001B[39;00m\n\u001B[32m     32\u001B[39m \u001B[38;5;66;03m# sections = extract_flat_sections_with_subtext(tei) # extract sections with their text in a dictionaty\u001B[39;00m\n\u001B[32m     33\u001B[39m \u001B[38;5;66;03m# ranked_sections = rank_sections_by_semantic_similarity([sec['title'] for sec in sections], [\"Experiments\",\"Evaluation\"],model = sim_model) # get the most similar sections to the queries\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     37\u001B[39m \u001B[38;5;66;03m# full text\u001B[39;00m\n\u001B[32m     38\u001B[39m \u001B[38;5;66;03m# raw_text = tei_to_full_raw_text(tei, remove_ref=True)\u001B[39;00m\n",
      "\u001B[31mIndexError\u001B[39m: pop index out of range"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Get the current working directory\n",
    "current_dir = Path(os.getcwd())\n",
    "responses_model = []\n",
    "\n",
    "documents = []\n",
    "grobid = GrobidService(config_path=\"./Grobid/config.json\")\n",
    "for i in range(len(df_unique_only)): #len(df)\n",
    "    paper_filename = df_unique_only.iloc[i]['filename']\n",
    "    # label = df.iloc[i]['category']\n",
    "\n",
    "    print(\"Processing paper:\", paper_filename)\n",
    "    start = time.time()\n",
    "    pdf_path = str(current_dir/paper_filename)\n",
    "\n",
    "    tei = grobid.process_full_text(pdf_path)\n",
    "    print(\"Grobid processing took:\", time.time() - start, \"seconds\")\n",
    "\n",
    "    try:\n",
    "        raw_text = extract_abstract(tei)\n",
    "        # if extract_abstract returns None or an empty string, treat as failure\n",
    "        if not raw_text or not raw_text.strip():\n",
    "            print(\"No abstract found, skipping.\")\n",
    "            labels_unique.pop(i)  # Remove the label for this paper\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting abstract ({e!r}), skipping.\")\n",
    "        labels_unique.pop(i)  # Remove the label for this paper\n",
    "        continue\n",
    "\n",
    "    # sections\n",
    "    # sections = extract_flat_sections_with_subtext(tei) # extract sections with their text in a dictionaty\n",
    "    # ranked_sections = rank_sections_by_semantic_similarity([sec['title'] for sec in sections], [\"Experiments\",\"Evaluation\"],model = sim_model) # get the most similar sections to the queries\n",
    "    # best_match_section, best_score = ranked_sections[0]\n",
    "    # raw_text = sections[[sec['title'] for sec in sections].index(best_match_section)]['text']\n",
    "\n",
    "    # full text\n",
    "    # raw_text = tei_to_full_raw_text(tei, remove_ref=True)\n",
    "    documents.append(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents, labels_unique, stratify=labels_unique, test_size=0.20, random_state=42 )\n",
    "print(\"Number of training samples:\", len(X_train))\n",
    "print(\"Number of test samples:\", len(X_test))\n",
    "\n",
    "model = make_pipeline(\n",
    "    TfidfVectorizer(ngram_range=(1,2), max_features=50000),\n",
    "    LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    ")\n",
    "print(cross_val_score(model, X_train, y_train, cv=5, scoring='f1_macro').mean())\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Test F1:\", f1_score(y_test, model.predict(X_test), average='macro'))\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "average_recall = recall_score(y_test, y_pred, average='macro')\n",
    "average_precision = precision_score(y_test, y_pred, average='macro')\n",
    "average_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Average Recall: {average_recall:.4f}\")\n",
    "print(f\"Average Precision: {average_precision:.4f}\")\n",
    "print(f\"Average F1: {average_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
