{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a3d48d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.tei_extraction import extract_sections_fulltext, extract_abstract, tei_to_full_raw_text, extract_flat_sections_with_subtext, rank_sections_by_semantic_similarity\n",
    "from utils.grobid_service import GrobidService\n",
    "\n",
    "\n",
    "from rapidfuzz import fuzz, process\n",
    "import ast\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from grobid_client.grobid_client import GrobidClient\n",
    "from bs4 import BeautifulSoup\n",
    "import Levenshtein\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004a2487",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def deduplicate_fuzzy(list, threshold=80):\n",
    "    unique = []\n",
    "    for name in list:\n",
    "        if all(fuzz.ratio(name, existing) < threshold for existing in unique):\n",
    "            unique.append(name)\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1b07860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_dir = Path(os.getcwd())\n",
    "parent_dir = current_dir.parent\n",
    "\n",
    "\n",
    "with open(\"../data/papers_data_copy.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    papers_list = json.load(f)\n",
    "\n",
    "# remove if Local PDF Path is None\n",
    "papers = [paper for paper in papers_list if paper.get(\"Local PDF Path\") is not None]\n",
    "authors = []\n",
    "datasets = []\n",
    "tasks = []\n",
    "\n",
    "# get the authors from papers\n",
    "for i, paper in enumerate(papers):\n",
    "    authors.append(paper['Authors'])\n",
    "    datasets.append(paper['Datasets'])\n",
    "    tasks.append(paper['Tasks'])\n",
    "references = [authors, datasets, tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae108e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['ARC'],\n",
       "  ['Question Answering'],\n",
       "  ['Knowledge Graphs'],\n",
       "  ['AI2 Reasoning Challenge'],\n",
       "  ['Knowledge Graph Embeddings']],\n",
       " [['Knowledge Graphs'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Link Prediction'],\n",
       "  ['Entity Embeddings']],\n",
       " [['Contrastive Learning'],\n",
       "  ['Knowledge Graphs'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Word Embeddings'],\n",
       "  ['Learning Word Embeddings']],\n",
       " [['Graph Embedding'],\n",
       "  ['Knowledge Base Completion'],\n",
       "  ['Knowledge Graphs'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Link Prediction'],\n",
       "  ['Knowledge Graph Embedding']],\n",
       " [['Knowledge Graphs'], ['Knowledge Graph Embeddings'], ['Link Prediction']],\n",
       " [['Knowledge Graphs'],\n",
       "  ['Retrieval'],\n",
       "  ['Graph Embedding'],\n",
       "  ['Image Retrieval'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Knowledge Graph Embedding'],\n",
       "  ['Zero-Shot Learning'],\n",
       "  ['Representation Learning']],\n",
       " [['Graph Embedding'],\n",
       "  ['Question Answering'],\n",
       "  ['Knowledge Graph Completion'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Link Prediction'],\n",
       "  ['Knowledge Graph Embedding'],\n",
       "  ['Triplet']],\n",
       " [['Knowledge Graph Embeddings'], ['Vocal Bursts Type Prediction']],\n",
       " [['Knowledge Graphs'], ['Knowledge Graph Embeddings'], ['Link Prediction']],\n",
       " [['Knowledge Graphs'],\n",
       "  ['Graph Embedding'],\n",
       "  ['reinforcement-learning'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Diversity'],\n",
       "  ['Knowledge Graph Embedding'],\n",
       "  ['Reinforcement Learning'],\n",
       "  ['Reinforcement Learning (RL)']],\n",
       " [['Knowledge Graph Embeddings']],\n",
       " [['Knowledge Graph Embeddings'],\n",
       "  ['model'],\n",
       "  ['Question Answering'],\n",
       "  ['Knowledge Base Completion'],\n",
       "  ['General Classification']],\n",
       " [['Knowledge Graphs'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Link Prediction'],\n",
       "  ['Articles'],\n",
       "  ['Entity Resolution']],\n",
       " [['Knowledge Graphs'],\n",
       "  ['Entity Alignment'],\n",
       "  ['Translation'],\n",
       "  ['Knowledge Graph Embeddings']],\n",
       " [['Knowledge Graph Embeddings']],\n",
       " [['Knowledge Graph Embeddings'], ['Inductive Bias']],\n",
       " [['Knowledge Graphs'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Inductive Learning']],\n",
       " [['Knowledge Graph Embeddings'], ['Decoder'], ['Translation']],\n",
       " [['Knowledge Graphs'],\n",
       "  ['Prediction'],\n",
       "  ['Relation'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Link Prediction']],\n",
       " [['Knowledge Graphs'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Reinforcement Learning'],\n",
       "  ['Reinforcement Learning (RL)']],\n",
       " [['Knowledge Graphs'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Link Prediction'],\n",
       "  ['Knowledge Graph Embedding'],\n",
       "  ['Graph Embedding']],\n",
       " [['Knowledge Graphs'],\n",
       "  ['Triple Classification'],\n",
       "  ['Prediction'],\n",
       "  ['Type prediction'],\n",
       "  ['Relation'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Link Prediction']],\n",
       " [['Recommendation Systems'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Word Embeddings']],\n",
       " [['Knowledge Graphs'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Link Prediction'],\n",
       "  ['Word Embeddings']],\n",
       " [['Entity Disambiguation'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Image Captioning'],\n",
       "  ['Opinion Mining']],\n",
       " [['Active Learning'], ['Knowledge Graph Embeddings']],\n",
       " [['Knowledge Graphs'],\n",
       "  ['Question Answering'],\n",
       "  ['Knowledge Graph Embeddings']],\n",
       " [['Machine Translation'],\n",
       "  ['Entity Linking'],\n",
       "  ['Information Retrieval'],\n",
       "  ['Link Prediction'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Metric Learning'],\n",
       "  ['Sentiment Analysis']],\n",
       " [['Quantization'],\n",
       "  ['Knowledge Graphs'],\n",
       "  ['Knowledge Graph Completion'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Tensor Decomposition']],\n",
       " [['Relation'], ['Knowledge Graph Embeddings'], ['Relation Extraction']],\n",
       " [['Knowledge Graphs'],\n",
       "  ['Knowledge Graph Completion'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Link Prediction'],\n",
       "  ['Knowledge Graph Embedding'],\n",
       "  ['Graph Embedding'],\n",
       "  ['Representation Learning']],\n",
       " [['Relation'], ['Knowledge Graph Embeddings'], ['Link Prediction']],\n",
       " [['Relation Prediction'],\n",
       "  ['Knowledge Base Completion'],\n",
       "  ['Knowledge Graphs'],\n",
       "  ['Knowledge Graph Completion'],\n",
       "  ['Relation'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Link Prediction']],\n",
       " [['Knowledge Graphs'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Link Prediction'],\n",
       "  ['Variational Inference']],\n",
       " [['Knowledge Graphs'], ['Link Prediction'], ['Knowledge Graph Embeddings']],\n",
       " [['Knowledge Graphs'],\n",
       "  ['Translation'],\n",
       "  ['Link Prediction'],\n",
       "  ['Knowledge Graph Embeddings']],\n",
       " [['Knowledge Graphs'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['BIG-bench Machine Learning'],\n",
       "  ['Marketing']],\n",
       " [['Knowledge Graph Embeddings']],\n",
       " [['Knowledge Graphs'],\n",
       "  ['Information Retrieval'],\n",
       "  ['General Knowledge'],\n",
       "  ['Retrieval'],\n",
       "  ['Entity Embeddings'],\n",
       "  ['Language Modelling'],\n",
       "  ['Knowledge Graph Embeddings'],\n",
       "  ['Recommendation Systems'],\n",
       "  ['Knowledge Graph Embedding'],\n",
       "  ['Node Classification'],\n",
       "  ['Language Modeling']],\n",
       " [['Knowledge Graph Embeddings'],\n",
       "  ['Link Prediction'],\n",
       "  ['Knowledge Base Completion']]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f65ba85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Loading configuration file from ./Grobid/config.json\n",
      "INFO - Configuration file loaded successfully\n",
      "2026-01-06 15:51:56,820 - INFO - Logging configured - Level: INFO, Console: True, File: disabled\n",
      "2026-01-06 15:51:56,872 - INFO - GROBID server http://localhost:8070 is up and running\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yuyu Zhang', 'Hanjun Dai', 'Toraman Kamil', 'Le Song']\n",
      "['Agustinus Kristiadi', 'Mohammad Asif Khan', 'Denis Lukovnikov', 'Jens Lehmann', 'Asja Fischer']\n",
      "['Avishek Joey Bose', 'Huan Ling', 'Yanshuai Cao', 'Borealis Ai']\n",
      "['Liwei Cai', 'William Yang Wang']\n",
      "['Tim Dettmers', 'Pasquale Minervini', 'Pontus Stenetorp', 'Sebastian Riedel']\n",
      "['Daniel Oñoro-Rubio', 'Mathias Niepert', 'Alberto García-Durán', 'Roberto González-Sánchez', 'Roberto J López-Sastre']\n",
      "['Tommaso Soru', 'Stefano Ruberto', 'Diego Moussallem', 'André Valdestilhas', 'Alexander Bigerl', 'Edgard Marx', 'Diego Esteves']\n",
      "['Bhushan Kotnis', 'Vivi Nastase']\n",
      "['Bhushan Kotnis', 'Vivi Nastase']\n",
      "['Wenhan Xiong', 'Thien Hoang', 'William Yang Wang']\n",
      "['Tathagata Sengupta', 'Cibi Pragadeesh', 'Partha Pratim Talukdar']\n",
      "['Armand Joulin', 'Piotr Bojanowski', 'Maximilian Nickel', 'Tomas Mikolov']\n",
      "['Théo Trouillon', 'Maximilian Nickel']\n",
      "['Muhao Chen', 'Yingtao Tian', 'Mohan Yang', 'Carlo Zaniolo']\n",
      "['He He', 'Anusha Balakrishnan', 'Mihail Eric', 'Percy Liang']\n",
      "['Naoya Takeishi', 'Kosuke Akimoto']\n",
      "['Yunpu Ma', 'Volker Tresp', 'Erik A Daxberger']\n",
      "['Yue Liu', 'Tongtao Zhang', 'Zhicheng Liang', 'Heng Ji', 'Deborah L Mcguinness']\n",
      "['Ivana Balažević', 'Carl Allen', 'Timothy M Hospedales']\n",
      "['Victoria Xi', 'Richard Lin', 'Caiming Socher', 'Xiong']\n",
      "['Hao Yu', 'Vivek Kulkarni', 'William Yang Wang']\n",
      "['Haoyu Wang', 'Vivek Kulkarni', 'William Yang Wang']\n",
      "['Thomas Ager', 'Ondřej Kuželka', 'Steven Schockaert']\n",
      "['Aditya Sharma', 'Partha Talukdar']\n",
      "['Seungwhan Moon', 'Leonardo Neves', 'Vitor Carvalho']\n",
      "['Gabriel Stanovsky', 'Daniel Gruhl', 'Pablo N Mendes']\n",
      "['Jay Pujara', 'Eriq Augustine', 'Lise Getoor']\n",
      "['Zhiting Hu', 'Poyao Huang', 'Yuntian Deng', 'Yingkai Gao', 'Eric P Xing']\n",
      "['Koki Kishimoto', 'Katsuhiko Hayashi', 'Genki Akai', 'Masashi Shimbo', 'Kazunori Komatani']\n",
      "['Ningyu Zhang', 'Shumin Deng', 'Zhanlin Sun', 'Guanying Wang', 'Xi Chen', 'Wei Zhang', 'Huajun Chen']\n",
      "['Shuai Zhang', 'Yi Tay', 'Lina Yao', 'Qi Liu']\n",
      "['Canran Xu', 'Ruijiang Li']\n",
      "['Deepak Nathani', 'Jatin Chauhan', 'Charu Sharma', 'Manohar Kaul']\n",
      "['Alexander I Cowen-Rivers', 'Pasquale Minervini', 'Tim Rocktäschel', 'Matko Bošnjak', 'Sebastian Riedel', 'Jun Wang']\n",
      "['Robert Bamler', 'Farnood Salehi', 'Stephan Mandt']\n",
      "['Mojtaba Nayyeri', 'Xiaotian Zhou', 'Sahar Vahdati', 'Shariat Hamed', 'Yazdi', 'Jens Lehmann']\n",
      "['Md. Rezaul Karim', 'Michael Cochez', 'Joao Bosco Jares', 'Mamtaz Uddin', 'Oya Beyan', 'Stefan Decker']\n",
      "['Afshin Sadeghi', 'Jens Lehmann']\n",
      "['Freddy Lecue', 'Achim Rettinger', 'Petar Ristoski', 'Jessica Rosati', 'Tommaso Di Noia', 'Renato De Leone', 'Heiko Paulheim']\n",
      "['Prodromos Kolyvakis', 'Alexandros Kalousis', 'Dimitris Kiritsis']\n"
     ]
    }
   ],
   "source": [
    "current_dir = Path(\".\")\n",
    "grobid = GrobidService(config_path=\"./Grobid/config.json\")\n",
    "authors_grobid = []\n",
    "\n",
    "for paper in papers:\n",
    "    pdf_path = str(current_dir / paper[\"Local PDF Path\"])\n",
    "    authors = grobid.extract_authors_from_pdf(pdf_path)\n",
    "    authors_grobid.append(authors)\n",
    "    print(authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b588d",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc1dc1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "dataset_mentions = []\n",
    "with open('./datasets.json', \"r\") as file:\n",
    "    dataset_mentions = json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee2c5a7",
   "metadata": {},
   "source": [
    "## Model task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f5f5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(str(current_dir) + '/SciREX-master/test_outputs/pdfs/ner_predictions_abstract.jsonl', 'r') as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ff99838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_0000:\n",
      "doc_0001:\n",
      "doc_0002:\n",
      "doc_0003:\n",
      "doc_0004:\n",
      "doc_0005:\n",
      "doc_0006:\n",
      "doc_0007:\n",
      "doc_0008:\n",
      "doc_0009:\n",
      "doc_0010:\n",
      "doc_0011:\n",
      "doc_0012:\n",
      "doc_0013:\n",
      "doc_0014:\n",
      "doc_0015:\n",
      "doc_0016:\n",
      "doc_0017:\n",
      "doc_0018:\n",
      "doc_0019:\n",
      "doc_0020:\n",
      "doc_0021:\n",
      "doc_0022:\n",
      "doc_0023:\n",
      "doc_0024:\n",
      "doc_0025:\n",
      "doc_0026:\n",
      "doc_0027:\n",
      "doc_0028:\n",
      "doc_0029:\n"
     ]
    }
   ],
   "source": [
    "tasks_per_doc = {}\n",
    "\n",
    "for entry in data:\n",
    "    doc_id = entry[\"doc_id\"]\n",
    "    words = entry[\"words\"]\n",
    "    ner_spans = entry.get(\"ner\", [])\n",
    "\n",
    "    tasks = set()\n",
    "    for start, end, label in ner_spans:\n",
    "        if label == \"Task\":\n",
    "            span_text = \" \".join(words[start:end])\n",
    "            tasks.add(span_text)\n",
    "    tasks = list(tasks)\n",
    "\n",
    "    tasks_per_doc[doc_id] = tasks\n",
    "\n",
    "for i in tasks_per_doc:\n",
    "    tasks_per_doc[i] = deduplicate_fuzzy(tasks_per_doc[i], threshold=80)\n",
    "\n",
    "for doc_id, tasks in tasks_per_doc.items():\n",
    "    print(f\"{doc_id}:\")\n",
    "    for task in tasks:\n",
    "        print(f\"  - {task}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c85b6c",
   "metadata": {},
   "source": [
    "## Model taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cef4a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the responses to a JSON file\n",
    "import json\n",
    "\n",
    "# open the CSV file\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"model_taxonomy_survey_2022\\index.csv\")\n",
    "df.head(5)\n",
    "labels = []\n",
    "# Create a mapping from category names to numerical labels\n",
    "category_mapping = {\n",
    "    \"Semantic matching models\": 0,\n",
    "    \"Translation models\": 1,\n",
    "    \"Internal side information inside KGs\": 2,\n",
    "    \"External extra information outside KGs\": 3,\n",
    "    \"Other models\": 4\n",
    "}\n",
    "# Initialize labels based on the category mapping\n",
    "labels = [category_mapping.get(category, 4) for category in df['category']]\n",
    "\n",
    "\n",
    "mask = ~df['url'].duplicated(keep=False)\n",
    "df_unique_only = df[mask]\n",
    "\n",
    "# Initialize labels based on the category mapping\n",
    "labels_unique = [category_mapping.get(category, 4) for category in df_unique_only['category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22d4a4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Loading configuration file from ./Grobid/config.json\n",
      "INFO - Configuration file loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1901.09590.pdf\n",
      "Grobid processing took: 5.154153347015381 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/0a1bf96b7165e962e90cb14648c9462d-Paper.pdf\n",
      "Grobid processing took: 2.6073012351989746 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1506.00999.pdf\n",
      "Grobid processing took: 4.542264223098755 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/liu17d.pdf\n",
      "Grobid processing took: 2.59675669670105 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1412.6575.pdf\n",
      "Grobid processing took: 2.2232553958892822 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/trouillon16.pdf\n",
      "Grobid processing took: 2.336663007736206 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1802.04868.pdf\n",
      "Grobid processing took: 2.3163814544677734 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/ds-paper-620.pdf\n",
      "Grobid processing took: 2.5679242610931396 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1705.10744.pdf\n",
      "Grobid processing took: 1.7733349800109863 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1805.02408.pdf\n",
      "Grobid processing took: 2.5506958961486816 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/lacroix18a.pdf\n",
      "Grobid processing took: 2.207603693008423 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1912.02686.pdf\n",
      "Grobid processing took: 2.5462777614593506 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1904.10281.pdf\n",
      "Grobid processing took: 2.213043689727783 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1910.11583.pdf\n",
      "Grobid processing took: 1.795302152633667 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/b337e84de8752b27eda3a12363109e80-Paper.pdf\n",
      "Grobid processing took: 2.1272480487823486 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/45634.pdf\n",
      "Grobid processing took: 2.581368923187256 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1808.04122.pdf\n",
      "Grobid processing took: 2.1660776138305664 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1703.06103.pdf\n",
      "Grobid processing took: 2.1793768405914307 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1911.03082.pdf\n",
      "Grobid processing took: 2.6513943672180176 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1711.04071.pdf\n",
      "Grobid processing took: 1.938723087310791 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/85.pdf\n",
      "Grobid processing took: 1.8068642616271973 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf\n",
      "Grobid processing took: 2.0091307163238525 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/P15-1067.pdf\n",
      "Grobid processing took: 1.9992444515228271 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/N16-1105.pdf\n",
      "Grobid processing took: 2.0158185958862305 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1801.08641.pdf\n",
      "Grobid processing took: 1.727839469909668 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1606.08140.pdf\n",
      "Grobid processing took: 2.0756397247314453 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/12887-57589-1-PB.pdf\n",
      "Grobid processing took: 1.6198914051055908 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/PACLIC_28_Fan.pdf\n",
      "Grobid processing took: 1.9558179378509521 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1704.05908.pdf\n",
      "Grobid processing took: 2.260603427886963 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/0596.pdf\n",
      "Grobid processing took: 2.0471460819244385 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1904.12211.pdf\n",
      "Grobid processing took: 1.5398190021514893 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1509.05490.pdf\n",
      "Grobid processing took: 1.8381195068359375 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1902.10197.pdf\n",
      "Grobid processing took: 2.3807170391082764 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1709.04676.pdf\n",
      "Grobid processing took: 2.2584307193756104 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1708.04828.pdf\n",
      "Grobid processing took: 2.6432018280029297 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/W18-3017.pdf\n",
      "Grobid processing took: 1.603663444519043 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1609.07028.pdf\n",
      "Grobid processing took: 1.9436557292938232 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/S18-2027.pdf\n",
      "Grobid processing took: 1.9913887977600098 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1809.01341.pdf\n",
      "Grobid processing took: 2.379446506500244 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1903.05485.pdf\n",
      "Grobid processing took: 5.97087287902832 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1802.00934.pdf\n",
      "Grobid processing took: 2.048593759536743 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/D14-1165.pdf\n",
      "Grobid processing took: 2.338787317276001 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1508.02593.pdf\n",
      "Grobid processing took: 2.03605055809021 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/paperID314.pdf\n",
      "Grobid processing took: 2.248919725418091 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/P17-2051.pdf\n",
      "Grobid processing took: 2.127558946609497 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/download_file.pdf\n",
      "Grobid processing took: 1.5869791507720947 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/P15-1125.pdf\n",
      "Grobid processing took: 2.3216099739074707 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/P15-1009.pdf\n",
      "Grobid processing took: 3.443997859954834 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/2018.KDD%202018%20Hierarchical%20Taxonomy%20Aware%20Network%20Embedding.pdf\n",
      "Grobid processing took: 3.7979671955108643 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1805.09547.pdf\n",
      "Grobid processing took: 2.425297260284424 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/P19-1431.pdf\n",
      "Grobid processing took: 1.7312769889831543 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1906.01195.pdf\n",
      "Grobid processing took: 2.1465632915496826 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1808.09040.pdf\n",
      "Grobid processing took: 2.2424747943878174 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1911.04910.pdf\n",
      "Grobid processing took: 1.9291939735412598 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/document.pdf\n",
      "Grobid processing took: 3.726109027862549 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/D11-1049.pdf\n",
      "Grobid processing took: 2.205716371536255 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/D15-1173.pdf\n",
      "Grobid processing took: 2.1925463676452637 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1504.06662.pdf\n",
      "Grobid processing took: 2.1042845249176025 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1506.01094.pdf\n",
      "Grobid processing took: 2.0357699394226074 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1607.01426.pdf\n",
      "Grobid processing took: 1.8377251625061035 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/jiang17a.pdf\n",
      "Grobid processing took: 2.123798131942749 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/W17-2608.pdf\n",
      "Grobid processing took: 2.149970054626465 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1806.04523.pdf\n",
      "Grobid processing took: 1.893697738647461 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/KR2ML_2019_paper_31.pdf\n",
      "Grobid processing took: 1.6901543140411377 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/P16-1136.pdf\n",
      "Grobid processing took: 2.2565038204193115 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/s00521-018-3384-6.pdf\n",
      "Grobid processing took: 9.70444369316101 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/S19-1016.pdf\n",
      "Grobid processing took: 1.973909854888916 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1909.11864.pdf\n",
      "Grobid processing took: 2.0863664150238037 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/kuzelka20a.pdf\n",
      "Grobid processing took: 2.2432045936584473 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/2001.11850.pdf\n",
      "Grobid processing took: 2.601844072341919 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/0e55666a4ad822e0e34299df3591d979-Paper.pdf\n",
      "Grobid processing took: 1.8344066143035889 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/0297.pdf\n",
      "Grobid processing took: 2.034559726715088 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/Tim_aitp.pdf\n",
      "Grobid processing took: 1.5073928833007812 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1807.08204.pdf\n",
      "Grobid processing took: 1.6154603958129883 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/0c72cb7ee1512f800abe27823a792d03-Paper.pdf\n",
      "Grobid processing took: 1.9436817169189453 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/264.pdf\n",
      "Grobid processing took: 1.9604523181915283 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/ijcai2016.pdf\n",
      "Grobid processing took: 1.9647903442382812 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1903.03772.pdf\n",
      "Grobid processing took: 2.8768701553344727 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/content.pdf\n",
      "Grobid processing took: 1.3669970035552979 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1903.08948.pdf\n",
      "Grobid processing took: 3.1458821296691895 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/13e5ebb0fa112fe1b31a1067962d74a7-Paper.pdf\n",
      "Grobid processing took: 2.184983253479004 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/2004.04412.pdf\n",
      "Grobid processing took: 2.411350965499878 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/document.pdf\n",
      "Grobid processing took: 3.561159610748291 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/download.pdf\n",
      "Grobid processing took: 2.247227907180786 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/meilicke18ruleemb.pdf\n",
      "Grobid processing took: 2.0682642459869385 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/N18-1068.pdf\n",
      "Grobid processing took: 2.5845909118652344 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1605.05416.pdf\n",
      "Grobid processing took: 1.5743505954742432 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1611.08661.pdf\n",
      "Grobid processing took: 1.7974367141723633 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1807.11761.pdf\n",
      "Grobid processing took: 1.237837791442871 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/SemaPro_2019_Analogy_inference_on_context_graphsJCM06092019.pdf\n",
      "Grobid processing took: 1.6599583625793457 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1909.03193.pdf\n",
      "Grobid processing took: 1.931776523590088 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/2004.14781.pdf\n",
      "Grobid processing took: 3.421921968460083 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/D16-1260.pdf\n",
      "Grobid processing took: 1.6914911270141602 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/D18-1225.pdf\n",
      "Grobid processing took: 2.2611682415008545 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1807.00228.pdf\n",
      "Grobid processing took: 2.5130279064178467 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1809.03202.pdf\n",
      "Grobid processing took: 1.5319993495941162 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/2004.04926.pdf\n",
      "Grobid processing took: 1.7978522777557373 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/trivedi17a.pdf\n",
      "Grobid processing took: 2.3377926349639893 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1904.05530.pdf\n",
      "Grobid processing took: 2.8706891536712646 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/2003.13432.pdf\n",
      "Grobid processing took: 2.336528778076172 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/2010.03526.pdf\n",
      "Grobid processing took: 2.527238130569458 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/P16-1137.pdf\n",
      "Grobid processing took: 2.167100191116333 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/K18-1014.pdf\n",
      "Grobid processing took: 1.9690744876861572 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1906.05317.pdf\n",
      "Grobid processing took: 2.5116207599639893 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/2001.04170.pdf\n",
      "Grobid processing took: 2.2405548095703125 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/1604.08642.pdf\n",
      "Grobid processing took: 2.0143966674804688 seconds\n",
      "Processing paper: model_taxonomy_survey_2022/model_type_pdfs/rosso2020www_0.pdf\n",
      "Grobid processing took: 2.883578300476074 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Get the current working directory\n",
    "current_dir = Path(os.getcwd())\n",
    "responses_model = []\n",
    "\n",
    "documents = []\n",
    "grobid = GrobidService(config_path=\"./Grobid/config.json\")\n",
    "for i in range(len(df_unique_only)): #len(df)\n",
    "    paper_filename = \"model_taxonomy_survey_2022/\" + df_unique_only.iloc[i]['filename']\n",
    "    # label = df.iloc[i]['category']\n",
    "\n",
    "    print(\"Processing paper:\", paper_filename)\n",
    "    start = time.time()\n",
    "    pdf_path = str(current_dir/paper_filename)\n",
    "\n",
    "    tei = grobid.process_full_text(pdf_path)\n",
    "    print(\"Grobid processing took:\", time.time() - start, \"seconds\")\n",
    "\n",
    "    try:\n",
    "        raw_text = extract_abstract(tei)\n",
    "        # if extract_abstract returns None or an empty string, treat as failure\n",
    "        if not raw_text or not raw_text.strip():\n",
    "            print(\"No abstract found, skipping.\")\n",
    "            labels_unique.pop(i)  # Remove the label for this paper\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting abstract ({e!r}), skipping.\")\n",
    "        labels_unique.pop(i)  # Remove the label for this paper\n",
    "        continue\n",
    "\n",
    "    # sections\n",
    "    # sections = extract_flat_sections_with_subtext(tei) # extract sections with their text in a dictionaty\n",
    "    # ranked_sections = rank_sections_by_semantic_similarity([sec['title'] for sec in sections], [\"Experiments\",\"Evaluation\"],model = sim_model) # get the most similar sections to the queries\n",
    "    # best_match_section, best_score = ranked_sections[0]\n",
    "    # raw_text = sections[[sec['title'] for sec in sections].index(best_match_section)]['text']\n",
    "\n",
    "    # full text\n",
    "    # raw_text = tei_to_full_raw_text(tei, remove_ref=True)\n",
    "    documents.append(raw_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf91e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 85\n",
      "Number of test samples: 22\n",
      "0.6319047619047619\n",
      "Test F1: 0.6409803921568628\n",
      "Average Recall: 0.6129\n",
      "Average Precision: 0.7200\n",
      "Average F1: 0.6410\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents, labels_unique, stratify=labels_unique, test_size=0.20, random_state=42 )\n",
    "print(\"Number of training samples:\", len(X_train))\n",
    "print(\"Number of test samples:\", len(X_test))\n",
    "\n",
    "model = make_pipeline(\n",
    "    TfidfVectorizer(ngram_range=(1,2), max_features=50000),\n",
    "    LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    ")\n",
    "print(cross_val_score(model, X_train, y_train, cv=5, scoring='f1_macro').mean())\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Test F1:\", f1_score(y_test, model.predict(X_test), average='macro'))\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "average_recall = recall_score(y_test, y_pred, average='macro')\n",
    "average_precision = precision_score(y_test, y_pred, average='macro')\n",
    "average_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Average Recall: {average_recall:.4f}\")\n",
    "print(f\"Average Precision: {average_precision:.4f}\")\n",
    "print(f\"Average F1: {average_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b04d78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 36.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 47.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 76.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 55.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 41.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 64.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 94.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Recall: 0.7976\n",
      "Average Precision: 0.8000\n",
      "Average F1: 0.7767\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk import sent_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Load a pre-trained sentence embedding model\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 2. Helper to encode one document\n",
    "def doc_to_vec(doc):\n",
    "    sentences = sent_tokenize(doc)\n",
    "    sent_embs = embedder.encode(sentences)   # shape: (n_sentences, dim)\n",
    "    return sent_embs.mean(axis=0)           # mean pooling → (dim,)\n",
    "\n",
    "# 3. Prepare embeddings for all docs\n",
    "X = [doc_to_vec(d) for d in documents]\n",
    "y = labels_unique\n",
    "\n",
    "# 4. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.20, random_state=42)\n",
    "\n",
    "# 5. Simple classifier\n",
    "clf = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "clf.fit(X_train, y_train)\n",
    "# calculate the average metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "average_recall = recall_score(y_test, y_pred, average='macro')\n",
    "average_precision = precision_score(y_test, y_pred, average='macro')\n",
    "average_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Average Recall: {average_recall:.4f}\")\n",
    "print(f\"Average Precision: {average_precision:.4f}\")\n",
    "print(f\"Average F1: {average_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb44915",
   "metadata": {},
   "source": [
    "## full process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b49dce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors_from_paper(paper_directory):\n",
    "    \"\"\"\n",
    "    Extract authors from a paper directory.\n",
    "    \"\"\"\n",
    "    grobid = GrobidService(config_path=\"./Grobid/config.json\")\n",
    "    authors = grobid.extract_authors_from_pdf(paper_directory)\n",
    "    return authors\n",
    "\n",
    "def get_datasets_from_paper(grobid_dataset_filepath):\n",
    "    \"\"\"\n",
    "    Extract datasets from a dataset.json.\n",
    "    \"\"\"\n",
    "    with open(grobid_dataset_filepath, \"r\") as file:\n",
    "        dataset_mentions = json.load(file)\n",
    "    return dataset_mentions\n",
    "\n",
    "def get_tasks_from_paper(task_filepath):\n",
    "    \"\"\"\n",
    "    Extract tasks from a task.json.\n",
    "    \"\"\"\n",
    "    with open(task_filepath, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    tasks_per_doc = {}\n",
    "\n",
    "    for entry in data:\n",
    "        doc_id = entry[\"doc_id\"]\n",
    "        words = entry[\"words\"]\n",
    "        ner_spans = entry.get(\"ner\", [])\n",
    "\n",
    "        tasks = set()\n",
    "        for start, end, label in ner_spans:\n",
    "            if label == \"Task\":\n",
    "                span_text = \" \".join(words[start:end])\n",
    "                tasks.add(span_text)\n",
    "        tasks = list(tasks)\n",
    "\n",
    "        tasks_per_doc[doc_id] = tasks\n",
    "    for i in tasks_per_doc:\n",
    "        tasks_per_doc[i] = deduplicate_fuzzy(tasks_per_doc[i], threshold=80)   \n",
    "    return tasks_per_doc \n",
    "\n",
    "def get_model_taxonomy_from_paper(model, paper_directory, extraction_method='abstract'):\n",
    "    \"\"\"\n",
    "    Extract model taxonomy from a paper directory.\n",
    "    \"\"\"\n",
    "    grobid = GrobidService(config_path=\"./Grobid/config.json\")\n",
    "    tei = grobid.process_full_text(paper_directory)\n",
    "    if extraction_method == 'abstract':\n",
    "        try:\n",
    "            raw_text = extract_abstract(tei)\n",
    "            # if extract_abstract returns None or an empty string, treat as failure\n",
    "            if not raw_text or not raw_text.strip():\n",
    "                print(\"No abstract found, skipping.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting abstract ({e!r}), skipping.\")\n",
    "            return None\n",
    "    elif extraction_method == 'sections':\n",
    "        try:\n",
    "            sections = extract_flat_sections_with_subtext(tei)  # extract sections with their text in a dictionary\n",
    "            ranked_sections = rank_sections_by_semantic_similarity([sec['title'] for sec in sections], [\"Experiments\", \"Evaluation\"], model=None)  # get the most similar sections to the queries\n",
    "            best_match_section, best_score = ranked_sections[0]\n",
    "            raw_text = sections[[sec['title'] for sec in sections].index(best_match_section)]['text']\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting sections ({e!r}), skipping.\")\n",
    "            return None\n",
    "    elif extraction_method == 'full_text':\n",
    "        try:\n",
    "            raw_text = tei_to_full_raw_text(tei, remove_ref=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting full text ({e!r}), skipping.\")\n",
    "            return None\n",
    "    prediction = model.predict(raw_text)\n",
    "    category_mapping = {\n",
    "    \"Semantic matching models\": 0,\n",
    "    \"Translation models\": 1,\n",
    "    \"Internal side information inside KGs\": 2,\n",
    "    \"External extra information outside KGs\": 3,\n",
    "    \"Other models\": 4\n",
    "    }\n",
    "    return category_mapping.get(prediction, 4)  # Default to 'Other models'\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
