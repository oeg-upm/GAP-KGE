{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc0809f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\M\\Desktop\\Curro\\Proyecto\\GAP-KGE\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.tei_extraction import extract_sections_fulltext, extract_abstract, tei_to_full_raw_text, extract_flat_sections_with_subtext, rank_sections_by_semantic_similarity\n",
    "from utils.grobid_service import GrobidService\n",
    "\n",
    "\n",
    "from rapidfuzz import fuzz, process\n",
    "import ast\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from grobid_client.grobid_client import GrobidClient\n",
    "from bs4 import BeautifulSoup\n",
    "import Levenshtein\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sentence_transformers import util\n",
    "\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0927470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, tokenizer, max_tokens=8000, overlap=200):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    \n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_tokens, len(tokens))\n",
    "        chunk = tokenizer.decode(tokens[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += max_tokens - overlap  # Overlapping context\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def deduplicate_fuzzy(list, threshold=80):\n",
    "    unique = []\n",
    "    for name in list:\n",
    "        if all(fuzz.ratio(name, existing) < threshold for existing in unique):\n",
    "            unique.append(name)\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc4a89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the name of datasets used in the paper?\"\n",
    "question2 = \"What are the tasks that the model is trained for?\"\n",
    "question3 = \"Who are the authors of the paper?\"\n",
    "question4 = \"Given the model definitions mentioned before, choose one of the following taxonomy as the taxonomy of the model mentioned in this paper: Semantic matching model, Translation models, Internal side information inside KGs model, External extra information outside KGs or Other KGC Technologies ? \"\n",
    "questions = [question, question2, question3, question4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64a9ceec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` requires `accelerate`. You can install it with `pip install accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# load the tokenizer and the model\u001B[39;00m\n\u001B[0;32m      7\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name)\n\u001B[1;32m----> 8\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m     12\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m max_context_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m32768\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m2048\u001B[39m\n",
      "File \u001B[1;32mc:\\Users\\M\\Desktop\\Curro\\Proyecto\\GAP-KGE\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    602\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mconfig_class \u001B[38;5;241m==\u001B[39m config\u001B[38;5;241m.\u001B[39msub_configs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext_config\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    603\u001B[0m         config \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mget_text_config()\n\u001B[1;32m--> 604\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m    605\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    606\u001B[0m     )\n\u001B[0;32m    607\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    609\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    610\u001B[0m )\n",
      "File \u001B[1;32mc:\\Users\\M\\Desktop\\Curro\\Proyecto\\GAP-KGE\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:277\u001B[0m, in \u001B[0;36mrestore_default_dtype.<locals>._wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    275\u001B[0m old_dtype \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mget_default_dtype()\n\u001B[0;32m    276\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 277\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    278\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    279\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_default_dtype(old_dtype)\n",
      "File \u001B[1;32mc:\\Users\\M\\Desktop\\Curro\\Proyecto\\GAP-KGE\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:4806\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   4804\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDeepSpeed Zero-3 is not compatible with passing a `device_map`.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   4805\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_accelerate_available():\n\u001B[1;32m-> 4806\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   4807\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   4808\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequires `accelerate`. You can install it with `pip install accelerate`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   4809\u001B[0m         )\n\u001B[0;32m   4811\u001B[0m \u001B[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001B[39;00m\n\u001B[0;32m   4812\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m load_in_4bit \u001B[38;5;129;01mor\u001B[39;00m load_in_8bit:\n",
      "\u001B[1;31mValueError\u001B[0m: Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` requires `accelerate`. You can install it with `pip install accelerate`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import accelerate\n",
    "\n",
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "max_context_tokens = 32768 - 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa76daca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "def load_model_and_tokenizer(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device: str = \"cuda\"\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Load a model and tokenizer from pretrained checkpoints.\n",
    "\n",
    "    Args:\n",
    "        model_name:   HuggingFace model identifier.\n",
    "        tokenizer_name: If not provided, defaults to model_name.\n",
    "        device:       Device string, e.g. 'cuda' or 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        model, tokenizer\n",
    "    \"\"\"\n",
    "    # tokenizer_name = tokenizer_name or model_name\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    model = model\n",
    "    tokenizer = tokenizer\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def run_qa_on_chunk(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    chunk: str,\n",
    "    question: str,\n",
    "    model_taxonomy: bool = False,\n",
    "    unique_labels: list = None,\n",
    "    device: str = \"cuda\",\n",
    "    gen_kwargs: dict = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Run a single questionâ€“answer generation on one text chunk.\n",
    "    Returns the raw decoded content (Python list format expected).\n",
    "    \"\"\"\n",
    "    # Build chat history\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\":\n",
    "            \"You are an assistant for QA tasks. Use only provided context.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Context chunk: {chunk}\"}\n",
    "    ]\n",
    "\n",
    "    model_context = '''The semantic matching models and The translation models only use the structure information of internal facts in KGs. The semantic matching models generally use semantic matching-based scoring functions and further consists of tensor/matrix factorization models and neural network models. The translation models apply distance-based scoring functions.\n",
    "    While Internal side information inside KGs and External extra information outside KGs outside KGs cooperate with additional information (the inside or outside information of KGs except for the structure information) to achieve KGC. Internal side information inside KGs involved in KGs, including node attributes information, entity-related information, relation-related information, neighborhood information, relational path information; External extra information outside KGs outside KGs, mainly including two aspects: rule-based KGC and third-party data sources-based KGC. \n",
    "    And if it is not any of the previous models, then it is Other KGC technologies.'''\n",
    "\n",
    "    # Add question prompt\n",
    "    if model_taxonomy:\n",
    "        prompt = (\n",
    "           f\"Now, given this context for model taxonomy: {model_context} Answer this question: {question} \"\n",
    "           +\"Give back the answer only and only in a correct Python list format, for example: ['A']. If you don't know the answer, just return an empty list.\"\n",
    "\n",
    "        )\n",
    "    if unique_labels and question and isinstance(unique_labels, list):\n",
    "        prompt = (\n",
    "            f\"Now, given this question: {question}, and those possible tasks: {unique_labels}. \"\n",
    "            + \"Return answer only in a Python list format, e.g. ['A','B'].\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            f\"Now, given this question: {question}. \"\n",
    "            + \"Return answer only in a Python list format, e.g. ['A','B'].\"\n",
    "        )\n",
    "    chat.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # Format for model\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "    inputs = tokenizer([formatted], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generation kwargs\n",
    "    gen_kwargs = gen_kwargs or {\n",
    "        \"max_new_tokens\": 8192,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.8,\n",
    "        \"top_k\": 20\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "    # Strip input prefix and parse\n",
    "    output_ids = outputs[0][inputs.input_ids.shape[-1]:].tolist()\n",
    "    # detect </think> token id if present\n",
    "    try:\n",
    "        think_idx = len(output_ids) - output_ids[::-1].index(tokenizer.convert_tokens_to_ids('</think>'))\n",
    "    except ValueError:\n",
    "        think_idx = 0\n",
    "    raw = tokenizer.decode(output_ids[think_idx:], skip_special_tokens=True).strip()\n",
    "        # Parse JSON list\n",
    "    try:\n",
    "        answer_list = json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        # fallback to Python literal_eval\n",
    "        from ast import literal_eval\n",
    "        try:\n",
    "            answer_list = literal_eval(raw)\n",
    "        except Exception:\n",
    "            answer_list = []\n",
    "    return answer_list\n",
    "\n",
    "def process_paper(\n",
    "    pdf_path: str,\n",
    "    grobid: GrobidService,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    questions: list,\n",
    "    answers: list,\n",
    "    max_context_tokens: int,\n",
    "    unique_labels: list = None,\n",
    "    device: str = \"cuda\",\n",
    "    extraction_method: str = \"full_text\",\n",
    "    section_name: list = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Process one paper: extract text, chunk, run QA, and collect responses.\n",
    "    Returns a dict with keys 'dataset', 'task', 'authors'.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing paper: {pdf_path}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Extract full text via Grobid\n",
    "    tei = grobid.process_full_text(pdf_path)\n",
    "    print(f\"Grobid processing took: {time.time() - start_time:.2f}s\")\n",
    "    if extraction_method == \"full_text\":\n",
    "        # Extract full text from TEI XML\n",
    "        raw_text = tei\n",
    "    elif extraction_method == \"abstract\":\n",
    "        # Extract abstract from TEI XML\n",
    "        raw_text = extract_abstract(tei, remove_ref=True)\n",
    "    elif extraction_method == \"flat_sections\":\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        sim_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        sections = extract_flat_sections_with_subtext(tei) # extract sections with their text in a dictionaty\n",
    "        ranked_sections = rank_sections_by_semantic_similarity([sec['title'] for sec in sections], section_name,model = sim_model) # get the most similar sections to the queries\n",
    "        best_match_section, best_score = ranked_sections[0]\n",
    "        raw_text = sections[[sec['title'] for sec in sections].index(best_match_section)]['text']\n",
    "\n",
    "\n",
    "\n",
    "    chunks = chunk_text(raw_text, tokenizer, max_tokens=max_context_tokens, overlap=200)\n",
    "    responses = {}\n",
    "    for i in answers:\n",
    "        responses[i] = []\n",
    "    \n",
    "\n",
    "    # responses = {\"dataset\": [], \"task\": [], \"authors\": []}\n",
    "    for i, question in enumerate(questions):\n",
    "        print(f\"  Question {i+1}/{len(questions)}: {question}\")\n",
    "        selected_chunks = chunks[:1] if i == 2 else chunks\n",
    "        responses_llm = [\n",
    "            run_qa_on_chunk(\n",
    "                model, tokenizer, chunk, question,\n",
    "                model_taxonomy=True if i == 3 else False,\n",
    "                unique_labels=unique_labels if i == 1 else None,\n",
    "                device=device\n",
    "            ) for chunk in selected_chunks\n",
    "        ]\n",
    "        key = answers[i]\n",
    "        responses[key].append(responses_llm)\n",
    "        print(f\"Completed in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    return responses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd2df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the JSON file with paper metadata from paperswithcode\n",
    "with open(\"../data/papers_data copy.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    papers_list = json.load(f)\n",
    "# remove if Local PDF Path is None\n",
    "papers_list = [paper for paper in papers_list if paper.get(\"Local PDF Path\") is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc39d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1f88245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Loading configuration file from ./Grobid/config.json\n",
      "INFO - Configuration file loaded successfully\n",
      "2025-10-29 18:22:59,112 - INFO - Logging configured - Level: INFO, Console: True, File: disabled\n",
      "2025-10-29 18:22:59,141 - INFO - GROBID server http://localhost:8070 is up and running\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "    model, tokenizer, device=\"cuda\"\n",
    ")\n",
    "grobid = GrobidService(config_path=\"./Grobid/config.json\")\n",
    "\n",
    "# Define or load your papers_list, questions, unique_labels\n",
    "# papers_list = [...]  # List[dict]\n",
    "# questions = [...]    # List[str]\n",
    "# unique_labels = [...]\n",
    "\n",
    "all_results = []\n",
    "for paper in papers_list[0:1]:\n",
    "    pdf_path = str(Path(os.getcwd()) / paper['Local PDF Path'])\n",
    "    res = process_paper(\n",
    "        pdf_path,\n",
    "        grobid,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        questions,\n",
    "        [\"dataset\", \"task\", \"authors\", \"taxonomy\"],\n",
    "        2048,\n",
    "        None,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "    all_results.append(res)\n",
    "\n",
    "# Now you have a list of results per paper\n",
    "# You can save or further process all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10983bbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mIndexError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mall_results\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "\u001B[31mIndexError\u001B[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "all_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e920a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_results)):\n",
    "    all_results[i]['dataset'] = deduplicate_fuzzy(\n",
    "        list(chain.from_iterable(all_results[i]['dataset'][0]))\n",
    "    )\n",
    "    all_results[i]['task'] = deduplicate_fuzzy(\n",
    "        list(chain.from_iterable(all_results[i]['task'][0]))\n",
    "    )\n",
    "    all_results[i]['authors'] = deduplicate_fuzzy(\n",
    "        list(chain.from_iterable(all_results[i]['authors'][0]))\n",
    "    )\n",
    "    all_results[i]['taxonomy'] = deduplicate_fuzzy(\n",
    "        list(chain.from_iterable(all_results[i]['taxonomy'][0]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb33d732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dataset': ['ARC Challenge Set',\n",
       "   'Advances in neural information processing systems',\n",
       "   'A large annotated corpus for learning natural language inference',\n",
       "   'An analysis of open information extraction based on semantic role labeling',\n",
       "   'Combining retrieval, statistics, and inference to answer elementary science questions',\n",
       "   'Think you have solved question answering? try arc, the ai2 reasoning challenge',\n",
       "   'Discriminative embeddings of latent variable models for structured data',\n",
       "   'Gake: Graph aware knowledge embedding',\n",
       "   'arXiv:1704.01212',\n",
       "   'arXiv:150',\n",
       "   'Answering complex questions using open information extraction',\n",
       "   'Scitail: A textual entailment dataset from science question answering',\n",
       "   'Squad: 100,000+ questions for machine comprehension of text',\n",
       "   'Markov logic networks',\n",
       "   'arXiv:1611.01603',\n",
       "   'arXiv:1709.04071'],\n",
       "  'task': ['Learning to reason with contextual knowledge graphs',\n",
       "   'Learning to reason with neural embeddings of both knowledge graphs',\n",
       "   'Generating Hypothesis',\n",
       "   'Searching Potential Supports',\n",
       "   'Constructing Knowledge Graphs',\n",
       "   'Learning with Graph Embeddings',\n",
       "   'Experiments',\n",
       "   'Setup',\n",
       "   'Baselines',\n",
       "   'Results and Analysis',\n",
       "   'Conclusion and Future Work',\n",
       "   'reasoning over contextual knowledge graphs',\n",
       "   'answering science exam questions',\n",
       "   'natural language inference',\n",
       "   'open information extraction',\n",
       "   'elementary science questions',\n",
       "   'question answering',\n",
       "   'reasoning challenge',\n",
       "   'structured data',\n",
       "   'knowledge embedding',\n",
       "   'retrieval',\n",
       "   'statistics',\n",
       "   'inference',\n",
       "   'Neural message passing for quantum chemistry',\n",
       "   'Learning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings',\n",
       "   'Question answering via integer programming over semi-structured knowledge',\n",
       "   'Markov logic networks for natural language question answering',\n",
       "   'Answering complex questions using open information extraction',\n",
       "   'Scitail: A textual entailment dataset from science question answering',\n",
       "   'Demonyms and compound relational nouns in nominal open ie',\n",
       "   'A decomposable attention model for natural language inference',\n",
       "   'Squad: 100,000+ questions for machine comprehension of text',\n",
       "   'Markov logic networks',\n",
       "   'Variational reasoning for question answering with knowledge graph'],\n",
       "  'authors': ['Yuyu Zhang', 'Hanjun Dai', 'Toraman Kamil', 'Le Song'],\n",
       "  'taxonomy': ['Other KGC Technologies', 'Semantic matching model']}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
